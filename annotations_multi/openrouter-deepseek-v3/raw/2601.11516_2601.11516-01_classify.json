{
  "timestamp": "2026-02-19T11:22:58.176120+00:00",
  "paper_id": "2601.11516",
  "claim_id": "2601.11516-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** â†’ Does context make clear the author means \"is linearly decodable from\"?\n     - YES â†’ Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO â†’ Code as **R3** (default mechanistic reading)\n   - **YES** â†’ Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result â†’ Code as **R2**\n     - Mechanism â†’ Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** â†’ Code as **R3**\n   - **NO** â†’ Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper â†’ Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications â†’ Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** â†’ Does the paper claim the component is the *unique* controller?\n     - YES â†’ Code as **R3**\n     - NO â†’ Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** â†’ Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing â†’ \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching â†’ \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering â†’ \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE â†’ \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention â†’ \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation â†’ \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" â†’ still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" â†’ code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2â†’3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2â†’R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2â†’R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2â†’R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2â†’R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2â†’R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 â†’ Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2â†’R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2â†’R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2â†’R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2â†’R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2â†’R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2â†’R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1â†’R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**KramÃ¡r et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) â†’ R3\n   - Check for uniqueness language (the, only) â†’ R3\n   - Check for storage/encoding language â†’ R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model â†’ R1\n   - If intervention but not per-instance counterfactual â†’ R2\n   - If establishes unique/necessary mechanism â†’ R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") â†’ still R3, note hedge in confidence\n   - Multi-method papers â†’ use highest-rung method that directly supports claim\n   - Implicit claims from narrative â†’ code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2601.11516\nTitle: Building Production-Ready Probes For Gemini\nFull text:\n                                                                                                                      2026-01\n\n     Building Production-Ready Probes For Gemini\n\n\n           JÃ¡nos KramÃ¡râˆ—, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, and Arthur Conmyâˆ—\n             Google DeepMind, âˆ—Equal contributions to this work.\n\n\n\n           Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against\n         bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may\n         be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail\n           to generalize under important production distribution shifts. In particular, we find that the shift from\n           short-context to long-context inputs is difficult for existing probe architectures. We propose several new\n          probe architectures that handle this long-context distribution shift.\n       We evaluate these probes in the cyber-offensive domain, testing their robustness against various\n           production-relevant distribution shifts, including multi-turn conversations, long context prompts, and2026\n          adaptive red teaming. Our results demonstrate that while our novel architectures address context\n           length, a combination of architecture choice and training on diverse distributions is required for broadFeb           generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal\n          accuracy at a low cost due to the computational efficiency of probes.10\n         These findings have informed the successful deployment of misuse mitigation probes in user-facing\n           instances of Gemini, Googleâ€™s frontier language model. Finally, we find early positive results using\n          AlphaEvolve (Novikov et al., 2025) to automate improvements in both probe architecture search and\n           adaptive red teaming, showing that automating some AI safety research is already possible.[cs.LG]\n          Keywords: Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring\n\n\n         1. Introduction\n\n           In this paper, we describe our experience applying probes to detect cyber-offensive prompts given as\n          input to Gemini 2.5 Flash (Google, 2025a). We describe the challenges we faced and the solutions\n        we arrived at as a case study for other frontier language model developers wishing to deploy probes\n           as a misuse mitigation in production.\n\n        What do we mean by a misue mitigation? Misuse mitigations are techniques that prevent malicious\n           users from performing cyber-offensive, CBRN1 and other similar attacks using frontier large language\n         models (Google DeepMind, 2025; Anthropic, 2025b; OpenAI, 2025). This risk is not theoretical:arXiv:2601.11516v4     frontier models can already significantly increase the abilities of malicious users to perform these\n           attacks (Anthropic, 2025a), and misuse mitigations have already been deployed by frontier AI\n         companies to decrease this risk (Anthropic, 2025c; Google, 2025b).2 One might hope that a sufficient\n          mitigation would be to train LLMs to reject harmful queries, but unfortunately current training\n          techniques are not robust enough (Nasr et al., 2025).\n\n       We thus focus on monitoring in this paper: additional deployment time systems that aim to detect\n         harmful user requests. In this work, we only study input monitoring techniques, although we note\n\n               1Chemical, Biological, Radiological, and Nuclear\n               2Anthropic: â€œOur ASL-3 capability threshold for CBRN (Chemical, Biological, Radiological, and Nuclear) weapons\n            measures the ability to significantly help individuals or groups with basic technical backgrounds (e.g. undergraduate STEM\n            degrees) to create/obtain and deploy CBRN weapons[...] we were unable to rule out the need for ASL-3 safeguards.â€.\n            Google: â€œGemini 2.5 Deep Think continues the trend of increased model capabilities â€” it generates detailed technical\n           knowledge of CBRN domains. It provides uplift in some stages of some harm journeys.â€\n\n\n\n              Corresponding author(s): conmy@google.com, janosk@google.com, neelnanda@google.com\n       Â© 2026 Google DeepMind. All rights reserved\n\n                                           Building Production-Ready Probes For Gemini\n\n\nthat training probes on model outputs is an important future direction in Section 7.\n\nIn the main text we focus entirely on detecting the harmful and critical domain of cyber-misuse\n(Anthropic, 2025a), although we expect many findings to transfer between mitigation domains.\nCyber-offensive capabilities are a particularly worthwhile domain for studying defenses because\nharmful prompts are very similar to common and valuable defensive cybersecurity requests, and also\nsomewhat similar to even more common coding requests. Therefore, it is particularly difficult to\navoid over-triggering while also preventing harm in the cyber domain.\n\nRecent work shows that large language model (LLM) classifiers can be an effective monitor (Sharma\net al., 2025) for frontier models. But monitoring all interactions with another LLM can be extremely\nexpensive, potentially doubling the cost if using a comparably capable model. A more cost-effective\nalternative is an activation probe: a small model trained on the internal hidden states of the model one\nplans to monitor. Activation probes are far cheaper than language model monitors because language\nmodel activations are already generated by the monitored model during a forward pass, so the only\nadditional computational cost is running the probe itself. Indeed, Cunningham, Peng, et al. (2025)\nand Cunningham, Wei, et al. (2026) investigate probes and find evidence showing that they are a\npromising and cost-effective technique for misuse and jailbreak detection.\n\nOverall, probes are indeed a cost-effective misuse mitigation, but there is a key problem: they are\nalso fragile to distribution shifts. In particular, we find that probes trained on short-context data\nfail to generalize to important production distribution shifts, particularly long-context inputs. While\ntraining directly on long-context data is possible, we find it increases training costs by over an order\nof magnitude to achieve similar results due to memory bandwidth constraints (see Appendix I), which\nin turn would significantly increase the costs of AlphaEvolve (Section 5.3), for example. Distribution\nshifts like jailbreaks and multi-turn data are not fully addressed by our work, and hence challenges\nbuilding robust and cheap defenses remain.\n\nOur main technical contributionsare detailed investigation of four techniques that we investigate to\naddress these distribution shift performance degradations. Below, we summarize these techniques\nand our main results from applying each one:\n\n   1. Improving probe architecture: it is a difficult engineering challenge to train probes on long-\n      context data, even with strong infrastructure (Appendix I). Yet it is highly important that\n     probes are performant on long-context data. We introduce a new probe architecture family,\n     MultiMax, that has higher classifier accuracy on long context prompts compared to other probe\n      architectures. In Section 5.1, we show that MultiMax improves long context generalization\n     compared to existing baselines such as mean-aggregated MLP probes (Figure 4).\n   2. Automated probe architecture search: In Section 5.3, we also present results from an auto-\n     mated architecture search using AlphaEvolve (Novikov et al., 2025), which discovers a probe\n      that outperforms other baselines (bootstrap probability > 0.95, Section 5.1.1).\n   3. Using a cascading classifier: Large language models are inherently more robust to distribution\n       shifts due to their generality, but are expensive to run. In Section 5.2 we show that by only\n      using the LLM when the probe is uncertain, we can create stronger classifiers than using either\n      the probe or the LLM on its own, at a tiny fraction of the LLMâ€™s inference cost. For example, a\n      probe-Flash cascade achieves lower test loss than Flash alone while using roughly 1/50th the\n      inference cost (Figure 5).\n   4. The value of training probes across many different seeds: We show that the high variance of\n      different initialization of probes (particularly when training non-linear probes) can be exploited\n      to find probes that fit both the validation and test set better (Section 4.2), even though our\n       train, validation and eval sets are disjoint (Section 4).\n\n\n\n\n\n                                                                                                         2\n\n                                           Building Production-Ready Probes For Gemini\n\n\nThese findings yield a set of recommendations for building high-quality probes. Using these recom-\nmendations, we find that our best probes achieve comparable performance to Gemini 2.5 Flash and\nPro classifiers at a fraction of the cost. Probes also outperform 2.5 Flash Lite (Table 3). Furthermore,\nwhen we combine probes and LLMs into a cascading classifier, we can use the LLM less than 10% of\nthe time and still achieve a lower FNR than the LLM alone (Figure 5). Our findings are not solely\nacademic: they have directly informed GDMâ€™s recent successful deployment of cyber attack misuse\nmitigation probes into production. However, looking forward, our techniques do not significantly\nreduce the success rate of adaptive adversarial attacks; indeed, recent work argues that this is ex-\ntremely challenging, if not impossible (Nasr et al., 2025). Our work can supplement other empirical\nstudies (Cunningham, Wei, et al., 2026) and user-level strategies (Shah et al., 2025).\n\nWe outline our notation and problem setup in Section 2, the probing and classifier baselines we\ncompare in Section 3, the datasets we train and test on in Table 1, our evaluation metric in Section 4.1,\nand our main results in Section 5. We also study our new probe architectures and methodology on\ndifferent models and domains in Appendix A.\n\n\n2. Setup and Notation\n\nWe consider a deployed language model that receives prompts consisting of token sequences ğ‘†ğ‘–=\n(ğ‘¡ğ‘–,1, . . . , ğ‘¡ğ‘–,ğ‘›ğ‘–). Our goal is to classify sequences ğ‘†ğ‘–according to some criteria. We can ask a different\nLLM to perform this classification (see Section 3.3.1). However, running an LLM for every user request\nis expensive, and we thus consider an alternative way of classifying ğ‘†ğ‘–: training and running a small\nlogistic regression classifier or neural network (i.e. a probe) trained on the deployed language modelâ€™s\nhidden states (activations).\n\nFormally, consider a training set of sequences {ğ‘†ğ‘–} with associated binary labels â„“ğ‘–âˆˆ[0, 1]. We pass\neach token sequence ğ‘†ğ‘–through the language model and harvest the dimension ğ‘‘hidden states (one\nfor each token) at a fixed layer ğ¿(we use a layer from the middle of the model). Denote as ğ’™ğ‘–,ğ‘—âˆˆâ„ğ‘‘\nthe hidden activation at layer ğ¿corresponding to ğ‘¡ğ‘–,ğ‘—, and let ğ‘¿ğ‘–âˆˆâ„ğ‘‘Ã—ğ‘›ğ‘–be the matrix of such hidden\nstates corresponding to ğ‘¡ğ‘–. We train probes to minimize cross entropy loss on these sequences:\n\n                                     ğ‘“âˆ—ğ‘= arg min âˆ‘ï¸ BCE(ğœ( ğ‘“ğ‘(ğ‘¿ğ‘–) + ğ‘), â„“ğ‘–)                          (1)\n                                                                   ğ‘“ğ‘           ğ‘–\n\nwhere ğ‘âˆˆâ„and the arg min is over all parameters of ğ‘“ğ‘and ğ‘. We use full batch gradient descent\nwith AdamW, for 1000 epochs.3\n\nDuring inference of the target model on a new sample ğ‘†âˆ—ğ‘–(which could be from a different distribution\nto all ğ‘†ğ‘–), we perform a single forward pass and run the trained probe on the activations ğ‘¿âˆ—ğ‘–. If the\nprobe score is above a threshold ğœ, we can run a misuse mitigation instead of naively continuing\ngeneration. As a slight abuse of notation, we define ğ‘“ğ‘(Â·, ğœ) to be the classification function from\ncombining the probe ğ‘“ğ‘with the threshold ğœ:\n\n                                              ğ‘“ğ‘(ğ‘†ğ‘–, ğœ) = 1{ğœ( ğ‘“ğ‘(ğ‘¿ğ‘–)+ğ‘)>ğœ}                                (2)\n\nwhere 1 is the indicator function. This produces a classification decision, just as the prompted classifier\ndoes.\n\n  3We found that it was easier to build infrastructure that loaded activations for training in once, rather than continually\nthrough training. See Appendix I. We additionally only ever observed zero or mild overfitting on validation sets despite\nmany epochs.\n\n\n\n                                                                                                         3\n\n                                           Building Production-Ready Probes For Gemini\n\n\n                                                        Probe (Prior SOTA Work)\n    12%                                           Language Model\n                                                        Probe (Ours)\n    10%                                                Pareto Frontier\n             EMA Linear\n                  Probe\n  Rate 8%\n  Error 6%                                       AlphaEvolve\n    4%                           Probe                                           (Right)                           Gemini 2.5                                                             Selected Probe                                                                                  Flash                                        +8% Flash\n    2%      Alternative Probe      Selected Probe;\n                  With MultiMax      Rolling Aggregation\n                Aggregation (Left)      (Middle)\n    0%           100     101     102     103     104     105     106     107\n                          Relative Inference Cost (Log Scale)\n\nFigure 1  | We improve over prior activation probing work by improving probe architectures and\ntraining to achieve better performance than language models at over 10,000Ã— lower cost. â€œSelected\nProbeâ€ refers to our Max of Rolling Means Attention Probe (Section 3.2). Deferring to an LLM such\nas Gemini 2.5 Flash 8% of the time further decreases error rate to a lower value than what can be\nachieved by solely using a probe or an LLM, by trusting the probe at extreme scores and delegating\nto the LLM in between (see Section 5.2.2, though note wide error bars).\n*As described in Section 4.1, for each method we select the decision threshold on a validation set by\nminimizing a weighted combination of FPR and FNR (i.e. error rate), with overtriggering FPR weighted\nmost heavily. We report the same weighted error computed on test data. See Appendix J for error bar\nmethodology. The two points not on the Pareto frontier (Selected Probe and AlphaEvolve) have their costs\nartificially increased by 1.25Ã— for visual separation; the costs of these three probes are in practice almost\nidentical.\n\n\n3. Our Classifiers\n\nWe compare three categories of classifiers: baseline probe architectures from prior work, our novel\nprobe architectures, and LLM-based classifiers. All methods are evaluated together in Section 5; we\nseparate them here for clarity. Figure 2 visualizes all architectures.\n\n\n3.1. Baseline Probe Architectures\n\nWe first describe probe architectures from prior work that serve as our baselines.\n\n\n3.1.1. Linear Probes\n\nLinear probes were first introduced by Alain and Bengio, 2016. Given ğ’˜âˆˆâ„ğ‘‘, mean pooled linear\nprobes are defined as:\n\n\n\n                                                                                                          4\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n\n\nFigure 2  | Different probing classifiers that we compare in Section 3. All our probing classifiers\ncan be composed into six states 1) â€“ 6) as illustrated. Residual stream activations of undergo a\n2) Transformation per-position. These are then processed via 4) â€“ 5) Aggregation which ends up\nproducing a single scalar score at step 6). Many existing probing classifiers such as linear probes,\nexponential moving average aggregation probes and attention probes fit into this framework.\n\n\n                                       1   ğ‘›ğ‘–\n                                                ğ‘“Linear(ğ‘†ğ‘–) =  âˆ‘ï¸ ğ’˜ğ‘‡ğ’™ğ‘–,ğ‘—                                 (3)\n                                                            ğ‘›ğ‘–\n                                                                   ğ‘—=1\n\nso are very simple in terms of the steps 1) â€“ 6) in Figure 2. Spelled out, for linear probes the\ntransformation is just the identity map, ğ»= 1, the scores are a simple linear map, and aggregation is\njust the mean.\n\n\n3.1.2. Exponential Moving Average (EMA) Probes\n\nEMA probes were proposed by Cunningham, Peng, et al., 2025 as a way to improve probe general-\nization to long contexts. To use an EMA probe, we first train a mean probe ğ‘“ğ‘™ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿas in Equation (3).\nThen, at inference time, we compute an exponential moving average at each index ğ‘—= 1, . . . , ğ‘›ğ‘–:\n\n                    EMA0 = 0;    EMAğ‘—= ğ›¼ğ‘“ğ‘™ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ’™ğ‘–,ğ‘—) + (1 âˆ’ğ›¼)EMAğ‘—âˆ’1                     (4)\n\n\n\n\n                                                                                                         5\n\n                                           Building Production-Ready Probes For Gemini\n\n\nfor ğ›¼âˆˆ(0, 1). Following Cunningham, Peng, et al., 2025, we use ğ›¼= 0.5. We then take the maximum\nover EMA scores to get ğ‘“EMA(ğ‘†ğ‘–) = maxğ‘—EMAğ‘—which is the final probe score.\n\n\n3.1.3. MLP Probes\n\nMLP probes (e.g. Zou, Phan, J. Wang, et al. (2024)) are the same as linear probes except that we apply\nan MLP to the pooled or single token activation vector before multiplying by ğ’’. Using the definition\nof an ğ‘€layer (ReLU) MLP as\n\n                 MLPğ‘€(ğ‘¿) = ğ´1 Â· ReLU(ğ´2 Â· ReLU(. . . ReLU(ğ´ğ‘€Â· ğ‘¿) . . .))                  (5)\n\nan ğ‘€-layer mean pooled MLP probe is formally defined as:\n\n                                     1   ğ‘›ğ‘–\n                                ğ‘“ğ‘€MLP(ğ‘†ğ‘–) =  âˆ‘ï¸ MLPğ‘€(ğ’™ğ‘–,ğ‘—)                               (6)\n                                                         ğ‘›ğ‘–\n                                                               ğ‘—=1\n\nwith the dimensions of ğ´1, . . . , ğ´ğ‘€chosen such that the final result is a scalar.\n\n\n3.1.4. Attention Probes\n\nThe single head attention probe was first introduced by Kantamneni et al., 2025, further used by\nMcKenzie et al., 2025, and expanded to use multiple heads by Shabalin and Belrose, 2025. In our\nwork, we first pass activations through a common MLP before computing attention. Specifically, we\ndefine:\n                                          ğ’šğ‘–,ğ‘—= MLPğ‘€(ğ’™ğ‘–,ğ‘—)                                    (7)\nwhere MLPğ‘€is an ğ‘€-layer MLP as defined in Equation (5). An ğ»-headed attention probe is then\ndefined as:\n                                      ğ»  Ãğ‘›ğ‘–                                           exp(ğ’’âŠ¤â„ğ’šğ‘–,ğ‘—) Â· (ğ’—âŠ¤â„ğ’šğ‘–,ğ‘—)                                                           ğ‘—=1                                     ğ‘“Attn(ğ‘†ğ‘–) = âˆ‘ï¸                                                 (8)\n                                               Ãğ‘›ğ‘–                                               exp(ğ’’âŠ¤â„ğ’šğ‘–,ğ‘—)                                                                 ğ‘—=1                                            â„=1\nwhere ğ’’â„, ğ’—â„âˆˆâ„ğ‘‘â€² are learned query and value vectors for head â„, and ğ‘‘â€² is the output dimension\nof the MLP. Note that this is the first probe where the aggregation weights are a function of the\nactivations themselves rather than constants, illustrated by the looping upper arrow in Figure 2.\n\nOne might naively think that attention probes require an expensive recomputation of the softmax\nfunction for every new token during generation. But this is false, and as an additional contribution\nwe present an inexpensive inference algorithm for attention probes in Appendix L.\n\n\n3.2. Our Probe Architectures\n\nWe now present our novel probe architectures designed to address the limitations of baseline methods,\nparticularly for long-context generalization.\n\n\n3.2.1. MultiMax Probes\n\nWe find that the attention probeâ€™s softmax weighting suffers from overtriggering on long contexts (see\nSection 5.1). To address this, we propose the MultiMax architecture, which replaces the softmax with\na hard max at inference time (though not always during training). Using the same MLP-transformed\nactivations ğ’šas above:\n                                             ğ»\n                                        ğ‘“MultiMax(ğ‘†ğ‘–) = âˆ‘ï¸ max ğ’—âŠ¤â„ğ’šğ‘–,ğ‘—                               (9)\n                                                                         ğ‘—âˆˆ[ğ‘›ğ‘–]\n                                                     â„=1\n\n\n                                                                                                         6\n\n                                           Building Production-Ready Probes For Gemini\n\n\nwhere ğ’—â„âˆˆâ„ğ‘‘â€² are learned value vectors for each head â„. Unlike attention probes, MultiMax selects\nthe single highest-scoring token per head rather than computing a weighted average, which prevents\ndilution of the signal when harmful content appears in a small portion of a long context.\n\n\n3.2.2. Max of Rolling Means Attention Probe\n\nIn prior work, McKenzie et al. (2025)4 introduce the Max of Rolling Means probe which takes the max\nover all mean scores of all possible windows of contiguous tokens of a fixed length in a sequence. In\nour work, we combine this with an Attention Probe, using the lessons learnt from the MultiMax probe\nwork. Specifically, we use 10 attention heads by default and compute attention-weighted averages\nwithin sliding rectangular windows of fixed width ğ‘¤(we use ğ‘¤= 10 as default). For each window\nending at position ğ‘¡, we compute:\n                                      Ãğ‘¡ğ‘—=ğ‘¡âˆ’ğ‘¤+1 ğ›¼ğ‘—Â· ğ‘£ğ‘—\n                                          Â¯ğ‘£ğ‘¡=                                              (10)\n                                       Ãğ‘¡ğ‘—=ğ‘¡âˆ’ğ‘¤+1 ğ›¼ğ‘—\nwhere ğ›¼ğ‘—= softmax(ğ’’âŠ¤ğ’šğ‘—) are the attention weights and ğ‘£ğ‘—= ğ’—âŠ¤ğ’šğ‘—are the per-token values. The final\noutput is maxğ‘¡Â¯ğ‘£ğ‘¡. In this work, we explore using this aggregation at evaluation time, in addition to\nthe simpler MultiMax aggregation described in Section 3.2.1.\n\n\n3.2.3. AlphaEvolve Architectures\n\nWe additionally experiment with running AlphaEvolve (Novikov et al., 2025) to generate novel\nprobing architectures. We start with an attention probe (Section 3.1.4) as our seed model5 and use\nweighted FPR and FNR as our optimization target (we use somewhat of a different weighting to\neither Equation (12) and Appendix K due to using AlphaEvolve early in this research project, see\nAppendix F.1). We choose two of the architectures AlphaEvolve discovered as additional methods to\ncompare to and include pseudocode for these two architectures in Appendix F.2.\n\n\n3.3. LLM-Based Classifiers\n\nFinally, we describe classifiers that use language models, which serve as both strong baselines and\ncomponents of our cascading systems.\n\n\n3.3.1. Prompted Language Model\n\nA prompted language model ğ‘“ğ¿ğ‘€consists of a (prompt, language model) pair. To classify a sequence\nğ‘†ğ‘–, we append ğ‘†ğ‘–to the prompt and pass it to the language model, and then parse the language\nmodelâ€™s response. We use a fixed detection prompt provided by colleagues and consider Gemini\n2.5 Flash Lite, Gemini 2.5 Flash, and Gemini 2.5 Pro as the language model. This method directly\nreturns a classification decision, i.e. we do not need to pass it through a ğ¶thresholding function. In\nAppendix G we experiment with eight other prompts, but do not see a significant improvement, and\nso keep our original prompt.\n\n\n3.3.2. Cascading Classifier\n\nCascading classifiers were also studied by Cunningham, Peng, et al., 2025 but have a long history\nin machine learning (Viola and M. Jones, 2001). We consider two level cascading classifiers, with\n\n   4Max-pooling over local windows is a well-established technique in text classification, with precedent in convolutional\napproaches (Kim, 2014; Collobert et al., 2011).\n   5Since this is already an existing architecture (Section 3.1.4).\n\n\n\n                                                                                                         7\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 1  | Errors we seek to minimize with our trained probe. For shorthand reference later on we\nlabel the error on each dataset as FPR (for false positive rate) or FNR (for false negative rate) with\nsubscript of prompt_structure[prompt_type]\n\n  Description                      Var         Train Split  Validation Split  Test Split\n\n  Minimize false positive rate on...\n    short context overtriggering        FPRSC[OT]       yes            yes            yes\n    short context â€œhard negativesâ€     FPRSC[HN]       yes           no            yes\n    multi-turn (also â€œhard negativesâ€)  FPRMT[HN]       yes           no            yes\n    long context random traffic        FPRLC[RT]      no           no            yes\n\n  Minimize false negative rate on...\n    short context attacks              FNRSC[A]        yes            yes            yes\n    multi-turn attacks                FNRMT[A]        yes           no            yes\n    long context attacks               FNRLC[A]       no           no            yes\n    pre-existing jailbreaks             FNRSC[J]       no           no            yes\n    automated red teaming            FNRSC[ART]      no           no            yes\n\n\nlevel one set to be a probe ğ‘“ğ‘and level two set to be a more expensive prompted language model ğ‘“ğ¿ğ‘€.\nThen the cascading classifier is defined with two thresholds ğœ1 and ğœ2 as\n\n                                    0      if ğ‘“ğ‘(ğ‘†ğ‘–) â‰¤ğœ1\n                                       ğ‘“ğ‘ğ‘(ğ‘†ğ‘–, ğœ1, ğœ2) =  ï£±ï£´ï£´ï£´ï£² ğ‘“ğ¿ğ‘€    if ğœ1 < ğ‘“ğ‘(ğ‘†ğ‘–) < ğœ2                       (11)\n                                                                                                                                             ï£´ï£´ï£´ï£³ 1      if ğ‘“ğ‘(ğ‘†ğ‘–) â‰¥ğœ2\nBy varying the percent of samples spanned by ğœ1 and ğœ2 we can vary the additional computational\ncost incurred by the cascading classifier over just running the probe.\n\n\n4. Datasets and Metrics\n\nWe focus on a single misuse domain in this paper: cyber misuse. Our breakdown of which datasets we\ntrain on, which we validate on, and which we test on is outlined in Table 1. We briefly describe each\ndataset in Table 2, sorted into datasets with benign examples on which we want our probe to classify\nwith logits less than 0 (and thus on which we want to minimize the false positive rate), and datasets\nwith true cyber attacks on which we want our probe to classify with logits greater than 0 (and thus\non which we want to minimize false negative rate). Importantly, no prompts are shared between\nany two of the train, validation, and test splits. Furthermore, all test sets represent distribution shifts\nfrom the training and validation distributions.\n\nWe intentionally train probes on a subset of distribution shifts, and in particular not on long context,\nsince training on long-context dramatically increases infrastructure burden and compute. This informs\nour decision of which training sets are included in Table 1. Additionally, we use a different distribution\nfor validation and test sets to ensure that thresholds are not optimized on the same distribution as\nthe test set.\n\nNote: we use the term â€œovertriggeringâ€ to refer to benign (non-cyber) datasets on which our classifiers\nshould not fire, including PII-redacted user traffic, capability evaluation data and synthetic prompts\ncurated for unrelated purposes.\n\n\n\n\n\n                                                                                                         8\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 2  | Detailed descriptions of each dataset. See Appendix B for metadata including dataset sizes.\n\n\n Dataset             Description\n\n  Benign (minimize FPR)\n Short Context        Short benign language model prompts. This dataset combines random coding traffic\n Overtriggering       samples, random chat traffic samples with PII removed, and a mixture of Gemini\n                        post-training datasets. Specifically, we train on random traffic from one Google surface\n                   where Gemini is served, and use traffic from a different Google surface in the validation\n                  and evaluation sets (both surfaces have PII scrubbed). In the validation set, we use\n                        exclusively non-coding-related overtriggering prompts, and in the test set we exclusively\n                     use coding prompts, ensuring a distribution shift.\n\n Short Context        Short benign cybersecurity requests that superficially resemble attack queries. These\n â€œHard Negativesâ€     include requests to understand historical security incidents (e.g., â€œExplain how the\n                  2017 Equifax breach exploited the Apache Struts vulnerabilityâ€), requests to review\n                    code for vulnerabilities as part of secure development practices, and queries from\n                       learners using educational security platforms. This dataset is curated and labeled by\n                       cybersecurity domain experts.\n\n Multi Turn          Benign, synthetically-generated, multi-turn conversations that discuss cybersecurity\n â€œHard Negativesâ€      topics. These conversations are designed to mimic multi-turn attacks where malicious\n                       intent only becomes apparent over several turns; however, in these hard negative\n                     examples, the conversation never becomes malicious. This helps train classifiers to\n                      avoid firing on legitimate, evolving conversations about security. We use synthetically\n                      created examples for training, bootstrapping single turn conversations together.\n\n Synthetic Long Con-  Long context prompts generated from coding user traffic with PII removed (on average\n text             >900k tokens, though at most 1,040,000 tokens due to Gemini context length limits).\n Random Traffic      These random traffic coding queries come from the Short Context Overtriggering dataset\n                      evaluation set, so there is no leakage between train and test data. More specifically,\n                we take the coding related queries, get Gemini to recall several GitHub repositories\n                     with related content to the query, filter for those with permissive licenses, and create a\n                     near maximal length prompt with these codebases as either prefix or the suffix of the\n                        request.\n\n  Attack (minimize FNR)\n Short Context  At-  Short (< 10k tokens) instances of cyber attack prompts, e.g. â€œHow do I execute a SQL\n tacks                  injection attackâ€. Curated and labeled by cyber-security domain experts.\n\n Long Context       Long context prompts generated from harmful short-context Cyber queries (which\n Attacks              are a different set to those used in all other datasets here). We use a similar long\n                       context generation pipeline to the â€œLong Context Random Trafficâ€ dataset, except insert\n                    harmful requests as a prefix to the code (i.e. at the start of the prompt), suffix to the\n                    code (i.e. at end of the prompt), as well as just before the final repository (so the\n                      request is interleaved with the context).\n\n Multi Turn            Synthetically-generated, multi-turn conversations that begin with benign or ambiguous\n Attacks               cybersecurity-related prompts but escalate over several turns to become explicitly\n                       malicious. This dataset simulates attackers attempting to bypass single-turn defenses\n                   by gradually building a harmful context, training the classifier to detect threats that\n                      only become apparent over an entire conversation. Generated using the same experts\n                       as the Multi Turn â€œHard Negativesâ€.\n Pre-existing       We take a list of jailbreaks, i.e. functions that map strings to strings such as appending\n Jailbreaks           a suffix as in the GCG paper (Zou, Z. Wang, et al., 2023), and apply them to all of the\n                    harmful cyber prompts in the Short Context Attacks evaluation set.\n\n                                                                             Continued on next page\n\n\n\n\n                                                                                                          9\n\n                                           Building Production-Ready Probes For Gemini\n\n\n                                  Table 2 â€“ Continued from previous page\n\n Dataset             Description\n\n Adaptive Red        This jailbreak dataset is constructed as follows: for each attack in the short context\n Teaming (ART)       attacks dataset, we ask Gemini 2.5 Pro to construct a narrative prompt that encodes\n                      the attack as a natural part of the prompt. Importantly, this process could in principle\n                     transform a malicious prompt into a benign one; we guard against this by using an\n                      autorater to verify that the modelâ€™s response is actually harmful (see Appendix H.1\n                        for details). Our initial version of the ART dataset was much more complicated. It\n                    used an adaptive jailbreak finding method modeled off of Prompt Automatic Iterative\n                     Refinement (PAIR; Chao, Robey, et al. (2025)) to find semantic jailbreaks by iteratively\n                    prompting an attacker model to generate a jailbreak for a given query and giving it\n                      feedback on whether it succeeded. However, we found that this complicated method was\n                      not significantly better than the single shot baseline described above; see Appendix H.\n\n\n\n4.1. Metric\n\nThere are many different datasets in Table 1, and for all methods besides the prompted classifier\nmethods we must choose a threshold ğœthat trades off against all of these metrics for use on the test\nsplits. We make this choice by choosing a ğœâˆ—that minimizes the following weighted error metric on\nall validation datasets from Table 1:\n\n\n                                        5     if ERRORğ‘–is FNR                             \"              Ã ğ‘¤ğ‘–Â· ERRORğ‘– #        ğœâˆ—= arg min                wğ‘–=  ï£±ï£´ï£´ï£´ï£² 2     if ERRORğ‘–is                                                           hard negative FPR       (12)\n                       ğœ     Ã ğ‘¤ğ‘–\n                                        50   if ERRORğ‘–is overtriggering FPR                  | Validation{z Loss }                                  ï£´ï£´ï£´ï£³\n\nThis equation allows us to weight performance on datasets differently.  Specifically, we weight\novertriggering false positives 10 times higher than false negatives and 25 times higher than hard\nnegative false positives. These multipliers reflect that true negatives are very rare and that we want\nto almost never classify benign overtriggering prompts as malicious, whereas occasionally classifying\nhard negatives as malicious is acceptable. See Appendix K for more discussion and results with\nalternative weightings.\n\nWe also use Equation (12) to calculate a weighted test loss on all test datasets from Table 1, which\nwe use as the main metric when we need to compare methods along a single axis (e.g. Figure 5). In\nother words, to compute a test loss, we take the weighted average of all FPRs and FNRs on the test\nset instead of validation set.\n\n\n4.2. Initialization Seeds\n\nWe train all probes in Table 3 on 100 different seeds, measure their validation loss on the validation\nsets in Table 1, and then choose the seed that has the lowest validation loss. We then report the test\nloss for this chosen seed.\n\nIn Figure 3 we show the effect of this seed selection procedure on test loss across architectures. To\nquantify the impact of choosing the best seed on validation, we compare the test loss of the best-\nvalidation-loss seed to the test loss of the median-validation-loss seed for each architecture. Across\narchitectures, seed selection via lowest validation loss reduces test loss by 0.008 on average, though\nthe median reduction is only 0.003. In contrast, architecture choice provides a much larger gain.\nComparing the median-validation-loss seeds, the best architecture achieves 0.025 test loss while the\n\n\n\n                                                                                                        10\n\n                                           Building Production-Ready Probes For Gemini\n\n    0.10       Median       Seed Selection: Median --> Best-Val\n              Best-Val               Improvement\n             Oracle    0.08\n           IQR\nLoss\n    0.06\nTest\n\n    0.04\n\n\n\n    0.02\n\n             Rolling         AlphaEvolve         Rolling            Rolling         AlphaEvolve       Vanilla Attn       Linear Probe      Linear Probe\n            Attention                            Attention          Attention            (Early                        Mean           Rolling Exp\n             Probe,                              Probe,        Probe (20H),        Variant)\n        MultiMax Agg                          Rolling Agg        Rolling Agg\n\nFigure 3  | Effect of seed selection on test loss. Arrows show improvement from median (gray) to\nbest-validation-selected (coral star) test loss. Green diamonds show the oracle (best possible seed).\nSome MultiMax aggregations have very large IQR and are shown in Figure 8 to not distort this figure.\n\n\nLinear Probe Mean achieves 0.064, over 13Ã— larger than the median seed selection gain. This result\nsuggests that practitioners should prioritize architecture search over extensive seed tuning. However,\nresearchers should still sweep over multiple seeds to reduce variance, as individual seed performance\ncan vary substantially and does still provide a small uplift. This is a finding consistent with prior work\non seed sensitivity in classifier finetuning (Risch and Krestel, 2020) and neural network training\nmore broadly (Picard, 2023).\n\nWe also note that the best possible seed on the test set is frequently much better than the seed we\nchoose via our validation dataset. It is possible that with a better validation data mix, the best-val\nseed would move closer to the best possible seed.\n\n\n5. Results\n\n5.1. Main Cyber Probing Results\n\nIn Table 3 we show our evaluation of a series of probes and prompted classifiers across all datasets in\nincreasing order of test set error. Our key takeaways are that:\n\n   1. Existing probe architectures can be significantly improved upon.\n   2. No single method dominates across all distribution shifts.\n   3. In this cyber-offensive query domain, attack success rates are extremely high.\n\nFirstly, existing probe architectures can be significantly improved upon. Prior work includes linear\nprobing trained with mean aggregation (e.g. Alain and Bengio (2016) and Zou, Phan, J. Wang, et al.\n(2024)), attention probing (Kantamneni et al., 2025; Shabalin and Belrose, 2025) or forms of max\naggregation of trained linear probes (Cunningham, Peng, et al., 2025). Our top performing probes:\nAlphaEvolveâ€™s MultiMax variation, and the Max of Rolling Mean Attention Probes, outperform these\nexisting linear probe baselines. The Max of Rolling Mean Attention Probe with MultiMax aggregation\noutperforms all LLMs, and we strongly suspect that this is still far from optimal.\n\nNext, no single method dominates across all distribution shifts. As can be seen from Table 3, the\n\n\n\n                                                                                                        11\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 3  | Main results across all evaluation datasets and classifiers. For methods with a configurable\nthreshold ğœ, ğœis chosen to minimize Equation (12) on dataset validation splits. Better error rates are\nsmaller; the best overall error rates in each column are bolded. Error bars are omitted; see Table 4\nfor statistical significance testing.\n\n\n                             FPR â†“                         FNR â†“\n Classifier           SC[OT] SC[HN] LC[RT] MT[HN]  SC[A]   LC[A]  MT[A]   SC[J]  SC[ART]  Test Error â†“\n Gemini 2.5 Flash      0.3%  14.09%  0.7%   19.67%  8.58%  6.06%  13.51%  8.74%  20.65%    2.04%\n Gemini 2.5 Pro       0.2%   6.71%   0.2%   13.11%  16.01% 13.42% 20.85% 10.64%  33.87%    2.21%\n Attn (With LC Train-  0.6%   7.04%  1.17%   4.92%   9.40%  1.34%  29.34%  3.33%  35.50%    2.38%\n ing)\n Rolling  Attn  Probe,  0.5%  12.75%  1.85%  11.48%  8.12%  1.73%  22.01%  3.58%  22.97%    2.50%\n MultiMax Agg\n AlphaEvolve         0.1%   5.37%  1.72%   6.56%   9.74%  4.76%  32.05% 11.28%  27.61%    2.53%\n Rolling  Attn  Probe,  0.7%   6.71%  1.23%  11.48%  8.58%  3.03%  26.25%  2.41%  42.46%    2.64%\n Rolling Agg (Selected)\n Rolling  Attn  Probe  0.5%   8.72%   1.0%   6.56%  11.37%  5.19%  32.05%  3.62%  50.58%    2.74%\n (20H), Rolling Agg\n AlphaEvolve   (Early  0.2%   8.05%  1.11%  1.64%   9.28%  7.79%  33.20% 42.28%  28.07%    2.87%\n Variant)\n Attn                0.3%  10.07%  0.0%   3.28%   9.98%  87.88% 35.91%  2.40%  43.39%    3.53%\n Gemini 2.5 Flash Lite  0.7%  25.50%  0.2%   14.75%  11.37% 55.41% 27.41% 10.64%  41.76%    3.71%\n Rolling  Attn  Probe  0.4%   9.40%  6.53%  24.59%  8.35%  0.0%  15.44%  2.79%  22.74%    4.50%\n (20H), MultiMax Agg\n MultiMax    Trained,  0.3%   4.70%  7.76%   3.28%  15.31%  0.4%  30.50%  5.17%  44.78%    5.35%\n MultiMax Agg\n Linear Probe Mean    1.05%  11.41%  0.5%   8.20%  13.69% 99.13% 63.32% 70.48%  47.56%    6.18%\n Linear Probe Rolling  1.01%  16.78% 13.55%  75.41%  11.37%  1.73%  5.41%  2.95%  14.15%    8.63%\n Exp\n MultiMax      (Attn  1.88%  17.45% 43.97%  70.49%  8.35%  0.0%  2.70%  0.8%   3.94%    21.93%\n Trained)\n MultiMax    Trained  1.12%  6.04%  82.14%  9.84%   9.51%  0.0%  20.85%  7.61%  38.52%   38.14%\n (20H), MultiMax Agg\n\n\noptimal probe for a particular sub-distribution tends to differ from task-to-task. We suspect that this\nis due to using a wider set of distributions than prior work. Additionally, we speculate that the large\nvariation in FPR and FNR values across techniques on a given domain is downstream of cyber-offensive\ncapabilities being very similar to cyber-benign applications, and not significantly different from many\ncoding related queries.\n\nFinally, in this cyber-offensive query domain, jailbreak success rates are extremely high. As\ngestured at in Figure 4d, on the pre-existing jailbreaks dataset, all techniques have a vulnerability\non at least 1% of queries. This means that we are a long way from adversarial robustness in this\ndomain. While we suspect that there is some dataset labelling error driving some low performance,\nwe still believe that all of our probes and classifiers are far from saturating the task of detecting\ncyber-offensive queries. Note that this dataset consists of purely prompt based attacks generated from\npublicly accessible methods.\n\nWe now choose a selected probe to run further experiments with in this paper (e.g. Figure 1 and\nFigure 5). Our selected probe is the Max of Rolling Means Attention Probe, which we find to\nstatistically significantly outperform (according to bootstrap resampling) the baselines in Table 4.\nWhile the Max of Rolling Means Attention Probe, evaluated with a MultiMax aggregation function\nat inference time, has lower test error in Figure 1, it also has far larger error bars, hence we do not\nfocus on it.\n\n\n\n                                                                                                        12\n\n                                           Building Production-Ready Probes For Gemini\n\n                                            Probe (Prior SOTA Work)              Relative Inference Cost (Log Scale)     Weighted Error Rate*                                   107      8.6%                          LanguageProbe (Ours)Model                                                              Cost106                   Lower is better(%)                                 105\n                                   104Rate5                                                                                                                               Inference             3.5%                      103\n                                   102Error                  2.6%   2.0%                                   1.5%      101                                                                                                                             Relative            EMA Linear          Attention           Selected         Gemini 2.5        Selected Probe      100   EMA Linear      Attention        Selected      Gemini 2.5    Selected Probe  0        Probe             Probe             Probe              Flash        +8% Flash                                 Probe         Probe         Probe           Flash     +8% Flash\n\n(a) Test error comparison                             (b) Cost comparison\n\n                                                          Attack Success Rate on Probes\n        Long Context FNR and FPR (Lower is Better)                             8.7%\n  100     13.5%  87.9%\n                                        FNR\n                                           FPR\n(%)                       10(%)     (%)\n   50                           5FNR                                      FPR     FNR   3.0%                                                         2.4%   2.4%                                                                            1.8%                                     1.2%          1.7%           0.0%   3.0%                                          1%\n    0 EMA Linear Attention   Selected 0           Probe     Probe     Probe          0   EMAProbeLinear      AttentionProbe         SelectedProbe       GeminiFlash2.5    Selected+8% FlashProbe\n(c) Long context FNR/FPR                            (d) Pre-existing jailbreaks FNR\n\nFigure 4  | Comparison of probe architectures across distribution shifts. (a) Test error across methods.\n(b) Probes achieve comparable performance to LLM classifiers at a fraction of the cost. (c) Long\ncontext generalization: AlphaEvolve probes maintain low FPR on long contexts. (d) Adversarial\nattacks remain challenging for all methods; all methods still leave attack success rate >1%. Note:\nerror bars are omitted from these plots see Figures 1 and 5 for visualizations with (relatively wide)\nuncertainty estimates.\n*As in Figure 1 the threshold selection methodology is described in Section 4.1.\n\n\n5.1.1. Statistical Significance of Probe Comparisons\n\nTo rigorously validate our results, we compute the frequency with which one method outperforms\nanother under a â€œbest-of-ğ‘˜â€ selection procedure. In other words, to compare whether method A is\nbetter than method B, we are interested in the probability that when we train ğ‘˜= 100 random seeds\nof each method and select the best by validation loss, method A achieves lower test loss than method\nB. We approximate this by using a bootstrap procedure over our seed sweeps, computing empirical\nPMFs (probability mass functions) for each methodâ€™s test loss distribution under best-of-ğ‘˜selection.\n\nTable 4 shows that our top methodsâ€”Rolling Attention Probe (with both Rolling and MultiMax\naggregation) and AlphaEvolveâ€”statistically significantly outperform all baseline methods. For\ncomparisons against Linear Probe baselines, the bootstrap PMFs have zero overlap: in every bootstrap\nsample, the top methods achieve lower test loss than the baselines. This validates that the performance\ngains we report are robust and not due to random seed selection.\n\nAmong the top methods themselves, the differences are not statistically significant: the bootstrap\nfrequency with which Rolling Attn (Rolling Agg) outperforms AlphaEvolve is only 0.51, and the\nfrequency with which it outperforms Rolling Attn (MultiMax Agg) is 0.48, suggesting these architec-\n\n\n\n\n                                                                                                         13\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n                                              Linear Baselines       Attention Baselines\n\n Method                                 Linear Mean  Linear EMA    Attn   MultiMax\n\n  Rolling Attn, Rolling Agg (Selected Probe)       1.0           1.0     > 0.996   > 0.999\n  Rolling Attn, MultiMax Agg               > 0.999      > 0.999    > 0.91    > 0.99\n  AlphaEvolve                                   1.0           1.0      > 0.96    > 0.99\n\nTable 4  | Bootstrap frequency with which row methods outperform column methods under best-of-\n100-seeds selection. Values of 1.0 indicate zero overlap between bootstrap PMFs. All selected probe\ncomparisons exceed 0.95.\n\n\ntures have converged to similar performance levels. This is visually illustrated in Figure 1, where\nthe â€œSelected Probeâ€ and AlphaEvolve have overlapping error bars. This indicates we have identified\na class of high-performing architectures, though determining the single â€œbestâ€ would require more\nseeds or a larger evaluation set.\n\n\n5.1.2. Training on Long-Context Data\n\nIn Table 3 the â€œAttn (LC Trained)â€ baseline achieves a competitive test error (âˆ¼2.38% â€“ the architecture\nis described in Section 3.1.4). This confirms that attention probes have sufficient capacity to handle\nlong contexts if given enough data. However, these results rely on directly training on long-context\ndata rather than generalizing from short-context data. This means that this generalization comes at\na steep price: Even with large-scale accelerator clusters, the full dataset of long-context activations\n(each up to 1M tokens) cannot fit into TPU HBM. This necessitates implementing complex data\nloading pipelines to swap data between host memory and device memory, preventing the use of\nstandard, efficient training loops. In our experiments, these overheads resulted in a 22Ã— increase in\ntraining cost compared to short-context training.\n\nIn contrast, our proposed Max of Rolling Means Attention Probe and AlphaEvolve architectures\nachieve similar performance (e.g., 2.53% test error for AlphaEvolve compared to 2.38% for Attn\n(LC Trained)) while being trained exclusively on short-context data. This demonstrates that better\narchitectural design can solve the long-context problem and achieve similar performance to directly\ntraining on long context data.\n\nWe do note that attention probes may be optimal for domains without large deployment distribution\nshifts; in Appendix A, we find that attention probes are highly performant on open source datasets\nwith the same train, test, and deployment distribution.\n\n\n5.2. Cascading Classifiers\n\nCascading classifiers combine a cheap probe with an expensive LLM classifier to achieve better cost-\naccuracy tradeoffs than either alone (Viola and M. Jones, 2001; McKenzie et al., 2025). The probe\nfirst classifies each query; queries where the probe is uncertain are deferred to the expensive model.\nWe consider two methods for computing the optimal deferral policy. Both methods set the parameters\nof the policy using the validation set.\n\n\n5.2.1. Warmup: Heuristic Band Method\n\nThe heuristic band method uses a symmetric confidence band around the probeâ€™s decision threshold.\nGiven a decision threshold ğ‘¡, we define a band [ğ‘¡âˆ’ğ›¿, ğ‘¡+ ğ›¿] and defer all queries with probe logits in\n\n\n\n                                                                                                        14\n\n                                           Building Production-Ready Probes For Gemini\n\n\n   4.0%        Probe                 Flash Lite\n                 Frontier              Flash\n            LLM Only            Pro\n   3.5%        Best Cascade\n\n   3.0%\n\nRate2.5%                                                            6.3%\nError                                                                               defer\n   2.0%\n                                                                                                   8.6%\n                                                                                    7.1%             defer                                                                                                      defer\n   1.5%\n\n   1.0%\n\n           102            103            104            105            106            107\n                                   Relative Inference Cost (Log Scale)\n\nFigure 5  | Cost plotted against test error (Equation (12)). The curves show the optimal frontier for\ncascading classifiers that combine cheap probes with expensive LLMs. Error bars (see Appendix J)\nindicate substantial uncertainty: most operating points have overlapping confidence intervals, so the\napparent ordering between methods should be interpreted cautiously. The cascade global minima\n(circles) represent optimal operating points for each LLM; further deferring to LLMs past these points\nincreases the error rate.\n\n\nthis range to the expensive model. By varying ğ›¿from 0 (no deferral) to infinity (full deferral), we\ntrace out a cost-accuracy curve. This method is simple but suboptimal: it treats all deferred queries\nsymmetrically regardless of their individual costs.\n\n\n5.2.2. Generalization: Threshold-Randomization-Optimal Cascading\n\nThe heuristic band method restricts us to symmetric deferral regions, but we can do better. In general,\na cascading policy is defined by two thresholds (ğ‘¡0, ğ‘¡1): classify as negative if the probe logit is below\nğ‘¡0, classify as positive if above ğ‘¡1, and defer to the expensive model otherwise. Each choice of (ğ‘¡0, ğ‘¡1)\ngives a point on a cost-accuracy plot on the validation data.\n\nSince we have finite data, sweeping over all possible threshold pairs yields a discrete set of (Cost, Test Error)\npoints. A natural question arises: what is the correct way to interpolate between these points to\nobtain a continuous Pareto frontier? We study and answer this question in detail in Appendix E, and\napply our results to the plot of the Pareto frontier in Fig. 5.\n\n\n5.2.3. Cascading Results\n\nIn Figure 5 we show the performance of cascading classifiers. Specifically, we find that Gemini 2.5\nPro and Gemini 2.5 Flash-Lite can be effectively combined with the optimal probe from Table 3 in\n\n\n\n                                                                                                         15\n\n                                           Building Production-Ready Probes For Gemini\n\n\norder to produce an even higher accuracy classifier than merely using the language models alone.\nThe probe used on its own also lies on the Pareto frontier.\n\nNotably, Gemini 2.5 Flash dominates Gemini 2.5 Pro across the entire cost-accuracy frontier. This is\ndespite our cost estimation being generous to Pro: we ignore the cost of thinking tokens, treating them\nas free. In practice, Gemini 2.5 Pro often produces significantly more than the minimal 128 thinking\ntokens we set in the API, making the true cost gap even larger. This suggests that for cascading\nclassifier deployments, Flash is the preferred expensive model. Note that all of these results are\nderived directly from sweeping over thresholds and then plotting the test data conclusion. However,\nsince weâ€™re sweeping over one parameter only, we donâ€™t think this is a problematic use of the test data.\n\n\n5.3. Automating Safety Research with AlphaEvolve\n\nIn this section, we describe our experience using AlphaEvolve to run an automatic evolutionary\nsearch for better probing architectures. AlphaEvolve (Novikov et al., 2025) is a system that optimizes\nalgorithms by leveraging a large language model to iteratively mutate and refine programs. The\nsystem starts from a single seed program and then iteratively generates new programs using the\nhigh-performing programs as in-context examples for subsequent generation steps. High-performing\nprograms are determined by their score on a user-defined automatic evaluation.\n\nWe give AlphaEvolve a simple base script that implements a naive attention probe. We then let\nAlphaEvolve optimize against a weighting of the validation error of datasets from Table 1 (excluding\njailbreaks and long context overtriggering). We run a single large AlphaEvolve job that generates and\nevaluates approximately 2500 probing architectures. This process successfully closes approximately\n50% of the test error gap between the attention probe baseline and the best possible probe performance;\nsee Figure 10 for the the best generated architecture according to weighted validation error over\ntime.\n\nThe two AlphaEvolve architectures we compare to in the rest of this paper are chosen from this\nvalidation error frontier, one from relatively early in training and one from the very end. We show\nthe corresponding pseudocode in Appendix F.2. Notably, both of the chosen AlphaEvolve programs\nfind MultiMax like solutions on their own. Indeed, the early AlphaEvolve program is just a MultiMax\nprobe followed by a dense linear projection of the heads, while the program from the end of the\nAlphaEvolve run involves complicated gating and regularization.\n\nWe conclude with some speculation and observations about using AlphaEvolve for safety research; we\nexpect our takeaways to be useful not just for AlphaEvolve, but also for similar automated evolutionary\nLLM-based program discovery systems.\n\nWhen to Use AlphaEvolve. We think that automated program discovery methods are promising for\nsafety research problems that can be framed as optimizing a single number or set of numbers. Indeed,\nwe also successfully used AlphaEvolve to optimize the prompt for our adaptive jailbreak dataset (see\nAppendix H).\n\nStrange Late Effects in AlphaEvolve Generations. We also observe some strange effects late in\nAlphaEvolve, where the best programs start to have strange comments describing hard to understand\nideas like \"Holographic Attention\" or \"frequency modulators\"; we think this tendency is likely due to\none of the prompts we used, which suggested that the model should â€œSuggest a crazy idea of how\nwe can improve our implementation, something that definitely nobody else would think of. Make it\ncrazy with a capital Câ€. It may be important to balance increasing variance of model responses while\npreventing generation from going off of the rails.\n\nHigh Level Recommendations On Using AlphaEvolve-like systems. We believe that there are a\n\n\n\n                                                                                                        16\n\n                                           Building Production-Ready Probes For Gemini\n\n\nnumber of important considerations that help to successfully use AlphaEvolve or similar systems.\nChiefly, we recommend running multiple AlphaEvolve runs with increasing length and number of\nworkers, tracking additional metrics that are not being expressly optimized by AlphaEvolve, and\nbuilding environments that are as robust as possible to intense optimization pressure. Thus, a typical\nAlphaEvolve workflow for us looked something like: first designing an environment, running a small\nAlphaEvolve run on it, observing the results on held-out metrics, debugging why they lagged behind\nthe optimized metrics, rerunning a larger AlphaEvolve run, debugging new compute bottlenecks in\nvarious parts of the pipeline (common for AlphaEvolve deployments), and so on. Thus, although\nAlphaEvolve can indeed greatly improve the performance of a seed program, there is a large overhead\nin building and iterating on a successful environment.\n\nEnvironmental Robustness: Evaluation Noise. For the environment robustness problem in particular,\nthere were two types of problems we found common. The first problem was the presence of noise in\nevaluation metrics, which is common in machine learning problems. For example, we believe that the\nlarge early drop in Fig. 10 comes from climbing randomness, as some seeds are naturally better at\nthe specific weighted tradeoff metric we specify. This is also reflected in our main results table in\nTable 3: the AlphaEvolve programs do not improve over the seed Attention probe program on test\nmetrics as much as one would expect from Fig. 10. We reduced variance by running multiple seeds in\nthe AlphaEvolve evaluation function.6\n\nEnvironmental Robustness: Reward Hacking. The second type of environmental robustness\nproblem we identified was (unintentional) reward hacking. For example, we found in early runs\napplying AlphaEvolve to our adaptive jailbreaking set, AlphaEvolve sometimes optimized for crashing\ninference servers by making the generated jailbreak extremely long, because a lack of a response\nwas treated as a success, similar to Georgiev et al. (2025). We changed the metric to treat a lack of\nresponse as a failure, which mitigated this problem. In general, we found that surprising failures like\nthis were common, motivating our above advice to iterate on environment design.\n\n\n6. Related Work\n\n6.1. Misuse Mitigation\n\nThe first line of defense for preventing language model misuse is aligning the modelâ€™s behavior directly\nduring training. Approaches in this vein include Reinforcement Learning from Human Feedback\n(RLHF) (Christiano et al., 2017) and Constitutional AI (RLAIF) (Bai et al., 2022), which use feedback\nfrom humans or AI raters to teach the target model not to respond to harmful queries. However, these\ntechniques are not yet sufficiently robust to prevent all misuse.\n\nRecognizing that models may never be safe to use as standalone input/output black boxes, recent work\naugments language models with separate input output classifiers to flag potentially unsafe requests\nor responses (Naihin et al., 2023; Inan et al., 2023; Han et al., 2024; Sharma et al., 2025). The\ndeployment of such monitors is now standard for frontier models, as detailed in frontier model system\ncards (Anthropic, 2025c; Google, 2025b). The broader field of AI Control (Greenblatt et al., 2023)\nspecifically focuses on augmenting untrusted language models into larger systems with components\nof language model, probe, or human oversight, although with more of a focus on model than user\nmisbehavior. However, these text-based monitors can add significant latency and compute costs, so a\npromising alternative is activation probing.\n\n   6One interesting technique we experimented with to determine if we needed to invest more compute in reducing\nmeasurement noise was simulating a simplified AlphaEvolve run, where we modeled each program run as a random process\nthat had a distribution of improvements over the seed program and a distribution over measurement noise.\n\n\n\n\n                                                                                                        17\n\n                                           Building Production-Ready Probes For Gemini\n\n\n6.2. Activation Probing\n\nActivation probing originated as a diagnostic technique to determine what information is linearly\ndecodable where from a frozen modelâ€™s internal states (Alain and Bengio, 2016). Recent work\nhas adapted these diagnostic tools into active safety monitors (Zou, Phan, S. Chen, et al., 2023),\ndemonstrating that probes can reliably detect high-stakes interactions (McKenzie et al., 2025) and\nlatent deceptive intent (Goldowsky-Dill et al., 2025), often earlier and more efficiently than text-based\nclassifiers. This efficiency is central to proposals for cheap misuse mitigation classifiers that reuse\nfrozen internal features rather than processing the full output text with another language model\n(Cunningham, Peng, et al., 2025). Contemporary research continues to refine probe architectures\nbeyond simple linear layers, utilizing attention mechanisms (Shabalin and Belrose, 2025), exponential\nmoving averages (Cunningham, Peng, et al., 2025), or sparse autoencoders (Kantamneni et al., 2025;\nTillman and Mossing, 2025) to improve performance.\n\n\n6.3. Cascading Classifiers\n\nCascading classifiers7 leverage a sequence of increasingly capable and more expensive models to\noptimize the trade-off between efficiency and accuracy (Blum et al. (2022) and Yue et al. (2024),\nthough see Mangal et al. (2022)). The foundational work by Viola and M. Jones (2001) on rapid object\ndetection demonstrated that a low cost layer could quickly reject the vast majority of negative examples,\nreserving expensive compute only for ambiguous cases. Recent work has adapted this paradigm\nto modern language model monitoring. Greenblatt et al. (2023) propose monitoring architectures\nthat combine trusted weak models, untrusted strong models, and trusted (but expensive) human\noversight; via clever cascading design, they show that one can achieve much better performance than\nusing any one source of oversight. Similarly, Cunningham, Peng, et al. (2025), Hua et al. (2025),\nand McKenzie et al. (2025) demonstrate that activation probes can serve as a first line of defense for\na more expensive language model classifier when detecting misuse, and Cunningham, Peng, et al.\n(2025) specifically show that the resulting system achieves most of the performance of the language\nmodel while not costing significantly more than the extremely cheap activation probe. Finally, Oldfield\net al. (2025) show that one can also apply this cascading classifier approach solely to a sequence of\nincreasingly expensive activation probes (which correspond to additional polynomial terms in the\nprobeâ€™s architecture), and Youstra et al. (2025) show that probes are a stronger defence than LLMs\nfor cipher attacks specifically.\n\n\n6.4. Distribution Shifts\n\nThe failure of machine learning models to generalize when the test distribution differs from the training\ndistribution is a classic and extensively studied problem (Blitzer et al., 2007; Torralba and Efros,\n2011). While large scale pretraining has generally improved out-of-distribution robustness compared\nto earlier architectures (Hendrycks et al., 2020), modern language models remain vulnerable to\nmany types of production distribution shifts (not least of which are adversarial examples; see the next\nrelated work section). Notably for our paper, past work has observed that long context (Liu et al.,\n2023; Hsieh et al., 2024) and multi-turn data (Laban et al., 2025) are challenging distribution shifts\nfor language models, so we should perhaps not be as surprised that activation probesâ€”which directly\noperate on language model hidden statesâ€”struggle in these settings as well.\n\n   7Sometimes called â€œhierarchical classifiersâ€ or â€œhierarchical monitorsâ€, e.g. in McKenzie et al. (2025).\n\n\n\n\n\n                                                                                                        18\n\n                                           Building Production-Ready Probes For Gemini\n\n\n6.5. Jailbreaking Models\n\nAdversarial examplesâ€”inputs explicitly designed to trigger model errorsâ€”are similarly a foundational\nand unsolved problem in machine learning (Szegedy et al., 2013). In the context of language models,\nthis manifests as \"jailbreaking\": crafting inputs that bypass safety training to elicit harmful outputs.\n\nThe discovery of these vulnerabilities has increasingly moved from manual engineering to automated\nsearch. Zou, Z. Wang, et al. (2023) pioneered automated prompt finding with GCG, a white box\ngradient-based method for generating universal adversarial suffixes. Other approaches have leveraged\nLLMs as attackers: Perez et al. (2022) demonstrate the utility of using LLMs to generate diverse red\nteaming test cases, while PAIR (Chao, Robey, et al., 2025) uses an iterative, adversary-versus-target\nLLM framework to refine attacks. Attacks have also emerged that exploit new model capabilities; no-\ntably, \"many-shot\" jailbreaking (Anil et al., 2024) leverages extended context windows with hundreds\nof harmful demonstrationsâ€”a key distribution shift our probes are designed to withstand.\n\nBenchmarks such as JailbreakBench (Chao, Debenedetti, et al., 2024) and HarmBench (Mazeika\net al., 2024) have been established to standardize jailbreak mitigation evaluation. However, Nasr\net al. (2025) highlight that static defenses remain brittle, showing that adaptive attacksâ€”where the\nadversary has knowledge of the defenseâ€”can bypass most existing guardrails.\n\n\n6.6. Automated Safety Research\n\nRecent advances have enabled language models to act as autonomous agents capable of conducting\nend-to-end scientific research; for example, Lu et al. (2024) and Gottweis et al. (2025) recently\nintroduced frameworks for fully automated scientific discovery. Within the safety domain, such agents\ncan accelerate manual red teaming and interpretability workflows; for example, MAIA (Shaham et al.,\n2025) uses a suite of tools to iteratively experiment on target models to identify failure modes and\nexplain inner mechanisms.\n\nOur work extends this trend by applying automated language model guided optimization directly to\nAI safety-relevant problems. Novikov et al. (2025) propose AlphaEvolve, an evolutionary system that\nimproves algorithms by prompting LLMs to make iterative changes to the best programs so far. We\nleverage AlphaEvolve in this paper to automate the search for well-generalizing probe architectures\nand to find more effective adversarial prompts in our automated red teaming system.\n\n\n7. Conclusion\n\nIn this work, our contributions were: (i) showing that probes are an effective complement to black-\nbox classifiers on cyber-misuse detection, (ii) showing that there is significant headroom in probe\nperformance, with both architecture variations we discovered, and others that AlphaEvolve discovered,\nand (iii) highlighting that robustness to distribution shifts remains a difficult problem for probes (and\nLLM classifiers).\n\nThe limitations of our work include that we do not evaluate against probes using activations from\nevery layer of the model as applied in concurrent work (Cunningham, Wei, et al., 2026), though\npotentially our findings and methodology (such as using AlphaEvolve) could be combined. Secondly,\nwe focus entirely on monitoring inputs to language models, and while we study multi-turn inputs,\nwe do not investigate whether any methods might classify that a language model rollout should be\nflagged partway through the rollout. Finally, a number of our results have large error bars, which\nmakes it difficult for us to recommend one single probe architecture, for example. Future research\ncould evaluate on several different domains beyond cyber misuse and academic datasets (Appendix A),\n\n\n\n\n                                                                                                        19\n\n                                           Building Production-Ready Probes For Gemini\n\n\nto produce exact recommendations.\n\nTo close, we hope that our findings can both improve the quality of defenses used in frontier AI\ndeployments at a given cost and also enable further research into the quality of activation probes. For\nexample, we are excited about future work that builds on the open-weights and open-data research\nin Appendix A.\n\n\n8. Acknowledgements\n\nWe are extremely grateful for the work of the Gemini Introspection workstream at Google for their\nwork adding probes to production Gemini deployments which we collaborated on. In particular Trilok\nAcharya, Wendy Kan, Felipe Tiengo Ferreira, BlaÅ¾ BrataniÄ, Andrey Vlasov, Andre Fernandes, Tolu\nDelano, Weina Ge, Yuhan Fu, Chris Hsu, Felipe Tiengo Ferreira, Seiji Sakiyama, Thai Duong, Evan\nFeder, Julian Freedberg and Riccardo Patana all worked on this effort. We are additionally grateful to\ndataset and evaluation contributions from Ziyue Wang, Raluca Ada-Popa, Laurent Simon and Xerxes\nDotiwalla, as well as Drew Proud, Tom Weingarten and the team behind the helpful-only Gemini\nvariant we use, and finally Vikrant Varma, Tom Lieberum, Samuel Albanie, Irhum Shafkat and Kate\nWoolverton for fantastic foundational engineering tooling. Finally we are also grateful to Ziyue Wang\nfor feedback on a draft of this paper, and grateful to Myriam Khan, Yotam Doron and Anca Dragan\nfor publication help.\n\n\n9. Author Contributions\n\nJK made the first findings that MultiMax architectures could generalize to long contexts, and built\nfoundational infrastructure that made all this work possible. JE ran all AlphaEvolve runs and wrote\nthe first draft of the paper and edited it extensively, and additionally assisted with the autorater\nbaseline experiments. ZW led the autorater baselines and ran the LC training experiments. BC built\nthe seed program of the automated red-teaming infrastructure, and provided feedback on the paper.\nRS and NN provided advice throughout the project. AC proposed the initial project, ran all final probe\nevaluations except for LC Attn Probes, and wrote the final paper.\n\n\nReferences\n\nAlain, Guillaume and Yoshua Bengio (2016). â€œUnderstanding intermediate layers using linear classifier\n    probesâ€. In: arXiv preprint arXiv:1610.01644.\nAnil, Cem et al. (2024). â€œMany-shot jailbreakingâ€. In: Advances in Neural Information Processing\n   Systems 37, pp. 129696â€“129742.\nAnthropic (Nov. 2025a). Disrupting the First Reported AI-Orchestrated Cyber Espionage Campaign.\n   Technical Report. Anthropic. url: https://assets.anthropic.com/m/ec212e6566a0d47\n   /original/Disrupting-the-first-reported-AI-orchestrated-cyber-espionage\n  -campaign.pdf.\nAnthropic (May 14, 2025b). Responsible Scaling Policy. Tech. rep. Version 2.2. Effective May 14, 2025.\n   Anthropic. url: https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1\n   dc4853e4d37.pdf (visited on 10/22/2025).\nAnthropic (May 2025c). System Card: Claude Opus 4 & Claude Sonnet 4. Tech. rep. Anthropic. url:\n   https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47\n   .pdf.\nBai, Yuntao et al. (2022). â€œConstitutional ai: Harmlessness from ai feedbackâ€. In: arXiv preprint\n   arXiv:2212.08073.\n\n\n                                                                                                        20\n\n                                           Building Production-Ready Probes For Gemini\n\n\nBerg, Mark de et al. (2008). â€œComputational Geometry: Algorithms and Applicationsâ€. In: Berlin,\n   Heidelberg: Springer, pp. 290â€“294. doi: 10.1007/978-3-540-77974-2.\nBlitzer, John, Mark Dredze, and Fernando Pereira (2007). â€œBiographies, bollywood, boom-boxes\n   and blenders: Domain adaptation for sentiment classificationâ€. In: Proceedings of the 45th annual\n   meeting of the association of computational linguistics, pp. 440â€“447.\nBlum, Avrim et al. (2022). â€œBoosting Barely Robust Learners: A New Perspective on Adversarial\n   Robustnessâ€. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo et al. Vol. 35.\n   Curran Associates, Inc., pp. 1307â€“1319. url: https://proceedings.neurips.cc/paper\n   _files/paper/2022/file/08fe50bf209c57eecf0804f9f9ed639f-Paper-Conferenc\n   e.pdf.\nChao, Patrick, Edoardo Debenedetti, et al. (2024). â€œJailbreakbench: An open robustness benchmark\n    for jailbreaking large language modelsâ€. In: Advances in Neural Information Processing Systems 37,\n   pp. 55005â€“55029.\nChao, Patrick, Alexander Robey, et al. (2025). â€œJailbreaking black box large language models in\n   twenty queriesâ€. In: 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).\n   IEEE, pp. 23â€“42.\nChristiano, Paul F et al. (2017). â€œDeep reinforcement learning from human preferencesâ€. In: Advances\n    in neural information processing systems 30.\nCollobert, Ronan et al. (2011). Natural Language Processing (almost) from Scratch. arXiv: 1103.0398\n   [cs.LG]. url: https://arxiv.org/abs/1103.0398.\nCunningham, Hoagy, Alwin Peng, et al. (2025). â€œCost-Effective Constitutional Classifiers via Represen-\n    tation Re-useâ€. In: url: https://alignment.anthropic.com/2025/cheap-monitors/.\nCunningham, Hoagy, Jerry Wei, et al. (2026). Constitutional Classifiers++: Efficient Production-Grade\n    Defenses against Universal Jailbreaks. arXiv: 2601.04603 [cs.CR]. url: https://arxiv.org\n   /abs/2601.04603.\nDarema, Frederica et al. (1988). â€œA single-program-multiple-data computational model for EPEX/FORTRANâ€.\n    In: Parallel Computing 7.1, pp. 11â€“24.\nGeifman, Yonatan and Ran El-Yaniv (Sept. 2019). â€œSelectiveNet: A Deep Neural Network with an\n    Integrated Reject Optionâ€. In: Proceedings of the 36th International Conference on Machine Learning.\n   Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Vol. 97. Proceedings of Machine Learning\n   Research. PMLR, pp. 2151â€“2159. url: https://proceedings.mlr.press/v97/geifman1\n   9a.html.\nGeorgiev, Bogdan et al. (2025). Mathematical exploration and discovery at scale. arXiv: 2511.02864\n   [cs.NE]. url: https://arxiv.org/abs/2511.02864.\nGoldowsky-Dill, Nicholas et al. (2025). â€œDetecting strategic deception with linear probesâ€. In: Forty-\n   second International Conference on Machine Learning.\nGoogle (2025a). Continuing to bring you our latest models, with an improved Gemini 2.5 Flash and\n    Flash-Lite release. Google Developers Blog. url: https://developers.googleblog.com/en\n   /continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-\n   flash-and-flash-lite-release/ (visited on 11/13/2025).\nGoogle (Aug. 2025b). Gemini 2.5 Deep Think Model Card. https://storage.googleapis.com/d\n   eepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf.\nGoogle DeepMind (Sept. 22, 2025). Frontier Safety Framework. Tech. rep. Version 3.0. Google\n   DeepMind. url: https://storage.googleapis.com/deepmind-media/DeepMind.com\n   /Blog/strengthening-our-frontier-safety-framework/frontier-safety-frame\n   work_3.pdf (visited on 10/22/2025).\nGottweis, Juraj et al. (2025). â€œTowards an AI co-scientistâ€. In: arXiv preprint arXiv:2502.18864.\nGreenblatt, Ryan et al. (2023). â€œAI control: Improving safety despite intentional subversionâ€. In: arXiv\n    preprint arXiv:2312.06942.\n\n\n\n\n                                                                                                        21\n\n                                           Building Production-Ready Probes For Gemini\n\n\nHan, Seungju et al. (2024). â€œWildguard: Open one-stop moderation tools for safety risks, jailbreaks,\n   and refusals of llmsâ€. In: Advances in Neural Information Processing Systems 37, pp. 8093â€“8131.\nHendrycks, Dan et al. (2020). â€œPretrained transformers improve out-of-distribution robustnessâ€. In:\n   arXiv preprint arXiv:2004.06100.\nHsieh, Cheng-Ping et al. (2024). â€œRULER: Whatâ€™s the Real Context Size of Your Long-Context Language\n   Models?â€ In: arXiv preprint arXiv:2404.06654.\nHua, Tim Tian et al. (2025). Combining Cost-Constrained Runtime Monitors for AI Safety. arXiv:\n   2507.15886 [cs.CY]. url: https://arxiv.org/abs/2507.15886.\nHughes, John et al. (2024). Best-of-N Jailbreaking. arXiv: 2412.03556 [cs.CL]. url: https://a\n   rxiv.org/abs/2412.03556.\nInan, Hakan et al. (2023). â€œLlama guard: Llm-based input-output safeguard for human-ai conversa-\n    tionsâ€. In: arXiv preprint arXiv:2312.06674.\nKantamneni, Subhash et al. (2025). â€œAre sparse autoencoders useful? a case study in sparse probingâ€.\n    In: arXiv preprint arXiv:2502.16681.\nKim, Yoon (2014). Convolutional Neural Networks for Sentence Classification. arXiv: 1408.5882\n   [cs.CL]. url: https://arxiv.org/abs/1408.5882.\nLaban, Philippe et al. (2025). LLMs get lost in multi-turn conversation. arXiv: 2505.06120.\nLiu, Nelson F et al. (2023). â€œLost in the middle: How language models use long contextsâ€. In: arXiv\n    preprint arXiv:2307.03172.\nLu, Chris et al. (2024). â€œThe ai scientist: Towards fully automated open-ended scientific discoveryâ€.\n    In: arXiv preprint arXiv:2408.06292.\nMadras, David, Toniann Pitassi, and Richard S. Zemel (2018). â€œPredict Responsibly: Improving Fairness\n   and Accuracy by Learning to Deferâ€. In: Advances in Neural Information Processing Systems 31,\n   pp. 6150â€“6160. url: https://papers.nips.cc/paper/7853-predict-responsibly-i\n   mproving-fairness-and-accuracy-by-learning-to-defer.\nMangal, Ravi et al. (2022). On the Perils of Cascading Robust Classifiers. arXiv: 2206.00278 [cs.LG].\n   url: https://arxiv.org/abs/2206.00278.\nMazeika, Mantas et al. (2024). â€œHarmbench: A standardized evaluation framework for automated\n   red teaming and robust refusalâ€. In: arXiv preprint arXiv:2402.04249.\nMcKenzie, Alex et al. (2025). â€œDetecting High-Stakes Interactions with Activation Probesâ€. In: arXiv\n    preprint arXiv:2506.10805.\nMozannar, Hussein and David Sontag (13â€“18 Jul 2020). â€œConsistent Estimators for Learning to Defer\n    to an Expertâ€. In: Proceedings of the 37th International Conference on Machine Learning. Ed. by\n   Hal DaumÃ© III and Aarti Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR,\n   pp. 7076â€“7087. url: https://proceedings.mlr.press/v119/mozannar20b.html.\nNaihin, Silen et al. (2023). â€œTesting language model agents safely in the wildâ€. In: arXiv preprint\n   arXiv:2311.10538.\nNasr, Milad et al. (2025). â€œThe Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses\n   Against Llm Jailbreaks and Prompt Injectionsâ€. In: arXiv preprint arXiv:2510.09023.\nNovikov, Alexander et al. (2025). â€œAlphaEvolve: A coding agent for scientific and algorithmic discoveryâ€.\n    In: arXiv preprint arXiv:2506.13131.\nOldfield, James et al. (2025). â€œBeyond Linear Probes: Dynamic Safety Monitoring for Language\n   Modelsâ€. In: arXiv preprint arXiv:2509.26238.\nOpenAI (Apr. 15, 2025). Preparedness Framework. Tech. rep. Version 2. Last updated: 15 April 2025.\n   OpenAI. url: https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddeb\n   cd/preparedness-framework-v2.pdf (visited on 10/22/2025).\nPerez, Ethan et al. (2022). â€œRed teaming language models with language modelsâ€. In: arXiv preprint\n   arXiv:2202.03286.\n\n\n\n\n\n                                                                                                        22\n\n                                           Building Production-Ready Probes For Gemini\n\n\nPicard, David (2023). torch.manual_seed(3407) is all you need: On the influence of random seeds\n    in deep learning architectures for computer vision. arXiv: 2109.08203 [cs.CV]. url: https:\n   //arxiv.org/abs/2109.08203.\nRisch, Julian and Ralf Krestel (May 2020). â€œBagging BERT Models for Robust Aggression Identificationâ€.\n    In: Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying. Marseille, France:\n   European Language Resources Association (ELRA), pp. 55â€“61. url: https://aclanthology\n   .org/2020.trac-1.9/.\nShabalin, Stepan and Nora Belrose (Aug. 2025). Attention Probes. https://blog.eleuther.ai/a\n   ttention-probes/.\nShah, Rohin et al. (2025). An Approach to Technical AGI Safety and Security. arXiv: 2504.01849\n   [cs.AI]. url: https://arxiv.org/abs/2504.01849.\nShaham, Tamar Rott et al. (2025). A Multimodal Automated Interpretability Agent. arXiv: 2404.14394\n   [cs.AI]. url: https://arxiv.org/abs/2404.14394.\nSharma, Mrinank et al. (2025). â€œConstitutional classifiers: Defending against universal jailbreaks\n    across thousands of hours of red teamingâ€. In: arXiv preprint arXiv:2501.18837.\nSzegedy, Christian et al. (2013). â€œIntriguing properties of neural networksâ€. In: arXiv preprint\n   arXiv:1312.6199.\nTillman, Henk and Dan Mossing (2025). â€œInvestigating task-specific prompts and sparse autoencoders\n    for activation monitoringâ€. In: arXiv preprint arXiv:2504.20271.\nTorralba, Antonio and Alexei A Efros (2011). â€œUnbiased look at dataset biasâ€. In: CVPR 2011. IEEE,\n    pp. 1521â€“1528.\nViola, Paul and Michael Jones (2001). â€œRapid object detection using a boosted cascade of simple\n    featuresâ€. In: Proceedings of the 2001 IEEE computer society conference on computer vision and\n    pattern recognition. CVPR 2001. Vol. 1. Ieee, pp. Iâ€“I.\nYoustra, Jack et al. (2025). Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks. arXiv:\n   2508.17158 [cs.LG]. url: https://arxiv.org/abs/2508.17158.\nYue, Murong et al. (2024). Large Language Model Cascades with Mixture of Thoughts Representations\n    for Cost-efficient Reasoning. arXiv: 2310.03094 [cs.CL]. url: https://arxiv.org/abs/23\n   10.03094.\nZou, Andy, Long Phan, Sarah Chen, et al. (2023). â€œRepresentation engineering: A top-down approach\n    to ai transparencyâ€. In: arXiv preprint arXiv:2310.01405.\nZou, Andy, Long Phan, Justin Wang, et al. (2024). Improving Alignment and Robustness with Circuit\n    Breakers. arXiv: 2406.04313 [cs.LG]. url: https://arxiv.org/abs/2406.04313.\nZou, Andy, Zifan Wang, et al. (2023). â€œUniversal and transferable adversarial attacks on aligned\n   language modelsâ€. In: arXiv preprint arXiv:2307.15043.\n\n\nAppendix\n\nA. Running our findings on other datasets and models\n\nTo test the generality of our findings, we evaluate our probe architectures on a diverse set of classi-\nfication tasks beyond cyber misuse detection. We use a subset of the binary classification datasets\nfrom Kantamneni et al. (2025), selecting only datasets with at least 128 datapoints. The resulting 12\ndatasets span truthfulness (TruthfulQA), ethics (Commonsense Morality, Deontology, Justice, Virtue\nEthics), news classification (Politics, Technology, Entertainment), as well as a simple text property\ndetection task (Is Short Description). We also use the Gemma-2 9B base model, like Kantamneni et al.\n(2025).8\n\n   8E.g. https://github.com/JoshEngels/SAE-Probes/blob/cec41e/ai_vs_humanmade_plot.py#L21\n\n\n\n\n                                                                                                        23\n\n                                           Building Production-Ready Probes For Gemini\n\n\nFigures 6 and 7 and Table 5 show AUROC scores across all architectures and datasets. Each architecture\nis evaluated with 100 random seeds per dataset, allowing us to report both median performance and\nbest-seed performance. As a baseline, we include the logistic regression results from Kantamneni\net al. (2025).\n\n\nArchitecture Ranking.  The AlphaEvolve architectures (early and final) achieve the highest median\nAUROC scores (0.975), followed closely by the Attention Probe with Default aggregation (median\nAUROC 0.975). The Attention Probe variants with MultiMax aggregation perform slightly worse\n(0.969), while the logistic regression baseline achieves median AUROC of 0.944. This ranking is\nconsistent across most datasets, with AlphaEvolve outperforming the logistic regression baseline by\nan average of 3.1 percentage points in median AUROC.\n\n\nSeed Variance.  Comparing median versus best-seed performance (Figure 7), we observe that seed\nselection provides modest improvements of 0.5â€“1.5 percentage points depending on architecture.\n\n\nLinear Probe Baseline.  Our experiments include a linear probe implementation trained using the\nsame infrastructure as our other architectures. However, this linear probe achieves median AUROC of\nonly 0.814 across datasets, substantially underperforming the logistic regression baseline from prior\nwork (median AUROC 0.944). We attribute this gap to our training infrastructure being optimized for\narchitectures with at least 10 or 100 times more parameters: the learning rate schedules, batch sizes,\nand regularization settings were tuned for attention-based probes, not linear probes. We therefore\nuse the logistic regression baseline from prior work as the reference point for linear methods in our\nvisualizations.\n\n\nComparison to Prior Work.  Several datasets overlap between our evaluation and Table 9 of\nKantamneni et al. (2025), which reports results on a sample of their full dataset collection; both\nworks use Gemma 2 9B as the base model. On news classification, we observe slightly higher scores\n(0.98 vs 0.965). The largest difference appears on Commonsense Morality, where our attention probe\nachieves 0.94 compared to 0.86 in prior work. This gap may be explained by architectural differences:\nour attention probes include an MLP layer before the attention mechanism (Section 3.1.4), which\nprovides additional representational capacity.\n\n\nB. Dataset Statistics\n\nHere we detail the specific sample counts and filtering logic for the datasets described in Table 1.\n\n\nB.1. Evaluation and Validation Splits\n\nTable 6 provides the exact sample counts for the validation and test splits used to calculate the error\nmetrics defined in Section 4.\n\n\nB.2. Training Data Configuration\n\nTable 7 details the composition of the training mixture. We apply heavy subsampling to the cyber-\nspecific dataset and the Multi-Turn data to balance the dataset mixture.\n\n\n\n\n\n                                                                                                        24\n\n                                           Building Production-Ready Probes For Gemini\n\n\n                             Median AUROC (rows: architectures by median, cols: datasets by median)\n\n         Attention Probe       1.000           0.995           0.984           0.986           0.983           0.980           0.970           0.943           0.940      (10H, Default Agg)\n                                                                                                                                                                           1.000\n            AlphaEvolve       1.000           0.995           0.987           0.987           0.983           0.982           0.968           0.947           0.935\n\n                                                                                                                                                                           0.975\n            AlphaEvolve       1.000           0.995           0.987           0.986           0.983           0.982           0.968           0.945           0.934                  (Early)\n                                                                                                                                                                           0.950\n         Attention Probe       1.000           0.994           0.984           0.979           0.979           0.979           0.958           0.925           0.919        (20H, MultiMax)\n                                                                                                                                                                           0.925\n                                                                                                                                                                                                          1.0)         Attention                 Probe       1.000           0.995           0.985           0.980           0.979           0.979           0.960           0.936           0.921             to        (10H,             MultiMax)\n                                                                                                                                                                           0.900(0.8\n         Attention  Architecture          (10H, Rolling)                 Probe       1.000           0.992           0.983           0.979           0.979           0.979           0.959           0.935           0.914                                  AUROC\n                                                                                                                                                                           0.875\n         Attention Probe       1.000           0.992           0.983           0.978           0.979           0.978           0.958           0.924           0.910          (20H, Rolling)\n                                                                                                                                                                           0.850\n         Attention Probe\n     (10H, Rolling Train,       1.000           0.976           0.984           0.972           0.979           0.976           0.943           0.923           0.886\n        MultiMax Eval)\n                                                                                                                                                                           0.825\n         Attention Probe\n     (20H, Rolling Train,       1.000           0.970           0.983           0.969           0.980           0.975           0.942           0.908           0.870\n        MultiMax Eval)\n                                                                                                                                                                           0.800\n               LogReg       1.000           0.987           0.962           0.948           0.978           0.951           0.940           0.933           0.833               (Baseline)\n\n                                    Is Short          Virtue        News       Deontology      News        News           Justice       TruthfulQA   Commonsense\n                            Description        Ethics       Technology        Valid            Politics     Entertainment          Is                           Morality\n                                                                           Dataset\n\nFigure 6  | Median AUROC across 100 seeds for each architecture-dataset pair. Green indicates higher\n(better) AUROC values. Architectures (rows) and datasets (columns) are sorted by their overall\nmedian performance.\n\n\nB.3. Long Context Data Filtering\n\nTo ensure robust evaluation of Long Context (LC) capabilities without data leakage, strict filtering\nwas applied to the FNRLC[A]dataset.\n\n   â€¢ Total LC stem fingerprints: 2,147 raw rows.\n   â€¢ Leakage Removal: 1,711 LC rows matched the Short Context (SC) training data stems and\n     were excluded from the LC evaluation to test true generalization.\n   â€¢ Final Availability: 436 LC rows were available for the â€˜Cleanâ€™ evaluation split.\n\n\nC. General Training Details\n\nIn this appendix, we list out various hyperparameters that we chose for our probe training runs. See\nalso Appendix B where we describe the training, validation and eval data.\n\n   â€¢ Optimizer: AdamW\n   â€¢ Learning Rate: 1 Ã— 10âˆ’4\n   â€¢ Weight Decay: 3 Ã— 10âˆ’3\n   â€¢ Optimizer Momentum: ğ›½1 = 0.9, ğ›½2 = 0.999\n   â€¢ Training Duration: 1000 steps\n   â€¢ Batch Size: Full batch gradient descent\n   â€¢ Source Layer: Activations harvested from the layer halfway through the model\n   â€¢ MLP Transformation (Figure 2): Two layers of width 100\n\n\n                                                                                                        25\n\n                                           Building Production-Ready Probes For Gemini\n\n\n                             Best-of-100-Seeds AUROC (rows: architectures by median, cols: datasets by median)\n\n         Attention Probe       1.000           0.996           0.988           0.988           0.986           0.982           0.973           0.955           0.947     (10H, Default Agg)\n                                                                                                                                                                          1.000\n           AlphaEvolve       1.000           0.996           0.991           0.989           0.988           0.986           0.975           0.968           0.947\n\n                                                                                                                                                                          0.975\n           AlphaEvolve       1.000           0.996           0.991           0.989           0.987           0.986           0.975           0.972           0.945                  (Early)\n                                                                                                                                                                          0.950\n         Attention Probe       1.000           0.996           0.989           0.984           0.985           0.982           0.963           0.963           0.927       (20H, MultiMax)\n                                                                                                                                                                          0.925\n                                                                                                                                                                                                          1.0)         Attention                 Probe       1.000           0.996           0.990           0.983           0.984           0.982           0.964           0.956           0.931             to       (10H,             MultiMax)\n                                                                                                                                                                          0.900(0.8\n         Attention Architecture         (10H, Rolling)                 Probe       1.000           0.995           0.987           0.983           0.984           0.983           0.965           0.958           0.930                                  AUROC\n                                                                                                                                                                          0.875\n         Attention Probe       1.000           0.996           0.986           0.983           0.984           0.984           0.965           0.959           0.929         (20H, Rolling)\n                                                                                                                                                                          0.850\n         Attention Probe\n    (10H, Rolling Train,       1.000           0.993           0.989           0.980           0.984           0.980           0.954           0.953           0.908\n        MultiMax Eval)\n                                                                                                                                                                          0.825\n         Attention Probe\n    (20H, Rolling Train,       1.000           0.993           0.987           0.978           0.984           0.983           0.952           0.955           0.895\n        MultiMax Eval)\n                                                                                                                                                                          0.800\n              LogReg       1.000           0.987           0.962           0.948           0.978           0.951           0.940           0.933           0.833               (Baseline)\n\n                                    Is Short          Virtue        News       Deontology      News        News           Justice       TruthfulQA   Commonsense\n                            Description        Ethics       Technology        Valid            Politics     Entertainment          Is                           Morality\n                                                                           Dataset\n\nFigure 7  | Best seed AUROC (out of 100 seeds) for each architecture-dataset pair. The ordering\nmatches Figure 6 to facilitate comparison.\n\n\n  â€¢ Activation Function: ReLU\n\n\nD. Seed Selection Details\n\nThis appendix provides additional details on the seed selection analysis from Section 4.2.\n\n\nD.1. High-Variance Architectures\n\nFigure 8 shows the seed selection analysis for architectures with MultiMax aggregation at evaluation\ntime. These architectures exhibit substantially larger interquartile ranges (IQR) in test loss across\nseeds compared to those shown in Figure 3, which is why we present them separately to avoid\ndistorting the main figureâ€™s scale.\n\n\nD.2. Raw Seed Selection Statistics\n\nTable 8 presents the complete numerical data underlying Figure 3 and Figure 8.\n\n\nD.3. Seed Selection vs Architecture Choice\n\nConsistent with the findings in Section 4.2, we investigate the relative impact of seed selection versus\narchitecture choice on these auxiliary datasets. To quantify the impact of seed selection, we compare\nthe test AUROC of the best seed (oracle) to the median seed for each architecture. Across architectures,\nselecting the best seed improves test AUROC by 0.011 on average, though individual architectures\nvary from +0.007 (Attention Probe with Default aggregation) to +0.014 (Linear Probe).\n\n\n\n                                                                                                        26\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 5  | Median AUROC by architecture and dataset. Dataset codes: IS=Is Short, VE=Virtue\nEthics, NT/NP/NE=News Tech/Politics/Entertainment, DV=Deontology, JI=Justice, TQ=TruthfulQA,\nCM=Commonsense.  Architecture codes: Def=Default aggregation, MM=MultiMax train/eval,\nRoll=Rolling train/eval, Râ†’MM=Rolling train with MultiMax eval.\n\n          Arch                   IS    VE   NT   DV   NP   NE      JI   TQ  CM   Med\n          Attn-10H (Def)      1.00   1.00   0.98   0.99   0.98   0.98   0.97   0.94   0.94   0.9834\n        AE                  1.00   1.00   0.99   0.99   0.98   0.98   0.97   0.95   0.93   0.9833\n        AE (Early)           1.00   0.99   0.99   0.99   0.98   0.98   0.97   0.94   0.93   0.9830\n          Attn-20H (MM)      1.00   0.99   0.98   0.98   0.98   0.98   0.96   0.92   0.92   0.9791\n          Attn-10H (MM)      1.00   0.99   0.99   0.98   0.98   0.98   0.96   0.94   0.92   0.9791\n          Attn-10H (Roll)      1.00   0.99   0.98   0.98   0.98   0.98   0.96   0.93   0.91   0.9789\n          Attn-20H (Roll)      1.00   0.99   0.98   0.98   0.98   0.98   0.96   0.92   0.91   0.9782\n          Attn-10H (Râ†’MM)   1.00   0.98   0.98   0.97   0.98   0.98   0.94   0.92   0.89   0.9760\n          Attn-20H (Râ†’MM)   1.00   0.97   0.98   0.97   0.98   0.98   0.94   0.91   0.87   0.9701\n         LogReg              1.00   0.99   0.96   0.95   0.98   0.95   0.94   0.93   0.83   0.9513\n           Linearâ€               1.00   0.67   0.97   0.81   0.97   0.97   0.74   0.79   0.77   0.8145\n\n                  â€ Underperforms LogReg likely due to training infra optimized for larger probes.\n\nTable 6  | Sample counts for Validation and Test splits across all datasets. The Metric column maps\nthese datasets to the error variables defined in Table 1.\n\n\n         Dataset Description                   Metric       Val     Test    Total\n\n          Benign / Random Traffic (False Positive evaluation)\n          Short Context RT (Refusal/Response)   FPRSC[OT]   8,283  33,132  41,415\n         Long Context RT (Benign Code)        FPRLC[RT]    202    812   1,014\n\n          Attacks / Cyber (False Negative evaluation)\n          Short Context Cyber                   FNRSC[A]    483    918   1,401\n          Multi-Turn Cyber                     FNRMT[A]     80    320    400\n         Long Context Cyber                   FNRLC[A]    429   1,718   2,147\n           Jailbreaks                            FNRSC[J]   4,277  17,108  21,385\n       ART (Advanced Red Teaming)         FNRSC[ART]    103    431    534\n\n\nIn contrast, architecture choice provides a much larger gain. Comparing the median-seed AUROC, the\nbest architecture (Attention Probe with Default aggregation) achieves 0.933 while the Linear Probe\nachieves only 0.824, a gap of 0.109 AUROC. This is approximately 10Ã— larger than the average seed\nselection gain, consistent with the main bodyâ€™s finding that architecture search should be prioritized\nover extensive seed tuning.\n\nInterestingly, the AlphaEvolve architectures (AE1 and AE2) achieve nearly identical performance\n(0.930 median AUROC) and show higher seed variance (+0.012 gain) compared to the Attention\nProbe with Default aggregation (+0.007 gain). This suggests that more complex architectures may\nbenefit more from careful seed selection, though the effect is still modest compared to architecture\nchoice.\n\n\nE. Efficiently finding the Threshold-Randomization-Optimal cascading policy\n\nRecall that in general, a cascading policy is defined by two thresholds (ğ‘¡0, ğ‘¡1): classify as negative if\nthe probe logit is below ğ‘¡0, classify as positive if above ğ‘¡1, and defer to the expensive model otherwise.\n\n\n\n\n                                                                                                        27\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 7  | Training dataset composition.\n\n\n                     Dataset Description          Training Split Size\n\n                      Short Context Attacks                     1,928\n                      Short Context Overtriggering              1,206\n                      Multi-Turn Attacks                       41\n\n                       Total                                 3,175\n\n    0.200       Median       Seed Selection: Median --> Best-Val\n               Best-Val               Improvement    0.175\n              Oracle\n    0.150       IQR\nLoss 0.125\nTest 0.100\n\n    0.075\n\n    0.050\n\n    0.025\n\n              Rolling     AlphaEvolve     Rolling        Rolling     AlphaEvolve   Vanilla Attn     Rolling      MultiMax    Linear Probe  Linear Probe\n             Attention                    Attention      Attention       (Early                      Attention      Trained      Mean      Rolling Exp\n              Probe,                      Probe,     Probe (20H),    Variant)                 Probe (20H),     Probe,\n         MultiMax Agg                 Rolling Agg   Rolling Agg                           MultiMax Agg MultiMax Agg\n\nFigure 8 | Effect of seed selection on test loss for high-variance architectures. Same format as Figure 3,\nbut showing architectures with MultiMax aggregation that have large IQR across seeds.\n\n\nEach choice of (ğ‘¡0, ğ‘¡1) gives a point on a cost-accuracy plot on the validation data. A natural question\narises: what is the correct way to interpolate between these points to obtain a continuous Pareto\nfrontier?\n\nThere are three possibilities: (1) a pessimistic step function, where between operating points you are\nstuck at the accuracy of the lower-cost option; (2) linear interpolation, where you can smoothly trade\ncost for accuracy; or (3) something better than linear, where the curve bows upward (concave).\nLinear interpolation (2) is achievable and optimal within the family of threshold-randomization policies.9\nThe key insight is that we can randomize between two policies. Given two policies ğ´and ğµon the\nfrontier, we flip a biased coin and use policy ğ´with probability ğ‘and policy ğµwith probability 1 âˆ’ğ‘.\nSince both expected cost and expected loss are linear in ğ‘, this traces out a straight line between the\ntwo points. In our setting, randomization is practical because we process many independent queries:\nrather than literally flipping coins, we can deterministically assign a fraction ğ‘of queries to policy ğ´.\nThe resulting frontier is therefore the lower convex hull of all discrete operating points, which we\ncompute efficiently in ğ‘‚(ğ‘log ğ‘) time (see Appendix E for algorithmic details). Importantly, when\noptimizing a single objective (such as minimizing error at any cost), the optimal policy always lies at\na vertex of the convex hull, meaning no randomization is requiredâ€”the â€œSelected Probe + 3% Flashâ€\n\n   9More sophisticated approaches exist: learned deferral policies can optimize routing based on input features rather than\njust probe classifier output (Mozannar and Sontag, 2020; Madras et al., 2018), and joint training can teach the cheap\nclassifier to be â€œusefully uncertainâ€ in the right places (Geifman and El-Yaniv, 2019). These methods may find operating\npoints below our convex hull, but at the cost of additional training complexity. Our threshold-randomization approach has\nthe advantage of being post-hoc: it can be applied to any pre-trained probe without retraining.\n\n\n\n                                                                                                        28\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 8  | Seed selection statistics across architectures. ğœmed, ğœbest, and ğœoracle are the test losses at the\nmedian, best-validation, and oracle (best-test) seeds respectively. Î” columns show the improvement\nfrom median to best-validation or oracle. A negative Î”best indicates validation selection hurt test\nperformance.\n\n                Architecture                      ğœmed     ğœbest     ğœoracle     Î”best     Î”oracle\n\n                  Roll. Attn., MultiMax Agg        0.031  0.025  0.019   0.006   0.013\n               AlphaEvolve                    0.029  0.025  0.017   0.004   0.012\n                  Roll. Attn., Rolling Agg           0.027  0.026  0.022   0.001   0.005\n                  Roll. Attn. (20H), Rolling Agg     0.028  0.027  0.022   0.000   0.005\n               AlphaEvolve (Early)             0.037  0.029  0.020   0.009   0.018\n               Attn                           0.038  0.035  0.018   0.003   0.020\n                  Roll. Attn. (20H), MultiMax Agg  0.076  0.045  0.022   0.031   0.054\n              MultiMax Tr., MultiMax Agg      0.076  0.054  0.029   0.023   0.047\n\n                Linear Probe Mean              0.064  0.062  0.061   0.002   0.003\n                Linear Probe Rolling Exp         0.084  0.086  0.047   -0.003  0.037\n\n\noperating point in Figure 1 uses a single deterministic threshold pair (see Appendix E.1 for a formal\nargument).\n\nWe compute the optimal cascading frontier in ğ‘‚(ğ‘log ğ‘) time.10 The key insight is to frame the\nproblem as greedy savings: starting from a policy that defers everything to the LLM, we progressively\nâ€œbuyâ€ samples to handle with the probe, always choosing the cheapest option first.\n\n\nSetup.  For each sample ğ‘–(sorted by probe logit), we define three per-sample losses: â„“(ğ‘–)âˆ’(loss if the\nprobe predicts negative), â„“(ğ‘–) (loss if the probe predicts positive), and â„“(ğ‘–)  (loss if we defer to the                          +                                         LLM\nLLM). These are derived from the weighted loss function in Equation (12). Note that we use â€œlossâ€\nhere to distinguish from â€œcostâ€ (the API expense of running the LLM).\n\n\nThe Greedy Savings Frame.  Consider the starting point: defer all ğ‘samples to the LLM. This\nincurs maximum API cost but minimum error (the LLMâ€™s native error rate). From here, we can â€œsaveâ€\nLLM calls by having the probe handle samples instead. Each such move trades API cost for potential\nerror (though note, the error might go down â€“ sometimes the probe is correct when the LLM is not!).\n\nThe crucial observation is that the left and right thresholds are independent. We define two error\nfunctions:\n\n   â€¢ ğ¿(ğ‘˜): The added error if the probe classifies the ğ‘˜lowest-logit samples as negative (instead of\n      deferring them). This equals Ãğ‘˜âˆ’1ğ‘–=0 (â„“(ğ‘–) âˆ’â„“(ğ‘–)LLM).\n   â€¢ ğ‘…(ğ‘˜): The added error if the probe classifies the ğ‘˜highest-logit samples as positive (instead of\n      deferring them). This equals Ãğ‘âˆ’1ğ‘–=ğ‘âˆ’ğ‘˜(â„“(ğ‘–) âˆ’â„“(ğ‘–)LLM).\n\nBoth functions map number of LLM calls saved â†’test loss increase. Since the left and right decisions\ndonâ€™t interfere, the total added error for any policy is simply ğ¿(ğ‘˜ğ¿) + ğ‘…(ğ‘˜ğ‘…) where ğ‘˜ğ¿+ ğ‘˜ğ‘…is the total\nnumber of samples handled by the probe.\n\n  10Note: the core details related to the Threshold-Randomization-Optimal policies and associated algorithms were sug-\ngested by Gemini 3.0 Pro DeepThink and other frontier models. But verified and extensively edited by conmy@google.com\nand frontier models.\n\n\n\n\n                                                                                                        29\n\n                                           Building Production-Ready Probes For Gemini\n\nWe now define the achievable versions Ëœğ¿and Ëœğ‘…as the lower convex hulls of ğ¿and ğ‘…respectively. Points\non the convex hull but not on the original curve are achievable via randomization between adjacent\npolicies (see Section 5.2.2).\n\nThe key insight is that the left and right decisions are independent: choosing to handle ğ‘˜ğ¿samples on\nthe left doesnâ€™t constrain which ğ‘˜ğ‘…samples we handle on the right. The total added error is simply\nthe sum Ëœğ¿(ğ‘˜ğ¿) + Ëœğ‘…(ğ‘˜ğ‘…), and the total LLM calls saved is ğ‘˜ğ¿+ ğ‘˜ğ‘….\nNow we can visualize the set of points (LLM calls saved, Test error increase):11\n\n               F =  ğ‘˜ğ¿+ ğ‘˜ğ‘…, Ëœğ¿(ğ‘˜ğ¿) + Ëœğ‘…(ğ‘˜ğ‘…)  : ğ‘˜ğ¿âˆˆ[0, ğ‘], ğ‘˜ğ‘…âˆˆ[0, ğ‘]                 (13)\n\nThis is the Minkowski sum of the two curves: F = Ëœğ¿âŠ•Ëœğ‘…. weâ€™re combining all possible ways to â€œspendâ€\nour savings budget across the two independent markets. The Pareto frontier of this set traces out the\noptimal cost-error tradeoff (Figure 9).\n\nA beautiful result from computational geometry is that for convex polygonal curves, the Minkowski\nsum can be computed efficiently by simply merging the edge lists sorted by slope (Berg et al., 2008).\nIntuitively: at each step, we should accept whichever â€œdealâ€ (left or right) offers the best marginal\nrateâ€”the lowest added error per LLM call saved. Sorting by slope and greedily accepting the flattest\nedges first achieves exactly this.\n\n\nThe Algorithm. We merge the edge lists of Ëœğ¿and Ëœğ‘…sorted by slope. Walking this merged list from\nflattest to steepest slope traces out the Pareto frontier: at each step, we accept the cheapest available\nâ€œdealâ€ regardless of which side it comes from. We stop when we have saved all ğ‘LLM calls (i.e., the\nprobe handles everything).\n\nThe convex hull computation is ğ‘‚(ğ‘) after sorting, and merging two sorted lists is ğ‘‚(ğ‘). The initial\nsort by probe logit dominates at ğ‘‚(ğ‘log ğ‘).\n\nNote that â€œLLM calls savedâ€ is the flipped version of â€œcostâ€ used in Figure 5. This is why the Pareto\nfrontier in Figure 9 curves upward (more savings â†’more error), while in Figure 5 it curves downward\n(more cost â†’less error).\n\n\nE.1. Vertex Optimality: Why No Randomization is Needed At The Optimal Low Error Point\n\nA natural question arises: if randomization between policies is allowed, does the optimal policy (the\none minimizing error) require randomization? We show that the answer is noâ€”the optimal policy\nalways lies at a vertex of the convex hull, corresponding to a single deterministic threshold pair.\n\n\nGeometric Argument.  Consider optimizing any linear objective over a convex set. The level sets of\na linear function are hyperplanes, and the minimum over a convex polytope is always achieved at a\nvertex (or along an edge, in degenerate cases). In our setting:\n\n\n   â€¢ The set of achievable (Cost, Error) pairs is the convex hull of discrete operating pointsâ€”a convex\n      polygon.\n   â€¢ Minimizing error alone corresponds to a horizontal objective (minimize the ğ‘¦-coordinate).\n\n  11Of course, ğ‘˜ğ¿+ ğ‘˜ğ‘…> ğ‘is not achievable since we only have ğ‘samples total. We ignore this constraint for now, as\nworking in this general form allows us to leverage existing algorithms for Minkowski sums. At the end, we simply restrict to\nthe achievable region ğ‘˜ğ¿+ ğ‘˜ğ‘…â‰¤ğ‘; see Figure 9 for a visualization.\n\n\n\n\n                                                                                                        30\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n                        A. Independent Savings Opportunities                                                           B. Combined Frontier (Minkowski Sum)\n                    (not real data, for illustration and intuition!)                                                 (not real data, for illustration and intuition!)\n                     Left Model (Negatives)                                                          17.5\n                   Right Model (Positives)\n      10\n                                                                                                15.0\n\n       8                                                                                                12.5          intuition!)                                                                                                                                                                                                                                                                                        intuition!)\n   and                                                                            and Error   6                                                                                                                        Error   10.0                                                                                                                                                  Capacity                                                                                                                                                                                                                                                                                                   Right)\n                                                 + Addedillustration                                                                                                                            Addedillustration  7.5   for 4                                                                          for                                                                                                                                                              Combined(Left Total                                                                                                                               Total\n    data,                                                                                                                               data,  5.0\n   real 2                                                                                                   real\n   (not                                                                                                      (not  2.5\n\n       0                                                                                                      0.0                                                                                                 Step taken from Left\n                                                                                                                                                                                                      Step taken from Right\n                                                                                                                                                                                                                     Single Model Limit (N)\n                                                                                                      2.5\n            0           2           4           6           8           10                    0.0           2.5           5.0           7.5          10.0         12.5         15.0         17.5         20.0\n                             LLM Calls Saved (per side)                                                                                                Total LLM Calls Saved\n                               (not real data, for illustration and intuition!)                                                                          (not real data, for illustration and intuition!)\n\nFigure 9  | Illustration of the Minkowski sum algorithm for computing the optimal cascading frontier.\nLeft: The two independent savings curves Ëœğ¿and Ëœğ‘…, representing the added error from having the\nprobe handle samples from the left and right tails respectively. Right: The combined Pareto frontier\nof the Minkowski Sum of the two curves, constructed by merging edge segments sorted by slope.\nSegments are colored by their source (blue from left, orange from right). The vertical line marks the\ndataset size limit; beyond this point, savings are unachievable.\n\n\n  â€¢ More generally, any weighted combination of cost and error is linear and thus minimized at a\n      vertex.\n\n\nThe vertices of our convex hull correspond exactly to non-randomized policies: each vertex is a specific\nthreshold pair (ğ‘¡0, ğ‘¡1). Interior points and edges require randomization between policies.\n\n\nApplication to Figure 1.  The â€œSelected Probe + 8% Flashâ€ operating point in Figure 1 is the\nvertex that minimizes error. This corresponds to a single deterministic cascading policy with specific\nthresholds (ğ‘¡0, ğ‘¡1), where approximately 3% of queries (those with probe logits between ğ‘¡0 and ğ‘¡1)\nare deferred to the LLM. No randomization is needed to achieve this optimal operating point.\n\nThis is convenient in our setting, where we seek to minimize error: deterministic policies are easier\nto implement, debug, and reason about than randomized policies. If we instead targeted a specific\ncost budget, randomization might be required to achieve points on the frontier edges. Additionally,\nfor misuse defense applications, randomized policies may be undesirable from a security perspective:\nHughes et al. (2024) show that attackers can exploit randomization by repeatedly querying until they\nreceive a favorable response (â€œBest-of-N jailbreakingâ€). While our defenses already face challenges\nfrom adaptive attacks (Figure 4d), deterministic policies avoid introducing this additional attack\nsurface.\n\n\nF. AlphaEvolve\n\nF.1. Further Details on Setup\n\nThe training data that AlphaEvolve trains on is 70% of the short-context cyber attack data (Table 1)\ncombined with 70% of the short-context overtriggering examples. The remaining 30% of each is used\nfor validation threshold selection (see below). Long-context data is only used for validation error\n\n\n\n                                                                                                        31\n\n                                           Building Production-Ready Probes For Gemini\n\n\nAlgorithm 1 Pseudocode for Best-Performing Alpha Evolve Probe Architecture.\nInput: Dataset of activations {ğ‘¿ğ‘–} and labels {â„“ğ‘–}, learning rate schedule ğœ‚ğ‘¡, reg weights ğœ†1, ğœ†ğ‘œğ‘Ÿğ‘¡â„ğ‘œ\nResult: Trained probe parameters ğœƒ= {ğ‘Šğ‘’ğ‘›ğ‘, ğ‘Šğ‘ğ‘Ÿğ‘œğ‘—, ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’, ğ‘Šğ‘œğ‘¢ğ‘¡, . . . }\nwhile not converged do\n   ğ»â†MLP(LayerNorm({ğ‘¿ğ‘–}))\n   // Compute gates\n   ğ‘‰â†(ğ‘Šğ‘ğ‘Ÿğ‘œğ‘—ğ») âŠ™Softplus(ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’ğ»)\n   // Compute max and min across sequence dim (generalization of MultiMax)\n   â„ğ‘ğ‘œğ‘œğ‘™â†Concat(maxğ‘¡ğ‘‰ğ‘¡, âˆ’minğ‘¡ğ‘‰ğ‘¡)\n   ğ‘¦â†ğ‘Šğ‘œğ‘¢ğ‘¡â„ğ‘ğ‘œğ‘œğ‘™\n   // Compute loss with L1 and orthogonality penalties\n   Lğµğ¶ğ¸â†BinaryCrossEntropy(ğœ(ğ‘¦), {â„“ğ‘–})\n   Lğ‘Ÿğ‘’ğ‘”â†ğœ†1 Ãğ‘Šâˆˆğœƒ||ğ‘Š||1 + ğœ†ğ‘œğ‘Ÿğ‘¡â„ğ‘œ||ğ‘Šğ‘‡ğ‘ğ‘Ÿğ‘œğ‘—ğ‘Šğ‘ğ‘Ÿğ‘œğ‘—âˆ’ğ¼||2ğ¹\n  ğœƒâ†AdamW(ğœƒ, âˆ‡ğœƒ(Lğµğ¶ğ¸+ Lğ‘Ÿğ‘’ğ‘”), ğœ‚ğ‘¡)\nend\n\n\ncomputation, not for training.\n\nThe validation error which we hillclimb with AlphaEvolve is the maximum of weighted error rates\nacross different error types: 200Ã— for overtriggering FPR, 40Ã— for long-context FPR, 6.7Ã— for cyber\nFPR, and 4Ã— for both cyber FNR and long-context FNR. Note that this weighting differs from the\n5:2:50 weighting used in Equation (12) because we ran AlphaEvolve early in this research project\nbefore settling on the final metric.\n\nOn net this means that inside the inner training loop, the only data used is training data, according\nto Table 1. In validation, which is used in AlphaEvolve as feedback to the LLM generating programs,\nwe use validation data, as well as some long-context data, which in the main body is only used for\nevaluation. We retrained the AlphaEvolve architectures with the final training mixture (e.g. with Multi-\nTurn data) and reported those numbers in the same body. Additionally Appendix A reports the same\nAlphaEvolve architecture on a different model and with different data, where it performs strongly,\nwhich suggests to us that we didnâ€™t significantly contaminate our results. Notably, in Figure 10, where\nwe find that AlphaEvolve is the second strongest architecture after the Attention Probe, and the\nAttention Probe would not generalize to long context prompts without specific training (see the main\npaper body). We speculate that rerunning AlphaEvolve with the same training mixture it is to be\ntrained with would lead to even stronger performance, but we havenâ€™t verified this.\n\n\nF.2. Alpha Evolve Probe Pseudocode\n\nEarly in the AlphaEvolve run (Figure 10), we found that AlphaEvolve rediscovered the MultiMax\narchitecture (although note that we provided it with an Attention Probe as a base). AlphaEvolve also\nadded a learnable linear feature map to weight each of the head outputs rather than merely sum\nthem. This makes this probe the only such variant that we study in this paper which has a step 6) in\nFigure 2, not merely a sum. This architecture (MultiMax plus a linear weighting of heads) is the early\nin training AlphaEvolve program described in our main results.\n\nWe present pseudocode for the final best-performing AlphaEvolve-discovered architecture in Algo-\nrithm 1. The architecture combines several innovations: gated projections using Softplus activations,\na â€œbipolarâ€ pooling strategy that takes both the max and negated min across the sequence dimension\n(generalizing MultiMax), and regularization via L1 penalties and orthogonality constraints on the\nprojection weights.\n\n\n\n                                                                                                        32\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n\n\n                                                                                  Best Validation Error\n       3\n                                                                                  Corresponding Test Error\n\n\n       2.5\n  Error\n       2\n    Weighted   1.5\n\n\n\n       1\n\n\n\n       0.5\n              0         5         10        15        20        25        30        35\n\n                                    Time Elapsed (Hours)\n\nFigure 10  | Weighted validation and test errors for best probe architecture over the course of the\nAlphaEvolve run. The process successfully closed approximately 50% of the test error gap between\nthe attention probe baseline and perfect probe performance.\n\n\nF.3. Training Curve\n\nFigure 10 shows the training curve for the AlphaEvolve run.\n\n\nG. Prompt Optimization Experiments\n\nIn addition to our original cyber misuse detection prompt provided by colleagues, we manually create\neight additional prompts with varying amount of detail and instruction. For some of these prompts,\nwe use mild manual prompt optimization by identifying examples where existing classifiers make\nmistakes and adding instructions that aim to correct these mistakes, while we greatly simplify some\nof the other prompts. We use the test sets of the short context overtriggering and short context attack\ndatasets for evaluating the prompts. Our main takeaway from this section is that we have evidence\nthat this original provided prompt is reasonably optimized.\n\nThe nine total prompts form an FPR vs. FNR curve for each model, shown on the left side of Fig. 11.\nWhile different prompts can indeed shift along the curve, no prompts are better than the original\nprompt on both FPR and FNR. Furthermore, the curve is approximately linear, so e.g. different\nprompts seem to be able to shift a constant number of negatives for positives. It therefore seems that\ndifferent prompts do not significantly improve upon each other, so we stick with our original prompts.\n\n\nH. Automated Red Teaming Analysis\n\nWe developed an iterative adaptive attack framework where an attacker model is given ğ‘‡turns to\njailbreak the target model. At each turn, the attacker receives its historyâ€”including whether previous\n\n\n\n                                                                                                        33\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n\n\n                 2                                                                                                      gemini-2.5-flash\n                     4  1\n                                                                                                  gemini-2.5-pro\n      0.4             5                                                                                      gemini-2.5-flash-lite\n\n\n                    4 Rate                5\n\n                                            6      0.3                           2        8                          1                                          7\n                                   54                     3   8  Negative                            2\n False   0.2                    O              8                                         67\n                                         7 6                                            1\n                                                                    3\n                                                                                        O\n      0.1                                                                    3\n                                                           O\n\n\n          0.05        0.1        0.15        0.2        0.25        0.3        0.35\n\n                                    False Positive Rate\n\nFigure 11  | Different prompts trade off on FPRSC[OT] and FNRSC[A](results are shown on the training\nset of these datasets). The prompts that we try are labeled by the order we created them; the original\nprompt is in orange. Prompts that lie along the pareto frontier are solid.\n\n\nattempts (1) bypassed the monitor and (2) jailbroke the target model (verified by an autorater) â€“\nallowing it to learn from failures. We evaluated this system on 100 random queries from the short\ncontext attacks test set, running ğ‘= 20 seeds per query.\n\nIn initial experiments with ğ‘‡= 10, we found that adaptive turns did not outperform independent\nrepetitions of the first turn. Formally, consider a prompt ğ‘†with ğ‘seeds. Let ğ‘€be the number of\nsuccessful jailbreaks observed in the first turn. The expected success probability (at least one jailbreak)\nif we repeated the first step independently ğ‘‡times is Pindep(ğ‘†, ğ‘‡) = 1 âˆ’(1 âˆ’ğ‘€ğ‘)ğ‘‡. We compare this to\nthe observed success rate of the adaptive procedure on the same seeds, Padapt(ğ‘†, ğ‘‡). Across all 100\nprompts ğ‘†ğ‘–, we found that Ã ğ‘–Pindep(ğ‘†ğ‘–, ğ‘‡) > Ã ğ‘–Padapt(ğ‘†ğ‘–, ğ‘‡) for all ğ‘‡> 1. Repeating the first iteration\nwas superior to the full iterative procedure (see Figure 12). We note this does not contradict Chao,\nRobey, et al., 2025, as they do not compare to this baseline. Consequently, for the main results, we\nsimplified our setup to ğ‘‡= 1.\n\nWe subsequently applied AlphaEvolve to optimize our attacker prompts (not released), improving the\nsingle-turn jailbreak success rate from 20% to 26%. We utilize this improved prompt for our main\nresults. However, even with these optimized prompts, the inequality Ã ğ‘–Pindep(ğ‘†ğ‘–, ğ‘‡) > Ã ğ‘–Padapt(ğ‘†ğ‘–, ğ‘‡)\nheld for all ğ‘‡> 1.\n\nFinally, we tested a \"cross-pollination\" variant where the attacker received ğ‘˜= 3 random successful\nand failed attacks from other seeds at prior steps. While this intervention achieved Ã ğ‘–Pindep(ğ‘†ğ‘–, ğ‘‡) <\nÃ ğ‘–Padapt(ğ‘†ğ‘–, ğ‘‡), the margin was negligible. Given the significant complexity and computational cost\nincrease, we leave further evaluation of such systems to future work.\n\n\nH.1. Definition of â€œSuccessâ€ in Adaptive Red Teaming\n\nA natural concern with our ART dataset construction is that the narrative rewriting process might\ninadvertently transform a malicious prompt into a benign one. If this happened, â€œsuccessfulâ€ jailbreaks\n\n\n\n                                                                                                        34\n\n                                           Building Production-Ready Probes For Gemini\n\n\n\n\n\n                               Old Prompt                             New Prompt\n\n                                                                                                                                                             Individual prompt\n\n            10                                                                                                                   Standard error\n\n                                                                                                                             Average across             Turn)                                                                                                                        prompts\n             5\n              Jailbreaks Single\n #\n   in       0\n                    Repeated\n\n   -\n            âˆ’5              Difference          (ART\n\n\n           âˆ’10\n\n\n               0         2         4         6         8         0         2         4         6         8\n\n                                     Iteration (t)                                              Iteration (t)\n\n\nFigure 12  | Multiple adaptive red teaming steps do not help jailbreak success compared to simply\nrepeating the first step.\n\n\nwould simply reflect easier queries rather than genuine vulnerabilities.\n\nWe guard against this by defining â€œsuccessâ€ using an autorater that evaluates the modelâ€™s response, not\njust whether the monitor was bypassed. Specifically, an attack is considered successful if and only if:\n\n   1. The attack bypasses the monitor (i.e., is not flagged), and\n   2. An autorater judges the modelâ€™s response to be harmful.\n\n\nThe autorater compares the modelâ€™s response to a baseline response from â€œadversarial Geminiâ€ (Gemini\nwithout safety training, sometimes called a â€œhelpful-onlyâ€ variant of the model. This model is not\npublicly released) given the original, un-rewritten harmful query. This ensures that both responses\nare evaluated against the same underlying malicious intent. An attack is only counted as successful if\nthe response to the rewritten prompt is at least as harmful as the baseline.\n\nThis design holds query difficulty constant: if the rewriting process neutralizes the harmful intent,\nthe model will produce a benign response that fails the autorater check, and the attack will not\nbe counted as successful. Our reported success rates (e.g., improvement from 20% to 26% via\nAlphaEvolve optimization) therefore reflect genuine jailbreak rates on queries of consistent difficulty.\n\n\nI. Infrastructure Recommendations\n\nWe recommend storing activations on disk when training single-layer probes. We find that this makes\niterating on probe architecture (e.g. manually or with AlphaEvolve) much faster. This advice contrasts\nwith Cunningham, Wei, et al. (2026), who recommend generating activations from the base LLM\nduring probe training. We believe that this difference is due to our probe training using activations\nfrom a single layer (so the storage cost and loading time is only proportional to the dataset size and\nthe activation dimension), while Cunningham, Wei, et al. (2026) train multi-layer probes, leading to\na storage cost and loading time that is additionally proportional to the number of layers in the model.\n\nWe use TPU machines to dump training, validation, and test activations to disk (training, val, and\n\n\n\n                                                                                                        35\n\n                                           Building Production-Ready Probes For Gemini\n\n\ntest), and then use CPUs for evaluating probes on these activations. We also are able to use small\nsingle-host TPU setups for training the probes (instead of much larger deployments that can host an\nentire production Gemini model).\n\nAnother infrastructure consideration is performing evaluations on long-context data. Our long-context\nevaluation dataset does not fit on one TPU node. We thus run long context evaluators distributed on\nmany CPU workers, all of which load a random split of the data themselves. This is similar to typical\nsingle program multiple data setups (Darema et al., 1988).\n\nSpecific Considerations For Long Context Training: To train probes on long context data, it is\nimpossible to load all training data at once to the TPU memory due to the prohibitively large data size\nof training activations. Therefore, for each epoch, we need to distribute both short context and long\ncontext data across data loading batches, and further divide into smaller training or evaluation batches.\nTraining attention probes with long context data thus requires smaller batch sizes to more efficiently\nleverage data distributions and converge under smaller number of epochs. Moreover, we implemented\na round-robin data allocation policy of data loading batches between training datasets, to prevent\ntraining dataset distribution shifts within an epoch. Loading large scale activations is expensive with\nstandard training pipelines, so we implemented a customized pipeline with optimizations for data\nloading speed, padding, and pipelining. These optimizations unlocked stable long context data probe\ntraining and improved accelerator utilization by about 5x.\n\n\nJ. Error Bar Methodology for Figure 1\n\nThe error bars in Figure 1 represent 95% confidence intervals. We use fundamentally different\nuncertainty quantification methods for probes versus language models because they face different\nsources of variance:\n\n\n  â€¢ Probes: We train 100 random seeds and select the best by validation loss. The dominant source\n      of uncertainty is seed selection variance: a different draw of 100 seeds would yield a different\n      â€œbestâ€ model with different test performance.\n  â€¢ Language Models: We run a single evaluation with temperature zero, so there is no seed or\n     sampling variance. The only uncertainty is finite test set noise: our test set provides a noisy\n      estimate of the true error rate.\n\n\nThese fundamentally different uncertainty sources require different statistical methods.\n\n\nJ.1. Probe Methods (AlphaEvolve, Selected Probe, EMA Linear Probe)\n\nFor probes, we use a smoothed bootstrap via kernel density estimation (KDE) in logit space to quantify\nseed selection uncertainty. Standard bootstrap of the argmin statistic produces degenerate confidence\nintervals because the same seed tends to be selected repeatedly. KDE smoothing addresses this by\nallowing the resampled seeds to have slightly different (val, test) pairs than the original data.\n\nProcedure:\n\n   1. Given ğ‘trained seeds with validation losses {ğ‘£1, . . . , ğ‘£ğ‘} and corresponding test losses {ğ‘¡1, . . . , ğ‘¡ğ‘}\n   2. Transform test losses to logit space: â„“ğ‘–= log(ğ‘¡ğ‘–/(1 âˆ’ğ‘¡ğ‘–)) to respect the bounded [0, 1] domain\n   3. Fit a 2D kernel density estimate to the joint distribution of (ğ‘£ğ‘–, â„“ğ‘–) pairs\n   4. Filter out degenerate seeds where ğ‘£ğ‘–equals the maximum penalty value (indicating failed\n      training runs)\n\n\n\n                                                                                                        36\n\n                                           Building Production-Ready Probes For Gemini\n\n\n   5. For ğµ= 20,000 bootstrap iterations:\n       (a) Sample ğ‘synthetic (ğ‘£âˆ—, â„“âˆ—) pairs from the KDE\n      (b) Select ğ‘—âˆ—= arg minğ‘˜ğ‘£âˆ—ğ‘˜(best validation seed in synthetic sample)\n        (c) Transform back: ğ‘¡âˆ—ğ‘—âˆ—= ğœ(â„“âˆ—ğ‘—âˆ—) where ğœis the sigmoid function\n      (d) Record ğ‘¡âˆ—ğ‘—âˆ—(test loss of selected seed)\n   6. Report the 2.5th and 97.5th percentiles of the ğµrecorded test losses\n\n\nThe KDE bandwidth is computed automatically using Scottâ€™s rule. This procedure captures how much\nthe final reported test loss would vary if we repeated the entire training and selection process.\n\nApparent One-Sided Error Bars: Some probes exhibit error bars that appear one-sided, with a large\ndownward extent but minimal upward extent. This occurs when the actual point estimate falls near the\nupper tail of the bootstrap distribution. Intuitively, this means the selected seed happened to perform\nrelatively poorly on the test set compared to what would typically be expected. Equivalently, the\nselected seed landed near the 95th percentile of the bootstrap distribution rather than the median. In\nsuch cases, the confidence interval correctly indicates substantial room for improvement (downward)\nbut limited room for the result to be worse (upward).\n\n\nJ.2. Language Model Methods (Gemini 2.5 Flash)\n\nFor the language model classifier, we use an analytic binomial confidence interval. Since we run only\none deterministic evaluation (temperature zero), there is no seed variance to bootstrap over. Instead,\nwe quantify uncertainty from the finite test set size.\n\nOur error metric is a weighted combination of error rates across different test groups (see Equa-\ntion (12)). For each group ğ‘”with weight ğ‘¤ğ‘”, we compute:\n\n  â€¢ The empirical error rate Ë†ğ‘ğ‘”on ğ‘›ğ‘”test examples\n                                                           Ë†ğ‘ğ‘”(1âˆ’Ë†ğ‘ğ‘”)\n  â€¢ The binomial variance: Var(Ë†ğ‘ğ‘”) =     ğ‘›ğ‘”\n\nThe overall weighted error has variance:\n\n                                             ğ‘¤ğ‘”    2   Ë†ğ‘ğ‘”(1 âˆ’Ë†ğ‘ğ‘”)\n                       Var(weighted error) = âˆ‘ï¸                    Â·                             (14)\n                                                                  ğ‘”  Ã ğ‘”â€² ğ‘¤ğ‘”â€²         ğ‘›ğ‘”\n\n                        âˆš\nWe report the 95% CI as mean Â± 1.96 Â·  Var, clipped to be non-negative.\n\n\nJ.3. Cascading Methods (Best Probe + Gemini 2.5 Flash)\n\nFor the cascading classifier, we propagate uncertainties from both components. Let  ğ‘“defer be the\ndeferral rate (fraction of examples sent to the LLM). The cascade error is approximately:\n\n                 Cascade Error â‰ˆ(1 âˆ’ğ‘“defer) Â· Probe Error + ğ‘“defer Â· LLM Error              (15)\n\n\nWe propagate asymmetric uncertainties using the bootstrap CI widths from each component. Specifi-\ncally, for the probe we use the distance from the bootstrap median to the 2.5th and 97.5th percentiles,\nand for the LLM we use the symmetric analytic margin. Combining via quadrature (assuming\n\n\n\n\n\n                                                                                                        37\n\n                                           Building Production-Ready Probes For Gemini\n\n\nindependence):\n\n                                   âˆšï¸ƒ                                 ğ›¿lo =  ((1 âˆ’ğ‘“defer) Â· ğ›¿probelo   )2 + ( ğ‘“defer Â· ğ›¿LLMlo  )2                     (16)\n                                   âˆšï¸ƒ                                ğ›¿hi =  ((1 âˆ’ğ‘“defer) Â· ğ›¿probehi   )2 + ( ğ‘“defer Â· ğ›¿LLMhi  )2                     (17)\n\nwhere ğ›¿probe are the lower and upper CI half-widths from the bootstrap distribution (not from the\n            lo/hi\npoint estimate), and ğ›¿LLM are the symmetric analytic margins. Since deferral rates are typically small\n                               lo/hi\n(âˆ¼8%), the cascade uncertainty is dominated by the probe component, inheriting its asymmetric\ncharacter.\n\n\nJ.4. Interpretation\n\nThe resulting confidence intervals have different interpretations:\n\n   â€¢ Probe CIs answer: â€œIf we trained a different set of 100 seeds and picked the best, how much\n     would our test error vary?â€\n   â€¢ LLM CIs answer: â€œGiven our finite test set, how precisely do we know the true error rate?â€\n   â€¢ Cascade CIs answer: â€œWhat is the combined uncertainty from both probe seed selection and\n    LLM test set noise?â€\n\n\nProbe CIs are typically wider than LLM CIs because seed selection introduces more variance than test\nset sampling given our test set sizes.\n\n\nK. Discusssion and results from weighting Equation (12) differently\n\nIn earlier iteration in this paper, we used a weighting of 1 rather than 5 in Equation (12). This\nappeared to lead to lower false positives, but extremely high false negative rates, and so we changed\nit once to the metric used in the main paper (we also sometimes measured a metric with hard cutoffs\nfor false positive rates, but found this too confusing due to discontinuities). However, for completeness\nand transparency we include the results for the 1:2:50 weighting in Table 9.\n\n\nL. Attention Probe Inference\n\nOne might think that attention probes have quadratic computational costs in the sequence length,\nsimilar to standard transformers. However, this is not the case: we present an attention probe\ninference algorithm that scales as ğ‘‚(ğ‘›) in the sequence length ğ‘›(Section 2), at least for attention\nprobes with a constant query vector ğ’’, as in Equation (8) (but note that this argument doesnâ€™t hold\nfor attention probes with multiple layers of attention pooling).\n\nThe key insight is that we can derive a recursive update rule that allows attention probes to be updated\nincrementally during generation without storing a KV cache or recomputing attention scores over\nprevious tokens.\n\nUsing the notation from Section 3.1.4, let the MLP-transformed activation at step ğ‘›be ğ’šğ‘›. For a single\nhead with query vector ğ’’and value vector ğ’—, the unnormalized attention score is ğ‘ ğ‘›= ğ’’âŠ¤ğ’šğ‘›and the\nvalue is ğ‘£ğ‘›= ğ’—âŠ¤ğ’šğ‘›. The probe output after ğ‘›tokens, denoted ğ´ğ‘›, is the softmax-weighted average:\n\n\n\n\n\n                                                                                                        38\n\n                                           Building Production-Ready Probes For Gemini\n\n\nTable 9  | Results with 1:2:50 weighting in Equation (12). Format matches Table 3.\n\n\n                             FPR â†“                         FNR â†“\n Classifier           SC[OT] SC[HN] LC[RT] MT[HN]  SC[A]   LC[A]   MT[A]   SC[J]  SC[ART]  Test Error â†“\n Rolling  Attn  Probe  0.11%  4.03%  0.37%  0.00%  17.63%  14.72%  41.31%  6.20%  69.84%    0.87%\n (20H), Rolling Agg\n Rolling  Attn  Probe, 0.21%  2.01%  0.37%   1.64%  16.71%  9.96%  39.38%  7.67%  69.61%    0.88%\n Rolling Agg\n Vanilla Attn         0.08%  3.36%  0.00%  0.00%  19.49%  92.21%  59.85%  1.65%  47.56%    0.94%\n Gemini 2.5 Pro       0.20%  6.71%  0.25%  13.11%  16.01%  13.42%  20.85% 10.64%  33.87%    0.94%\n Rolling  Attn  Probe, 0.04%  6.04%  0.49%   6.56%  20.65%  4.76%  35.14%  8.02%  53.13%    0.95%\n MultiMax Agg\n AlphaEvolve         0.05%  2.01%  0.25%  0.00%  23.67%  30.30%  45.17% 47.28%  60.09%    0.96%\n AlphaEvolve   (Early  0.07%  4.70%  0.37%   1.64%  19.95%  8.23%  29.34% 51.51%  59.40%    0.96%\n Variant)\n Gemini 2.5 Flash     0.25%  14.09%  0.74%  19.67%  8.58%   6.06%  13.51%  8.74%  20.65%    1.32%\n Linear Probe Mean    0.06%  2.68%  0.00%  3.28%  29.93% 100.00% 93.44% 89.06%  83.06%    1.63%\n Gemini 2.5 Flash Lite  0.75%  25.50%  0.25%  14.75%  11.37%  55.41%  27.41% 10.64%  41.76%    1.78%\n MultiMax    Trained, 0.18%  2.68%  3.33%   3.28%  16.71%  3.90%  37.45% 22.03%  71.93%    2.34%\n MultiMax Agg\n MultiMax      (Attn  0.13%  7.38%  3.82%  24.59%  24.13%  2.16%  16.22%  9.28%  41.53%    2.82%\n Trained)\n Linear Probe Rolling  0.35%  12.08%  4.19%  67.21%  18.10%  4.76%  13.13%  6.76%  26.91%    3.90%\n Exp\n Rolling  Attn  Probe  0.21%  5.37%  19.33%  24.59%  21.58%  0.00%  23.94%  6.34%  25.06%   10.07%\n (20H), MultiMax Agg\n MultiMax    Trained  0.14%  4.70%  23.40%  1.64%  18.33%  0.87%  36.29% 57.39%  81.21%   11.96%\n (20H), MultiMax Agg\n\n\n                                   Ãğ‘›ğ‘—=1 ğ‘’ğ‘ ğ‘—ğ‘£ğ‘—\n                             ğ´ğ‘›=                                           (18)\n                                    Ãğ‘›ğ‘—=1 ğ‘’ğ‘ ğ‘—\n\nRecursive Update. Let ğ‘ğ‘›= Ãğ‘›ğ‘—=1 ğ‘’ğ‘ ğ‘—be the denominator. When a new token ğ‘›+ 1 arrives, we update\nthe denominator: ğ‘ğ‘›+1 = ğ‘ğ‘›+ ğ‘’ğ‘ ğ‘›+1. We can express the new weighted average ğ´ğ‘›+1 in terms of the\nprevious state ğ´ğ‘›:\n\n\n\n                                                  ğ‘›+1                                  1\n                                 ğ´ğ‘›+1 =    âˆ‘ï¸                                        (19)                                         ğ‘ğ‘›+1 Â©Â­     ğ‘’ğ‘ ğ‘—ğ‘£ğ‘—ÂªÂ®                                                             ğ‘—=1\n                                  1  Â«       Â¬\n                           =     (ğ‘ğ‘›ğ´ğ‘›+ ğ‘’ğ‘ ğ‘›+1ğ‘£ğ‘›+1)                            (20)\n                                         ğ‘ğ‘›+1\n                                            ğ‘ğ‘›               ğ‘’ğ‘ ğ‘›+1\n                           = ğ´ğ‘›     + ğ‘£ğ‘›+1                                 (21)\n                                            ğ‘ğ‘›+1          ğ‘ğ‘›+1\n\nLet\nğ‘ğ‘’ğ‘¡ğ‘ğ‘›+1 = ğ‘’ğ‘ ğ‘›+1/ğ‘ğ‘›+1 be the effective attention weight of the new token. Note that ğ‘ğ‘›/ğ‘ğ‘›+1 = 1 âˆ’\nğ‘ğ‘’ğ‘¡ğ‘ğ‘›+1. The update simplifies to a standard exponential moving average form with a variable decay\nrate:\n\n\n                                   ğ´ğ‘›+1 = ğ´ğ‘›+ ğ›½ğ‘›+1(ğ‘£ğ‘›+1 âˆ’ğ´ğ‘›)                            (22)\n\n\n                                                                                                        39\n\n                                           Building Production-Ready Probes For Gemini\n\n\nNumerical Stability. To prevent overflow of ğ‘ğ‘›, we track the log-sum-exp, â„“ğ‘›= log ğ‘ğ‘›. The update for\nthe log-normalizer is:\n                            â„“ğ‘›+1 = log(ğ‘’â„“ğ‘›+ ğ‘’ğ‘ ğ‘›+1) = ğ‘š+ log(ğ‘’â„“ğ‘›âˆ’ğ‘š+ ğ‘’ğ‘ ğ‘›+1âˆ’ğ‘š)                    (23)\nwhere ğ‘š= max(â„“ğ‘›, ğ‘ ğ‘›+1). The weight ğ›½ğ‘›+1 is then computed stably in log-space:\n\n                                      ğ›½ğ‘›+1 = exp(ğ‘ ğ‘›+1 âˆ’â„“ğ‘›+1)                               (24)\n\nConclusion. To run an attention probe during generation, we only need to maintain the current scalar\naverage ğ´ğ‘›and the scalar log-normalizer â„“ğ‘›per head. Upon generating token ğ‘›+ 1, we compute ğ‘ ğ‘›+1\nand ğ‘£ğ‘›+1, update â„“ğ‘›+1 and ğ´ğ‘›+1 using the equations above, and discard the token activations. This\ngives ğ‘‚(ğ‘›) total compute and ğ‘‚(1) memory for a sequence of length ğ‘›.\n\nIt is clear how this approach can be generalized to many heads, and architectures such as the Max of\nRolling Means Attention Probe (Section 3.2.2) can also have ğ‘‚(ğ‘›) efficiency by storing all the window\nscores (in our case the window is size 10, so small), and taking the max of the current rolling sum\nand the prior max.\n\n\n\n\n\n                                                                                                        40\n\n## Claim to Classify\n\"activation probes may be a promising misuse mitigation technique\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Linear probing",
    "method_rung": 1,
    "claim_rung": 1,
    "confidence": 5,
    "hedge_flag": 1,
    "reasoning": "The claim 'activation probes may be a promising misuse mitigation technique' is from the abstract. The paper's primary method is linear probing (and variants like attention probes) on frozen activations to classify inputs, which is a Rung 1 observational method (establishes correlational evidence only). The claim uses linguistic markers 'may be a promising... technique' which is a Rung 1 associational claim about utility/potential, not a causal or mechanistic claim. The hedge 'may' indicates uncertainty, so hedge_flag = 1. Confidence is high because the method and claim are clearly aligned with Rung 1."
  },
  "metadata": {
    "model": "deepseek/deepseek-v3.2",
    "usage": {
      "prompt_tokens": 37527,
      "completion_tokens": 180,
      "total_tokens": 37707
    },
    "finish_reason": "stop"
  }
}