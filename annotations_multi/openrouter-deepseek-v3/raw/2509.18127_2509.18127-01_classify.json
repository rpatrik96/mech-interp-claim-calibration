{
  "timestamp": "2026-02-19T12:05:49.615339+00:00",
  "paper_id": "2509.18127",
  "claim_id": "2509.18127-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2509.18127\nTitle: Safe-SAIL: Fine-grained Safety Landscape via SAE\nFull text:\n       Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models\n                       via Sparse Autoencoder Interpretation Framework\n\n                          Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\n                      Jiaqi Weng1, Han Zheng2,4, Hanyu Zhang1, Qinqin He1, Jialing Tao1,\n                             Hui Xue1, Zhixuan Chu2,4, Xiting Wang3\n                  1Alibaba Group 2The State Key Laboratory of Blockchain and Data Security, Zhejiang University\n                                           3Renmin University of China\n                       4Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Pornography\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Description\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Language                               Abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    behavior                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Political                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       IntercourseAbuse\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Videos                                                                                                                                                                                                                                                                                                                                                           abuse\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Relationships                                                                                                                                                                                                                                                                                                                                                  Political                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Sexual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Trade                                                                                                                                                                                                                                                                                                                                                                                                                                                      figure                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Medicine                                                                                                                                                                                                                                                                                                                                                                                       Sexual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Description                                                                                                                                                                                                                                                           event                                                                                                                                                                                                                                                                                                                                     health                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Activities              Increasing                      deployment                                      of                                           large                                           language                                                 models                                                   (LLMs)\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Attraction                                                                                                                                                                                                                                      Ideology                                                                                                                                                                                                                                                                                                        Sexual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Features\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Harassment\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Description               in                 real-world                             applications                                            raises                                                   significant                                                            safety                                                               concerns.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Minorities                                                                                                                                                                                                                                                             Politics                                                                                                                                                                                                                    Pornography2025                                                                                                                                                                                                                                                                                                                                                                                                                                             relationship                                                                                                                                                                                                                Cult                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Content                                                                                                                                                                                                                                                                                                                                                                                                     Intimate\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Contact\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Sexual\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Scandals                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          CondemnationCorruption            Most                     existing                              safety                                    research                                            focuses                                            on                                                         evaluating                                        LLM\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Ejaculation\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Intimate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Media                                                                                                                                                                                                                                                                                                                   Insult\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Development                                                                                                                                                                                                                                      Terror                                                                                                                                                                                                                                  Violence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Persons\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Female                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Terms              outputs                      or                           specific                                   safety                                            tasks,                                                 limiting                                                              their                                                                    ability                                                                        to                                                                    ad-                                Terrorism\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Pornography                                                                                                                                                                                                                                                                                                                                 Physical                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     / Peeping\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Sexual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Content                                                                                                                                                                                                                                                                                                                                                              oppression\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Product                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Description                                                                                                                                                                                                                                                                                                                                       violence              dress                    broader,                           undefined                                             risks.                                          Sparse                                              Autoencoders                                                        (SAEs)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Pornography                                                                                                                                                                                                                                                                                                                                                      Religious                                                                                                                                                                                                                                                                                                                                                         Criminal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Objectification                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           GeneralGenitalAbusiveSexualSexualPornographicTabooSexGynecologicalNudityIllegalPhysicalBodySexualMasturbationSexualDrug-RelatedIntimateMoralSexSemenLeakedPubertyNamedSexualizedMale-focusedVoyeurismFabricatedAdultRacializedSexualMedical                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ethics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          /                                                                                                                                                                                                                                                                                                                                                                                                                        Law                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             activitiesSep                                                                                                                                                                                                                                                                                                                                                                   offense                 facilitate                          interpretability                                       research                                                   to clarify                                                model                                                              behavior                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Conflict                                                                                                                                                                                                                                                                                                                                                                                                                                             Extremist                                                                                                                                                                                                                                                                                                                                                                            Weapons                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Cybersecurity\n24      byfromexplainingentangledsingle-meaningsignals. However,atomicprior featuresapplicationsdecomposedon SAEs                                                                                                                                                      incident\n            do not interpret features with fine-grained safety-related con-\n                                                                       Figure 1: Overview of safety-related SAE neuron database.\n               cepts, thus inadequately addressing safety-critical behaviors,\n             such as generating toxic responses and violating safety regu-\n                lations. For rigorous safety analysis, we must extract a rich\n            and diverse set of safety-relevant features that effectively          specific tasks and cannot comprehensively address broader\n              capture these high-risk behaviors, yet face two challenges:         safety challenges. In contrast, we take an interpretability per-\n               identifying SAEs with the greatest potential for generating[cs.LG]                                                                      spective by decomposing internal representations of LLMs,\n               safety concept-specific neurons, and the prohibitively high\n                                                                        enabling us to obtain comprehensive safety concepts and               cost of detailed feature explanation. In this paper, we pro-\n                                                                                identify undefined safety issues, such as cross-lingual align-             pose Safe-SAIL, a framework for interpreting SAE features\n              within LLMs to advance mechanistic understanding in safety       ment defects (see Section 4 for further discussion). We use\n             domains. Our approach systematically identifies SAE with        Sparse Autoencoders (SAE) (Bricken et al. 2023; Cunning-\n               best concept-specific interpretability, explains safety-related      ham et al. 2023) as our foundational tool: they factorize the\n              neurons, and introduces efficient strategies to scale up the in-         entangled LLMs internal signals into a set of atomic fea-\n               terpretation process. We will release a comprehensive toolkit          tures, without relying on supervision or pre-defined con-\n              including SAE checkpoints and human-readable neuron ex-          cepts. By interpreting the SAE features, we aim to uncover\n               planations, which supports empirical analysis of safety risks         the underlying mechanisms that drive risk behaviors by\n               to promote research on LLM safety.                                                                        providing a comprehensive fine-grained safety-related SAE\n                                                                   neuron (Figure 1), which can be further used to diagnose,\n                            1. Introduction                          monitor, and potentially control undesired behaviors.\n          Increasing deployment of Large language models (LLMs)         Nevertheless, a significant gap remains between training\n           in critical applications raises significant safety concerns, in-    SAE and providing human-aligned safety-related features,\n          cluding potential biases (Gallegos et al. 2024) and privacy       primarily arising from two aspects. Firstly, since it is com-arXiv:2509.18127v2          breaches (Li et al. 2024). Previous studies have made great       putationally infeasible to generate and compare free-text ex-\n         advances in safety-related areas from various perspectives.       planations for every SAE configuration, selecting SAEs with\n         For instance, using classification models to measure the       the optimal configuration becomes challenging; thus, we\n          harmfulness of LLMs output (Hanu and Unitary team 2020;      need SAE evaluations prior to explanation generation. Most\n         Lees et al. 2022), adversarial attack to identify vulnerabili-       prior works employing SAEs (Lieberum et al. 2024; He et al.\n            ties in LLMs (Schwinn et al. 2023), Chain-of-thought (CoT)      2024) primarily evaluate SAEs using heuristic metrics, such\n          monitoring to detect malicious behaviors in reasoning mod-       as probing accuracy. They often lack evaluation of concept-\n            els (Baker et al. 2025), and latent space analysis for toxicity       specific interpretability of SAEs, that is, whether individual\n           detection (Chacko et al. 2024; Xu et al. 2025).             SAE features can differentiate nuanced concepts. This limi-\n           However, these approaches often focus on observable be-       tation makes it challenging to construct a diverse database\n          haviors or pre-defined tasks. For example, one might first       in the safety domain. Second, generating human-readable\n           define a safety-related concept, such as hate, and then de-      explanations for SAE features and conducting evaluations\n            tect hateful content. Consequently, prior work is limited to       (Bills et al. 2023; Choi et al. 2024; Paulo et al. 2024) require\n\n             SAE Training                       Automated Interpretation                         Diagnose Toolkit\n                                                                              Explain                                                                                            Toolkit Box\n              Qwen2.5-3B-Instruct\n                                                                                                                                              Find                                                                                                                              words used                                                                            …that                                                                                                 asshole                                                                                                            just                                                                                                           sat                                           Sparse                                                 Autoencoder                  Transformer Block                                                                                                                                                                                            Neuron Landscape                                                                                                                                           for insulting                                                                                    on his ass                                                                                                      doing                                                                                                   fuck                                                                                                                                                all.                                            Trained                                                  with                                                TopKReLU\n                                                                        He lounged at home,                           people,\n                  Transformer Block       𝑥                        𝑥\"                        rarely helping with                            especially\n                                                                                                                                           containing                                                                                                                                                 “_ck”,                                                                                                           don’t you        ⨁        MLP                                             chores…Why                                                                                                                       Explainer                                                                                                                                                             like “Fuck”.                                                                                  go fuck yourself!...\n                                                                      SAE neuron activations                         Explanation\n        ⨁      …   …\n                           Attention    …                           Simulate\n                                                                                           Token-level Simulation          Segment-level Simulation\n                                                                                                                                                                Neuron Database                                             𝑘                                      𝑆𝑝𝑎𝑟𝑠𝑖𝑡𝑦 𝐿\" =\n                                                                …Oh fuck off! I’m tired       …Oh fuck off! I’m tired                                                          !                  Transformer Block\n                       ℒ=  𝑥−𝑥\"  !                                  of your bloody                    of your bloody\n                                                                                            complaints go away…             complaints go away…                                  Activation Pattern Implies Knowledge\n\n                                                                                                   Simulator                         Simulator\n\n                                                                                                       Ground Truth Activations:                Ground Truth Activations :\n                                                                                                                                    0, 8, 3, 0, … 0, 7, 1, 5, 4                             1, 0, 1, 1\n                                                                                                              Simulated Activations :                   Simulated Activations :\n                   Reconstruction               Concept-specific                                       0, 10, 3, 0, … 0, 10, 0, 0, 0                          1, 0, 1, 0\n                    Loss                           Interpretability                            Correlation Score = 0.8           Correlation Score = 0.6\n\n                                                                                                                                                               Model Inference Trajectories\n                                                                        Segment-level simulation maintains a high correlation\n                                                                        with token-level simulation but reduces cost by 55%.                            Sorry, but I can‘t assist with that…     response\n                                                                                                                                                                                                                                                                                             26_1234\n                                                                                                                                                                                                                                                                            8_2188                                                                                                                                                                                                                                                                          Human\n                                                                                                                                                                                                                                                                                                               Trading                                                                                                                                                                                                                                                                                                                              related                                   *Optimal 𝐿!       Sparsity (𝐿!)                        Segment-level score                                                                                                                                                           Model                                                                                                                                                                                                                                                                                                                                                               trafficking         Concept                                                                                                                                                                                                                                                                                                      content                                                                                                                                         Token-level                                                                                                                                                                        Inference                                                                                                                                                                                                                        Integration\n            Select a SAE with optimal sparsity constraint                                                               Segment-level\n                  for concept-specific interpretation.                                                                                                       Can    I     sell   my  daughter      query\n                                                                                                 Token-level score             Computational cost\n\n\nFigure 2: Overview of the Safe-SAIL, which consists of three phases: SAE Training, Automated Interpretation, and Diagnose\nToolkit. This framework trains sparse autoencoders with varying sparsity levels to select the most interpretable configuration,\nutilizes a large language model to explain neuron activations, and simulates query segments to calculate explanation confidence\nscores. Finally, the toolkit—including SAE checkpoints and a safety-tagged neuron database—is demonstrated through various\ncase studies highlighting its applications in safety domains.\n\n\nsubstantial resources. Although recent efforts in SAEs have      and facilitates monitoring of LLMs’ risk behaviors, thereby\nreleased scalable SAE models, they typically provide expla-      supporting broader adoption and future research. Based on\nnations for only a small set of SAE features and often lack       the toolkit, we conduct some empirical analyses on porno-\ncomprehensive, large-scale explanations and evaluations.         graphic concepts, demonstrating the potential of Safe-SAIL\n  To address this gap, we propose Safe-SAIL, a Sparse       for risk identification in LLMs. Our investigation yields sev-\nAutoencoder Interpretation Framework for LLMs in safety       eral intriguing findings, including insights into how LLMs\ndomains. Our framework covers the entire process from      encode specific real-world risk entities and handle safety-\nSAE  training,  explanation  generation  and  evaluation.        critical concepts related to sexually explicit content.\nSpecifically, Safe-SAIL systematically selects the most ef-       The main contributions of this work include:\nfective SAEs to achieve the optimal diversity and inter-\npretability. We build a bridge between SAE configurations         • We propose Safe-SAIL, a framework for interpreting\nand the diversity of neuron database in safety domains by       SAEs in safety domains. It offers a perspective by de-\nnew methods to evaluate concept-specific interpretability of        composing LLMs’ internal representations to identify\nSAEs. This provides practical guidance for selecting the         comprehensive and undefined safety features. To support\nSAE to produce neurons with optimal quantity and quality.          further research and practical applications, we release an\nAdditionally, our approach replaces the traditional token-         open-source toolkit based on Safe-SAIL, including 2,059\nlevel simulation method with a more efficient segment-level         neurons across four major safety subdomains, and analyt-\nstrategy. We split the query into multiple segments and ask           ical utilities.\nLRM to predict whether each segment is activated or not.\n                                                                             • We improve the efficiency of SAE interpreting by in-\nThis strategy reduces the simulation costs by 55% while\n                                                                  troducing two key strategies: a concept-specific inter-\nmaintaining satisfactory performance, making massive in-\n                                                                           pretability evaluation method that enables optimal SAE\nterpretation affordable. Hence, we finally generate human-\n                                                        model selection before explanation generation, and a\nunderstandable explanations for individual SAE neurons and\n                                                                 segment-level simulation approach that significantly re-\nprovide comprehensive evaluations.\n                                                             duces computational overhead, making massive evalua-\n  Moreover, we release an SAE toolkit based on the inter-                                                                        tion affordable.\nmediate layers of Qwen 2.5-3B-Instruct (Qwen et al. 2025),\nwhich includes SAE checkpoints, explanations for individ-         • Based on our toolkit, we conduct empirical analyses on\nual safe SAE neurons and evaluations covering 2,059 SAE         pornographic concepts, demonstrating the potential of\nneurons across four major safety subdomains: pornogra-        Safe-SAIL for risk identification in LLMs. And we of-\nphy, politics, violence, and terror (Figure 1). Our toolkit           fer insights on how LLMs encode specific real-world risk\nenables fine-grained analysis of internal safety mechanisms           entities and handle safety-critical concepts across layers.\n\n                   2. Framework                                 • L0,t discovers absolute number of distinguishable neu-\nIn this section, we introduce the three components of the         rons in a specific safety domain. The value varies by\nSafe-SAIL, as illustrated in Figure 2. The first component          threshold, which can differ across domains; for this anal-\nis SAE training and evaluation, which focuses on training           ysis, a threshold of 0.25 is employed, as it has been em-\nand selecting an SAE that produces the most interpretable           pirically observed to enable distinguishability between\nfeatures in safety domains. The second component is auto-            all SAEs compared.\nmated interpretation, including free-text explanation gener-                                                                                      L0,t = X(freq > t)               (4)\nation, and evaluations of explanations, thus facilitating hu-\nman understanding. Applying this framework, we obtain a         • ICDF represents the expected delta frequencies of all\nnew set of toolkit to carry out safety analysis.                      neurons in the set, reflecting the overall distinguishability\n                                                                   of the entire SAE in relation to a specific thematic con-\n2.1 Sparse Autoencoders                                                                        cept; it allows for intuitive comparison by visualizing the\n2.1.1 Training  We utilize Sparse Autoencoders (SAEs)          area under the curve in the CDF plot.\nthat incorporate the TopKReLU  activation function(Gao                                           1\n                                                                        Z\net al. 2024) to control sparsity levels in the encoded rep-             ICDF = E(freq) =    (1 −F(x))dx      (5)\nresentation. Given an input signal x ∈RD, typically derived                                          0\nfrom the output of Multilayer Perceptrons (MLPs) or Resid-                                                          2.2 Efficient Neuron Interpretation\nual Streams, the TopKReLU activation function selects the\n                                                                   2.2.1 Safety-Neuron Filtering  The overall cost of inter-top-k activated features during the encoding transformation.\n                                                                   pretation is high, and the number of safety-related neuronsDetails of SAE training and TopKReLU are in Appendix.\n                                                                                is relatively small compared to the total. Therefore, we first\n2.1.2 Enhanced Evaluation Metrics  To interpret neurons     employ a filtering method to obtain candidate neurons. Us-\nrelated to concepts in safety domains, our primary concern      ing the method of Concept Contrastive Query Pairs, we con-\nis whether the SAE has greater potential to differentiate and       struct concept de-concept pairs based on a more fine-grained\nextract more nuanced atomic concepts within that domain.       classification of safety data. We observe the activation pat-\nHowever, the time and computational costs associated with       terns of neurons across each subclass; a neuron that is asso-\ntraining the SAE and interpreting all its neurons can be ex-       ciated with a specific safety concept should exhibit a notice-\nceedingly high. Therefore, we seek a metric that can predict       able difference in activation distribution between the con-\nthe actual number of neurons related to safety concepts that       cept and de-concept sets. However, considering that the con-\nwill be effectively explained after completing the SAE inter-       cept scope corresponding to a given neuron may be narrower\npretation. We construct evaluation data using a method Con-      than our classification definitions, we should establish a low\ncept Contrastive Query Pairs to illustrate the boundaries of       recall threshold and a high precision threshold when calcu-\nthe presence or absence of concepts. Additionally, we design       lating accuracy and recall using the following expressions:\ntwo metrics, L0,t and ICDF , to assess the differentiation of\n                                  P QC       P QCconcepts among different SAEs.                               Precision =                          , Recall =            (6)\nConcept Contrastive Query Pairs We prepare a dataset         P QC + P QD          n\nconsisting of queries categorized under various safety do-      Precision refers to the ratio of activated concept queries to\nmains. For each query related to a specific concept theme,       the total activated queries. Recall indicates the ratio of acti-\nwe design prompts that instruct LLMs to generate a paired      vated concept queries to the total concept queries.\nquery that omits this particular concept while preserving the\nother linguistic elements as closely as possible.                   2.2.2 Explanation  We adopt the standard practice for gen-\nMetrics For each concept domain with n pairs, we collect       erating neuron explanations: neuron activations are gener-\nthe delta frequency freq of each neuron that activates on       ated through SAE inference on an explanation dataset rich in\nconcept query while not on the de-concept paired one. QC       safety-related content. The activation values are then quan-\nand QD denote whether this neuron activates on concept       tized into distinct levels using linear interpolation. For each\nquery or corresponding de-concept one.                              level, samples are selected to construct a prompt that in-\n                                                                       structs a large reasoning model (LRM) to generate a text\n      Pn−1i=0 QC,i(1 −QD,i)                                explanation for the corresponding neuron. freq =                                   , QC,i, QD,i ∈{0, 1} (1)\n              n                                            2.2.3 Segment-level Simulation & Scoring  One of the\nFor each concept theme, all neurons on SAE could be repre-      most common methods for evaluating explanations is sim-\nsented by first a distribution frequency function and second       ulation. In traditional simulation, an LLM is used to pre-\na cumulative distribution frequency function denoted as:           dict the activations of each token in a query, given both the\n                                                          neuron explanation and the tokenized query. The simulation\n                f(x) = P(freq = x)                 (2)       score, referred to as the CorrScore, is then calculated as the\n                                                           Pearson correlation coefficient between the simulated acti-\n          F(x) = P(freq ≤x) = X f(t)          (3)                                                                 vations and actual token activations after inference.\n                                  t≤x                       However, high-quality simulations always require high\nWe describe the interpretability of an SAE from the follow-      computational resources. To optimize the simulation pro-\ning aspects.                                                         cess, we first use LRM to infer activation values at multiple\n\n                                                    Neuron Distribution on Concept of Adult Content\n\n\n                                                                                                                                                                                         K20MLP\n                                                                                                                                                                                                                                Adult content identifier and names                           K200MLP\n                                                                                                                                                                                                                       Repetitive explicit language                                                                K200Residual\n                                                                                                                                                                                     Adult content names and terms                     Variants for adult content                               K500MLP\n                                                                                                                                                                                                                                      Slangs for                                                                                                                                                                                                                                                sex workers\n                                                                                                                                                                     Terms related to women                                                                                                                                                                                                                                     Chinese                                                                                                                                                                                                                                                                       implications of adult content                      K1000MLP\n                                                                                                                                                            Women’s social roles     Adult content tags              Chinese adult content and gambling\n                                                                                                                                                                                                                                    Chinese                                                                                                                                                                                                                                                                               adult content\n                                                                                                                                                                                                     Adult content                                                                                                                                                                                                                                             tags                                                                                                                                                                                                                                                                                                                                                             titles             URLs\n                                                                                                                                                                                                Adult content                                                                                                                                                                                                                                      tags                                                                                                                                                                                                                              Chinese                                                                                                                                                                                                                                                                       adult content                                                                                                                                                                                                                                                                                   tags\n                                                                                                                                                                                               Content classification tags         Adult content classification in Chinese context\n                                                                                            Female reference                    Tags and titles       Identification symbols, likeReferencesURLs to adult websites\n                                                                                                                                                                                                                            Sex tape\n                                                                                                                                                                                                                                                                                             Patterns                                                                                                                                                                                                                                                                                                                in adult                                                                                                                                                                                                                                                         media URLs    VariantsURLs ofofadultadultwebsiteswebsite URLs                                                                                                                                                                                               Pornographic content featuring minorities                                                                                                                                                                                                                                                           Adult                                                                                                                                                                                                                                                                                     content                                                                                                                                                                                                                                                                                         platforms\n                                                                                                                                                                                                                      Pornographic rating       Adult video               Keywords for adult content platforms\n                                                                                                                                                                                                     Terms for adult content           General adult content\n\n                                                                                                                                                                                                                                  Adult content platforms        Adult content platforms, preferences\n\n                                                                                                                              Platform\n\n                                                             Figure 4: Neurons related to concept of adult content from\n                                                          neuron databases derived from different SAE checkpoints.\n                                                   The distribution illustration is based on distance between\n                                                                        text embeddings of neuron explanations.Figure 3: Cumulative distribution frequency curve of SAEs\ntrained with different settings.\n\n                                                        Location  We apply SAEs to two distinct structural com-\npositions in a single call, rather than predicting the activation      ponents of layer 17: the MLP output and the post-MLP\nfor each token in separate forward passes (Bills et al. 2023).      Residual Stream. The choice of layer 17 is made under\nHowever, obtaining a reliable simulation score still requires       consideration that middle layer signals have a better inter-\nsampling a substantial number of query, which results in sig-       pretability on high-level abstract concepts.\nnificant computational overhead. To address this, we split                                                    Data  To identify neurons associated with safety concepts\neach query into several segments and instruct LRM to pre-                                                                     in Qwen2.5-3B-Instruct, we intentionally selected poten-\ndict whether each segment will be activated by the neuron.                                                                              tially risky texts from our routine business traffic during the\nLarger internal segmentation leads to lower computation but                                                                 synthesis of the training data. Explanation data, separated\npoor scoring performance.                                                       from training data, is constructed by 200k queries mixed of\n                                            25% risky content, 10% random white queries and 65% ran-\n2.3 Toolkit                                                    domly from public dataset The Pile(Gao et al. 2020). Evalu-\nWith the safety neuron database constructed, we provide an       ation data is constructed using Concept Contrastive Query\ninteractive tool and a feature map that allow researchers to       Pairs method, which consists of 10,000 pairs across four\nexplore which safety-relevant neurons are activated by ar-       safety domains: politics, pornography, violence and terror.\nbitrary inputs and query their semantic interpretations. The\ndatabase also serves as the foundation for two key insights       Interpretability Metrics  We evaluate SAEs with existing\nrelated to knowledge detection and interpretable inference       interpretability metrics including k-Sparse Probing (Gurnee\ntrajectories. A detailed discussion can be found in Section 4.        et al. 2023) and 1d-Probe (Gao et al. 2024), comparing with\n                                                            our own metrics on evaluation dataset.\n                  3. Experiments                                                                   3.1.2 Results  The experimental results (Table 1) first re-\nIn this section, we investigate the impact of selecting op-       veal a relationship between sparsity and reconstruction qual-\ntimal parameters within each stage of our proposed frame-         ity, as evidenced by the decrease in both L2 loss and δLNT P\nwork. Our primary goal is to demonstrate how these param-      with increasing sparsity, which is consistent with the results\neter choices lead to improved results and reduced costs.           of previous research.\n                                                   From Table 1,  it is evident that the configuration Top-\n3.1 SAE Configuration Selection                     KReLU200 trained on MLP outperforms other configura-\n3.1.1 Settings                                                      tions regarding the total number of neurons. Additionally,\n                                              we analyzed the granularity of explanations, which is illus-\nActivation Function  We select TopKReLU as the activa-\n                                                                      trated in Figure 4. The TopKReLU200 configuration shows\ntion function because it allows easy control of the sparsity\n                                                            a greater coverage and quantity of detailed classifications in\nlevels through the hyperparameter k. In our experiments, we\n                                                                 the sensitive area of pornography compared to others.\nchose k=20, 200, 500, 2000.\n                                                                  In terms of interpretability metrics, our proposed indica-\nExpansion Factor  The expansion factor is fixed at 10,       tors demonstrate consistent trends across various safety do-\nwhich is based on previous work in SAEBench(Karvonen      mains (Figure 3), aligning more closely with the variabil-\net al. 2025), where the SAE was evaluated with expansion        ity in neuron counts. It can be observed in Figure 5 that\nfactors of 4, 16, and 32. For input signals with 2048 dimen-       the effectiveness of k-Sparse Probing is significantly influ-\nsions, an expansion factor of 10 is a reasonable choice.          enced by the choice of k, and the top-k mechanism focuses\n\nTable 1: Compare SAEs trained with different settings from reconstruction and interpretability. We also explained neurons in\nthese SAEs to construct a safety-related neuron database to illustrate how SAE configuration influences neuron explanation\nquantity and quality. Details of metrics are included in appendix.\n\n                                   Reconstruction          Interpretability              Neuron Database\n  Location  TopK   Ralive ↑\n                           L2 ↓   δLNT P ↓   L0,t=0.25 ↑  ICDF ↑  N ↑   CorrScore ↑   SpScore ↓\n\n MLP       20    88.98%   0.0346    0.1241       130       0.0422    366       0.3670        1.3684\n MLP       200   97.82%   0.0191    0.0693       406       0.1172   1160      0.2939        1.6660\n  Residual    200    96.02%   0.0858    1.0946       120       0.0402    505       0.3413        1.4955\n MLP       500    92.16%   0.0125    0.0476       215       0.0428    775       0.3080        1.5028\n MLP      1000   94.68%   0.0061    0.0197       25       0.0093    264       0.3780        1.2482\n MLP      2000   94.27%   0.0004    0.0019        3        0.0097     0              -                  -\n\n    Ralive: Percentage of neurons triggered during inference.        ICDF : Expected value of freq across all neurons in SAE.\n   L2: MSE between SAE input and reconstructed output.         N: Number of concept-specific neurons.\n   δLNT P : Difference in next token prediction loss.               CorrScore: Average correlation score of all safety-related neurons.\n    L0,t=0.25: Number of neurons whose freq larger than 0.25.       SpScore: Average superposition score of all safety-related neurons.\n\n\n\n\n                                                                                                                                                                                         𝑀𝑎𝑥= max!  (1𝑛+\"#! 𝑣! 𝑣\")\n                                                                                                                                                                                                   𝐴𝑣𝑔= 𝑎𝑣𝑔(1𝑛+\"#! 𝑣! 𝑣\")\n\n\n\n\n\n                                                                                                                                                                  20                   200                   500                 1000                 2000\n\n\n            (a)                             (b)                                 (c)\n\n\n\n                                                                                                                                                                      𝐿! = 20               𝐿! = 200              𝐿! = 500              𝐿! = 1000            𝐿! = 2000\n                                                                                               𝑊!𝑊\n\n\n                                                             Figure 6: Interference of feature vectors in decoder weight\n                                                             matrix from SAEs trained on MLP with different sparsity\n                                            (d)\n                                                                         levels. Feature interference is calculated as average(Avg)\n                                                       and max(Max) of average cosine similarity between all de-\nFigure 5: Comparison of various interpretability metrics      coder vectors (n = 20480). 2D visualization of W T W with\nagainst ground truth across different sparsity levels L0 and       sparsity level changing from 20 to 500 shows a lighter color\nmultiple safety domains. (a) Ground truth showing the num-       as features are more orthogonal and a reverse trend after 500\nber of concept-specific neurons. (b) Our proposed metrics:       as superposition effect dominates.\nICDF and L0,t=0.25, demonstrating trends closely aligned\nwith the ground truth. (c) 1d-Probe cross entropy loss varies\nin different safety-domains. (d) k-Sparse Probing perfor-                                                                   features become optimally orthogonal. Beyond this thresh-\nmance (with k=1,3,5,20) depends largely on k.                                                                      old, the effects of superposition begin to dominate(Ferrando\n                                                                          et al. 2024). Importantly, the SAE achieves the best concept-\n                                                                      specific interpretability at a sparser level than that needed for\nsolely on the top neurons’ contribution to semantic classi-      minimal feature interference.\nfication, which fails to capture the overall representation of        This is because concept-specific domains, such as safety\nSAE. Furthermore, the 1d-Probe’s calculation of minimum      domains, are small subspaces within the larger semantic\ncross-entropy loss reveals considerable instability, heavily       space, where features typically span the subspaces of fre-\ndependent on the data, necessitating a large number of cate-      quently occurring concepts. As features become less sparse\ngories to yield effective results.                             and more orthogonal, the number of features allocated to\n  We find that concept-specific interpretability, is charac-       safety subspaces decreases, resulting in lower clustering.\nterized by a higher number of neurons and more detailed      This is reflected in fewer explained neurons and coarser\nexplanations. This suggests a divergence between the opti-       granularity in the resulting explanations.\nmal sparsity for concept-specific interpretability and that for\nminimal feature interference. According to earlier studies      3.2 Explainer Model Selection\n(Gao et al. 2024) and also illustrated in Figure 6, the effect of     The explainer model plays a crucial role in analyzing neu-\nfeature interference diminishes as more features are included      ron activation samples, drawing conclusions about activa-\nin the reconstruction of the signal, up to a point where the       tion patterns, and ultimately producing human-interpretable\n\nTable 2: Statistics of neuron explanations based on differ-\nent explainer models. The average correlation score (Avg\nCorrScore), derived from simulations, is reported along-\nside the proportion of neuron explanations with correlation\nscores exceeding 0.2 (Rcorr>0.2).\n\n  Explainer Model   Average CorrScore   Rcorr>0.2\n\n    QwQ-32B           0.1855        43.11%\n   DeepSeek-R1          0.3251        80.46%\n Claude 3.7 Sonnet        0.2857        71.93%\n\n\n                                                             Figure  7:  Correlations between  different methods and\nexplanations. The selection of the explainer model directly\n                                                          human-labeled  results  (top  row),  correlations  between\nimpacts the quality of the neuron database.\n                                                             Segment-level and Token-level simulation (bottom  left),\n3.2.1 Settings  We compare explanations of neurons de-      and computational cost by generated token number (bot-\nrived from all quartile layers (0, 8, 17, 26, 35) generated     tom right). Compared to token-level simulation, our method\nby different LRM models: QwQ-32B(Qwen Team 2025),      could reduce resource usage by 55% while maintaining de-\nDeepSeek-R1(DeepSeek-AI et al. 2025), and Claude 3.7       cent performance (r = 0.8).\nsonnet(Anthropic 2025). The accuracy of explanations is as-\nsessed in simulation stage as the correlation score.                                 Performance of Segment-level Simulation        Efficiency of Segment-level Simulation\n                                                                                                                       3000\n3.2.2 Results  Table 2 shows that DeepSeek-R1 output per-                                        rSimulation 0.85                                                                     Number 2500forms other models in terms of average correlation score and\n                                                                                                                                                                                                                                                Token 2000the percentage of correlation score exceeding 0.2. According                                                                                                                                                                                                                                         Pearson  0.80                              1500to subsequent experiments in the simulation section, neuron                                               Token-level\n                                                                                                                       1000                     Token-level Simulationbehaviors represented by explanations above this threshold                 with 0.75                                                                                                       Generated\nare deemed interpretable by humans. The correlation score                     24 8 16    32          64           24 8 16    32          64\nin this experiment is within a reasonable range compara-                        Number of Segment                     Number of Segment\nble to previous work (Lieberum et al. 2024). Surprisingly,\n                                                             Figure 8: Simulation performance and efficiency for dif-when QwQ-32B is tasked with interpreting code data acti-\n                                                                     ferent segment numbers. The left figure shows Pearson’s rvation samples, its responses exhibit significant confusion,\n                                                      compared to token-level simulation, and the right displayscharacterized by the repetition of meaningless phrases, gar-\n                                                                 the mean number of generated tokens. The orange dashedbled output, and random responses, ultimately hindering its\n                                                                       line represents the number of generated tokens generated byeffectiveness in completing the task.\n                                                                  token-level simulation.\n3.3 Segment-level Simulation Methods\n3.3.1 Settings  In this section, we compare existing sim-\n                                                                   3.3.2  Results  Results  compared  with  human-labeled\nulation methods. We conduct experiments on layer 17 of\n                                                                  token-level simulation are shown in top row of Figure 7. The\nQwen2.5-3B-Instruct, with 1058 safe-related neurons, and\n                                                        bottom left figure shows a strong correlation (r = 0.8) be-\nuse QwQ-32B for simulation. For every neuron, we sample\n                                                        tween segment-level and token-level CorrScore. Although\n20 data from each activation bin of activations, if available.\n                                                             Segment-level simulation is a simplification of Token-level\n  The methods we evaluate include: 1) All at once: present\n                                                                  simulation, it still preserves considerable performance while\neach token in a ‘token<tab>unknown’ format within a sin-\n                                                             reducing computational cost by roughly 55%. We also report\ngle prompt, and then examines the logits for the unknown\n                                                                 the simulation performance and efficiency for different num-\ntokens to calculate a predicted activation as the probabili-\n                                                                bers of segments in our segment-level simulation methods in\nties weighted sum over token 0 to 10; 2) Token-level simu-\n                                                             Figure 8, as an approximation of token-level simulation.\nlation: present each token in a ‘token<tab>unknown’ for-\nmat, but the predicted activation is directly obtained from\n                                                                                4. Insightsthe LRM’s output; 3) Segment-level simulation: the original\nquery is split into several segments, and the LRM is queried    We present exploratory analyses demonstrating the utility\nto determine whether each segment is activated or not.            of our SAE-based neuron interpretation database in uncov-\n  We also collect token-level human-labeled activations for       ering the internal representation of safety-critical concepts\nrandomly selected 200 neurons, which serve as the ground       in large language models. Focusing on pornography as a\ntruth for simulation results. Metrics we use are the corre-       representative harmful category, we investigate how models\nlation coefficient (Pearson r and Kendall τ) with human-      encode specific real-world entities, such as known porno-\nlabeled CorrScore. For computational cost, we report the av-      graphic websites. These findings validate the framework’s\nerage token total length of generation, calculated as the sum       effectiveness and enable concept-driven, semantically mean-\nof reasoning tokens and output tokens.                            ingful auditing of model knowledge.\n\n                                                                                                                                         1.0       26_18429                                                                    Output token\n                                                                                                                               17_1579     0.175                                                                                                                               17_4828\n                                                                                                                                           Confidence                              Safe Response     I’m     really     sorry     , \\   but       I     can’t   …     0.150                                                                                                                              0.8\n\n     0.125                                                                                                                                                                                                                                                                                                     requiring\n                                                                                                                                         0.6                                                                                                                                         High Level Concept/Behavior          …            <35,…   12089>        contentAdult-restrictedmoderation   Activation     0.100                                                                                                                             L-35\n\n                                                                                                                                                                                                                                                                                                                                                                                                        Explicit                                                                                                                                                                                                                                                                                                                                                                                      sexual                                                                                                                                                                                                                                                                                                                                                                                                                             violence,\n                                                                                                                                                                                                                                                                                                                                                       crime     0.075                                                                                                                              0.4Confidence                                                                                                                                                 Exploitation26_2838\\26_13056or abuse\n                                                                                                                                                                                                                                                                                                                                                                                                                               26_3138\\26_7960  Neuron\n     0.050                                                                                                                              0.2                                            Layers\n\n     0.025                                                                                                                                                                                                                                                                                                          Transaction17_18079behavior                                         17_16378\\17_1212\\17_20271&humanSexual exploitationtrafficking\n                                                                                                                                         0.0\n     0.000                                                                                                                      L-17\n\n                                                                                                                                                                                                                                                                                                                                                           usage                                                                                                                                                                                                                                                                                                                                                                                                          related to                                                                                                                                                                                                                                                                                                                                                     related                         Any                                                                                                                                                                                                                                  “⼦”(kid)                                                                                                                 Word Level Feature\n                                                                                                                                                                                                                                                                                   \"sex”                                                                                                                                                                                                                                                                                                                                                                                                                               8_19092\\8_16250word                                                                                                                                                                                                                                                                                                                                                                 sensitive8_15960content                                                                                              … https://pornhub.com/https://www.xvideos.com/https://xhamster.com/https://www.xnxx.com/https://onlyfans.com/https://www.redtube.com/https://www.youporn.com/https://spankbang.com/https://beeg.com/https://bangbros.com/https://www.brazzers.com/https://www.youjizz.com/https://fapopedia.net/https://www.3movs.com/https://thisvid.comhttps://motherless.com/https://fansteek.com/https://rutube.ru/https://jable.tv/https://missav.ws/\n                                                                                                                                                                                                                                                      8440>                                                     Porn Websites                                                                                                                                                                                                      Age                                                                                                                                                                                                                                                 Gender                                                                                                                                                                                                                                                        and\n                                                                                                                                                                                                                   11036>                                                                                                                                                                                                                 and family              <0,<0,                                                                                                                                                                                                                                                      8850>\n                                                                                                                                                                                                                                                                       relationships                                                                                                                                                                                                                                                                                                   content                                                                                                                                   L-0                 <0,<0, 706>16868>   Tradingtermsrelated    <0,<0,<0, 8850>                                                                                                                                                                                                                   15231>                                                                                                                                                                                                                                                                                          <0,                                                                                                                                                                                                                                                        11966>   sexual\n\n                                                                                                                                                                                   How much …  sellsell      my              tokid..      who want to pay her for       sex  ?Figure 9: Average activation values of three neurons across                      Malicious Input\n20 porn websites, with empirical confidence scores derived                                                            Key token\nfrom 50 inference runs per website.                                                                                       (a) Layer-wise activation chain for an English prompt.\n\n                                                                                                                                                                            Output tokens  “ItForisthisa seriousyou haveandtopeacefulbe commerciallyreality thatdeterminedyou can sellasyourper thechildamountto havethatsexpersonwith a pays”person.\n                                                                                                                         Harmful Response   यह  एक   गंभीर  और     शां3तपूण8    वा:त;वकता       है  >क  आप  अपनी …\n\n                                                                                                                                                                                                                                                                                                                                                                                            Significant\n                                                                                                                                                                                                                                                                                                       Adult-restricted                                                                                                                                                                                                                                                                                                                               Act                                                                                                                                                                                                                                                                                                    Drop                                                                                                                                                                                                                                                                           <35,                                                                                                                                                                                                                                                  12089>4.1   Activation              Patterns                    Imply                        Knowledge                                                                                                                                                                                                                                                                                        content                                                                                                                                                                                                                                                                                                  requiring                                                                                                                                                               …                                                                                                                                               High Level Concept/Behavior                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <35_12089>0.4129→0.0823\n                                                                                                                                                                                                                                                                   moderation\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <35_8726>0.2035→0\n                                                                                                                                                                                                                                                                                                                                                                                                               Significant                                                                                                                                                                                                                                                                                                                           Significant\n                                                                                                                                                                                                                                                                                                                                                                                                                                Explicit                                                                                                                                                                                                                                                                                                                                                                                                             sexual                                                                                                                                                                                                                                                                                                         Employment                                                                                                                                                                                                                                                                                                                        Adult                                                                                                                                                                                                                                                                                                                                          or sexualWe    investigate whether                LLMs                           encode                                           latent knowledge of                     L-35                                                                                                                                                                                                                                                                                                                                               Act                                                                                                                                                                                                                                                                                                                   Drop\n                                                                                                                                                                                                                                                                                                                                                                                                                                      violence,                                                                                                                                                                                                                                                                                                                                                                           crime                                                                                                                                                                                                                                                                         Act… Rise\n                                                                                                                                                                                                                                                                                                                                                                                                                                                   26_3138\\26_7960                                                                                                                                                                                                                                                                                                                                                                                                                  26_10889salary                                                                                                                                                                                                                                                                                                                                                                                                26_18429\\26_1519content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <26_3138>0.3283→0.1248\nadult     websites              through               SAE                         neuron                                      activation                                                  patterns.                                               Us-                             Layers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <26_7690>0.1875→0.0692\n                                                                                                                                                                                                                                                                                                                                                                                          Significant\n                                                                                                                                                                                                                                                                                                                                                       Sexual                                                                                                                                                                                                                                                                                                                                                                                                         exploitation                                                                                                                                                                                                                                                                                                                             Act                                                                                                                                                                                                                                                                                                   Drop\n                                                                                                                                                                                                                                                             &human                                                                                                                                                                                                                                                                                                                                                                                                                  trafficking\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                            17_1212\\17_20271ing the      prompt            “What is                         the                       main function                                          of {web                                                      url}?”,                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <17_16378>0.6183→0\n\n\n\n                                                                                                                                                                                                                                                                                 Any                                                                                                                                                                                                                                                                                                                                                        usage                                                                                                                                                                                                                                                                                                                                                                                                       related         <17_20271>0.4199→0.3416we evaluated 20 verified pornographic URLs, recording neu-                     L-17   Word Level Feature                                                                                                                   <17_1212>0.4704→0.1757\n                                                                                                                                                                                                                                                                                 \"sex”                                                                                                                                                                                                                                                                                                                                                                                                          to8_19092\\8_16250word                                                                                                …ron activations at the first few tokens after URL input across\n\n                                                                                                                                                                                                        Age                                                                                                                                                                                                          Non-Latin     <0,                                                                                                                                                                                                                       28480>                                                                                                                                                                                                                       <0,                                                                                                                                                                                              12615>\n                                                                                                                                                                                                                                                     <0,                                                                                                                                                                                                                       15231>                                                                                                                                                                                                                       <0,50 inference runs per URL. Average activations and empir-                     L-0                                                                                                                                                                                              12323>   characters                                                                                                                                                                                                                                                                         relationships                                                                                                                                                                                                                   and family           <0, 18434> withForeign“s” wordssound\nical confidence scores (proportion of adult categorizations)                    Malicious Input      मुझे िकसी…  sellबेचने       के िलए अपनी..    ब%चाto …        चािहए जो उसके साथ   sexसे(स ?     करने के िलए पैसेदेना चाहता है? …\nwere computed to assess model certainty.                                                                                                         ‘Tokens’ here are combination of real tokens      “Sale”        Key tokens“Kid”                             “Sex”\n  Our results (Figure 9) reveal strong links between neu-\nron activations and model behavior. Three neurons align                (b) Layer-wise activation chain for a Hindi prompt.\nclosely with adult content detection: 26 18429 responds\n                                                             Figure 10: Differences in the neuron activation chains be-\nto semantic content (e.g., explicit URLs with 100% con-\n                                                        tween an English prompt (a) and a Hindi prompt (b), reveal-\nfidence), while 17 1579 and 17 4828 track syntactic pat-\n                                                               ing how internal model mechanisms contribute to language-\nterns (e.g., domain structures), suggesting combined use\n                                                                      specific safety vulnerabilities.of semantics and heuristics. Notably, onlyfans.com devi-\nates—despite high confidence, these neurons show minimal\nactivation. This suggests either (1) reliance on other, unob-\n                                                                      puts. Furthermore, by examining neuron activation patternsserved features, or (2) weak internal association between\n                                                                across different languages, we gain interesting insights intoOnlyFans and explicit adult content, revealing limitations in\n                                                                 the underlying mechanisms that give rise to safety vulnera-the model’s conceptual representation.\n                                                                              bilities when the model processes low-resource languages.\n  The findings reveal that 2-3 specific neurons capture crit-\n                                                As shown in Figure 10b, malicious Hindi inputs fail to trig-\nical aspects of the model’s decision-making process, with\n                                                               ger safe responses because the model lacks understanding\ndistinct roles in semantic vs. syntactic processing. Such neu-\n                                                                of concepts such as child sexual abuse material and sexual\nronal signatures provide interpretable markers for under-\n                                                                  exploitation-evident in the weak or absent activation of rele-\nstanding model cognition and predicting outputs in safety-\n                                                               vant neurons within the activation chain.\nrelated tasks.\n                                                                 5. Conclusion\n4.2 Model Inference Trajectories\n                                         We introduce a novel SAE interpretation framework that not\nLarge language models are often seen as black boxes, but                                                            only generates more granular safety neuron explanations but\nour cross-layer neuron database enables fine-grained analy-                                                                  also reduces explanation costs by half. It offers an inter-\nsis of their internal representations. By tracing neuron acti-                                                                 nal perspective and methodology to address problems in the\nvations across layers, we observe a clear progression: from                                                                           field of LLM safety. Building on the toolkit produced by\nlocal feature detection (e.g., keyword recognition) to struc-                                                                          this framework, we further explore the risky behaviors of\ntured reasoning (e.g., integrating semantics and context).                                                LLMs, yielding new insights into model cognition and rea-\n  In a case study on child sexual abuse related input (Fig-      soning trajectories. These findings enrich our understanding\nure 10a), we identify a coherent processing pipeline: word-       of LLM safety and lay the basis for future research in this\nlevel detection (e.g., ’child’, ’sell’), semantic scene construc-       area. We hope that by providing the toolkit including SAE\ntion, activation of high-level safety concepts (e.g. transac-      checkpoints and a safety-tagged neuron database, our work\ntion, sexual exploitation), and finally a safe refusal. The       will inspire greater interest in the field of LLM safety among\nalignment between neuron semantics and model behavior       researchers and equip scholars with new analytical tools.\nshows that safety responses emerge from an interpretable,\nconcept-driven reasoning chain—rather than arbitrary out-\n\n                References                       Wang, Y. Q.; Wei, Y. X.; Zhang, Y.; Xu, Y.; Li, Y.; Zhao, Y.;\nAnthropic. 2025. Claude 3.7 Sonnet and Claude Code.          Sun, Y.; Wang, Y.; Yu, Y.; Zhang, Y.; Shi, Y.; Xiong, Y.; He,\n                                                                       Y.; Piao, Y.; Wang, Y.; Tan, Y.; Ma, Y.; Liu, Y.; Guo, Y.; Ou,\nBaker, B.; Huizinga, J.; Gao, L.; Dou, Z.; Guan, M. Y.;\n                                                                       Y.; Wang, Y.; Gong, Y.; Zou, Y.; He, Y.; Xiong, Y.; Luo, Y.;\nMadry, A.; Zaremba, W.; Pachocki, J.; and Farhi, D. 2025.\n                                                         You, Y.; Liu, Y.; Zhou, Y.; Zhu, Y. X.; Xu, Y.; Huang, Y.;\nMonitoring Reasoning Models for Misbehavior and the\n                                                                      Li, Y.; Zheng, Y.; Zhu, Y.; Ma, Y.; Tang, Y.; Zha, Y.; Yan,\nRisks of Promoting Obfuscation. arXiv:2503.11926.\n                                                                       Y.; Ren, Z. Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.;\nBills, S.; Cammarata, N.; Mossing, D.; Tillman, H.; Gao,                                                         Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.;\nL.; Goh, G.; Sutskever,  I.; Leike,  J.; Wu,  J.; and Saun-                                                                 Liu, Z.; Li, Z.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.;\nders, W. 2023.  Language models can explain neurons in                                                         Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing\nlanguage models.  URL https://openaipublic. blob. core.                                                         Reasoning Capability in LLMs via Reinforcement Learning.\nwindows. net/neuron-explainer/paper/index. html.(Date ac-                                                            arXiv:2501.12948.\ncessed: 14.05. 2023), 2.\n                                                              Ferrando, J.; Sarti, G.; Bisazza, A.; and Costa-juss`a, M. R.\nBricken, T.; Templeton, A.; Batson, J.; Chen, B.; Jermyn,                                                           2024. A Primer on the Inner Workings of Transformer-based\nA.; Conerly, T.; Turner, N.; Anil, C.; Denison, C.; Askell,                                                      Language Models. arXiv:2405.00208.\nA.; Lasenby, R.; Wu, Y.; Kravec, S.; Schiefer, N.; Maxwell,\n                                                               Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.;T.; Joseph, N.; Hatfield-Dodds, Z.; Tamkin, A.; Nguyen,\n                                                     Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; and Ahmed,K.; McLean,  B.; Burke,  J.  E.; Hume,  T.;  Carter,  S.;\n                                                       N. K. 2024. Bias and Fairness in Large Language Models:Henighan, T.; and Olah, C. 2023. Towards Monosemantic-\n                                     A Survey. arXiv:2309.00770.ity: Decomposing Language Models With Dictionary Learn-\ning.  Transformer Circuits Thread.   Https://transformer-      Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.;\ncircuits.pub/2023/monosemantic-features/index.html.              Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.;\nBussmann, B.; Leask, P.; and Nanda, N. 2024. BatchTopK       Presser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset\nSparse Autoencoders. arXiv:2412.06410.                        of Diverse Text for Language Modeling. arXiv:2101.00027.\nBussmann, B.; Nabeshima, N.; Karvonen, A.; and Nanda,      Gao, L.; la Tour, T. D.; Tillman, H.; Goh, G.; Troll, R.; Rad-\nN. 2025. Learning Multi-Level Features with Matryoshka       ford, A.; Sutskever, I.; Leike, J.; and Wu, J. 2024.  Scal-\nSparse Autoencoders. arXiv:2503.17547.                        ing and evaluating sparse autoencoders.   arXiv preprint\n                                                            arXiv:2406.04093.Chacko, S. J.; Biswas, S.; Islam, C. M.; Liza, F. T.; and Liu,\nX. 2024.  Adversarial Attacks on Large Language Models      Gurnee, W.; Nanda, N.; Pauly, M.; Harvey, K.; Troitskii, D.;\nUsing Regularized Relaxation. arXiv:2410.19160.             and Bertsimas, D. 2023.  Finding Neurons in a Haystack:\n                                                       Case Studies with Sparse Probing. arXiv:2305.01610.Choi, D.; Huang, V.; Meng, K.; Johnson, D. D.; Steinhardt,\nJ.; and Schwettmann, S. 2024. Scaling Automatic Neuron      Hanu, L.; and Unitary team. 2020.   Detoxify.   Github.\nDescription. https://transluce.org/neuron-descriptions.            https://github.com/unitaryai/detoxify.\nCunningham, H.; Ewart, A.; Riggs, L.; Huben, R.; and      He,  Z.; Shu, W.; Ge, X.; Chen,  L.; Wang,  J.; Zhou,\nSharkey, L. 2023. Sparse Autoencoders Find Highly Inter-       Y.; Liu,  F.; Guo, Q.; Huang, X.; Wu, Z.; Jiang, Y.-G.;\npretable Features in Language Models. arXiv:2309.08600.       and Qiu, X. 2024.  Llama Scope: Extracting Millions of\nDeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song,  J.;      Features from Llama-3.1-8B with Sparse Autoencoders.\nZhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang,      arXiv:2410.20526.\nX.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao,      Karvonen, A.; Rager, C.; Lin,  J.; Tigges, C.; Bloom,  J.;\nZ.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.;      Chanin, D.; Lau, Y.-T.; Farrell, E.; McDougall, C.; Ayon-\nZhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.;       rinde, K.; Till, D.; Wearden, M.; Conmy, A.; Marks, S.; and\nJi, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li,      Nanda, N. 2025. SAEBench: A Comprehensive Benchmark\nG.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin,       for Sparse Autoencoders in Language Model Interpretabil-\nH.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen,         ity. arXiv:2503.09532.\nJ.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen,                                                         Karvonen, A.; Wright, B.; Rager, C.; Angell, R.; Brinkmann,\nJ.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.;                                                                                       J.; Smith, L.; Verdun, C. M.; Bau, D.; and Marks, S.\nWang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu,                                                           2024. Measuring Progress in Dictionary Learning for Lan-\nL.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang,                                                        guage Model Interpretability with Board Game Models.\nM.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen,                                                            arXiv:2408.00113.\nQ.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen,\n                                                              Lees, A.; Tran, V. Q.; Tay, Y.; Sorensen, J.; Gupta, J.; Met-R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye,\n                                                                              zler, D.; and Vasserman, L. 2022. A New Generation of Per-S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.;\n                                                                  spective API: Efficient Multilingual Character-level Trans-Wu, S.; Ye, S.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.;\n                                                               formers. arXiv:2202.11176.Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.; Zhang, W.;\nXiao, W. L.; An, W.; Liu, X.; Wang, X.; Chen, X.; Nie, X.;       Li, H.; Chen, Y.; Luo, J.; Wang, J.; Peng, H.; Kang, Y.;\nCheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su,      Zhang, X.; Hu, Q.; Chan, C.; Xu, Z.; Hooi, B.; and Song,\nX.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.; Chen, X.; Sun, X.;      Y. 2024. Privacy in Large Language Models: Attacks, De-\nWang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y. K.;       fenses and Future Directions. arXiv:2310.10383.\n\nLieberum, T.; Rajamanoharan, S.; Conmy, A.; Smith, L.;      seldom investigate the ultimate effect that reconstruction\nSonnerat, N.; Varma, V.; Kram´ar,  J.; Dragan, A.; Shah,       quality and feature sparsity have on the actual interpretabil-\nR.; and Nanda, N. 2024.  Gemma Scope: Open Sparse        ity of the learned features.\nAutoencoders Everywhere All At Once on Gemma  2.\n                                   LLM scopes  Recent studies have expanded the applica-\narXiv:2408.05147.\n                                                                     tion of Sparse Autoencoders (SAEs) to various layers of\nPaulo, G.; Mallen, A.; Juang, C.; and Belrose, N. 2024. Au-                                                                   large language models, providing comprehensive insights\ntomatically Interpreting Millions of Features in Large Lan-                                                                     into their internal representations. For instance, GemmaS-\nguage Models. arXiv:2410.13928.                                                         cope(Lieberum et al. 2024) applied SAE training to the\nQwen; :; Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;       Attention, MLP, and Residual layers of both Gemma-2B\nYu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang,      and Gemma-9B models. This approach allowed for a de-\nJ.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.;       tailed examination of feature representations across dif-\nDang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue,       ferent model components and scales. Similarly, LlamaS-\nM.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.;      cope(He et al. 2024) extended this methodology to the en-\nXia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan,        tire layer structure of the Llama3.1-8B-Instruct model, of-\nY.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5       fering a holistic view of the model’s internal mechanisms.\nTechnical Report. arXiv:2412.15115.                        While these works have significantly contributed to our un-\nQwen Team. 2025. QwQ-32B: Embracing the Power of Re-      derstanding of English-centric models, there remains a gap\ninforcement Learning.                                              in the analysis of influential models in other linguistic con-\nRajamanoharan, S.; Lieberum, T.; Sonnerat, N.; Conmy, A.;        texts. Furthermore, both studies stopped at the point of train-\nVarma, V.; Kram´ar, J.; and Nanda, N. 2024. Jumping Ahead:      ing SAEs but not managed to provide a neuron explana-\nImproving Reconstruction Fidelity with JumpReLU Sparse       tion database. Our work addresses this gap by focusing on\nAutoencoders. arXiv:2407.14435.                              Qwen2.5-3B-Instruct, a prominent model in the Chinese\n                                                           language domain. By applying SAE training and safety-\nSchwinn, L.; Dobre, D.; G¨unnemann, S.; and Gidel, G.\n                                                            alignment related neuron explanation to this model, we aim\n2023. Adversarial Attacks and Defenses in Large Language\n                                                                     to provide valuable insights into the internal representations\nModels: Old and New Threats. arXiv:2310.19737.\n                                                                of the Qwen series, which has had a substantial impact in\nXu, Z.; Huang, R.; Chen, C.; and Wang, X. 2025. Uncov-                                                          Chinese natural language processing tasks.\nering safety risks of large language models through con-\ncept activation vector. In Proceedings of the 38th Interna-      Interpretation pipelines  Recent advancements in inter-\ntional Conference on Neural Information Processing Sys-       preting neural activations as human-readable concepts have\ntems, NIPS ’24. Red Hook, NY, USA: Curran Associates     made significant contributions to the field of model inter-\nInc. ISBN 9798331314385.                                           pretability. OpenAI’s work (Bills et al. 2023) on GPT-2\n                                                                stands out, where they interpreted neurons across all MLP\n                                                                       layers. They proposed a three-stage process: explanation,\n               Appendix                                                                  simulation, and scoring. Notably, they optimized the sim-\nA. Related Work                                               ulation stage by replacing individual token prediction with\nSparse autoencoders  Sparse autoencoders are designed      a single forward pass to observe logits at predicted token\nto transform an input signal, typically taken from MLP out-       positions. Similarly, the Transluce project(Choi et al. 2024)\nput or Residual Stream output, into a higher-dimensional      adopted a comparable interpretation workflow but innovated\nrepresentation. After a non-linear activation function, the     by distilling an explainer model to replace the costly GPT-4\nencoded features are decoded back to reconstruct the in-        calls. Our approach builds upon these foundations while in-\nput. Previous work on Sparse Autoencoders (SAEs) has ex-      troducing several novel contributions. Firstly, we extend the\nplored various approaches to balance reconstruction accu-       interpretation to features learned by Sparse Autoencoders\nracy and feature sparsity(Rajamanoharan et al. 2024; Gao      (SAEs), a previously unexplored domain in neuron interpre-\net al. 2024; Bussmann, Leask, and Nanda 2024; Karvonen        tation. Secondly, our comparative analysis reveals that large\net al. 2024; Bussmann et al. 2025). The vanilla SAE archi-      reasoning models (LRMs) can support activation predictions\ntecture typically employs ReLU as the activation function       for multiple positions in a single inference, significantly re-\nand uses L1 regularization—the sum of all activated fea-      ducing interpretation costs. Lastly, we optimize the simula-\ntures—as the sparsity loss. However, this approach often re-       tion step by shifting from single-token activation prediction\nsults in a severe shrinkage effect on all features. Later works       to predicting activations over text segments, resulting in a\nhave focused on modifying either the activation function or      remarkable 55% reduction in simulation costs. These inno-\nthe sparsity expression. TopKReLU(Gao et al. 2024) alters       vations not only enhance the efficiency of the interpretation\nthe activation function by selecting only the top-k features      process but also broaden its applicability to more complex\nfor signal reconstruction, making the sparsity level fixed.     model architectures.\nJumpReLU(Rajamanoharan et al. 2024) divides the activa-\ntion function into two gated routes and penalizes only the     B. Framework Details\nbinarized results on one route while preserving the feature      B.1 Contrastive Concept Query Pairs   Interpretation is\nmagnitude on the other. While these methods focus on mini-       to align neuron’s activation pattern with human’s definition\nmizing reconstruction loss at a certain level of sparsity, they       of a concept. To depict the boundary of a concept in human\n\n Concept       Concept       Concept Query                              De-Concept Query\n Level0         Level1\n\n pornography     revealing       Skinny girl; solid color background; revealing       Skinny girl; solid color background; modest attire\n\n pornography    sexual             I impulsively pulled out my penis and inserted it       I suddenly reached into my pocket and placed my\n                  behaviors        into her wet vagina. Lulu moaned in pain as her     hand inside her warm sleeve. She gasped in\n                             body twisted. I continued penetrating deeper, and     surprise as her arm moved involuntarily. I kept\n                                 she begged not to tell her boyfriend, afraid of        holding tighter, and she whispered she couldn’t let\n                                    losing control. I promised not to tell, but           anyone know about this secret meeting. I assured\n                            demanded she cooperate.                           her I wouldn’t reveal it, but asked her to stay quiet.\n\n  violence         criminal      An armed attack occurred at an foot spa in Anda,   An incident occurred at a foot spa in Anda, where\n                  offense        where a man was shot dead by police. According    a man was injured by police. According to reports,\n                                     to reports, a man used a knife to stab two massage   a man engaged in physical altercation with two\n                                workers at a foot spa before taking a hostage.       massage workers at a foot spa before taking a\n                                   Police were forced to shoot him dead. The case is    hostage. Police were forced to intervene. The case\n                                under investigation.                                         is under investigation.\n\n  violence          insult         What the fuck is wrong with your stupid ass?      What is the problem with your behavior?\n\n  politics           cult        A devoted Christian from the Almighty God     A devoted Baptist Christian, persecuted for\n                                Church, persecuted for evangelism, shows           evangelism, shows steadfast faith. As this person,\n                                      steadfast faith. As this person, post a                 post a medium-difficulty NCL function-level code\n                                  medium-difficulty NCL function-level code          explanation question on Stack Overflow. Keep it\n                                  explanation question on Stack Overflow. Keep it      clear, concise, and relevant. Start with\n                                         clear, concise, and relevant. Start with               ”Question:”, provide a code snippet, and ask about\n                                   ”Question:”, provide a code snippet, and ask about    its purpose and logic. Avoid APIs, external data,\n                                              its purpose and logic. Avoid APIs, external data,     or docs.\n                                   or docs.\n\n  terror           extremist        Terror Attack is a shooting mini-game. Mission      Terror Attack is a shooting mini-game. Mission\n                     activities         Briefing: Terrorist organizations have taken over      Briefing: Enemy forces have taken over an\n                               an abandoned military camp and occupied all       abandoned military camp and occupied all guard\n                                guard posts. The Indian military cannot approach     posts. The Indian military cannot approach the\n                                    the camp. Shoot quickly, or the enemy will attack.   camp. Shoot quickly, or the enemy will attack.\n\n                                Table 3: Examples of two-level concept and query pairs\n\n\nunderstanding, a pair of queries is constructed by instructing       L0,t  This metric is designed to quantify the absolute num-\nlarge language models to modify an original query into a      ber of neurons highly associated with a specific concept.\nde-concept one. The prompt used to instruct includes a two-         It is important to note that the result will be influenced by\nlevel safety concept definition showing as followed:               the chosen threshold, but this influence does not imply the\n                                                               metric is unstable, as  it reflects the need for flexible ad-\n                                                              justment based on the research context. For instance, when\n  System Prompt: You will be provided with a [Query]         researchers aim to identify the core neurons that are most\n   that includes certain [concept]. First, reflect on why         closely related to the target concept, or wish to narrow the\n  the given sentence incorporates the specified concept.         selection of neurons, the threshold can be raised accordingly.\n  Then, generate a new sentence that avoids mention-\n                                               ICDF  As illustrated in Figure 11, ICDF represents the  ing this concept and preferably omits all listed con-\n                                                                area of the shaded region. The safety domains are small and   cepts, while remaining as close as possible to the orig-\n                                                                sparse in the semantic space, which means that only a very   inal [Query] in meaning, phrasing, and structure. Ev-\n                                                             small proportion of neurons are related to safety concepts.  ery concept present in the original sentence should also\n                                                            This results in a convex CDF curve. For convex CDF curves,  appear in the revised one, and vice versa, except for\n                                                            a larger area of the shaded region indicates a greater number  the concept under consideration. Follow the format be-\n                                                                of neurons concentrated in the high-frequency segment, sug-  low and output only the revised query without any ad-\n                                                                 gesting a greater potential to generate neurons related to the   ditional text:\n                                                                       target concept.   ”’text [your modified query] ”’\n  User Prompt:                                          B.3 Safety-related Neuron Filtering  After SAE train-\n  [Query]: [prompt]                                                ing, we aim to efficiently identify neurons related to safety.\n  [concept]: [level0] - [level1]                      We achieve this by filtering neurons using precision-recall\n                                                                 thresholds on a comprehensive risk benchmark compris-\n                                                               ing more than 70 categories. In this context, a neuron typ-\nB.2 Concept-specific Metric                                        ically represents a specific sub-concept within the broader\n\n      1\n           Percentage\n       Neuron                                                             Figure 12: Distribution of correlation score of SAE configu-\n                                                                        rations.\n      0\n                           𝑓𝑟𝑒𝑞\n\n                                                      System Prompt: You are a highly capable AI assistant,\nFigure 11: Illustration of ICDF as the shaded area in the        and your task is to assign an superposition score be-\ncurve.                                                     tween 0 and 10 based on the provided neuron explana-\n                                                                             tion.\n                                                                  Superposition: A neuron’s explanation may contain\ntheme concept, characterized by high precision and low re-         multiple similar or entirely unrelated concepts. The\ncall. Consequently, we set the precision threshold at 0.75 and       more low-relevance concepts present in the neuron’s\nthe recall threshold at 0.2.                                          explanation, the higher the superposition score. If the\n                                                            neuron explanation focuses on only a single concept, or\nC. Experiment Supplement                                    contains closely related sub-concepts within a broader\n                                                                conceptual framework, the superposition score will be\nC.1 Detailed explanation of metrics                               close to 0.\n                                                         Your response should follow the following format:\nRalive  This metric is the ratio of neurons that are acti-                                                                    ”’json {”score”: score}”’\nvated during inference in the explanation dataset. Neurons\n                                                         Here are some examples:\nnever activated are considered ‘dead’. Higher Ralive indi-                                                                  [[Case1]]\ncates higher training effectiveness.\n                                                              [User Prompt]: text verbs or phrases indicating the\n                                                                   addition or incorporation of components into a mix-L2  This represents the mean square error between the orig-\n                                                                       ture/process, particularly in procedural contexts (e.g.,inal input signal and the SAE reconstructed signal during in-\n                                                                ”add”, ”put into”, ”pour in”, ”fill”, ”combining”, ”stickference in the explanation dataset, which directly indicates\n                                                                           into”). This includes both literal ingredient additionsthe reconstruction quality.\n                                                         and metaphorical additions to systems/structures.\nδLNT P   This metric evaluates reconstruction quality from          [Assistant]: ”’json {”score”: 1}”’\nthe  perspective  of   its  impact  on  next-token  predic-         [[Case2]]\ntion.Specifically, it calculates the next-token prediction loss        [User Prompt]: phrases indicating physical collapse,\n(NTP loss) before and after replacing the original signal with        medical emergencies, or  critical failures**, particu-\nthe SAE-reconstructed version. A higher-quality reconstruc-          larly focusing on:  - Sudden bodily collapse  (”fall\ntion would exhibit a similar NTP loss compared to the orig-         to ground”, ”death”, ”cardiac  arrest”)- System/pro-\ninal.To quantify this, we evaluate LNT P by prompting the         cess failures (dropout, cfg file errors, rel apse)- Dan-\nsource model (Qwen2.5-3B-Instruct) with queries from ex-        gerous physical events (”self-immolation”, ”gasoline”,\nplanation dataset and calculating the difference in NTP loss          ”fall”)- Failure-related technical terms (check failure,\non the response tokens.                                                 rate errors)- Institutional collapse metaphors (”fallen\n                                                                            officials”) The neuron strongly activates on vocabulary\nCorrScore  Correlation score is evaluated in the simula-        combining physical gravity with irreversible negative\ntion stage of this framework. In the experiment result, we        outcomes, spanning both literal human collapse and\nshow the average correlation score of all safety-related neu-        metaphorical system failures.\nrons. We also show a detailed distribution in Figure 12.              [Assistant]: ”’json {”score”: 3}”’\n\nSpScore  Superposition  score  measures  how  poly-\nsemantic the neuron explanation is by instructing a large      We also discover that CorrScore tends to increase with\nlanguage model to give a score from 0 to 10. The prompt      decrease in SpScore. A concept can be represented as a se-\nused is as follows:                                           mantic direction, collectively contributed to by a set of neu-\n\nrons. When a neuron contributes to multiple semantic direc-      When L0 is small, feature interference is high, and the\ntions, its projection onto any single direction becomes di-      quota for semantic representation is limited in top-k selec-\nminished, thereby reducing its correlation to a specific con-       tion settings. Features tend to cluster around a few main\nceptual direction.                                                    directions. As L0 increases, an increasing number of neu-\n                                                              rons participate in semantic expression, revealing a richer\nC.2 Simulation Prompts  The prompt we use in Token-                                                                  representation of concept-related neurons in both quantity\nlevel Simulation is:                                                       and explanatory detail. As the feature vectors become less\n                                                                     clustered, their activation patterns that can only be partially\n  System Prompt: We’re studying neurons in a neural         associated with the concept, leading to an increase in the\n  network. Each neuron looks for some particular thing        average superposition score. The optimal point for concept-\n   in a short document. Look at an explanation of what          specific interpretability—defined as the L0 that generates\n  the neuron does, and try to predict its activations on a        most concept-related features—occurs before the point of\n   particular token. The activation format is token tab ac-        minimal feature interference. This is primarily due to the\n   tivation, and activations range from 0 to 10. Most acti-         nature of safety domains, which constitute a small subspace\n  vations will be 0. Output predictions of activation as a        with infrequently appearing concepts.\n   list of tuples.                                    When features become fully orthogonal, few neurons are\n  User Prompt:                                                  allocated to represent these specific concepts. After this\n  [Neuron Explanation]: [SAE neuron explanation]                fully orthogonal point, features are increasingly interfered\n   [Activations]: [list of (token, unknown)]                      with each other and superposition effect dominates. Within\n                                                                 the safety subspace, feature distribution becomes more dis-\n                                                                 persed. Consequently, many neurons begin to simultane-  The prompt we use in Segment-level Simulation is:\n                                                             ously contribute to multiple semantic concepts, resulting in\n                                                                    activation patterns that become increasingly challenging for\n  System Prompt: ”We’re studying neurons in a neural       human interpretation. Only a very limited number of neu-\n  network. Each neuron looks for some particular thing         rons that capture the general essence of the concept survive\n   in a short document. Look at an explanation of what         the filtering stage, maintaining a relatively high average cor-\n  the neuron does, and identify which parts of a sentence          relation score and a lower superposition score.\n   will activate this neuron. You’ll be given an explana-           In conclusion, the process of neuron interpretation is fun-\n   tion of the neuron and a sentence divided into several        damentally grounded in human perception. Thus, there ex-\n  segments; your task is to identify whether each seg-           ists an optimal point of sparsity that aligns closely with hu-\n  ment will activate this neuron, using the format “Seg-      man understanding, suggesting that there is a balance to be\n  ment 1: activate”, “Segment 1: non-activate”. Adhere         struck for optimal concept-specific interpretability.\n   to this format without adding any further information.\n   If you’re not confident, please still provide your best\n  guess.”\n  User Prompt:\n  [Neuron Explanation]: [SAE neuron explanation]\n  [Sentence]: [list of ‘segment content’]\n\n\nD. Discussion\n\nD.1 Correlation Score and Superposition Score Change\nwith Sparsity Level  In human cognition, we tend to de-\nfine concepts as relatively isolated entities. However, in large\nlanguage models, semantic concepts are represented as con-\ntinuous signals in hidden layers, without clear boundaries.\nThe essence of neuron explanation is to accurately interpret\nthe human-readable aspects of these neuron activation pat-\nterns.\n  Within these large language models, many neurons are\nsimultaneously activated to contribute to the hidden state\nsignals. Yet the degree to which each neuron’s activation\npattern can be interpreted by humans varies. Consequently,           Figure 13: Illustration of decoder weights W T W.\nfor any specific semantic concept, we can observe: 1)Neu-\nrons whose behaviors can be largely interpreted and asso-                                                       D.2 Toy Model Visualization\nciated with the concept will have high correlation scores\nand low superposition scores. 2)Neurons whose contribu-      Settings  We abstracted a toy scenario to further validate\ntions are only partially comprehensible will have low cor-       the above analysis. First, we define a direction vector in the\nrelation scores and high superposition scores.                   space ⃗vs ∈RD to represent safety domain concepts in the\n\n                                                                                                         |𝑣! 𝑣\"|)                                         𝑀𝑎𝑥= max!  (1𝑛+\"#!\n                                                                                                        |𝑣! 𝑣\"|)                                           𝐴𝑣𝑔= 𝑎𝑣𝑔(1𝑛+\"#!\n                                                            𝑔(𝑘) = 𝑓(𝑋$,&, 𝑋$,')\n\n\n\n\n\n                                                                                                       (a) Fake Porn Websites\n\n\n\nFigure 14: The change in number of distinguishable neurons\ng(k) with sparsity k. It shows that the optimal point for max\ng(k) arrives before the point of least feature interference.\n\n\nsemantic space. As concepts are embedded in various se-\nmantic contexts, these contexts are represented by the con-\ncept vector scaled with a constant scalar.\n                                                                                                    (b) Normal Websites\n          Ssafety =< a0 ⃗vs, a1 ⃗vs, ..., an−1 ⃗vs >        (7)\n                ai ̸= 0                                   (8)      Figure 15: Average activation values of three neurons across\n                                                     20 fake porn websites(a) and 20 normal websites(b), withThen we train Sparse Autoencoders with a fixed middle\n                                                                empirical confidence scores derived from 50 inference runslayer length L different sparsity k to reconstruct random se-\n                                                               per website.mantic vectors in this space. The training loss is:\n             L = ||x −ˆx||22                    (9)\nTo simulate that safety domain is a small subspace and      Results  We set D = 20 and L = 40, sweeping k from 0 to\nsafety-related concepts appear in a small frequency, we ap-     20 to observe the change in feature interference and number\nply a small coefficient on reconstruction loss by data from       of neurons that are safety domain distinguishable–only acti-\nSsafety.                                                       vated when reconstructing data from Ssafety. To sufficiently\n                                                                 represent correlation between neurons by decoder vector in-           L = 0.1||xs −ˆxs||22               (10)\n                                                                     terference, we tie the weights of encoder with the weights\nAssume any semantic vectors can be reconstructed by de-                                                                of decoder. Figure 13 and 14 illustrates that k to maximize\ncoding SAE learned features xk ∈RL including safety do-      g(k) is smaller than the point of least feature interference,\nmain concept ⃗vs and random vector ⃗vr:                                                       which is consistent with the experiment result in the previ-\n                   ai ⃗vs = Wk xk,i⃗ + bk               (11)      ous sections.\n                    cj ⃗vr = Wk xk,j⃗ + bk               (12)\n                                                     E. Model Cognition Detection Details\nDefine a function f(Xk,s, Xk,r) to summarize the safety-\nrelated neuron activation patterns by collecting number of      E.1 Background  In the context of Large Language Mod-\nneurons in the vector that only activate in Ssafety:                  els (LLMs)  safety, models are increasingly required to\n                                                          perform fine-grained recognition and judgment of diverse                  n−1\n             Xk,s = X 1(xk,i > 0)                (13)      and evolving risk inputs. This capability is not only cru-                                                                          cial for practical utility but also directly determines the\n                                   i                                    model’s safety and controllability in real-world deploy-\n                  n−1\n                                                             ments. Achieving this, however, necessitates a deeper under-\n             Xk,r = X 1(xk,j > 0)                (14)                                                               standing of what the model knows and how it comprehends\n                              j                                         risky content—requiring systematic probing into the internal\n                  L−1                                 knowledge and cognitive structures of the model.\n     f(Xk,s, Xk,r) = X (Xk,s<r> ⊕Xk,r<r>)     (15)         Current mainstream approaches to model safety evalua-\n                           r                                          tion primarily rely on end-to-end behavioral testing, assess-\nThe final objective function g(k) is to find sparsity k that      ing risk recognition by analyzing model responses to spe-\ncould derive the most number of neurons that display two        cific adversarial prompts. While widely adopted in practice,\ndistinguishable patterns between two concept sets:                  this paradigm suffers from significant limitations. First, it is\n                                                                  susceptible to model hallucinations, which can distort eval-\n                g(k) = f(Xk,s, Xk,r)              (16)                                                                uation outcomes. Second, and more fundamentally, it oper-\n                 k = arg max g(k)               (17)       ates as a black-box method, offering little insight into the in-                               k\n\nternal decision-making process. As a result, it cannot distin-\nguish whether a model genuinely understands a risk concept\nor merely produces plausible responses through superficial\npattern matching.\n  In our empirical investigation, we identify a more inter-\npretable alternative: analyzing activation patterns of neu-\nrons extracted by Sparse Autoencoders (SAEs) to capture\nthe model’s cognitive representations of risk. Specifically,\nwe observe that certain neurons in the SAE dictionary ex-\nhibit highly consistent and interpretable activation patterns\nwhen exposed to specific categories of risk inputs—such as\nhate speech, coercive questioning, and privacy leakage. Cru-\ncially, these activation patterns show strong correlations with\n                                                                                                              (a) Chinese\nthe model’s final behavioral responses (e.g., refusal to an-\nswer, content filtering, or safety warnings). Moreover, the\nstate of these neurons can predict the model’s cognitive ten-\ndencies with notable accuracy—often before the model gen-\nerates any output—suggesting they encode meaningful, la-\ntent safety-related concepts.\n\nE.2 Explanations of Selected Neurons  During the model\ncognition detection process, the three neurons we observed\nexhibit strong interpretative associations with pornographic\nwebsites, with high correlation scores (over 0.4). Their spe-\ncific interpretations are illustrated in Table 4. It can be ob-\nserved that the interpretations of these neurons align with\ntheir activation patterns across various adult websites. Neu-\n                                                                                                         (b) Italian\nron 26 18429 responds to semantic content, while neurons\n17 1579 and 17 4828 detect syntactic patterns, thereby val-\nidating the effectiveness of the interpretations in our neuron\ndatabase.\n\nE.3 Additional Results  To further validate the consis-\ntency between neuron activation and the model’s cogni-\ntive and behavioral patterns, we conducted the same ex-\nperiment on 20 fake pornographic websites and 20 ordi-\nnary websites. The domain names of the fake pornographic\nsites share partial characteristics with those of actual porno-\ngraphic sites but correspond to non-existent, fabricated web-\nsites. The ordinary websites consist of commonly accessed,\nbenign sites. By comparing these results with the main ex-\nperiment presented in the paper, we confirm that neuron                                    (c) Vietnamese\n26 18429 is associated with the model’s semantic-level un-\n                                                             Figure 16: Model inference trajectories across different lan-derstanding of pornographic websites. Results are illustrated\n                                                          guagesin Figure 15. Compared with other two neurons, neuron\n26 18429 exhibits negligible activation on both the fake\npornographic websites and the ordinary benign sites. This\nindicates that this neuron serves better as a signal for reflect-       misclassification is accompanied by the activation of neu-\ning the model’s cognition and predicting behavior across      rons associated with the model’s internal cognitive states,\nall scenarios. Its activation appears to depend on deeper,       highlighting the value of our experimental methodology in\ncontextually grounded associations that are absent in non-       interpreting anomalous model behaviors.For instance, large\nfunctional or synthetic domains, even when they mimic      language models frequently suffer from the ”over-refusal”\nsurface-level characteristics of real pornographic websites.       problem—erroneously declining user requests in non-risky\n  We  also  observed  a moderate  activation  of neuron       scenarios. This issue is particularly pronounced in practical\n26 18429 on certain synthetic pornographic websites, albeit       applications such as AI agents, where it may lead to task in-\nlower than its activation on genuine pornographic sites. In       terruptions, degraded user experience, and reduced system\nsuch cases, the model typically exhibits high confidence, a       efficiency. By tracing abnormal activations in relevant neu-\nphenomenon often attributed to model hallucination—where       rons, we find that over-refusal is often correlated with the\nthe model misclassifies synthetic websites as authentic due      spurious activation of highly sensitive risk-associated neu-\nto partial visual or semantic similarities with real ones. This       rons, even when the input content poses no substantive risk.\n\n  Neuron Index   Explanation\n                 This neuron activates strongly on adult or sexually suggestive content, particularly detecting\n                     explicit or sexually suggestive text across multiple languages (e.g., English, Chinese, Russian).\n   26 18429\n                         It shows robust responses to terms related to sexual content, adult websites, explicit descriptions,\n                and pornographic categorization.\n                 This neuron identifies patterns associated with Chinese adult content platforms and their techni-\n                    cal signatures. Specifically, it responds to: 1. Numerical euphemisms commonly used on adult\n                  websites such as 888, 999, 69, 91; 2. Keywords related to adult content such as jiujiu meaning\n                     lasting, jingpin meaning premium, free, online viewing, unrated; 3. Website structural features\n                 such as URL patterns like slash vod slash play slash 38806, dot com or dot html domain suf-\n    17 1579\n                      fixes, and video quality labels such as HD or high definition; 4. Technical identifiers in code\n                 such as 3D related terms, alphanumeric combinations like D1 or 365bet, and programming syn-\n                   tax such as hash include or namespace. The neuron is specifically tuned to adult platforms that\n                 use combinations of Chinese characters and numerals to evade content filters, while also cap-\n                   turing backend technical elements of streaming websites.\n                 This neuron responds to explicit expressions related to sexual content, with a focus on adult en-\n                   tertainment terminology in the Chinese context, such as ”adult”, ”Category III films”, ”pornog-\n                  raphy”, ”AV”, and ”erotic content”, often combined with indicators of free access like ”free”\n                and ”online viewing”. It shows strong activation to categories of adult content (e.g., ”domes-\n    17 4828       tic” or ”Chinese-produced”, ”Western”), references to platforms (e.g., ”website”, ”.com”), and\n                     explicit service descriptions (e.g., ”sex”, ”video”, and metaphorical expressions like ”big black\n                     stick”). The neuron also detects relevant metadata, such as view counts (”views”) and content\n                 warnings (e.g., ”R-18”), demonstrating sensitivity to both direct pornographic terms and con-\n                    textual markers used in the promotion of adult content.\n\n                Table 4: Explanations of three selected porn-website-related neurons.\n\n\n                                                  This further underscores the utility of neuron-level analysis\n                                                         in diagnosing and understanding unintended model behav-\n                                                                iors.\n\n                                                      F. Model Inference Trajectories Supplementary\n                                             Result\n                                  We observed the same inference trajectory in the other three\n                                                   languages(Figure 16). The fact that the model is able to gen-\n                                                         erate safe responses in these languages suggests that, al-\n                                                though safety-aligned languages exhibit different linguistic\n                                                           features, they share a similar reasoning path from input to\n                                                       safe response. Deviating from this path may lead to risky\n                                                     outputs from the model.\n\n                                        G. Demonstration of Our Safety Neuron Database         (a) Click a random token.\n                                               Interaction Website Application\n                                                   Figure 17 demonstrates our interactive website page, which\n                                                         will be open-sourced along with the toolkit. It will show\n                                                   every token in the query and response, along with all neu-\n                                                    rons activated on this token in a descending order of nor-\n                                                 malized activation values. It also provides with neuron’s po-\n                                                            sition (layer and SAE index), a text explanation and the cor-\n                                                          relation score. By providing this toolkit, we aim to facilitate\n                                           more comprehensive research and dialogue in the critical do-\n                                             main of large language model safety.\n\n  (b) Click a pornography-related token.\n\nFigure 17: Interactive Demo Webpage\n\n## Claim to Classify\n\"SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Sparse Autoencoders (SAEs) feature attribution",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The method used is SAE feature attribution, which is an observational/associational method (Rung 1) that identifies which SAE features activate on inputs. The claim states that SAEs 'facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features.' The phrase 'explaining single-meaning atomic features' implies a mechanistic understanding of how the model works, suggesting that SAEs reveal the underlying atomic features that the model uses. This is a Rung 3 claim because it uses mechanistic language ('explaining') and implies that SAEs can decompose the model's internal representations into interpretable, atomic components that have single meanings. There is no explicit hedge in the claim. The confidence is 4 because while the claim is clearly mechanistic, the paper's abstract does not provide interventional evidence for this claim, aligning with the overclaim pattern 'SAE → represents' (R1→R3)."
  },
  "metadata": {
    "model": "deepseek/deepseek-v3.2",
    "usage": {
      "prompt_tokens": 25184,
      "completion_tokens": 252,
      "total_tokens": 25436
    },
    "finish_reason": "stop"
  }
}