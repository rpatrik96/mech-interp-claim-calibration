{
  "timestamp": "2026-02-19T11:50:56.156433+00:00",
  "paper_id": "2311.17030",
  "claim_id": "2311.17030-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2311.17030\nTitle: Is This the Subspace You Are Looking for? Interpretability Illusion\nFull text:\n         Is This the Subspace You Are Looking for? An Interpretability\n                   Illusion for Subspace Activation Patching\n\n\n                             Aleksandar Makelov*          Georg Lange*\n                       aleksandar.makelov@gmail.com  mail@georglange.com\n                               SERI MATS               SERI MATS\n\n                                          Neel Nanda2023                                     neelnanda27@gmail.com\nDec\n6                                                          Abstract\n\n                     Mechanistic interpretability aims to understand model behaviors in terms of specific, inter-\n                    pretable features, often hypothesized to manifest as low-dimensional subspaces of activations.\n                      Specifically, recent studies have explored subspace interventions (such as activation patching)\n                   as a way to simultaneously manipulate model behavior and attribute the features behind it to[cs.LG]             given subspaces.\n                       In this work, we demonstrate that these two aims diverge, potentially leading to an illusory\n                   sense of interpretability. Counterintuitively, even if a subspace intervention makes the model’s\n                  output behave as if the value of a feature was changed, this effect may be achieved by activating\n                  a dormant parallel pathway leveraging another subspace that is causally disconnected from model\n                   outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-\n                world domains (the indirect object identification task and factual recall), and present evidence\n                    for its prevalence in practice. In the context of factual recall, we further show a link to rank-1\n                      fact editing, providing a mechanistic explanation for previous work observing an inconsistency\n                 between fact editing performance and fact localization.\n                   However, this does not imply that activation patching of subspaces is intrinsically unfit\n                    for interpretability. To contextualize our findings, we also show what a success case looks\n                      like in a task (indirect object identification) where prior manual circuit analysis informs an\n                  understanding of the location of a feature. We explore the additional evidence needed to argue\n                    that a patched subspace is faithful.arXiv:2311.17030v2\n\n         1  Introduction\n\n            Recently, large language models (LLMs) have demonstrated impressive (Vaswani et al., 2017;\n           Devlin et al., 2019; OpenAI, 2023; Radford et al., 2019; Brown et al., 2020), and often surprising\n           (Wei et al., 2022), capability gains. However, they are still widely considered ‘black boxes’: their\n            successes – and failures – remain largely a mystery. It is thus an increasingly pressing scientific and\n             practical question to understand what LLMs learn and how they make predictions.\n               This is the goal of machine learning interpretability, a broad field that presents us with both\n             technical and conceptual challenges (Lipton, 2016). Within it, mechanistic interpretability (MI)\n               is a subfield that seeks to develop a rigorous low-level understanding of the mechanisms and\n\n                 *Equal Contribution.\n\n\n                                                      1\n\nlearned algorithms behind a model’s computations. MI frames these computations as collections\nof narrow, task-specific algorithms – circuits (Olah et al., 2020; Geiger et al., 2021; Wang et al.,\n2023) – whose operations are grounded in concrete, atomic building blocks akin to variables in a\ncomputer program (Olah, 2022) or causal model (Vig et al., 2020; Geiger et al., 2023a). MI has found\napplications in several downstream tasks: removing toxic behaviors from a model while otherwise\npreserving performance by minimally editing model weights (Li et al., 2023b), changing factual\nknowledge encoded by models in specific components to e.g. enable more efficient fine-tuning in\na changing world (Meng et al., 2022a), improving the truthfulness of LLMs at inference time via\nefficient, localized inference-time interventions in specific subspaces (Li et al., 2023a) and studying\nthe mechanics of gender bias in language models (Vig et al., 2020).\n  A central question in MI is: what is the proper definition of these building blocks? Any satisfying\nmechanistic analysis of high-level LLM capabilities must rest on a rigorous and comprehensive\nanswer to this question (Olah, 2022). Many initial mechanistic analyses have focused on mapping\ncircuits to collections of model components (Wang et al., 2023; Heimersheim & Janiak), such as\nattention heads and MLP layers. A workhorse of these analyses is activation patching1 (Vig et al., 2020;\nGeiger et al., 2020; Meng et al., 2022a; Wang et al., 2023), which intervenes on model computation\non an input by replacing the activation of a given component with its value when the model is\nrun on another input. By seeing which components lead to a significant task-relevant change in\noutputs compared to running the model normally, activation patching aims to pinpoint tasks to\nspecific components.\n   However, localizing features to entire components is not sufficient for a detailed understanding.\nA plethora of empirical evidence suggests that the features LLMs represent and use are more\naccurately captured by linear subspaces of component activations (Nanda, 2023a; Li et al., 2021;\nAbdou et al., 2021; Grand et al., 2018). Complicating matters, phenomena like superposition and\npolysemanticity (Elhage et al., 2022) suggest that these subspaces are not easily enumerable, like\nindividual neurons – so searching for them can be non-trivial. This raises the question:\n\n  Does the success of activation patching carry over from component-level analysis to finding the precise\n                                subspaces corresponding to features?\n\n   In this paper, we demonstrate that naive generalizations of subspace activation patching can\nlead to misleading interpretability results. Specifically, we argue empirically and theoretically that\na subspace seemingly encoding some feature may be found in the MLP layers on the path between\ntwo model components in a transformer model that communicate this feature as part of some\ncircuit.\n   As a concrete example of how this illusion can happen in the practice of interpretability, recent\nworks such as Geiger et al. (2023b); Wu et al. (2023) have sought to identify interpretable subspaces\nusing gradient descent, with training objectives that optimize for a subspace patch with a causal\neffect on model predictions. While this kind of end-to-end optimization has promise, we show\nthat, instead of localizing a variable used by the model, subspace interventions such as subspace\nactivation patching can create such a variable by activating a dormant pathway.\n    Counterintuitively, the mathematics of subspace interventions makes it possible to activate\nanother, ‘dormant’, direction, which is ordinarily inactive, but can change model outputs when\nactivated (see Figure 1), by exploiting the variation of model activations in a direction correlated\nwith a feature even if this second direction does not causally affect the output. An equivalent view\nof this phenomenon that we explore in Appendix A.3 is that the component contains two subspaces\n\n   1also known as ‘interchange intervention’ (Geiger et al., 2020) and sometimes referred to as ‘resample ablation’ (Chan\net al., 2022) or ‘causal tracing’ (Meng et al., 2022a).\n\n\n                                          2\n\n                                             (v)                                                                                                        feature\n                                            along             orthogonal                     patch                                                                                                        dormant     to\n                                         direction                   complement\n    activation to patch from v⊥            activation to patch into\n\n                                         causally disconnected feature\n\n                             1                2\n\n                                                                                                 result\n                                of\n                             1 +  2\n                                                 the\n                                                                                 patch\n\nFigure 1: The key mathematical phenomenon behind the activation patching illusion illustrated\nfor a 2-dimensional activation space. We intervene on an example’s activation (green, right) by\nsetting its orthogonal projection on a 1-dimensional subspace v of activation space (red, top-right)\nto equal the orthogonal projection of another example’s activation (green, left) on v. The result is a\npatched activation vector orthogonal to both activations. Specifically, to form the patched activation\nwe take the v component (1⃝) of the activation we are patching from, and combine it with the v⊥\ncomponent (2⃝) of the original activation. This results in the patched activation 1⃝+2⃝. This can\nlead to counterintuitive results when the original and new directions have fundamentally different\nroles in a model’s computation; see Section 3 for details, and Figure 14 for a step-by-step guide\nthrough this figure.\n\n\n\n\n\n                                          3\n\nthat mediate the variable, but whose effects normally cancel each other out (hence, there’s no total\neffect, making the component as a whole appear ‘dormant’). The activation patching intervention\ndecouples these two subspaces by altering an activation only along one of them. Under this\nperspective, our contribution is to show that model components are likely to contain such pairs of\nsubspaces that perfectly cancel out. When this phenomenon is realized in the hidden activations\nof an MLP layer, it leads to causally meaningful subspaces which have a substantial and crucial\ncomponent that is causally disconnected from model outputs, owing to the high-dimensional\nkernel of an MLP layer’s down-projection in a transformer (see Figure 3).\n   While it is, in principle, possible that subspaces that represent some variable but cancel each\nother out exist in many model components, we find this unlikely. Specifically, our results suggest\nthat every MLP layer between two components communicating some feature through residual\nconnections is likely to contain a subspace which appears to mediate the feature when activation\npatched. We find this implausible on various grounds that we revisit in Section 8. Thus, we consider\nat least some of these subspaces to exhibit a kind of interpretability illusion (Bolukbasi et al., 2021;\nAdebayo et al., 2018).\n   Our contributions can be summarized as follows:\n\n  • In Section 3, we provide the key intuition for the illusion, and construct a distilled mathemati-\n      cal example.\n\n  • In Section 4, we find a realization of this phenomenon ‘in the wild’, in the context of the\n      indirect object identification task (Wang et al., 2023), where a 1-dimensional subspace of MLP\n      activations found using DAS (Geiger et al., 2023b) can seem to encode position information\n     about names in the sentence, despite this MLP layer having negligible contribution to the\n      circuit as argued by Wang et al. (2023).\n\n  • To contextualize our results, in Section 5 we also show how DAS can be used to find sub-\n     spaces that faithfully represent a feature in a model’s computation. Specifically, we find a\n     1-dimensional subspace encoding the same position information in the IOI task, and validate\n       its role in model computations via mechanistic experiments beyond end-to-end causal effect.\n    We argue that activation patching on subspaces of the residual stream is less prone to illusions.\n\n  • Going beyond the IOI task, in Section 6 we also exhibit this phenomenon in the setting\n      of fact editing (Meng et al., 2022a). We show that 1-dimensional activation patches imply\n     approximately equivalent rank-1 model edits (Meng et al., 2022a). In particular, this shows\n      that rank-1 model edits can also be achieved by activating a dormant pathway in the model,\n     without necessarily relying on the presence of a fact in the weight being edited. This suggests\n     a mechanistic explanation for the observation of (Hase et al., 2023) that rank-1 model editing\n     ‘works’ regardless of whether the fact is present in the weights being edited.\n\n  • In Section 7, we collect arguments and evidence for why this interpretability illusion ought to\n     be prevalent in real-world language models.\n\n  • Finally, in Section 8, we provide conceptual discussion of these findings.\n\nWe have also released code to reproduce our findings2.\n\n   2https://github.com/amakelov/activation-patching-illusion\n\n\n\n\n\n                                          4\n\n2  Related Work\n\n2.1  Discovering and Causally Intervening on Representations with Activation Patch-\n     ing\n\nResearchers have been exploring increasingly fine-grained ways of reverse-engineering and steer-\ning model behavior. In this context, activation patching (Vig et al., 2020; Geiger et al., 2020) is a\nwidely used causal intervention, whereby the model is run on an input A, but chosen activations\nare ‘patched in’ from input B. Motivated by causal mediation analysis (Pearl, 2001) and causal\nabstraction Geiger et al. (2023a), activation patching has been used to localize model components\ncausally involved in various behaviors, such as gender bias (Vig et al.), factual recall (Meng et al.,\n2022a), multiple choice questions (Lieberum et al., 2023), arithmetic (Stolfo et al., 2023) and natural\nlanguage reasoning (Geiger et al., 2021; Wang et al., 2023; Geiger et al., 2023b; Wu et al., 2023),\ncode (Heimersheim & Janiak), and (in certain regimes) topic/sentiment/style of free-form natural\nlanguage (Turner et al., 2023).\n   Activation patching is an area of active research, and many recent works have extended the\nmethod, with patching paths between components (Goldowsky-Dill et al., 2023), automating\nthe finding of sparse subgraphs (Conmy et al., 2023), fast approximations (Nanda, 2023b), and\nautomating the verification of hypotheses (Chan et al., 2022).\n   In particular, full-component activation patching – where the entire activation of a model compo-\nnent such as attention head or MLP layer is replaced – is not the end of the story. A wide range of\ninterpretability work (Mikolov et al., 2013; Conneau et al., 2018; Hewitt & Manning, 2019; Tenney\net al., 2019; Burns et al., 2022; Nanda et al., 2023) suggests the linear representation hypothesis: models\nencode features as linear subspaces of component activations that can be arbitrarily rotated with\nrespect to the standard basis (due to phenomena like superposition, polysemanticity (Arora et al.,\n2018; Elhage et al., 2022) and lack of privileged bases (Smolensky, 1986; Elhage et al., 2021)).\n   Motivated by this, recent works such as Geiger et al. (2023b); Wu et al. (2023); Lieberum\net al. (2023) have explored subspace activation patching: a generalization of activation patching that\noperates only on linear subspaces of features (as low as 1-dimensional) rather than patching entire\ncomponents.\n   Our work contributes to this research direction by demonstrating both (i) a common illusion to\navoid when looking for such subspaces and (ii) a detailed case study of successfully localizing a\nbinary feature to a 1-dimensional subspace.\n\n\n2.2  Interpretability Illusions\n\nDespite the promise of interpretability, it is difficult to be rigorous and easy to mislead yourself. A\ncommon theme in the field is identifying ways that techniques may lead to misleading conclusions\nabout model behavior (Lipton, 2016). In computer vision, Adebayo et al. (2018) show that a popular\nat the time class of pixel attribution methods is not sensitive to whether or not the model used to\nproduce is has actually been trained or not. In Geirhos et al. (2023), the authors show how a circuit\ncan be hardcoded into a learned model so that it fools interpretability methods; this bears some\nsimilarity to our illusion, especially its fact editing counterpart.\n   In natural language processing, Bolukbasi et al. (2021) show that interpreting single neurons\nwith maximum activating dataset examples may lead to conflicting results across datasets due\nto subtle polysemanticity (Elhage et al., 2022). Recently, McGrath et al. (2023) demonstrated that\nfull-component activation patching in large language models is vulnerable to false negatives\ndue to (ordinariliy dormant) backup behavior of downstream components that activates when a\n\n\n\n                                          5\n\ncomponent is ablated.\n  We contribute to the study of interpretability illusions by demonstrating a new kind of illusion\nwhich can arise when intervening on model activations along arbitrary subspaces, by demonstrating\nit in two real-world scenarios, and providing recommendations on how to avoid it.\n\n\n2.3  Factual Recall\n\nA well-studied domain for discovering and intervening on learned representations is the localiza-\ntion and editing of factual knowledge in language models (Wallat et al., 2020; Meng et al., 2022b; Dai\net al., 2022; Geva et al., 2023; Hernandez et al., 2023). A work of particular note is Meng et al. (2022a),\nwhich localizes factual information using a variation of full-component activation patching, and\nthen edits factual information with a rank-1 intervention on model weights. However, recent work\nhas shown that rank-1 editing can work even on weights where the fact supposedly is not encoded\n(Hase et al., 2023), and that editing a single fact often fails to have its expected common-sense effect\non logically related downstream facts (Cohen et al., 2023; Zhong et al., 2023).\n  We contribute to this line of work by showing a formal and empirical connection between\nactivation patching along 1-dimensional subspaces and rank-1 model editing. In particular, rank-1\nmodel edits can work by creating a dormant pathway of an MLP layer, regardless of whether the\nfact is stored there. This provides a mechanistic explanation for the discrepancy observed in Hase\net al. (2023).\n\n\n3 A Conceptual View of the Illusion\n\n3.1  Preliminaries: (Subspace) Activation Patching\n\nActivation patching (Vig et al., 2020; Geiger et al., 2020; Wang et al., 2023; Chan et al., 2022) is\nan interpretability technique that intervenes upon model components, forcing them to take on\nvalues they would have taken if a different input were provided. For instance, consider a model\nthat has knowledge of the locations of famous landmarks, and completes e.g. the sentence A =\n‘The Eiffel Tower is in’ with ‘Paris’.\n  How can we find which components of the model are responsible for knowing that ‘Paris’ is\nthe right completion? Activation patching approaches this question by\n\n   (i) Running the model on A;\n\n  (ii) Storing the activation of a chosen model component C, such as the output of an attention head,\n     the hidden activations of an MLP layer, or an entire residual stream (a.k.a. bottleneck) layer;\n\n (iii) Running the model on e.g. B = ‘The Colosseum is in’, but with the activation of C taken from\n     A.\n\n     If we find that the model outputs ‘Paris’ instead of ‘Rome’ in step (iii), this suggests that\ncomponent C is important for the task of recalling the location of a landmark.\n   The linear representation hypothesis proposes that linear subspaces of vectors will be the most\ninterpretable model components. To search for such subspaces, we can adopt a natural general-\nization of full component activation patching, which only replaces the values of a subspace U\n(while leaving the projection on its orthogonal complement U⊥unchanged). This was proposed in\nGeiger et al. (2023b), and closely related variants appear in Turner et al. (2023); Nanda et al. (2023);\nLieberum et al. (2023).\n\n\n                                           6\n\n   For the purposes of exposition, we now restrict our discussion to activation patching of a\n1-dimensional subspace (i.e., a direction) spanned by a unit vector v (i.e., ∥v∥2 = 1). We remark that\nthe illusion also applies to higher-dimensional subspaces (see Appendix A.1 for theoretical details;\nlater on, in Appendix B.6, we also show this empirically for the IOI task). If actA, actB ∈Rd are\nthe activations of a model component C on examples A, B and pA = v⊤actA, pB = v⊤actB are their\nprojections on v, patching from A into B along v results in the patched activation\n\n                                      actpatchedB   = actB + (pA −pB)v.                                   (1)\n\nFor a concrete scenario motivating such a patch, consider a discrete binary feature used by the\nmodel to perform a task, and prompts A, B which only differ in the value of this feature. A 1-\ndimensional subspace can easily encode such a feature (and indeed we explore an example of this\nin great detail in Sections 4 and 5).\n\n\n3.2  Intuition for the Illusion\n\nWhat would make activation patching a good attribution method? Intuitively, an equivalence is\nneeded: an activation patch should work if and only if the component/subspace being patched\nis indeed a faithful to the model’s computation representation of the concept we seek to localize.\nRevisiting Equation 1 with this in mind, it is quite plausible that, if v indeed encodes a binary\nfeature relevant to the task, the patch will essentially overwrite the feature with its value on A, and\nthis would lead to the expected downstream effect on model predictions3.\n   Going in the other direction of the equivalence, when will the update in Equation 1 change the\nmodel’s output in the intended way? Intuitively, two properties are necessary:\n\n  • correlation with the concept: v must be activated differently by the two prompts. Otherwise,\n     pA ≈pB, and the patch has no effect;\n\n  • potential for changing model outputs: v must be ‘causally connected’ to the model’s outputs;\n      in other words, it should be the case that changing the activation along v can at least in some\n      cases lead to a change in the next-token probabilities output by the model. Otherwise, if, for\n      instance, v is in the nullspace of all downstream model components, changing the activation’s\n      projection along v alone won’t have any effect on the model’s predictions.\n     For example, if the component C we are patching is the post-nonlinearity activation of an MLP\n       layer, the only way this activation affects the model’s output is through matrix multiplication\n     with a down-projection Wout. So, if v ∈ker Wout, we will have\n\n                         WoutactpatchedB   = WoutactB + (pA −pB)Woutv = WoutactB.\n\n      In other words, the activation patch leads to the exact same output of the MLP layer as when\n     running the model on B without an intervention. So, the patch will leave model predictions\n     unchanged.\n\n    3It is in principle possible that, even if the value of the feature is overwritten, this has no effect on model predictions.\nFor example, it is possible that v is not the only location in the model’s computation where this feature is represented;\nor, it may be that there are backup components that are normally inactive on the task, but activate when the value of\nthe subspace v is changed, as explored in McGrath et al. (2023). Such scenarios are beyond the scope of this simplified\ndiscussion.\n\n\n\n\n                                          7\n\nThe crux of the illusion is that v may obtain each of the two properties from two ‘unrelated’\ndirections in activation space (as shown in Figure 1) which ‘happen to be there’ as a side effect of\nlinear algebra. Specifically, we can form\n\n                                    1\n                                      villusory = √  (vdisconnected + vdormant) ,                               (2)\n                                     2\nfor orthogonal unit vectors v⊤disconnectedvdormant = 0 such that\n\n  • vdisconnected is a causally disconnected direction in activation space: it distinguishes between\n      the two prompts, but is in the nullspace of all downstream model components (e.g., a vector\n      in ker Wout for an MLP layer with down-projection Wout);\n\n  • vdormant is a dormant direction in activation space: it can in principle steer the model’s output\n      in the intended way, but is not activated differently by the two prompts (in other words,\n    v⊤dormantactA ≈v⊤dormantactB).\n\n   To illustrate this algebraically, consider what happens when we patch along villusory. We have\n\n                                     1\n                pA = v⊤illusoryactA = √  v⊤disconnectedactA + v⊤dormantactA\n                                      2\n                                     1\n                   pB = v⊤illusoryactB = √  v⊤disconnectedactB + v⊤dormantactB\n                                      2\nBy assumption, v⊤dormantactB = v⊤dormantactA. Thus,\n\n                               1\n                   pA −pB = √  v⊤disconnectedactA −v⊤disconnectedactB\n                                2\n\nso the patched activation is\n\n                               1\n                 actpatchedB   = actB + √  v⊤disconnectedactA −v⊤disconnectedactB   villusory.\n                                2\n\nIf for example villusory is in the space of post-nonlinearity activations of an MLP layer with down-\nprojection matrix Wout, and vdisconnected ∈ker Wout, the new output of the MLP layer after the patch\nwill be\n\n                                 1\n         WoutactpatchedB   = WoutactB + √  v⊤disconnectedactA −v⊤disconnectedactB  Woutvillusory\n                                  2\n\n                           1\n         = WoutactB +   v⊤disconnectedactA −v⊤disconnectedactB  Woutvdormant                (3)                           2\nwhere we used that Woutvdisconnected = 0. From this equation, we see that, by patching along\nthe sum of a disconnected and dormant direction, the variation in activation projections on the\ndisconnected part (which we assume is significant) ‘activates’ the dormant part: we get a new\ncontribution to the MLP’s output (along Woutvdormant) which can then possibly influence model\noutputs. This contribution would not exist if we patched only along vdisconnected (because it would\nbe nullified by Wout) or vdormant (because then we would have pA ≈pB).\n  We make the concepts of causally disconnected and dormant subspaces formal in Subsection\n3.5. We also remark that, under the assumptions of the above discussion, the optimal illusory\npatch will provably combine the disconnected and dormant directions with equal weight √1 as in\n                                                                                                              2\nEquation 2; the proof is given in Appendix A.2.\n\n\n                                          8\n\n3.3  The Illusion in a Toy Model\n\nWith these concepts in mind, we can construct a distilled example of the illusion in a toy (linear)\nneural network. Specifically, consider a network M that takes in an input x ∈R, computes a\nthree-dimensional hidden representation h = xw1, and then a real-valued output y = wT2 h. Define\nthe weights to be\n\n                       w1 = (1, 0, 1) ,  and  w2 = (0, 2, 1)\n\nand observe that M (x) = x, i.e. the network computes the identity function:\n\n                 x  7→  h = (x, 0, x)  7→   y = 0 × x + 2 × 0 + 1 × x = x.\n\nThis network is illustrated in Figure 2. We can analyze the 1-dimensional subspaces (directions)\nspanned by each of the three hidden activations:\n\n  • the h1 direction is causally disconnected: setting it to any value has no effect on the output;\n\n  • the h2 direction is dormant: it is constant (always 0) on the data, but setting it to some other\n     value will affect the model’s output;\n\n  • the h3 direction mediates the signal through the network: the input x is copied to it, and is in\n     turn copied to the output4.\n\nAs expected, patching along the direction h3 overwrites the value of the x feature (which in this\nexample is identical to the input). That is, patching along h3 from x′ into x makes the network\noutput x′ instead of x.\n   However, patching along the sum of the causally disconnected direction h1 and the dormant\ndirection h2 represented by the unit vector villusory =  √12, √12, 0  has the same effect: using\nEquation 1, patching from x′ into x along villusory results in the hidden activation\n\n                                               ⊤                                        x + x′  x′ −x\n                                    hpatched =               ,            , x\n                                          2     2\n\n                                                                 x′−xwhich when multiplied with w2 gives the final output 2 ×   2 + 1 × x = x′.\n\n3.4  Detecting the illusion in practice\n\nHow can we tell if this kind of phenomenon occurs for a given subspace activation patch? Given a\nsubspace spanned by a unit vector v, suppose that activation patching along this subspace has an\neffect on model outputs consistent with changing the property that varies between the examples\nbeing patched. We can attempt to decompose it orthogonally into a causally disconnected part\nand a dormant part, and argue that each of these parts has the properties described in the above\nsections.\n\n  4An important note on this particular example is that the distinction between causally disconnected, dormant and\nfaithful to the computation directions is artificial, and here it is only used for exposition. In particular, we show in\nAppendix A.3 that re-parametrizing the hidden layer of the network via a rotation makes villusory take the role of the\nfaithful direction e3, and the two other (rotated) basis vectors become a disconnected/dormant pair. By contrast, when\nwe exhibit the illusion in real-world scenarios, a reparametrization of this kind would need to combine activations\nbetween different model components, such as MLP layers and residual stream activations. We return to this point in\nSection 8.\n\n\n                                          9\n\n                                               h1 ←x\n\n                        ×1\n\n\n                           x               h2 ←0              y ←x                                       ×2\n\n\n                        ×1             ×1\n\n                                               h3 ←x\n\n\n\nFigure 2: A network M illustrating the illusion. The network computes the identity function:\nM(x) = x. The activation of the input, output and each hidden neuron for a generic input x are\nshown in the circles, with arrows indicating the weight of the connections (no arrow means a\nweight of 0). The hidden unit h3 stores the value of the input and passes this to the output, while\nthe unit h2 is dormant and h1 is disconnected from the output. However, activation patching the\n1-dimensional linear subspace spanned by the sum of the h1 and h2 basis vectors (defined by the\nunit vector v = ( √12, √12, 0)) has the same effect on model behavior as patching just the unit h3.\n\n\n    Specifically, when v is in the post-GELU activations of an MLP layer in a transformer with down-\nprojection Wout (almost all examples in this paper are of this form), it is clear that the orthogonal\nprojection of v on ker Wout is causally disconnected. This suggests writing v = vnull + vrow where\nvnull ∈ker Wout is the orthogonal projection on ker Wout, and vrow is the remainder, which is in\n(ker Wout)⊥, the rowspace of Wout. Using this decomposition, we can perform several experiments:\n\n  • compare the strength of the patch to patching along the subspace spanned by vrow alone,\n     obtained by removing the causally disconnected part of v. If vrow is indeed dormant as we\n     hope to show, this patch should have no effect on model outputs; in reality, vrow may only be\n     approximately dormant, so the patch may have a small effect. Conversely, if this patch has an\n      effect similar to the original patch along v, this is evidence against the illusion;\n\n  • check how dormant vrow is compared to vnull by comparing the spread of projections of the\n     examples on both directions.\n\n  We use these experiments, as well as others, throughout the paper in order to rule out or confirm\nthe illusion.\n\n\n3.5  Formalization of Causally Disconnected and Dormant Subspaces\n\nFor completeness, in this subsection we give a (somewhat) formal treatment of the intuitive\nideas introduced in the previous subsection. Readers may also want to consult Appendix A.1 for\nbackground on patching higher-dimensional subspaces, which is used to define these concepts.\n   Let M : X →O be a machine learning model that on input x ∈X outputs a vector y ∈O of\nprobabilities over a set of output classes. Let D be a distribution over X , and C be a component of\n\n\n                                          10\n\nM, such that for x ∼D the hidden activation of C is a vector cx ∈Rd. For a subspace UC ⊂Rd, we\nlet ux be the orthogonal projection of cx onto UC. Finally, let MUC←uy(x) be the result of running\nM with the input x and setting the subspace UC patched to uy.\n  We say UC is causally disconnected if MUC←u′(x) = M(x) for all u′ ∈UC. In other words,\nintervening on the model by setting the orthogonal projection of C’s activation on UC to any other\nvalue does not change the model’s outputs. For a concrete example of a causally disconnected\nsubspace, consider an MLP layer in a transformer model with an output projection matrix Wout;\nthen, ker Wout is a causally disconnected subspace of the hidden (post-nonlinearity) activations of\nthe MLP layer.\n  We say UC is dormant if MUC←uy(x) ≈M(x) with high probability over x, y ∼D, but there\nexists u′ ∈Rd such that MUC←u′(x) is substantially different from M(x) (e.g., significantly changes\nthe model’s confidence on the task’s answer). In other words, a dormant subspace is approximately\ncausally disconnected when we patch its value using activations realized under the distribution D,\nbut can have substantial causal effect if set to other values.\n\n\n4  The Illusion in the Indirect Object Identification Task\n\n4.1  Preliminaries\n\nIn Wang et al. (2023), the authors analyze how the decoder-only transformer language model GPT-2\nSmall (Radford et al., 2019) performs the indirect object identification task. In this task, the model is\nrequired to complete sentences of the form ‘When Mary and John went to the store, John gave a\nbottle of milk to’ (with the intended completion in this case being ‘ Mary’). We refer to the repeated\nname (John) as S (the subject) and the non-repeated name (Mary) as IO (the indirect object). For\neach choice of the IO and S names, there are two patterns the sentence can have: one where the IO\nname comes first (we call these ‘ABB examples’), and one where it comes second (we call these\n‘BAB examples’). Additional details on the data distribution, model and task performance are given\nin Appendix B.1.\n   Wang et al. (2023) suggest the model uses the algorithm ‘Find the two names in the sentence,\ndetect the repeated name, and predict the non-repeated name’ to do this task. In particular, they find\na set of four heads in layers 7 and 8 – the S-Inhibition heads – that output the signal responsible\nfor not predicting the repeated name. The dominant part of this signal is of the form ‘Don’t attend\nto the name in first/second position in the first sentence’ depending on where the S name appears\n(see Appendix A in Wang et al. (2023) for details). In other words, this signal detects whether the\nexample is an ABB or BAB example. This signal is added to the residual stream5 at the last token\nposition, and is then picked up by another class of heads in layers 9, 10 and 11 – the Name Mover\nheads – which incorporate it in their queries to shift attention to the IO name and copy it to the last\ntoken position, so that it can be predicted (Figure 3).\n\n\n4.2  Finding Subspaces Mediating Name Position Information\n\nHow, precisely, is the positional signal communicated? In particular, ‘don’t attend to the first/second\nname’ is plausibly a binary feature represented by a 1-dimensional subspace. In this subsection, we\npresent methods to look for such a subspace.\n\n   5We follow the conventions of Elhage et al. (2021) when describing internals of transformer models. The residual\nstream at layer k is the sum of the output of all layers up to k −1, and is the input into layer k.\n\n\n\n\n                                          11\n\n   Gradient of name mover attention scores.                                                Baseline    Patched\nAs shown in Wang et al. (2023), the three name                                                  962  RudiHim       924 Emmathem\nmover heads identified therein will attend to                                                   1 …         3 …\n                                                                                                                              1 Emma      1  Rudi\none of the names, and the model will predict\nwhichever name is attended to. The position\nfeature matters mechanistically by determining                                                               +\nwhether they attend to IO over S. This moti-\nvates us to consider the gradient vgrad of the                                                             MoverName\ndifference of attention scores of these heads on\nthe S and IO names with respect to the resid-                                                              +\nual stream at the last token, right after layer 8.                                                 W୭୳୲        gelu          ୰ୣୱ୧ୢ\nWe choose this layer because it right after the                        ୑୐୔        W୧୬\nS-Inhibition heads (in layers 7 and 8) and before\n                                                                                                                                           +\nthe name mover heads (in layers 9 and 10); see\nFigure 3. This gradient is the direction in the\n                                                                                                                                                                     S-\nspace of residual stream activations at this lo-                                                           Inhib\ncation that maximally shifts attention between\nthe two names (per unit ℓ2 norm), so we expect\nit to be a strong mediator of the position signal.\nImplementation details are given in Appendix     …                                                              Rudi and              says         toB.2.                                                                   Emma     Emma\n    Importantly, the transformation from resid-\nual stream activations to attention scores is an\napproximately linear map: it consists of layer                                              Figure 3: Schematic of the IOI circuit and locations\nnormalization followed by matrix multiplica-                                                of key interventions. As argued in Wang et al.\ntion. Layer normalization is a linear operation                                                   (2023), GPT2-Small predicts the correct name by\nmodulo the scaling step, and empirically, the                                                  S-inhibition heads writing positional information\nscales of different examples in a trained model                                                  to the residual stream, which is used by the name\nat inference time are tightly concentrated (see                                       movers to copy the non-duplicated name (green\nalso ‘Handling Layer Normalization’ in Elhage                                                arrows). Location of subspace interventions vMLP\net al. (2021)). This justifies the use of the gra-                                            (analyzed in Subsection 4.4) and vresid (analyzed\ndient – which is in general only locally mean-                                                in Section 5) are marked. Patching the illusory\ningful – as a direction in the residual stream                                          subspace vMLP adds a new path (red) along the\nglobally meaningful for the attention scores of                                               established one that is used to flip positional in-\nthe name mover heads.                                            formation when patched.\n   Distributed alignment search. We can also\ndirectly optimize for a direction that mediates\nthe position signal. This is the approach taken by DAS (Geiger et al., 2023b). In our context, DAS\noptimizes for an activation subspace which, when activation patched from prompt B into prompt\nA, makes the model behave as if the relative position of the IO and S names in the sentence is as in\nprompt B. Specifically, if we patch between examples where the positions of the two names are\nthe same, we optimize for a patch that maximizes the difference in predicted logits for the IO and\nS names. Conversely, if we patch between examples where the positions of the two names are\nswitched, we optimize to minimize this difference. This approach is based purely on the model’s\npredictions, and does not make any assumptions about its internal computations.\n  We let vMLP and vresid be 1-dimensional subspaces found by DAS in the layer 8 MLP activations\nand layer 8 residual stream output at the last token, respectively (see Figure 3). Both of these\nlocations are between the S-Inhibition and Name Mover heads; however, Wang et al. (2023) did not\n\n\n                                          12\n\nfind any significant contribution from the MLP layer, making it a potential location for our illusion.\nImplementation details are given in Apendix B.3.\n\n\n4.3  Measuring Patching Success via the Logit Difference Metric\n\nIn our experiments, we perform all patches between examples that only differ in the variable we\nwant to localize in the model, i.e. the position of the S and IO names in the first sentence. In other\nwords, we patch from an example of the form ‘Then, Mary and John went to the store. John gave\na book to’ (an ABB example) into the corresponding example ‘Then, John and Mary went to the\nstore. John gave a book to’ (a BAB example), and vice-versa. Our activation patches have the goal\nof making the model output the S name instead of the IO name.\n   Accordingly, we use the logit difference between the logits assigned to the IO and S names as\nour main measure of how well a patch performs. We note that the logit difference is a meaningful\nquantifier of the model’s confidence for one name over the other (it is equal to the log-odds between\nthe two names assigned by the model), and has been extensively used in the original IOI circuit\nwork Wang et al. (2023) to measure success on the IOI task.\n   Given a prompt x from the IOI distribution, let logitIO (x) , logitS (x) denote the last-token logits\noutput by the model on input x, for the IO and S names in the prompt x respectively (note that\nin our IOI distribution, all names are single tokens in the vocabulary of the model). The logit\ndifference\n\n                                 logitdiff (x) := logitIO (x) −logitS (x)\n\nwhen x is sampled from the IOI distribution is > 0 for almost all examples (99 + %), and is on\naverage ≈3.5 (for this average value, the probability ratio in favor of the IO name is e3.5 ≈33).\n    Similarly, for an activation patching intervention ι, let logitι(x←x′)IO    (x), logitι(x←x′)S     (x) denote the\nlast-token logits output by the model when run on input x but patching from x′ using ι. The logit\ndifference after intervening via ι is thus\n                                                  logitι(x←x′)IO    (x) −logitι(x←x′)S     (x)                          logitdiffι(x←x′) (x) :=\n\nOur main metric is the average fractional logit difference decrease (FLDD) due to the intervention\nι, where\n\n                                    logitdiff(x) −logitdiffι(x←x′)         logitdiffι(x←x′)\n             FLDDι(x←x′)(x) :=                = 1 −                              (4)                                             logitdiff(x)                   logitdiff(x)\n\nThe average FLDD is 0 when the patch does not, on average, change the model’s log-odds. The\nmore positive FLDD is, the more successful the patch, with values above 100% indicating that\nthe patch more often than not makes the model prefer the S name over the IO name. Finally, an\naverage FLDD below 0% means that the patch on average helps the model do the task (and thus\nthe patch has failed).\n  We also measure the interchange accuracy of the intervention: the fraction of patches for which\nthe model predicts the S (i.e., wrong) name for the patched run. This is a ‘hard’ 0-1 counterpart to\nthe FLDD metric.\n\n\nWhy prefer the FLDD metric to interchange accuracy? We argue that our main metric, which\nis based on logit difference (Equation 4), is a better reflection of the success of a patch than the\naccuracy-based interchange accuracy. Specifically, there are practical cases (e.g. the results in\n\n\n                                          13\n\n       160         Input\n                ABB (no intervention)\n                BAB (no intervention)       140                    patch BAB -> ABB\n                    patch ABB -> BAB\n       120                                       Patching          FLDD    Interchange\n                                            subspace                       accuracy       100   examples\n                                                         full MLP              -8%       0.0% of   80\n                                        vMLP                 46.7%       4.2%\n        60\n                                        vMLP rowspace        13.5%       0.2%\n        40                                   vMLP nullspace        0%       0.0%  Number\n        20                                                 full residual stream    123.6%      54.8%\n                                                            vresid                140.7%      74.8%\n         0\n               10.0   7.5   5.0   2.5   0.0   2.5   5.0   7.5             vresid rowspace       127.5%      63.1%\n            Activation projection                   vresid nullspace        13.9%       0.4%\n                                                    vgrad                111.5%      45.1%\nFigure 4:  Projections of the output of        vgrad rowspace       106.47%      40.6%\nthe MLP layer on the gradient direc-        vgrad nullspace         2.2%       0.0%\ntion vgrad before (blue/orange) and af-\nter (green/red) the activation patch along    Table 1: Effects of activation patching of full compo-\nvMLP. In the legend, ‘ABB’ denotes exam-   nents and 1-dimensional subspaces on the IOI task:\nples where the IO name comes before the    fractional logit difference decrease (FLDD, higher\nS name, and ‘BAB’ the other kind of ex-   means more successful patch; 0% means no change)\namples.                            and interchange accuracy (fraction of predictions\nWhile before the patch the contribution    flipped; higher means more successful patch).\nof the MLP layer to the causally relevant   The first 5 interventions are described in more detail\ndirection vgrad distinguishes between val-    in Section 4, and the next 6 in Section 5.\nues of the IO position in the prompt, after   An FLDD metric of > 100% indicates that the patch\nthe patch there is a strong distinction (in     is more successful than not on average; however,\nthe opposite direction). This shows that   an FLDD of ≈50% is still significant, even if the\nthe patch activates a potential mediator    associated interchange accuracy may be ≈0%. See\nof this feature that is normally dormant,   Subsection 4.3 for more on interpreting the FLDD\ntaking model activations off-distribution.    metric.\n\n\nSubsection 4.4) in which an intervention consistently achieves FLDD ≈50%, even though the\ninterchange accuracy is ≈0%. In practice, circuits often have multiple components contributing to\nthe same signal (including the IOI circuit found in Wang et al. (2023)). So a single non-residual-\nstream component consistently responsible for shifting 50% of the model’s log-odds towards\nanother prediction is significant (even more so for a low-dimensional subspace of the component).\nIndeed, even if this component’s contribution alone is insufficient to cause the predicted token to\nchange, three such components would robustly change the prediction.\n\n4.4  Results: Demonstrating the Illusion for the vMLP Direction\n\nWe now show that patching the vMLP direction exhibits the illusion from Section 3. By contrast,\nwe revisit vgrad and vresid in Section 5, where we show that both are representations of the name\nposition information that are highly faithful to the model’s computation.\n\n\n\n                                          14\n\nMethodology and interventions considered  To contextualize the effect of the vMLP patch, we\ncompare it to several additional subspace- and component-level activation patching interventions:\n\n  • full MLP: patching the full value of the hidden activation of the 8-th MLP layer at the last\n      token.\n\n  • vMLP: patching along the 1-dimensional subspace spanned by the direction vMLP found in\n     Subsection 4.2.\n\n  • vMLP nullspace: patching along the 1-dimensional subspace spanned by the causally dis-\n     connected component of vMLP. This is the orthogonal projection vnullspaceMLP    of vMLP on the\n     nullspace ker Wout of the down-projection Wout of the MLP layer. Note that Wout ∈R768×3072,\n     so its kernel occupies at least 2304 dimensions, or 3/4 of the total dimension of the space of\n    MLP activations.\n\n  • vMLP rowspace: patching along the 1-dimensional subspace spanned by causally relevant\n    component of vMLP. This is the orthogonal projection vrowspaceMLP    of vMLP on the rowspace of\n      Wout. Note that we have the orthogonal decomposition\n                               vMLP = vnullspaceMLP  + vrowspaceMLP      .\n\n  • full residual stream: patching the entire activation of the residual stream at the last token\n      after layer 8 of the model. This is indicated as the location of vresid in Figure 3.\n\n\nResults.  Metrics are shown in Table 1. In particular, we confirm the mechanics of the illusion are\nat play through the following observations.\n\n\n   The causally disconnected component of vMLP drives the effect.  While patching the vMLP\ndirection has a significant effect on the FLDD metric (46.7%), this effect is greatly diminished\nwhen we remove the component of vMLP in ker Wout whose activations are (provably) causally\ndisconnected from model predictions (13.5%), or when we patch the entire MLP activation (−8%,\nactually increasing confidence). By contrast, performing analogous ablations on vresid leads to\nroughly the same numbers for the three analogous interventions (140.7%/127.5%/123.6%; we refer\nthe reader to Section 5 for details on the vresid experiments).\n\n   Patching vMLP activates a dormant pathway through the MLP.  To corroborate these findings,\nin Figure 4, we plot the projection of the MLP layer’s contribution to the residual stream on the\ngradient direction vgrad before and after patching, in order to see how it contributes to the attention\nof name mover heads. We observe that in the absence of intervention, the MLP output is weakly\nsensitive to the name position information, whereas after the patch this changes significantly.\n\n\n   Further validations of the illusion. We observe that the disconnected-dormant decomposition\nfrom the illusion approximately holds: the causally disconnected component of vMLP (the one in\nker Wout) is significantly more discriminating between ABB and BAB examples than the component\nin (ker Wout)⊤, which is the one driving the causal effect (Figure 5); in this sense, the causally\nrelevant component is ‘dormant’ relative to the causally diconnected one6.\n\n   6The projection of vMLP onto ker Wout is substantial:  it has norm ≈0.65, and the orthogonal projection onto\n(ker Wout)⊤has norm ≈0.75 (as predicted by our model, the two components are approximately equal in norm;\nsee Appendix A.2).\n\n\n                                          15\n\n   While the contribution of the vMLP patch to the FLDD metric may appear relatively small, and\nthe interchange accuracy of this intervention is very low, in Appendix B.4 we argue that this is\nsignificant for a single component.\n  A potential concern when evaluating these results is overfitting by DAS. In our experiments, we\nalways evaluate trained subspaces on a held-out test dataset which uses different names, objects,\nplaces and templates for the sentences; this makes sure that we learn a general (relative to our IOI\ndistribution) subspace representing position information and not a subspace that only works for\nparticular names or other details of the sentence. We investigated overfitting in DAS further in\nAppendix B.7, and found that when DAS is trained on a dataset with a small number of names,\noverfitting is a real concern. However, the extent of overfitting is not such that DAS works in layers\nof the model where a generalizing DAS solution can also be found.\n   Another potential concerns is that the model could be somehow representing the position\ninformation in the MLP layer in a higher-dimensional subspace, and that our 1-dimensional\nintervention is perhaps not fit to illuminate the properties of that larger representation. In Appendix\nB.6, we show that the illusion occurs when patching 100-dimensional subspaces as well, and the\nquantitative effect of the illusion is just a little stronger than that for 1-dimensional subspaces (as\nmeasured by the FLDD metric).\n    Finally, in Appendix B.5, we show that we can find a direction within the post-gelu activations\nthat has an even stronger effect on the model’s behavior, even when we replace the MLP weights\nwith random matrices.\n\n                Projection onto nullspace component       Projection onto rowspace component\n               140                       pattern     500                       pattern\n                                  ABB                             ABB\n               120                       BAB                             BAB\n                                                                   400\n               100                   examples\n     of      80                                                 300\n\n                60\n                                                                   200              Number      40\n                                                                   100\n                20\n\n                 0                                                   0\n                     1.5   1.0   0.5  0.0   0.5   1.0   1.5   2.0   2.5   3.0        1.5   1.0   0.5  0.0   0.5   1.0   1.5   2.0   2.5   3.0\n\n\n                   Activation projection                  Activation projection\n\nFigure 5: Projections of dataset examples onto the two (normalized to unit ℓ2 norm) components of\nthe illusory patching direction found in MLP8: the nullspace (irrelevant) component (left), and the\nrowspace (dormant) component (right).\n\n\n\n\n\n                                          16\n\n5  Finding and Validating a Faithful Direction Mediating Name Position\n   in the IOI Task\n\nAs a counterpoint to the illusion, in this section we demonstrate a success case for subspace\nactivation patching, as well as for DAS as a method for finding meaningful subspaces, by revisiting\nthe directions vgrad and vresid defined in Subsection 4.2, and arguing they are faithful to the model’s\ncomputation to a high degree.\n    Specifically, we subject these directions to the same tests we used for the illusory direction\nvMLP, and arrive at significantly different results. Through these and additional validations, we\ndemonstrate that these directions possess the necessary and sufficient properties of a successful\nactivation patch – being both correlated with input variation and causal for the targeted behavior –\nin a way that does not rely on a large causally disconnected component for the effect.\n\n\n5.1  Ruling Out the Illusion\n\nIntuitively, the main property of vresid we want to establish in order to rule out the illusion is that it\nis simultaneously (1) strongly discriminating between ABB and BAB examples (i.e., projections of\nactivations on vresid separate these two classes well), and (2) is highly aligned with the direction\nvgrad that downstream model components read this information from in order to put attention\non the IO name and not the S name. To this end, we define a notion of causally disconnected\ncomponent for vresid, and we show that removing it does not diminish the effect of the patch;\nwe further show that vresid and vgrad are quite similar, and that vgrad is also strongly activated by\nposition information.\n\n\nWhat is the causally (dis)connected subspace of the residual stream?  While for an MLP layer\nit is clear that ker Wout is the subspace of post-GELU activations which is causally disconnected\nfrom model outputs, the residual stream after layer 8 has no subspace which is simultaneously in\nthe kernel of all downstream model components, or even of all the query matrices of downstream\nattention heads (we checked this empirically).\n   To overcome this, recall from Section 4 that Wang et al. (2023) argued that the three Name\nMover heads in layers 9 and 10 are mostly responsible for the IOI task specifically. Let WNMQ  ∈\nR(3×64)×768 = R192×768 be the stacked query matrices of the three name mover heads (which are\n                                             ⊤\nfull-rank). We use the 192-dimensional subspace  ker WNMQ     as a proxy for the causally relevant\nsubspace7 of the residual stream at the last token position after layer 8.\n   To further narrow down the precise subspace read by the Name Mover heads, we also compare\nvresid with the gradient vgrad, which is the direction that the Name Mover’s attention on the IO vs.\nS name is most sensitive to.\n\n\nResults.  In Table 1, we report the fractional logit difference decrease (FLDD, recall Subsection\n4.3) and interchange accuracy when patching vresid and vgrad, as well as their components along\n\n   7Note that, while technically all attention heads in layers 9, 10 and 11 read information from the residual stream\nafter layer 8, using their collective query matrices instead of just the name movers would lead to a vacuous concept of\na causally relevant subspace, because their collective query matrices’ rowspaces span the entire residual stream. As a\n                                q 192    1                     ⊤rough baseline, a random isotropic unit vector would have on average    768 = 2 of its ℓ2-norm in  ker WNMQ         . We\nalso note that this is on par with the decomposition of vMLP we considered in Section 4, where ker Wout occupied 3/4 of\nthe dimension of the full space of activations.\n\n\n                                          17\n\n                                                       ⊤\nker WNMQ  (denoted ‘nullspace’) and its orthogonal complement  ker WNMQ     (denoted ‘rowspace’).\nWe observe that the non-nullspace metrics are broadly similar8; in particular, removing the causally\ndisconnected component of vresid does not significantly diminish the effect of the patch in terms of\nthe logit difference metrics (as it does for vMLP).\n   Furthermore, we find that the cosine similarity between vresid and vgrad is ≈0.78, which is\nsignificant (the baseline for random vectors in the residual stream is on the order of √ 1  ≈0.03).\n                                                                                                        768\n                                                            ⊤\nBoth vresid and vgrad have a significant fraction of their norm in the  ker WNMQ     subspace (91%\nand 98%, respectively). These results suggest that this vresid and vgrad are highly similar directions,\nand that they’re both strongly causally connected to the model’s output.\n    In Figure 6, we also find that both directions are strongly discriminating between ABB and BAB\nexamples.\n\n\nDiscussion. A key observation about the residual stream at the last token is that it is a full\nbottleneck for the model’s computation over the last token position: all updates to that position are\nadded to it. This provides another viewpoint on why the successful patches we find don’t rely on a\ndormant subspace: there can be no earlier model component that activates the directions we find in\na way that skips over the patch via a residual connection (unlike for vMLP). Indeed, in Figure 20 in\nAppendix C we show that the vresid direction gets written to by the S-Inhibition heads.\n\n5.2  Additional Validations\n\nIn Appendix C, we further validate these directions’ faithfulness to the computation of the IOI\ncircuit from Wang et al. (2020) by finding the model components that write to them and studying\nhow they generalize on the pre-training distribution (OpenWebText); representative samples\nannotated with attention scores are shown in Figures 23, 21, 22 in Appendix C.\n\n\n6  Factual Recall\n\nThis section has two major goals. One is to show that the interpretability illusion can also be\nexhibited for the factual recall capability of language models, a much broader setting than the\nIOI task. The other is to exhibit in practice an approximate equivalence between two seemingly\ndifferent interventions: rank-1 weight editing and interventions on 1-dimensional subspaces of\nactivations. We do this in several complementary ways:\n\n   1. we show that DAS (Geiger et al., 2023b) finds illusory 1-dimensional subspace patches that\n     change factual recall (e.g., to make a model complete ‘The Eiffel Tower is in’ with ‘ Rome’\n     instead of ‘ Paris’). The patches found strongly update the model’s confidence towards the\n      false completion, but the effect disappears when the causally disconnected component of the\n     subspace is removed, or when the entire MLP activation containing the subspace is patched\n      instead.\n\n   2. we show that for a wide range of layers in the middle of the model (GPT2-XL Radford et al.\n      (2019)), rank-1 fact editing using the ROME method (Meng et al., 2022a) is approximately\n\n  8We observe that the vresid patch is more successful than the vgrad patch; we conjecture that this is due to vresid being\nable to contribute to all downstream attention heads, not just the three name-mover heads. In particular, the original\nIOI paper Wang et al. (2023) found that there is another class of heads, Backup Name Movers, which act somewhat like\nName Mover heads.\n\n\n                                          18\n\n                                                                    70                                   pattern       pattern\n                60                       ABB         ABB\n                                  BAB        60   BAB\n                50\n                                                                    50                    examples                40\n     of                                                          40\n\n                30\n                                                                    30\n\n                20               Number                                                          20\n\n                10                                                  10\n\n                 0                                                   0\n                       10    5   0    5    10   15   20   25           20    15    10    5    0     5    10    15\n\n\n                   Activation projection                  Activation projection\n\nFigure 6: Projections of dataset examples’ activations in the residual stream after layer 8 onto\nthe vresid direction found by DAS (left) and the vgrad direction (right) which is the gradient for\ndifference in attention of the name mover heads to the two names.\n\n\n      equivalent to a 1-dimensional subspace intervention that generalizes activation patching. The\n     same arguments from Sections 3 and 8 apply to this intervention, suggesting that it is likely\n      to work successfully in a wide range of MLP layers, regardless of the role of these MLP layers\n      for factual recall.\n\n    3. Finally, we show that the existence of the illusory patches from 1. implies the existence of rank-\n     1 weight edits which have identical effect at the token being patched, and comparable overall\n       effect on the model. This provides the other direction of an approximate equivalence between\n     1-dimensional subspace interventions and rank-1 editing, which may be of independent\n       interest.\n\n    In particular, these findings provide a mechanistic explanation for the observation of prior work\n(Hase et al., 2023) that ROME works even in layers where the fact is supposedly not stored. As we\ndiscuss in Section 8, we expect that in practice all MLP layers between two model components\ncommunicating some feature are likely to contain an illusory subspace – and, by virtue of the\napproximate equivalence we demonstrate, rank-one fact edits will exist in these MLP layers,\nregardless of whether they are responsible for recalling the fact being edited.\n\n\n6.1  Finding Illusory 1-Dimensional Patches for Factual Recall\n\nGiven a fact expressed as a subject-relation-object triple (s, r, o) (e.g., s =  ‘Eiffel Tower’, r =\n‘is in’, o = ‘Paris’), we say that a model M recalls the fact (s, r, o) if M completes a prompt expressing\njust the subject-relation pair (s, r) (e.g., ‘The Eiffel Tower is in’) with the object o (‘Paris’).\n    Let us be given two facts (s, r, o) and (s′, r, o′) for the same relation that a model recalls correctly,\nwith corresponding factual prompts A expressing (s, r) and B expressing (s′, r) (e.g., r = ‘is in’, A =\n‘The Eiffel Tower is in’, B = ‘The Colosseum is in’). In this subsection, we patch from B into A, with\n\n\n\n                                          19\n\n                                     1.0                              Method\n                                                            DAS\n                                                                                                Full MLP patch\n                                     0.8                                                            DAS rowspace component\n\n                                     0.6                                                                         changed\n                                                    facts 0.4\n                     of 0.2\n\n                                     0.0\n                                    0    5    10   15   20   25   30   35   40   45                                                                                    Fraction\n                                    Intervention layer\n\nFigure 7: Fraction of successful fact patches under three interventions: patching along the direction\nfound by DAS (blue), patching the component of the DAS direction in the rowspace of Wout (green),\nand patching the entire hidden MLP activation (orange).\n\n\nthe goal of changing the model’s output from o to o′. Implementation details are given in Appendix\nD.1.\n   Results are shown in figure 7. We find a stronger version of the same qualitative phenomena\nas for the IOI illusory direction from Section 4: (i) the directions we find have a strong causal\neffect (successfully changing o to o′), but (ii) this effect disappears when we instead patch along\nthe subspace spanned by the component orthogonal to ker Wout, and (iii) patching the entire MLP\nactivation instead has a negligible effect on the difference in logits between the correct and incorrect\nobjects. Further experiments confirming the illusion are in Appendix D.2.\n  We conclude that it is possible to make a model output a different object for a given fact\nby exploiting a 1-dimensional subspace patch that activates a dormant circuit in the model; in\nparticular, using such a patch to localize the fact in the model is prone to interpretability illusions.\nNext, we turn to a more sophisticated intervetion that has been used to edit a fact in a more holistic\nway, so that related facts update accordingly while the model otherwise stays mostly the same.\n\n\n6.2  Background on ROME\n\nMeng et al. (2022a) propose an intervention to overwrite a fact (s, r, o) with another fact (s, r, o′)\nwhile minimally changing the model otherwise. The intervention is a rank-1 weight edit, which\nupdates the down-projection Wout of a chosen MLP layer to become W′out = Wout + ab⊤for some\na ∈Rdresid, b ∈RdMLP. The edit takes a ‘key’ vector k ∈RdMLP representing the subject (e.g., an\naverage of its last-token MLP post-gelu activations over many prompts containing it) and a ‘value’\nvector v ∈Rdresid which, when output by the MLP layer, will cause the model to predict the new\nobject o′ for the factual prompt (together with some other conditions incentivizing the model to not\nchange much otherwise).\n    Importantly, we demonstrate that ROME can be formulated as an optimization problem with a\nnatural objective, and this objective allows us to compare it to related interventions. Namely, the\nvectors a, b are the solution to\n\n                                         h     i               min trace Covx∼N (0,Σ) ab⊤x     subject to  W′outk = v.                    (5)\n                            a,b\n\n\n                                          20\n\nwhere Cov [r] = E h (r −µ) (r −µ)⊤i denotes the covariance matrix of a random vector r with\nmean µ, and Σ ⪰0 is an empirical (uncentered) covariance matrix for the MLP activations (proof in\nAppendix D.4). In words, the ROME update is the update that alters Wout so it outputs v on input\nk, and minimizes the total variance of the extra contribution of the update in the output of the MLP\nlayer under the assumption that the pre-Wout activations are normally distributed with mean zero\nand covariance Σ ⪰0.\n\n\n6.3  Rank-1 Fact Edits Imply Approximately Equivalent 1-Dimensional Subspace Inter-\n     ventions\n\nComparing the effect of a rank-1 edit to the MLP layer’s output with equation 3 expressing the\neffect of patching on the MLP’s outputs, we see that the two are quite similar. This leads to a\nnatural question: given a rank-1 weight edit W′out = Wout + ab⊤such as ROME, can we find a\n1-dimensional activation patch that has the same contribution to the MLP’s output for any MLP\nactivation x?\n\n\nMotivation and details.  As it turns out, finding a patch that has the same effect as a rank-1 edit is\nnot feasible in practice. For an activation x, the extra contribution to the MLP’s output due to a\nrank-1 edit is b⊤x  a, whereas the extra contribution of a 1-dimensional patch from activation x′ is\n v⊤x′ −v⊤x Wv, where crucially ∥v∥2 = 1. In particular, the vectors a, b are not norm-constrained,\nunlike v. This restricts the magnitude of the contribution of a patch, and we find this matters in\npractice.\n   To overcome this, we consider a closely related subspace intervention,\n\n                                xintervention = x + v⊤0 −v⊤x v = x − v⊤x v\n\nwhere v is no longer restricted to be unit norm, and x′ is chosen to be 0 to match the expectation\nof the rank-1 edit’s contribution (see Appendix D.7 for details). This intervention bears many\nsimilarities to subspace patching; in particular, this intervention leaves the projections on all\ndirections orthogonal to v the same, and the intuitions about the illusion from Sections 3 and 7\nalso apply to this intervention. We also remark that, at the same time, this intervention is exactly\nequivalent to the rank-1 edit W′′out = Wout + Woutv (−v)⊤in terms of contribution to the MLP\noutput.\n     It turns out that this more general intervention can often approximate the ROME intervention\nwell. Specifically, given a rank-1 edit Wout + ab⊤, we can treat the problem probabilistically over\nactivations x ∼N (0, Σ) like done in Meng et al. (2022a), and ask for a direction v with the following\nproperties:\n\n  • the expected value of both interventions is the same;\n\n  • the resulting extra contribution −v⊤x Woutv to the MLP’s output points in the same direc-\n      tion as the extra contribution b⊤x a of the rank-1 edit;\n\n  • the total variance trace Cov  −v⊤x Woutv − b⊤x a   of the difference of these two contri-\n     butions is minimized over x ∼N (0, Σ).\n\nDetails are given in Appendix D.7. The important takeaway is that the solution v has the form\n\n                       v = αW+outa + u  where  u ∈ker Wout\n\n\n                                          21\n\nfor a constant α ≥0 that is optimized. In particular, Woutv points in the direction a, and the\ncomponent u (which is causally disconnected) is a ’free variable’ that is essentially optimized to\nbring v⊤x close to −b⊤x/α (subject to accounting for Σ).\n\n\nMetrics and evaluation. We apply this method to find subspace interventions corresponding to\nedits extracted from the COUNTERFACT dataset (Meng et al., 2022a); see Appendix D.1 for details.\nSpecifically, we run ROME for all the edits, and we find the approximate subspace intervention\n(defined by a vector v) corresponding to each ROME edit. To compare the interventions, we\nconsider the following metrics:\n\n\n   Rewrite score.  Defined in Hase et al. (2023) (and a closely related metric is optimized by\nMeng et al. (2022a)), the rewrite score is a relative measure of how well a change to the model\n(ROME or our subspace intervention) increases the probability of the false target o′ we are trying to\nsubstitute for o. Specifically, if pclean (o) is the probability assigned by the model to output o under\nnormal operation, and pintervention (o) is the probability assigned when the intervention is applied,\nthe rewrite score is\n                                      pintervention (o′) −pclean (o′)                                     ∈(−∞, 1].\n                                1 −pclean (o′)\n\nwith a value of 1 indicating the model assigns probability 1 to o′ after the intervention. We measure\nthe rewrite score for the ROME intervention, our approximation of it, and also the corresponding\nsubspace intervention with the ker Wout component of v removed, in analogy with how we examine\nthe subspace patches in Sections 4 and 6.1. That is, if vnull is the orthogonal projection of v on\nker Wout and vrowsp = v −vnull, we apply the intervention\n\n                               xrowspace intervention = x − v⊤rowspx  vrowsp.\n\nResults comparing ROME and the subspace intervention we use to approximate it are shown in\nFigure 8. When using the rowspace intervention, all rewrite scores are less than 10−3, indicating a\nstrong reliance on the nullspace component.\n\n   Cosine similarity of v and b.  Our intervention contributes − v⊤x Wv, and the ROME edit\ncontributes b⊤x  a. Note that, by construction, the cosine similarity of Wv with a is 1. So, the\ncosine similarity of v and b measures how well the direction we are projecting the activation x on\nmatches that from the ROME edit. Results are shown in Figure 9 (left); in a range of layers we\nobserve cosine similarity significantly close to 1.\n\n\n   Overall change to the model relative to ROME.  This is the total variance introduced by this\nintervention as a fraction of the total variance introduced by the corresponding ROME edit. It\nmeasures the extent to which the subspace intervention damages the model overall, following our\nformulation of ROME as an optimization problem (see Appendix D.4). Results are shown in Figure\n9 (right). Note that this metric is a ratio of variances; a ratio of standard deviations can be obtained\nby taking the square root.\n   In conclusion, we observe that in layers 20-35 inclusive, the two interventions are very similar\naccording to all metrics considered.\n\n\n\n\n                                          22\n\nWhat is the interpretability illusion implied by this? An important difference between the\nIOI case study from Section 4 and the factual recall results from the current section is that, while\nactivating a dormant circuit is contrary to activation patching’s interpretability fact editing is, by\ndefinition, allowed to alter the model. In this sense, activating a dormant circuit via a rank-1 edit\nshould no longer be considered a sign of spuriosity.\n    Instead, we argue that the interpretability illusion is to assume that the success of ROME means\nthat the fact is stored in the layer being edited. This was already observed in Hase et al. (2023). Our\nwork provides a mechanistic explanation for this observation.\n  We also note that we have evaluated the success of ROME and our approximately-equivalent\nsubspace intervention only using the rewrite score metric and the measure of total variance implicit\nin the ROME algorithm. Ideally, there would be other validations of a fact edit that test the behavior\nof the intervened-upon model on other facts that should be changed by the edit.\n\n                      layer 20                                       layer 25\n    1.0                                          1.0\n  (ours)    0.8                                          0.8\n  score\n    0.6                                                 0.6   Rewrite\n    0.4\n              0.90       0.95       1.00              0.85    0.90    0.95    1.00\n                        layer 30                                        layer 35\n    1.00                                        1.00\n\n    0.75                                        0.75  (ours)\n                                                0.50  score  0.50\n\n                                                0.25    0.25   Rewrite\n                                                0.00\n          0.2    0.4    0.6    0.8    1.0          0.00   0.25   0.50   0.75   1.00\n             Rewrite score (ROME)                        Rewrite score (ROME)\n\nFigure 8: Rewrite score comparison between ROME (x-axis) and our approximation to it (y-axis)\nvia a subspace intervention for layers 20, 25, 30, 35.\n\n\n\n6.4  1-Dimensional Fact Patches Imply Equivalent Rank-1 Fact Edits\n\nFinally, we show that the existence of an activation patch as in Subsection 6.1 implies the existence\nof a rank-1 weight edit which has the same contribution to the MLP’s output at the token being\npatched, and otherwise results in very similar model outputs as the activation patch.\n\n\n                                          23\n\n     1.0                                                10\n     0.8\n                                                 8\n     0.6                                                                                                                                                                                                                                                                                      (ours/ROME)  6    similarity     0.4                                                                                                                              ratio  4\n     0.2  Cosine                                              2\n     0.0                                                                                                                                                                                                          Variance        0  5 10 15 20 25 30 35 40 45                  15    20    25    30    35\n               Intervention layer                                     Intervention Layer\n\nFigure 9: Comparisons between ROME rank-1 edits and our approximation via a subspace inter-\nvention. Left: cosine similarity between the vector v defining the subspace we intervene on and the\nvector b from the ROME edit (dashed horizontal line is at y = 1). Right: ratio of the total variance\nintroduced by the subspace intervention to the total variance of the ROME intervention (x-axis\nscale is restricted to make the plot readable; dashed horizontal line is at y = 1).\n\n\n    Intuitively, a ‘fact patch’ as in Subsection 6.1 should have a corresponding rank-1 edit with the\nsame effect: the last subject token MLP activation uA for prompt A takes the role of k, and the patch\nmodifies the MLP’s output (making it v) to change the model’s output to o′. We make this intuition\nformal in Appendix D.5, where we show that for each 1-dimensional activation patch between a\npair of examples in the post-GELU activations of an MLP layer, there is a rank-1 model edit to Wout\nthat results in the same change to the MLP layer’s output at the token where we do the patching,\nand minimizes the variance of the extra contribution in the sense of Equation 5.\n   While this shows that the patch implies a rank-1 edit with the same behavior at the token where\nwe perform the patch, the rank-1 edit is applied permanently to the model, which means that it (unlike\nthe activation patch) applies to every token. Thus, it is not a priori obvious whether the rank-1 edit\nwill still succeed in making the model predict o′ instead of o. To this end, in Appendix D.6, we\nevaluate empirically how using the rank-1 edit derived in Appendix D.5 instead of the activation\npatch changes model predictions, and we find negligible differences.\n\n\n7  Reasons to Expect the MLP-in-the-middle Illusion to be Prevalent\n\nWe only exhibit our illusion empirically in two settings: the IOI task and factual recall. However,\nwe believe it is likely prevalent in practice. In this section, we provide several theoretical, heuristic\nand empirical arguments in support of this.\n    Specifically, we expect the illusion to be likely occur whenever we have an MLP M which is not\nused in the model’s computation on a given task, but is between two components A and B which\nare used, and communicate by writing to / reading from the direction v via the skip connections of\nthe model. This structure has been frequently observed in the mechanistic interpretability literature\n(Lieberum et al., 2023; Wang et al., 2023; Olsson et al., 2022; Geva et al., 2021): circuits contain\ncomponents composing with each other separated by multiple layers, and circuits have often been\nobserved to be sparse, with most components (including most MLP layers) not playing a significant\nrole.\n\n\n\n                                          24\n\n7.1  Assumptions: a Simple Model of Linear Features in the Residual Stream\n\nThe linear representation hypothesis suggests a natural way to formalize this intuition. Namely,\nlet’s assume for simplicity that there is a binary feature C in the data, and the value of C influences\nthe model’s behavior on a task, by e.g. making some next-token predictions more likely than others.\nConcretely, there is a residual stream direction v ∈Rdresid that mediates this effect: projections on\nv (at an appropriate token position) linearly separate examples according to the value of C, and\nintervening on this projection by setting it to e.g. the mode of the opposite class has the same effect\non model outputs as changing the value of C in the input itself. Furthermore, we assume that this\ndirection has this property in all residual stream layers between some two layers a < b.\n  We note that these assumptions can be realized ‘in the wild’: the highly similar directions vgrad\nand vresid discussed in Section 5 are both examples of such directions v for the binary concept of\nwhether the IO name comes first or second in the sentence, as we argued empirically.\n\n\n7.2  Overview of Argument\n\nThe key hypothesis is that, given the setup from the previous Subsection 7.1, the post-nonlinearity\nactivations of every MLP layer between layers a and b are likely to contain a 1-dimensional subspace\nwhose patching will have the same effect (possibly with a smaller magnitude) on model outputs as\nchanging the value of C in the input. For this, it is sufficient to have two kinds of directions in the\nMLP’s activation space:\n\n  • a ‘causal’ direction, such that changing the projection of an activation along this direction\n      results in the expected change of model outputs. Such a direction will exist simply because\n     Wout is a full-rank matrix, so we can simply pick W+outv. We give an empirically-supported\n      theoretical argument for this in Appendix E.1.\n\n  • a ‘correlated’ direction that linearly discriminates between the values of C: such a direc-\n      tion will exist because the pre-nonlinearity activations (which are an approximately lin-\n     ear image of the residual stream) will linearly discriminate C, and the transformation\n     x 7→gelu(x) 7→projker Wout gelu (x) approximately preserves linear separability. We give\n     an empirically-supported theoretical argument for this in Appendix E.2.\n\n\n8  Discussion, Limitations, and Recommendations\n\nThroughout this paper, we have seen that interventions on arbitrary linear subspaces of model\nactivations, such as subspace activation patching, can have counterintuitive properties. In this\nsection, we take a step back and provide a more conceptual point of view on these findings, as well\nas concrete advice for interpretability practitioners.\n\n\nWhy should this phenomenon be considered an illusion?  One argument for the illusory nature\nof the subspaces we find is the reliance on a large causally disconnected component (in all our\nexamples, this component is in the kernel of the down-projection Wout of an MLP layer). In particu-\nlar, patching along only the causally-relevant component of the subspace (the one in (ker Wout)⊥)\ndestroys the effect of the subspace patch; we find this a convincing reason to be suspicious of the\nexplanatory faithfulness of these subspaces.\n   Beyond this argument, there are several more subtle considerations. For an explanation to\nbe ‘illusory’, there has to be some notion of what the ‘true’ explanation is. We admit that a\n\n\n                                          25\n\ndefinition of a ‘ground truth’ mechanistic explanation is conceptually challenging. In the absence\nof such a definition, our claims rest on various observations about model’s inner workings that we\nnow collect in one place and make more explicit. We believe these findings collectively point to\nmeaningful constraints on mechanistic explanations.\n   For example, the IOI circuit work of Wang et al. (2023) finds through various component-level\ninterventions that the layer 8 MLP as a whole does not contribute significantly to the model’s ability\nto do the IOI task. However, does this imply that there aren’t individual subspaces of the MLP\nlayer that mediate the model’s behavior on the IOI task? Not necessarily: there could be, for\nexample, two subspaces mediating the position signal, but which have opposite contributions to\nthe MLP’s output that cancel out. This is compatible with our model of the illusion from Section\n3: for example, we can form two ‘cancelling’ 1-dimensional subspaces by taking the sum and\ndifference of the causally disconnected and dormant directions in our model. From this point of\nview, our subspace intervention decouples these (ordinarily coupled) subspaces by changing the\nactivation only along one of them. This is impossible for an intervention that operates on entire\nmodel components.\n   Should we prefer the view under which the MLP layer simply does not participate in any\nmeaningful way in the IOI task, or the view under which it contains subspaces that mediate infor-\nmation about the IOI task, but whose contributions cancel out? Note that meaningful cancellation\nbehavior between entire model components has been observed to some extent in the mechanistic\ninterpretability literature, such as with negative heads (Wang et al., 2023), anti-induction heads\n(Olsson et al., 2022) and copy suppression heads (McDougall et al., 2023). Furthermore, it is not\nclear that, in general, a component-level explanation should take precendence over subspace-level\nexplanations. So, a priori, we have a conundrum: two different kinds of interventions arrive at\nconflicting interpretations.\n   Nevertheless, based on our experiments, we suggest that the view under which the MLP layer\ncontains meaningful subspaces that cancel out is the less likely mechanistic explanation for several\nreasons. A first argument is that, as we argue in Section 7, the existence of the illusory subspace only\nrelies on the existence of certain directions in the residual stream; the MLP weights themselves don’t\nplay a role. In some sense, the illusory subspace is a ‘necessity of linear algebra’. This is further\nreinforced by the fact that we find illusory directions even when the MLP weights are replaced\nby random matrices (see Appendix B.5). A second argument is that features that are individually\nstrong, but whose contributions almost exactly cancel out, seem unlikely to be prevalent.\n    Finally, we again remark that circuits for specific tasks have been observed to be sparse (recall\nSection 7). Our model of the illusion from Section 3 and the evidence from Section 7 suggest that\nany MLP layer between two circuit components using the residual stream as a communication\nbottleneck for some feature will contain a subspace that appears to mediate this feature. Thus, even\nif we cannot conclusively rule out any given MLP layer on the path as not being meaningfully\ninvolved in the computation, it would be quite surprising if always all of them are involved. So we\nexpect that at least some of these subspaces will be illusory.\n\n\nThe importance of correct model units. A further implicit assumption in our work is that model\ncomponents are meaningful boundaries for mechanistic explanation. As we illustrate in Appendix\nA.3, our toy example of the illusion can be considered in a rotated basis, in which the ‘illusory’\ndirection appears ‘meaningful’. In a similar way, if we allow ourselves to arbitrarily reparametrize\nspaces of activations by crossing the boundaries between e.g. attention heads and MLP layers,\ncalling the MLP subspace ‘illusory’ is much more tenuous.\n   To respond to this criticism, we point to the many observations in the mechanistic literature\n\n\n\n                                          26\n\nthat different components (like heads and MLP layers) perform qualitatively different functions in\na model. For example, tasks involving algorithmic manipulations of in-context text, such as the\nIOI task, often rely predominantly on attention heads (Wang et al., 2023; Heimersheim & Janiak).\nOn the other hand, MLP layers have so far been implicated in tasks having to do with recalling\nbigrams and facts (Meng et al., 2022a; Gurnee et al., 2023). On these grounds, mixing activations\nbetween them is likely to lead to less parsimonious and less principled mechanistic explanations.\n\n\nTakeaways and recommendations.  As we have seen, optimization-based methods using subspace\nactivation patching, such as DAS (Geiger et al., 2023b) can find both faithful (Section 5) and illusory\n(Section 4) features with respect to the model’s computation. We recommend running such methods\nin activation bottlenecks, especially the residual stream, as well as using validations beyond end-to-\nend evaluations to ascertain the precise role of such features.\n\n\n9  Acknowledgments\n\nWe are deeply indebted to Atticus Geiger for many useful discussions and helpful feedback, as well\nas help writing parts of the paper. We particularly appreciate his thoughtful pushback on framing\nand narrative, and commitment to rigour, without which this manuscript would be far poorer.\nWe would also like to thank Christopher Potts, Curt Tigges, Oskar Hollingsworth, Tom Lieberum,\nSenthooran Rajamanoharan and Peli Grietzer for valuable feedback and discussions. The authors\nused the open-source library transformerlens (Nanda & Bloom, 2022) to carry out the experiments\nrelated to the IOI task. AM and GL did this work as part of the SERI MATS independent research\nprogram, with support from AI Safety Support.\n\n\n10  Author Contributions\n\nNN was the main supervisor for this project, and provided high level feedback on experiments,\nprioritisation, and writing throughout. NN came up with the original idea of the illusion and the\nconceptual example. AM came up with the correspondence with factual recall, developed the\nfactual recall results, and ran the experiments for Sections 6, 7 and part of 5 (with the exception of\nexperiments from Appendix E.6 ran by GL), and wrote the majority of the paper and appendices,\nas well as the public version of the code for the paper. GL also ran the experiments for Sections 4\nand 5 with some guidance from AM, wrote and ran the experiments for Appendices B.5, B.6, B.7, C\nand E.6, and contributed to Section 5 and various other sections.\n\n\nReferences\n\nMostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders\n  Søgaard. Can language models encode perceptual structure without grounding? a case study in\n   color. arXiv preprint arXiv:2109.06129, 2021.\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim.\n  Sanity checks for saliency maps. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen\n  Grauman, Nicol0 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Process-\n  ing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,\n\n\n\n\n                                          27\n\n  December 3-8, 2018, Montreal, Canada, pp. 9525–9536, 2018. URL https://proceedings.neurips.\n  cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html.\n\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic\n  structure of word senses, with applications to polysemy.  Transactions of the Association for\n  Computational Linguistics, 6:483–495, 2018.\n\nMariette Awad, Rahul Khanna, Mariette Awad, and Rahul Khanna. Support vector machines for\n  classification. Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and\n  System Designers, pp. 39–66, 2015.\n\nTolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viégas, and Martin\n  Wattenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n  few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in\n  language models without supervision. arXiv preprint arXiv:2212.03827, 2022.\n\nLawrence Chan, Adria Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishin-\n  skaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: a method for\n  rigorously testing interpretability hypotheses, 2022. URL https://www.alignmentforum.org/\n  posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing.\n\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of\n  knowledge editing in language models. arXiv preprint arXiv:2307.12976, 2023.\n\nArthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià\n  Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. arXiv\n  preprint arXiv:2304.14997, 2023.\n\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loic Barrault, and Marco Baroni. What\n  you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\n  In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n   1: Long Papers), pp. 2126–2136, Melbourne, Australia, July 2018. Association for Computational\n  Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neu-\n  rons in pretrained transformers.  In Proceedings of the 60th Annual Meeting of the Association\n   for Computational Linguistics (Volume 1: Long Papers), pp. 8493–8502, Dublin, Ireland, May\n  2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL\n  https://aclanthology.org/2022.acl-long.581.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\n  bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the\n  North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\n  Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for\n  Computational Linguistics.  doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/\n  N19-1423.\n\n\n\n                                          28\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n  Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n  Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n  Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.\n URL https://transformer-circuits.pub/2021/framework/index.html.\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\n  Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\n  Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo-\n   sition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/toy_\n  model/index.html.\n\nAtticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models\n  partially embed theories of lexical entailment and negation. In Proceedings of the Third BlackboxNLP\n  Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 163–173, Online, November\n  2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.16. URL\n  https://www.aclweb.org/anthology/2020.blackboxnlp-1.16.\n\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural\n  networks. Advances in Neural Information Processing Systems, 34:9574–9586, 2021.\n\nAtticus Geiger, Christopher Potts, and Thomas Icard. Causal abstraction for faithful model inter-\n  pretation. Ms., Stanford University, 2023a. URL https://arxiv.org/abs/2301.04709.\n\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D Goodman. Finding\n  alignments between interpretable causal variables and distributed neural representations. arXiv\n  preprint arXiv:2303.02536, 2023b.\n\nRobert Geirhos, Roland S Zimmermann, Blair Bilodeau, Wieland Brendel, and Been Kim. Don’t\n  trust your eyes: on the (un) reliability of feature visualizations. arXiv preprint arXiv:2306.04719,\n  2023.\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers\n  are key-value memories.  In Proceedings of the 2021 Conference on Empirical Methods in Natu-\n   ral Language Processing, pp. 5484–5495, Online and Punta Cana, Dominican Republic, 2021.\n  Association for Computational Linguistics.  doi: 10.18653/v1/2021.emnlp-main.446. URL\n  https://aclanthology.org/2021.emnlp-main.446.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual\n  associations in auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.\n\nAaron Gokaslan and Vanya Cohen. Openwebtextcorpus, 2019. URL http://Skylion007.github.\n  io/OpenWebTextCorpus.\n\nNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model\n  behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.\n\nG Grand, I Blank, F Pereira, and E Fedorenko. Semantic projection: Recovering human knowledge\n  of multiple, distinct object features from word embeddings. arxiv. arXiv preprint arXiv:1802.01241,\n  2018.\n\n\n\n                                          29\n\nWes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris\n  Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint\n  arXiv:2305.01610, 2023.\n\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing?\n  surprising differences in causality-based localization vs. knowledge editing in language models.\n  arXiv preprint arXiv:2301.04213, 2023.\n\nStefan Heimersheim and Jett Janiak. A circuit for Python docstrings in a 4-layer attention-\n  only  transformer.   URL  https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/\n  a-circuit-for-python-docstrings-in-a-4-layer-attention-only.\n\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge representa-\n  tions in language models, 2023.\n\nJohn Hewitt and Christopher D Manning. A structural probe for finding syntax in word repre-\n  sentations. In Proceedings of the 2019 Conference of the North American Chapter of the Association\n   for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n  4129–4138, 2019.\n\nBelinda Z Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\n  language models. arXiv preprint arXiv:2106.00737, 2021.\n\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\n  intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341,\n  2023a.\n\nMaximilian Li, Xander Davies, and Max Nadeau. Circuit breaking: Removing model behaviors\n  with targeted ablation. arXiv preprint arXiv:2309.05973, 2023b.\n\nTom Lieberum, Matthew Rahtz, János Kramár, Geoffrey Irving, Rohin Shah, and Vladimir Miku-\n   lik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in\n  chinchilla. arXiv preprint arXiv:2307.09458, 2023.\n\nZachary C. Lipton. The mythos of model interpretability, 2016. URL https://arxiv.org/abs/\n  1606.03490.\n\nCallum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda. Copy\n  suppression: Comprehensively understanding an attention head. arXiv preprint arXiv:2310.04625,\n  2023.\n\nThomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. The hydra\n   effect: Emergent self-repair in language model computations. arXiv preprint arXiv:2307.15771,\n  2023.\n\nKevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual\n  associations in GPT. In Advances in Neural Information Processing Systems, 2022a.\n\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass editing\n  memory in a transformer. arXiv preprint arXiv:2210.07229, 2022b.\n\n\n\n\n\n                                          30\n\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed\n  representations of words and phrases and their compositionality.   In Christopher  J. C.\n  Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in\n  Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Pro-\n  cessing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada,\n  United States, pp. 3111–3119, 2013. URL https://proceedings.neurips.cc/paper/2013/hash/\n  9aa42b31882ec039965f3c4923ce901b-Abstract.html.\n\nNeel Nanda. Actually, othello-gpt has a linear emergent world representation. 2023a.\n\nNeel Nanda. Attribution patching: Activation patching at industrial scale, 2023b. URL https:\n  //www.neelnanda.io/mechanistic-interpretability/attribution-patching.\n\nNeel Nanda and Joseph Bloom.   Transformerlens.   https://github.com/neelnanda-io/\n  TransformerLens, 2022.\n\nNeel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world\n  models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023.\n\nChris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases.\n  https://www.transformer-circuits.pub, 2022. URL https://www.transformer-circuits.pub/2022/\n  mech-interp-essay.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n  Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001.\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\n  Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn\n  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones,\n  Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,\n  Jared Kaplan, Sam McCandlish, and Chris Olah.   In-context learning and induction\n  heads.  Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/\n  in-context-learning-and-induction-heads/index.html.\n\nOpenAI. Gpt-4 technical report, 2023.\n\nJudea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence, UAI’01, pp. 411–420, San Francisco, CA, USA, 2001. Morgan Kaufmann\n  Publishers Inc. ISBN 1558608001.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n  models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nP. Smolensky. Neural and conceptual interpretation of PDP models. In Parallel Distributed Processing:\n  Explorations in the Microstructure, Vol. 2: Psychological and Biological Models, pp. 390–431. MIT Press,\n  Cambridge, MA, USA, 1986. ISBN 0262631105.\n\nAlessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. Understanding arithmetic reasoning\n  in language models using causal mediation analysis. arXiv preprint arXiv:2305.15054, 2023.\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\n  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593–4601,\n\n\n                                          31\n\n  Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452.\n URL https://aclanthology.org/P19-1452.\n\nAlex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Acti-\n  vation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248,\n  2023.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n  Lukasz Kaiser, and Illia Polosukhin.  Attention is all you need.  In Isabelle Guyon, Ul-\n  rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,\n  and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:  Annual\n  Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,\n  CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n  3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason\n  Huang, Yaron Singer, and Stuart Shieber. Causal mediation analysis for interpreting neural nlp:\n  The case of gender bias. URL https://arxiv.org/abs/2004.12265.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason\n  Huang, Yaron Singer, and Stuart Shieber. Causal mediation analysis for interpreting neural nlp:\n  The case of gender bias. arXiv preprint arXiv:2004.12265, 2020.\n\nJonas Wallat, Jaspreet Singh, and Avishek Anand. BERTnesia: Investigating the capture and\n  forgetting of knowledge in BERT.  In Proceedings of the Third BlackboxNLP Workshop on An-\n  alyzing and Interpreting Neural Networks for NLP, pp. 174–183, Online, November 2020. As-\n  sociation for Computational Linguistics.  doi:  10.18653/v1/2020.blackboxnlp-1.17.  URL\n  https://aclanthology.org/2020.blackboxnlp-1.17.\n\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\n  Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The\n  Eleventh International Conference on Learning Representations, 2023. URL https://openreview.\n  net/forum?id=NpsVSN6o4ul.\n\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In\n  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\n  pp. 6151–6162, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n  emnlp-main.496. URL https://aclanthology.org/2020.emnlp-main.496.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\n  Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.\n  arXiv preprint arXiv:2206.07682, 2022.\n\nZhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D Goodman. Interpretability at scale:\n  Identifying causal mechanisms in alpaca. arXiv preprint arXiv:2305.08809, 2023.\n\nZexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen.\n  Mquake: Assessing knowledge editing in language models via multi-hop questions, 2023.\n\n\n\n\n\n                                          32\n\n                                                                                               (v)\n                                                                                                 along                                                                                                                                                                                                                                                                                                                                                                                    direction                                                                                              patch                                                                                                                direction                                                to\n                                                                                                                                                                                                                                                                                                  dormant                                                                                                            direction                                                                                       dormant                               example to patch from         example to patch into\n                                                                                              causally disconnected direction\n\n                              causally disconnected direction\n\n\n\n\n\n                                             Figure 11:  Suppose we have two examples\nFigure 10: Consider a 2-dimensional subspace  (green) which differ in their projection on the\nof model activations, with an orthogonal basis  causally disconnected direction (and have zero\nwhere the x-axis is causally disconnected (chang- projection on the dormant direction, by defini-\ning the activation along it makes no difference  tion).  Let’s consider what happens when we\nto model outputs) and values on the y axis are  patch from the example on the left into the ex-\nalways zero for examples in the data distribution  ample on the right along the 1-dimensional sub-\n(a specical case of a dormant direction).          space v spanned by the vector (1, 1) (red)\n\n                                        (v)                                                    (v)\n                                      along                                                      along                                                                                                                direction                                                                                                                                                                                                                                                            direction        orthogonal                   patch                               orthogonal                   patch\n                                                                                       dormant    to                                                                                                                                                                       dormant    to                                                                                                            direction                                  direction\n                                                                                    example to patch into                              example to patch into        example to patchcomplementfrom v⊥  example to patchcomplementfrom v⊥\n                              causally disconnected direction                                 causally disconnected direction\n\n                              1                2                                                                 1                2\n                                                                                                                                                                                                                                                                result\n                                                                                     of\n                                                                                                                 1 +  2   the\n                                                                                                                                                                                                                     patch\nFigure 12: To patch along v from the left into\nthe right example, we match the projection on v  Figure 13: This results in the patched activation\nfrom the left one, and leave the projection on v⊥ 1⃝+2⃝, which points completely along the dor-\nunchanged. In other words, we take the compo- mant direction. In this way, activation patch-\nnent of the left example along v ( 1⃝) and sum it  ing makes the variation of activations along the\nwith the v⊥component (2⃝) of the original acti-  causally disconnected x-axis result in activations\nvation.                                      along the previously dormant y-axis.\n\n\n          Figure 14: A step-by-step illustration of the phenomenon shown in Figure 1.\n\n\n\n\n                                          33\n\nA  Additional Notes on Section 3\n\nA.1  The Illusion for Higher-Dimensional Subspaces\n\nIn the main text, we mostly discuss the illusion for activation patching of 1-dimensional subspaces\nfor ease of exposition. Here, we develop a more complete picture of the mechanics of the illusion\nfor higher-dimensional subspaces.\n   Let C be a model component taking values in Rd, and let U ⊂Rd be a linear subspace. Let V be\na matrix whose columns form an orthonormal basis for U. If the C activations for examples A and\nB are actA, actB ∈Rd respectively, patching U from A into B gives the patched activation\n\n                  actpatchedB   = actB + VV⊤(actA −actB) = (I −VV⊤)actB + VV⊤actA\n\nFor intuition, note that VV⊤is the orthogonal projection on U, so this formula says to replace the\northogonal projection of actB on U with that of actA, and keep the rest of actB the same.\n   Generalizing the discussion from Section 3, for the illusion to occur for subspace S, we need S\nto be sufficiently aligned with a causally disconnected subspace Vdisconnected that is correlated with\nthe feature being patched, and a dormant but causal subspace Vdormant which, when set to out of\ndistribution values, can achieve the wanted causal effect.\n   For example, a particularly simple way in which this could happen is if we let Vdisconnected, Vdormant\nbe 1-dimensional subspaces (like in the setup for the 1-dimensional illusion), and we form S by\ncombining Vdisconnected ⊕Vdormant with a number of orthogonal directions that are approximately\nconstant on the data with respect to the feature we are patching. These extra directions effectively\ndon’t matter for the patch (because they are cancelled by the actA −actB term). Given a specific\nfeature, it is likely that such weakly-activating directions will exist in a high-dimensional activation\nspace. Thus, if the 1-dimensional illusion exist, so will higher-dimensional ones.\n\n\nA.2  Optimal Illusory Patches are Equal Parts Causally Disconnected and Dormant\n\nIn this subsection, we prove a quantitative corollary of the model of our illusion that suggests that\nwe should expect optimal illusory patching directions to be of the form\n\n                                    1\n                                       villusory = √  (vdisconnected + vdormant)\n                                     2\n\nfor unit vectors vdisconnected ⊥vdormant. In other words, we expect the strongest illusory patches to\nbe formed by combining a disconnected and illusory direction with equal coefficients, like depicted\nin Figure 1:\n\nLemma A.1. Suppose we have two distributions of input prompts DA, DB. In the terminology of Section\n3, let vdisconnected ⊥vdormant be unit vectors such that the subspace spanned by vdisconnected is a causally\ndisconnected subspace, and the subspace spanned by vdormant is strongly dormant, in the sense that the\nprojections of the activations of all examples Dsource ∪Dbase onto vdormant are equal to some constant c.\n    Let v =  vdisconnected cos α + vdormant sin α be a unit-norm linear combination of the two directions\nparametrized by an angle α. Then the magnitude of the expected change in projection along vdormant when\npatching from xA ∼DA into xB ∼DB is maximized when α = π4 , i.e. cos α = sin α = √12.\n\nProof. Recall that the patched activation from xA into xB along v is\n\n                                      actpatchedB   = actB + (pA −pB)v\n\n\n                                          34\n\nwhere pA = v⊤actA, pB = v⊤actB are the projections of the two examples’ activations on v. The\nchange along vdormant is thus\n          v⊤dormant  actpatchedB    −actB = (pA −pB) sin α = (v⊤actA −v⊤actB) sin α\n       = v⊤disconnected(actA −actB) cos α sin α\nwhere we used the assumption that v⊤dormantactA = v⊤disconnectedactB. Hence, the expected change is\n                        cos α sin α v⊤disconnectedEA∼DA,B∼DB [actA −actB] .\n\nThe function  f (α) = cos α sin α for α ∈[0, π/2] is maximized for α = π/4, concluding the\nproof.\n\n\nA.3  The Toy Illusion in a Rotated Basis\n\nThere is a subtlety in the toy example of the illusion from 3.3. Suppose we reparametrized the\nhidden layer of the network so that, instead of the standard basis (e1, e2, e3), we use a rotated basis\nwhere one of the directions is e1 + e2, the other direction is orthogonal to it and to w2 (hence will\nbe causally disconnected), and the last direction is the unique direction orthogonal to the first two.\n   The unit basis vectors for this new basis are given by\n                          √\n                                d1 = (e1 + e2) /  2,\n                              √\n                                d2 = (−e1 + e2 −2e3) /  6,\n                             √\n                                d3 = (−e1 + e2 + e3) /  3.\n\nIf we assemble these into the rows of a rotation matrix\n                              √1  √1   0 \n                                                        2     2\n                               √−1  √1  √−2                          R =                                                        6     6     6                                                                                                                                                                    √−1  √1  √1\n                                                        3     3     3\n\nthe re-parametrized network is then given by\n                        x  7→   h′ = Rw1x  7→   y = (Rw2)⊤h′\n\nand a diagram of this new network is shown in Figure 15. From this point of view, d1 takes the role\nthat e3 had before: the input is essentially copied to it (modulo scalar multiplication), and then read\nfrom it at the output. By contrast, d2 is now a causally disconnected direction, and d3 is a dormant\ndirection.\n\n\nB  Additional details for Section 4\n\nB.1  Dataset, Model and Evaluation Details for the IOI Task\n\nWe use GPT2-Small for the IOI task, with a dataset that spans 216 single-token names, 144 single-\ntoken objects and 75 single-token places, which are split 1 : 1 across a training and test set. Every\nexample in the data distribution includes (i) an initial clause introducing the indirect object (IO,\nhere ‘Mary’) and the subject (S, here ‘John’), and (ii) a main clause that refers to the subject a second\ntime. Beyond that, the dataset varies in the two names, the initial clause content, and the main\nclause content. Specifically, use three templates as shown below:\n\n\n                                          35\n\n                                   √\n                                                                           h′1 ←x/  2\n\n                                         √\n                         √              ×  2\n                               ×1/  2\n\n                         × −√ 3/2                  √   √\n                                    x                      h′2 ←−√ 3/2x            y ←   2h′1 +   3h′3\n\n\n\n                                        √\n                                          ×  3\n\n                                                                              h′3 ←0\n\n\n\n                                         Figure 15\n\n\n         Then, [ ] and [ ] had a long and really crazy argument. Afterwards, [ ] said to\n         Then, [ ] and [ ] had lots of fun at the [place]. Afterwards, [ ] gave a [object] to\n          Then, [ ] and [ ] were working at the [place]. [ ] decided to give a [object] to\n\nand we use the first two in training and the last in the test set. Thus, the test set relies on unseen\ntemplates, names, objects and places. We used fewer templates than the IOI paper Wang et al.\n(2020) in order to simplify tokenization (so that the token positions of our names always align), but\nour results also hold with shifted templates like in the IOI paper.\n  On the test partition of this dataset, GPT2-Small achieves an accuracy of ≈91%. The average\ndifference of logits between the correct and incorrect name is ≈3.3, and the logit of the correct\nname is greater than that of the incorrect name in ≈99% of examples. Note that, while the logit\ndifference is closely related to the model’s correctness, it being > 0 does not imply that the model\nmakes the correct prediction, because there could be a third token with a greater logit than both\nnames.\n\nB.2  Details for Computing the Gradient Direction vgrad\n\nFor a given example from the test distribution and a given name mover head, we compute the\ngradient of the difference of attention scores from the final token position to the 3rd and 5th token in\nthe sentence (where the two name tokens always are in our data). We then average these gradients\nover a large sample of the full test distribution and over the three name mover heads, and finally\nnormalize the resulting vector to have unit ℓ2 norm.\n  We note that there is a ‘closed form’ way to compute approximately the same quantity that\nrequires no optimization. Namely, for a single example we can collect the keys kS, kIO to the name\nmover heads at the first two names in the sentence (the S and IO name). Then, for a single name\nmover head with query matrix WQ, a maximally causal direction v in the residual stream at the\nlast token position after layer 8 will be one such that WQv is in the direction of kS −kIO, because\nthe attention score is simply the dot product between the keys and queries. We can use this to\n‘backpropagate’ to v by multiplying with the pseudoinverse W+Q . This is slightly complicated by the\nfact that we have been ignoring layer normalization, which can be approximately accounted for by\nestimating the scaling parameters (which tend to concentrate well) from the IOI data distribution.\nWe note that this approach leads to broadly similar results.\n\n\n\n                                          36\n\nB.3  Training Details for DAS\n\nTo train DAS, we always sample examples from the training IOI distribution. We sample equal\namounts of pairs of base (which will be patched into) and source (where we take the activation\nto patch in from) prompts where the two names are the same between the prompts, and pairs\nof prompts where all four names are distinct. We optimize DAS to maximize the logit difference\nbetween the name that should be predicted if the position information from the source example is\ncorrect and the other name.\n   For training, we use a learned rotation matrix as in the original DAS paper (Geiger et al., 2023b),\nparametrized with torch.nn.utils.parametrizations.orthogonal. We use the Adam optimizer\nand minibatch training over a training set of several hundred patching pairs. We note that results\nremain essentially the same when using a higher number of training examples.\n\n\nB.4  Discussion of the Magnitude of the Illusion\n\nWhile the contribution of the vMLP patch to logit difference may appear relatively small, we note\nthat this is the result of patching a direction in a single model component at a single token position.\nTypical circuits found in real models (including the IOI circuit from Wang et al. (2023)) are often\ncomposed of multiple model components, each of which contribute. In particular, the position\nsignal itself is written to by 4 heads, and chiefly read by 3 other heads. As computation tends to\nbe distributed, when patching an individual component accuracy may be a misleading metric (eg\npatching 1 out of 3 heads is likely insufficient to change the output), and a fractional logit diff\nindicates a significant contribution. By contrast, patching in the residual stream is a more potent\nintervention, because it can affect all information accumulated in the model that is communicated\nto downstream components.\n\n\nB.5  Random ablation of MLP weights\n\nHow certain are we that MLP8 doesn’t actually matter for the IOI task? While we find the IOI\npaper analysis convincing, to make our results more robust to the possibility that it does matter, we\nalso design a further experiment.\n   Given our conceptual picture of the illusion, the computation performed by the MLP layer\nwhere we find the illusory subspace does not matter as long as it propagates the correlational\ninformation about the position feature from the residual stream to the hidden activations, and as\nlong as the output matrix Wout is full rank (also, see the discussion in 8). Thus, we expect that if we\nreplace the MLP weights by randomly chosen ones with the same statistics, we should still be able\nto exhibit the illusion.\n    Specifically, we randomly sampled MLP weights and biases such that the norm of the output\nactivations matches those of MLP8. As random MLPs might lead to nonsensical text generation,\nwe don’t replace the layer with the random weights, but rather train a subspace using DAS on the\nMLP activations, and add the difference between the patched and unpatched output of the random\nMLP to the real output of MLP8. This setup finds a subspace that reduces logit difference even\nmore than the vMLP direction.\n   This suggests that the existence of the vMLP subspace is less about what information MLP8\ncontains, and more about where MLP8 is in the network.\n\n\n\n\n\n                                          37\n\nB.6  Generalization to high-dimensional Subspaces\n\nIn the main text, we focus on activation patching in one-dimensional subspaces for clarity. Here, we\nextend the discussion to higher-dimensional subspaces and show that the interpretability illusion\ngeneralizes to high-dimensional linear subspaces.\n  We investigate two different 100-dimensional subspaces UMLP8 in MLP8 and Uresid8 in the\noutput of layer 8. Specifically, we used DAS to find orthonormal bases VMLP and Vresid that align\nthe position information in these two locations, as explained in A.1. We found that these subspaces\nperformed slightly better compared to their 1-dimensional counterparts (for Vresid: 190% FLDD\nand 89% interchange accuracy; for VMLP: 62% FLDD and 13% interchange accuracy).\n  We hypothesize that the subspace trained on MLP8 is pathological while the subspace in the\nresidual stream is not. To test this, we decompose every basis vector v(d)          into its projection                                                                        {MLP8, resid8}\nvnullspace                                                                              such {MLP8, resid8}          on the nullspace ker{Wout, WQ} and its orthogonal complement vrowspace{MLP8, resid8}\nthat\n\n                              v(d)     = vnullspace   + vrowspace                             {MLP8, resid8}     {MLP8, resid8}    {MLP8, resid8}.\n\n   Note that WQ denotes the query weight of name mover head 9.9. We then patched the 200-\ndimensional subspace spanned by ˆV with\n\n                           ˆV = QR([vnullspace1           , ..., vnullspaced           , vrowspace1           , ..., vrowspaced        ])\n\ncomposed out of the decomposed subspace vectors and orthonormalized using QR-decomposition\n(see Figure 16). For patching the output of layer 8, FLDD and interchange accuracy remained simi-\nlarly high. (FLDD: 200%, interchange accuracy: 86%). However, patching on MLP8 mostly removes\nthe effect of the patch (FLDD: 17%, interchange accuracy: 2%). Thus, the causally disconnected\nsubspace is required for patching MLP8 which suggests that the interpretability illusion generalizes\nto higher-dimensional subspaces.\n\n\nB.7  Overfitting on Small Datasets\n\nHow important is a large and diverse dataset for training DAS? We initially hypothesized that for\nvery small datasets, it is possible to find working subspaces in all layers as there are only a few\nfixed activation vectors in each layer and we might be able to find subspaces that utilize this noise\nto overfit.\n   To test this, we created a small IOI dataset containing only two names from a fixed template.\nWe fitted a one-dimensional subspace using DAS for every layer on that dataset and the full dataset\nas a control (Figure 17). We repeated the experiment for subspaces in the MLP and residual stream\nand evaluated the subspaces on their train distribution and a test distribution containing all names\nand templates. FLDD was highest in layer 8, the component between S-inhibition heads and name\nmovers, and also high in neighboring layers that still contain IOI information (e.g. some of the\nS-inhibition heads are in layer 7 and some of the name movers are in layer 10). Moreover, train\nFLDD was significantly higher than test FLDD when trained on only 2 names.\n   Importantly, we also observe that subspaces optimized on the small dataset reached a FLDD\nbigger than zero in some of the other layers but contrary to our expectation, this was neither high\nin absolute terms nor compared to subspaces trained on the full distribution (see Figure 17).\n\n\n\n\n\n                                          38\n\n                      2.0\n\n                      1.5\n\n                       FLDD 1.0\n\n                      0.5\n\n                      0.0\n                              S-Inhib    S-inhib    MLP8    MLP8\n                             decomposed       decomposed\n\n\nFigure 16: Fractional logit difference decrease (FLDD) for patching a 100-dimensional subspace on\nthe S-inhibition heads or on MLP8; \"decomposed\" patches the 200-dimensional subspace made out\nof the nullspace projection vectors into WQ of name mover or Wout of MLP8, respectively, and their\northogonal complements\n\nC  Additional Details for Section 5\n\nWhich model components write to the vresid direction? To test how every attention head and MLP\ncontributes to the value of projections on vMLP, we sampled activations from head and MLP outputs\nat the last token position of IOI prompts, and calculated their dot product with vresid (Figure 20). We\nfound that the dot products of most heads and MLPs was low, and that the S-inhibition heads were\nthe only heads whose dot product differed between different patterns ABB and BAB. This shows\nthat only the S-inhibition heads write to the vresid direction (as one would hope). Importantly, this\ntest separates vresid from the interpretability illusion vMLP. While patching vMLP8 also writes to\nvresid8 (i.e. vMLP8Wout ≈vresid8), the MLP layer does not write this subspace on the IOI task (see\nFigure 4). This further supports the observation that the vMLP patch activates a dormant pathway\nin the model.\n   Generalization beyond the IOI distribution. We also investigate how the subspace generalizes.\nWe sample prompts from OpenWebText-10k and look at those with particularly high and low\nactivations in vsinhib. Representative examples are shown in Figure 21 together with the name\nmovers attention at the position of interest, how the probability changes after subspace ablation,\nand how the name movers attention changes.\n    Stability of found solution. Finally, we note that solutions found by DAS in the residual stream\nare stable, including when trained on a subset of S-inhibition heads (see Figure 18).\n\n\nD  Additional details for Section 6\n\nD.1  Dataset construction and training details\n\nWe use the first 1000 examples from the COUNTERFACT dataset (Meng et al., 2022a). We filter the\nfacts which GPT2-XL correctly recalls. Out of the remaining facts, for each relation we form all pairs\n\n\n                                          39\n\nof distinct facts, and we sample 5 such pairs from each relation with at least 5 facts. This results in a\ncollection of 40 fact pairs spanning 8 different relations. We then use these facts as follows:\n\n  • for the ROME experiments in Subsection 6.3, we define edits by requesting one of the facts in\n     each pair to be rewritten with the object of the other fact;\n\n  • for the activation patching experiments in Subsection 6.1, we patch from the last token of s′ in\n    B to the last token of s in A (prior work has shown that the fact is retrieved on s (Geva et al.,\n      2023)), and we again use DAS Geiger et al. (2023b) to optimize for a direction that maximizes\n      the logit difference between o′ and o.\n\n\nD.2  Additional fact patching experiments\n\nIn figure 24, we show the distribution of the fractional logit difference metric (see Subsection 4.2 for\na definition) when patching between facts as described in Subsection 6.1. Like in the related Figure\n7, we observe that, while patching along the directions found by DAS achieves strongly negative\nvalues (indicating that the facts are very often successfully changed by the patch), the interventions\nthat replace the entire MLP layer or only the causally relevant component of the DAS directions\nhave no such effect.\n   Next, we observe that the nullspace component of the patching direction is the one similar to\nthe variation in the inputs (difference of last-token activations at the two subjects). Specifically, in\nFigure 25, we plot the (absolute value of the) cosine similarity between the difference in activations\nfor the two last subject tokens, and the nullspace component of the DAS direction. We note that\nthis similarity is consistently significantly high (note that it can be at most 1, which would indicate\nperfect alignment).\n    Finally, we observe that the nullspace component of the patching direction is a non-trivial part\nof the direction in Figure 26, where we plot the distribution of the ℓ2 norm of this component.\n\n\nD.3 ROME implementation details\n\nROME takes as input a vector k ∈RdMLP representing the subject (e.g. an average of last-token\nrepresentations of the subject) and a vector v ∈Rdresid which, when output by the MLP layer, will\ncause the model to predict a new object for the factual prompt, but at the same time won’t change\nother facts about the subject. ROME modifies the MLP weight by setting W′out = Wout + ab⊤, where\na ∈Rdresid, b ∈RdMLP are chosen so that W′outk = v, and the MLP’s output is otherwise minimally\nchanged. Without loss of generality, the first condition implies that a = v −Woutk and b⊤k = 1;\nthe second condition is then modeled by minimizing the variance of b⊤x when x ∼N (0, Σ) for\nan empirical estimate Σ ∈RdMLP×dMLP of the covariance of MLP activations (see Lemma D.1 in\nAppendix D for details and a proof). In all our experiments involving ROME, we use GPT2-XL\n(Radford et al., 2019), and we use the precomputed values of Σ from Meng et al. (2022a) accessible\nonline here.\n\n\nD.4 ROME as an Optimization Problem\n\nWe now review the ROME method from Meng et al. (2022a) and show how it can be characterized\nas the solution of a simple optimization problem. Following the terminology of 6.4, let us have an\nMLP layer with an output projection Wout, a key vector k ∈RdMLP and a value vector v ∈Rdresid.\n\n\n\n\n                                          40\n\n   In Meng et al. (2022a), equation 2, the formula for the rank-1 update to Wout is given by\n\n                                            k⊤Σ−1\n                      W′out = Wout + (v −Woutk)                                          (6)                                           k⊤Σ−1k\n\nwhere Σ is an empirical estimate of the uncentered covariance of the pre-Wout activations. We\nderive the following equivalent characterization of this solution (which may be of independent\ninterest):\n\nLemma D.1. Given a matrix Wout ∈Rdresid×dMLP, a key vector k ∈RdMLP and a value vector v ∈Rdresid, let\nΣ ≻0, Σ ∈RdMLP×dMLP be a positive definite matrix (specifically, the uncentered empirical covariance), and\nlet x ∼N (0, Σ) be a normally distributed random vector with mean 0 and covariance Σ. Then, the ROME\nweight update is W′out = Wout + ab⊤where a ∈Rdresid, b ∈RdMLP solve the optimization problem\n\n               min trace(Covx W′outx −Woutx )   subject to  W′outk = v.\n                            a,b\n\nIn other words, the ROME update is the update that causes Wout to output v on input k, and minimizes the\ntotal variance of the extra contribution of the update in the output of the MLP layer under the assumption\nthat the pre-Wout activations are normally distributed with covariance Σ.\n\nProof. We have W′outx −Woutx = ab⊤x. Next, Using Ex[xx⊤] = Σ and the cyclic property of the\ntrace, we see that\n\n                          trace(Covx W′outx −Woutx ) = ∥a∥22b⊤Σb\n\nWe must have ab⊤k = v −Woutk, so without loss of generality we can rescale a, b so that a =\nv −Woutk. Then, we want to solve the problem\n\n                        min b⊤Σb  subject to  b⊤k = 1\n                                 b\n\nwhich we can solve using Lagrange multipliers. The Lagrangian is\n\n                                         1\n                               L(b, λ) = 2b⊤Σb −λb⊤k\n\nand the derivative w.r.t. b is Σb −λk = 0, which tells us that b is in the direction of Σ−1k. Then the\n                                                                                 k⊤Σ−1constraint b⊤k = 1 forces the constant of proportionality, and we arrive at b =                                                                               k⊤Σ−1k\n\nD.5  Connection between 1-dimensional activation patching and model editing\n\nLemma D.2. Given prompts A and B, two token positions tA, tB, and an MLP layer with output projection\nweight Wout ∈Rdresid×dMLP, let uA, uB ∈RdMLP be the respective (post-nonlinearity) activations at these\ntoken positions in this layer. If v is a direction in the activation space of the MLP layer, then there exists a\nROME edit W′out = Wout + ab⊤such that the activation patch from uB into uA along v and the edit result\nin equal outputs of the MLP layer at token tA when run on prompt A. Moreover, the ROME edit is given by\n\n                 a =  (uB −uA)⊤v  Woutv   and any b that satisfies  b⊤uA = 1.\n\n               Σ−1uAChoosing b =        minimizes the change to the model (in the sense of Meng et al. (2022a)) over all such\n                uTAΣ−1uA\nrank-1 edits.\n\n\n                                          41\n\nProof. The activation after patching from B into A along v is u′A = uA + ((uB −uA)⊤v)v, which\nmeans that the change in the output of the MLP layer at this token will be\n\n                           Woutu′A −WoutuA = ((uB −uA)⊤v)Woutv\n\nThe change introduced by a fact edit at this token is\n\n             W′outuA −WoutuA = ab⊤uA =  b⊤uA   (uB −uA)⊤v  Woutv\n\nand the two are equal because b⊤uA = 1.\n   To find the b that minimizes the change to the model, we minimize the variance of b⊤x when\nx ∼N (0, Σ) subject to b⊤uA = 1. The variance is equal to b⊤Σb, so we have a constrained (convex)\nminimization problem\n\n                                1\n                        min                                             subject to  b⊤uA = 1                           2b⊤Σb\n\nThe rest of the proof is the same as in Lemma D.1. Namely, we can solve this optimization problem\nusing Lagrange multiplies. The Lagrangian is\n\n                                        1\n                                 L(b, λ) = 2b⊤Σb −λb⊤uA\nand the derivative w.r.t. b is Σb −λuA = 0, which tells us that b is in the direction of Σ−1uA. Then\nthe constraint b⊤uA = 1 forces the constant of proportionality.\n\nD.6  Additional experiments comparing fact patching and rank-1 editing\n\nIn Figure 27, we plot the distributions of the logit difference between the correct object for a fact and\nthe object we are trying to substitute when patching the 1-dimensional subspaces found by DAS,\nand performing the equivalent rank-1 weight edit according to Lemma D.2. We observe that the\ntwo metrics quite closely track each other, indicating that the additional effects of using a weight\nedit (as opposed to only intervening at a single token) are negligible.\n    Similarly, in Figure 28, we show the success rate of the the two methods in terms of making the\nmodel output the object of the fact we are patching from. Again, we observe that they quite closely\ntrack each other.\n\n\nD.7  From Rank-1 Model Edits to Subspace Interventions\nIn this section, we describe how, given a rank-1 edit W′out = Wout + abT, to obtain a direction\nv ∈RdMLP such that intervening on the model by setting the projection on v to some constant c ∈R\n(at each token) is approximately equivalent to intervening via the rank-1 edit.\n    Specifically, given an activation x ∈RdMLP, the patched activation is x′ = x +  c −vTx v and\nthe extra contribution of the subspace intervention to the output of the MLP layer will be\n\n                       contribsubspace(x) = Woutx′ −Woutx =  c −vTx Wv.\n\nSimilarly, the extra contribution of the rank-1 edit to the output of the MLP layer is\n\n                            contribrank-1(x) = W′outx −Woutx =  bTx  a.\n\n\n                                          42\n\nRecall (see Appendix D.4) that the ROME method (Meng et al., 2022a) implicitly treats the activation\nx as a random vector sampled from N (0, Σ) where Σ is an empirical estimate of the covariance.\nIn particular, this distribution is used to quantify the amount to which a rank-1 edit changes the\nmodel.\n   Motivated by this, we formalize approximating the rank-1 edit by the subspace intervention\nusing the following criteria analogous to the ROME method:\n\n  • Ex∼N (0,Σ)  contribsubspace(x) = Ex∼N (0,Σ) [contribrank-1(x)], i.e. the interventions have the\n    same expectation;\n\n  • Woutv ∥a, i.e. the interventions point in the same direciton;\n\n  • trace Covx  contribsubspace(x) −contribrank-1(x)   is minimized, i.e. the two interventions\n      are maximally similar with respect to the activation distribution modeled as x ∼N (0, Σ)\n      (this is the criterion used by ROME; recall D.4).\n\n   The expectation of contribrank-1(x) is zero, while the expectation of contribsubspace(x) is cWoutv,\nand since Woutv = 0 would lead to a trivial intervention, we must have\n\n                                                  c = 0.\n\nNext, to ensure Woutv ∥a, we have to pick v = αW+outa + u where u ∈ker Wout. With this, the\ncovariance minimization can then be written as\n\n                          min ∥a∥22 (b + αv)T Σ (b + αv)                                                 α,v\n\n(this is a similar derivation to the one in Appendix D.4). After removing constant terms and setting\nw = u/α, we are left with\n\n              min                     h α4 W+outa + w T Σ W+outa + w + 2α2bTΣ W+outa + w i .                       α,w\n\nsubject to Woutw = 0. The Lagrangian is\n\n      L (α, w, λ) = α4 W+outa + w T Σ W+outa + w + 2α2bTΣ W+outa + w + λTWoutw\n\nwith the first-order conditions\n\n                   ∂L\n                = 2α4Σ W+outa + w + 2α2Σb + WToutλ = 0\n                   ∂w\nand ∂L/∂λ = Woutw = 0. Multiplying the w derivative with WoutΣ−1 on the left gives us a linear\nsystem for λ:\n\n                        WoutΣ−1WToutλ = −2α2Woutb −2α4a,\n\n                                                                   ∂L = 0 gives us w. In practice,which can be solved assuming we know α, and then substituting λ in ∂w\nwe guess several values for α (typically, α2 = 0.05 performs best) and pick the one resulting in the\nbest value for the objective.\n\n\n\n\n\n                                          43\n\nE  Additional Details for Section 7\n\nE.1  Prevalence of Causal Directions in MLP Layers\n\nGiven an MLP activation x and a vector u ∈RdMLP, changing the projection of x on u means\nreplacing x with the new activation x′ = x + αu for some α ∈R. This translates to the new output\nof the MLP layer being\n\n                              Wout (x + αu) = Woutx + αWoutu.\n\nUnder our assumptions, the direction u will be causally relevant if the extra contribution to the\nresidual stream αWoutu points along v; thus it suffices to find a u such that Woutu ∥v.\n   As it turns out, we can simply choose u = W+outv. Indeed, we empirically observe that Wout ∈\nRdresid×dMLP is a full-rank matrix9, with almost all singular values bounded well away from 0 (see\nAppendix E.5). Since dMLP > dresid, it follows that WoutW+outv = v. This establishes that u is a causal\ndirection.\n\n\nE.2  Prevalence of Directions Discriminating for C in MLP layers\n\nFor a feature u ∈RdMLP to discriminate between values of C, we need projections of the post-\nnonlinearity activations on u to linearly separate examples according to the values of C. By\nassumption, v is a good linear separator for the values of C in the residual stream. We can thus\nframe our goal as a more general question:\n\n If two sets of activations are linearly separable in the residual stream, are their images after the non-linearity\n                                   also (approximately) linearly separable?\n\nThe transformation from residual vectors x ∈Rdresid to post-nonlinearity activations is given by the\nsteps\n\n                    x 7→LayerNorm (Winx) 7→gelu (LayerNorm (Winx))\n\nThe composition of LayerNorm and Win is approximately a linear operation (Elhage et al., 2021),\nso the values of the concept C are also linearly separated in the pre-gelu activations. However, it is\nnot a priori clear if the gelu operation (approximately) preserves linear separability.\n  We show ample empirical evidence in Appendix E.4 that this transformation approximately\npreserves the Euclidean geometry of activations in a certain restricted sense; then, we prove that this\npreservation implies that points remain approximately linearly separable after this transformation.\nWe further argue this empirically in Appendix E.6, where we show that linear separability is\napproximately preserved in MLP activations for random directions v in the residual stream.\n\n\nE.3  Empirical Analysis of Distortion Introduced by the Non-linearity\n\nMethodology. We use the first 10K texts of OpenWebText dataset (Gokaslan & Cohen, 2019). Each\nof these texts contains 1,024 tokens; we pass each text through GPT-2 Small, and for each layer\n\n   9This is also heuristically plausible: models want to maximize their expressive capacity, and pre-training datasets are\nvery complex, so making Wout low-rank would not be preferred by optimization.\n\n\n\n\n\n                                          44\n\ncollect the pre-gelu activations xi of the MLP layer, as well as the values zi = projker Wout (gelu (xi)).\nWe sample 250 quadruples of distinct i, j, k, l per text, and compute the values\n                                                      aijkl =  xi −xj ⊤(xk −xl)\n                                                     bijkl =  zi −zj ⊤(zk −zl)\n\nWe collect these numbers across the first 1000 texts out of the first 10K, resulting in 250K datapoints\nper layer, and perform linear regression of bi against ai.\n  We note that there is some inherent linearity in the quantities aijkl, bijkl that could in principle\nskew the results of the linear regression towards a higher r2 statistic in the presence of enough\nsamples. In particular, there are linear dependencies of the form\n\n                                                            aijkl = aijkp + aijpl\n\nfor any p, and similarly for the bijkl quantities. This makes these quantities potentially misleading\ntargets for linear regression. However, in our regime, we sample 250 4-element subsets from the set\n{1, . . . , 1024}, and the probability of sampling quadruples that are linearly related is quite small.\n\nResults. We find that the coefficients of determination r2 for the linear regression are consistently\nhigh (≈0.8 or higher) for all layers except for layer 0, indicating a high degree of fit. The r2 values\nare given in Table TODO, and regression plots are shown in Figure TODO. We also remark that the\nintercept coefficients are ≈0 relative to the standard deviation in the dependent variable.\n\n\nE.4  Details for Subsection E.2\n\nWe will show the stronger property that activations remain approximately linearly separable even\nafter projecting on the kernel of Wout. Define the function\n                                                       gelu  x′                                            f  : x′ 7→gelu  x′  7→projker Wout\nwhere projker Wout is orthogonal projection on the kernel of Wout.\n   To overcome the non-linearity of f, we establish an empirical property of f on activations from\nthe model’s pre-training distribution. Specifically, we show that  f approximately preserves the\nEuclidean geometry of activations in a certain restricted sense:\n                                f (xi) −f (xj) ⊤( f (xk) −f (xl)) ≈λ  xi −xj ⊤(xk −xl) + η                   (7)\n\nfor λ > 0 and η ≈0 (relative to the standard deviation of the expression on the left-hand side of\nthe approximation). Specifically, we perform linear regression of   f (xi) −f (xj) ⊤( f (xk) −f (xl))\nusing  xi −xj ⊤(xk −xl) as the predictor variable in Appendix E.3, and find very high coefficients\nof determination (r2 ≈0.8) in all layers except for layer 0. To avoid relying solely on the coefficient\nof determination, we also generate regression plots for the data; see Figure 29 for regression lines\nover samples of 104 points from each layer of GPT2-Small.\n    Finally, we prove that f maintains linear separability if we assume that Equation 7 holds exactly\nwith η = 0:\nLemma E.1. Let xi ∈Rd, 1 ≤i ≤n be linearly separable with respect to binary labels yi ∈{−1, 1}. Let\n f  : Rd →Rd be a transformation with the property that\n                                   f (xi) −f (xj) ⊤( f (xk) −f (xl)) = λ  xi −xj ⊤(xk −xl)\n\nfor all distinct i, j, k, l and some λ > 0. Then, f (xi) are linearly separable with respect to the labels yi as well.\n\n\n                                          45\n\nProof. Consider the hard SVM objective for the points (xi, yi),\n\n                           1\n                    min  ∥w∥22   subject to   yi  w⊤xi + b ≥1.                                  w,b 2\n\nSince the examples are linearly separable, we know that the minimizer (w∗, b∗) exists and satisfies\nall constraints. Furthermore, from the optimality conditions of the dual formulation of the objective\nwe know that we can write w∗= ∑i αixi where ∑i αi = 0 (see for example Awad et al. (2015)). Let\nS = {s1, . . . , st} ⊂{1, . . . , n} be the support of α, i.e. the indices sj such that αsj ̸= 0. Since ∑i αi = 0,\nwe can rewrite w∗as\n\n                     w∗= ∑ βj   xsj −xsj+1\n                                                                                               j\n\nwith indices modulo |S|. Since (w∗, b∗) is a separating hyperplane for (xi, yi), we have\n\n                          (w∗)⊤xi ≥1 −b  when   yi = 1\n                          (w∗)⊤xi ≤−1 −b  when   yi = −1\n\nand thus\n\n                      (w∗)⊤  xi −xj ≥2  when   yi = 1, yj = −1.\n\nUsing the expansion of w∗as a linear combination of differences between examples, this says\n\n                            ⊤\n        ∑ βj   xsj −xsj+1      xi −xj ≥2  when   yi = 1, yj = −1.\n                                             j\n\nand thus\n\n                          ⊤\n    ∑ βj    f (xsj) −f (xsj+1)       f (xi) −f (xj) ≥2λ > 0  when   yi = 1, yj = −1.\n                         j\n\nNow we claim that\n\n                    = ∑ βj    f (xsj) −f (xsj+1)                                          bw                                                                                       j\n\nis a linear separator for ( f (xi), yi) for some bias to be determined later. Indeed, let M = minyi=1 bw⊤f (xi)\nand m = maxyi=−1 bw⊤f (xi). Then we have M −m ≥2λ > 0. Choosing any bb ∈(m, M), we have\n\n                                               0  when   yi = 1                            bw⊤f (xi) −bb ≥M −bb >\n                                             0  when   yi = −1                          bw⊤f (xi) −bb ≤m −bb <\nwhich shows that ( bw,bb) linearly separates the points ( f (xi), yi).\n\nE.5 MLP weights are full-rank matrices\n\nIn figure 30, we plot the 100 smallest singular values of the MLP weights in GPT2-Small for all 12\nlayers. We observe that they the vast majority are bounded well away from 0. This confirms that\nboth MLP weights are full-rank transformations.\n\n\n                                          46\n\nE.6  Features in the residual stream propagate to hidden MLP activations\n\nIntuition. Suppose we have two classes of examples that are linearly separable in the residual\nstream. The transformation from the residual stream to the hidden MLP activations is a linear map\nfollowed by a nonlinearity, specifically x 7→gelu(Winx). As we observed in E.5, the Win matrix is\nfull-rank, meaning that all the information linearly present in x will also be so in Winx. Even better,\nsince Win maps x from a dresid-dimensional space to a dMLP = 4dresid-dimensional space, this should\nintuitively make it much easier to linearly separate the points, because in a higher-dimensional\nspace there are many more linear separators. On the other hand, the non-linearity has an opposite\neffect: by compressing the space of activations, it makes it harder for points to be separable. So it is\na priori unclear which intuition is decisive.\n   Empirical validation. However, it turns out that empirically this is not such a problem. To\ntest this, we run the model GPT2-Small on random samples from its data distribution (we used\nOpenWebText-10k), and extract 2000 activations of an MLP-layer after the non-linearity. We\ntrain a linear regression with ℓ2-regularization to recover the dot product of the residual stream\nimmediately before the MLP-layer of interest and a randomly chosen direction. We repeat this\nexperiment with different random vectors and for each layer. We observe that all regressions are\nbetter than chance and explain a significant amount of variance on the held-out test set (R2 =\n0.71 ± 0.17, MSE = 0.31 ± 0.18, p < 0.005). Results are shown in Figure 31 (right) (every marker\ncorresponds to one regression model using a different random direction).\n   The position information in the IOI task is really a binary feature, so we are also interested in\nwhether binary information in general is linearly recoverable from the MLP activations. To test this,\nwe sample activations from the model run on randomly-sampled prompts. This time however, we\nadd or subtract a multiple of a random direction v to the residual stream activation u, and calculate\nthe MLP activations using this new residual stream vector u′:\n\n                                        u′ = u + y × z × ∥u∥2 × v\n\nwhere y ∈{−1, 1} is uniformly random, z is a scaling factor we manipulate, and v is a randomly\nchosen direction of unit norm. For each classifier, we randomly sample a direction v that we either\nadd or subtract (using y) from the residual stream. The classifier is trained to predict y. We rescale\nv to match the average norm of a residual vector and then scale it with a small scalar z.\n   Then, a logistic classifier is trained on 1600 samples. Again, we repeat this experiment for\ndifferent v and z, and for each layer. We observe that the classifier works quite well across layers\neven with very small values of z (still, accuracy drops for z = 0.0001). Results are shown in Figure\n31 (right), and Table 2.\n\n\n                       Table 2: Mean Accuracy for Different Values of z\n                                       z    Mean Accuracy\n                                   0.0001        0.69\n                                    0.001         0.83\n                                      0.01          0.87\n                                        0.1         0.996\n\n\n\n\n\n                                          47\n\n               MLP patch                    Residual stream patch\n\n\n        2.00\n\n set)    1.75\n        1.50\n\n\n        1.25   training\n        1.00\n\n\n        0.75  (small\n\n        0.50\n FLDD    0.25\n\n\n        0.00\n\n\n\n\n        2.00\n\n set)    1.75\n\n        1.50\n\n\n        1.25   training\n  (full    1.00\n        0.75\n\n\n        0.50 FLDD\n\n        0.25\n\n\n        0.00\n\n\n             0   1   2   3   4   5   6   7   8   9  10  11    0   1   2   3   4   5   6   7   8   9  10  11\n                                    Layer                                              Layer\n\n\nFigure 17: FLDD for different IOI-position subspaces: Subspaces were fitted to either a small\nversion of the IOI dataset that only contained 2 names (first row) or on the full dataset (second row)\nusing activations from the MLP (first column) or the residual stream (second column). Subspace\nperformance on the IOI task was evaluated on the training (blue) distribution and the full test\ndataset containing all names (orange)\n\n\n\n\n\n                                          48\n\nFigure 18: Cosine Similarity between learned position subspaces in the S-inhibition heads is high\neven when using only a subset of S-inhibition heads for training\n\n\n\n\n\n                  20\n                         ABB\n                         ABA\n\n                  10\n\n\n                   0                                                    Activation\n\n                  10                                          Subspace\n\n                  20\n\n\n                  30\n                      <|endoftext|>Then         ,   {S1 or IO}  and {S1 or IO}  had      lots      of      fun      at      the   {place}       .     {S2}   gave     a   {object}    to\n                                                                                                              Position\n\nFigure 19: The IOI position subspace activates at words that predict a repeated name. S-inhibition\nsubspace activations for different IOI prompts per position\n\n\n\n\n\n                                          49\n\n                      Layer 5                                Layer 6                                Layer 7                                Layer 8\n     4\n\n\n     2\n\n\n     0\n   Product     2\n Dot\n\n     4\n\n                    ABB\n     6               BAB\n                              S-Inhibition heads\n        0  1  2  3  4  5  6  7  8  9 10 11 mlp   0  1  2  3  4  5  6  7  8  9 10 11 mlp   0  1  2  3  4  5  6  7  8  9 10 11 mlp   0  1  2  3  4  5  6  7  8  9 10 11 mlp\n                        Head                                    Head                                    Head                                    Head\n\nFigure 20: S-Inhibition heads but not MLP8 write to the position subspace in the residual stream\nthat is causally connected to the name movers on the IOI task\n\n\n\n\n\n                                          50\n\n     OpenWebText-10k and                       Attention of Name Movers                               Logit Difference      Subspace Activation                            at 2nd last position\n       <|endoftext|>\n           An\n        Oklahoma\n           senate\n            panel\n         approved\n            a\n                   bill\n         Monday\n               to\n           prevent\n               city\n           and\n           county\n        governments\n           from\n             out\n           doing\n             state\n            laws\n            on\n       employment\n           and\n10          public\n      accommodations\n                        . 0\n\n           The10\n                   bill\n               initially\n         prevented\n              local\n        governments\n           from\n            going\n            further\n            than\n           any\n             state\n            law\n                        .\n           A\n         committee\n           substitute\n           focused\n                      it\n            on\n       employment\n           and                                                         Original             public                                         Mean Ablated      accommodations\n\n\n\n                        51\n\n  OpenWebText-10k and                        Attention of Name Movers                           Logit Difference   Subspace Activation                              at 2nd last position\n       morning\n            late\n            last\n       month\n                   ,\n         Rays\n         ha\n        wn\n       Chandler\n         and\n          her\n          wife\n         Avery\n       Chandler\n          pulled\n           into\n          the\n        parking\n              lot\n            at\n          the\n          7\n10       30\n         Mid\n 5      town\n         building\n 0       and\n        began\n 5      walking\n        towards\n10        the\n        entrance\n                   .\n\n\n\n        Baby\n                   ,\n         you\n          see\n          the\n        news\n         trucks\n           ?\n\n         Rays\n         ha\n        wn\n        asked                                                         Original         Avery                                         Mean Ablated                   .\n\n\n\n                        52\n\n    OpenWebText-10k and                       Attention of Name Movers                              Logit Difference     Subspace Activation                             at 2nd last position\n          AB\n           key\n           two\n           times\n           you\n                will\n           see\n             that\n                will\n          appear\n                  all\n            the\n        programs\n                     /\n            scripts\n             that\n          begins\n            with\n           gn\n        ome\n                    -\n           co\n         characters\n          and10             that\n            are\n 0         on\n          synaptic\n             or\n10            in\n            our\n         repositories\n           from\n          source\n                      .\n                  list\n                      .\n             This\n                  is\n            helpful\n         when\n           you\n          don\n                     't\n       remember\n           exactly\n            the\n        name\n              of\n            a\n         program                                                         Original             or                                         Mean Ablated        maybe\n\n\n\n                        53\n\n                                  0\n\n                                  5\n\n                                 10                                                                                                         difference\n                                 15\n                                                    logit                                  Method                                 20                                                             DAS\n                                                                                                  Full MLP patch\n                                 25                             DAS rowspace component\n                                     0    5    10   15   20   25   30   35   40   45                                                                                                         Fractional\n                                    Intervention layer\n\nFigure 24: Fractional logit difference distributions under three interventions: patching along the\ndirection found by DAS (blue), patching the component of the DAS direction in the rowspace of\nWout (green), and patching the entire hidden MLP activation (orange).\n\n\n\n\n\n                                     1.0\n\n\n                                     0.8\n\n\n                                     0.6                                                                                                         similarity\n                                                               cosine 0.4\n                                     0.2\n\n\n                                     0.0\n                                    0    5    10   15   20   25   30   35   40   45                                                                                    Absolute\n                                    Intervention layer\n\nFigure 25: Distribution of the absolute value of the cosine similarity between the nullspace compo-\nnent of the DAS fact patching directions and the difference in activations of the last tokens of the\ntwo subjects.\n\n\n\n\n\n                                          54\n\n                                     1.0\n\n\n                                     0.8\n\n\n                                     0.6                                                                                              component\n                                     0.4\n\n\n                                     0.2                                                                                              nullspace\n                     of                                     0.0\n                                    0    5    10   15   20   25   30   35   40   45\n                                    Intervention layer                                          Norm\n\nFigure 26: Distribution of the norm of the nullspace component of the DAS direction across\nintervention layers.\n\n\n\n\n\n                                10\n\n                                 0\n\n                                10\n\n                                20\n\n                                30                                                                                                        difference  40\n\n                                50                                                              Method                                                    Logit\n                                60                                          rank-1 model edit\n                                                                                  1-dimensional patch\n                                70\n                                     0    5    10   15   20   25   30   35   40   45\n                                    Intervention layer\n\nFigure 27: Comparison of logit difference between 1-dimensional fact patches and their derived\nrank-1 model edits\n\n\n\n\n\n                                          55\n\n                                     1.0                                Method\n                                                                                  rank-1 model edit\n                                                                                 1-dimensional patch\n                                     0.8\n                                          rate\n                                                                         success 0.60.4\n                                                    patch 0.2\n                                          Fact 0.0\n                                    0    5    10   15   20   25   30   35   40   45\n                                    Intervention layer\n\nFigure 28: Comparison of fact editing success rate between 1-dimensional fact patches and their\nderived rank-1 model edits\n\n\n\n\n\n                                          56\n\nFigure 29: Regression plots accompanying the experiments in Appendix E.4.\n\n\n\n\n\n                                 57\n\n                  4.0                                                   6\n\n                  3.5\n                                                                     5\n                  3.0\n                                                                     4            value      2.5\n\n                  2.0                                                   3\n\n                  1.5                   Singular                                                           2\n                  1.0\n                                                                     1                  0.5\n\n                  0.0                                                   0\n                    0       20      40      60      80      100          0       20      40      60      80      100\n\n\n                   Index (decreasing)                   Index (decreasing)\n\nFigure 30: Smallest 100 singular values of the Win (left) and Wout (right) MLP weights by layer in in\nGPT2-Small\n\n\n\n\n\nFigure 31: Recovering residual stream features linearly from hidden MLP activations: classification\n(left) and regression (right).\n\n\n\n\n\n                                          58\n\n## Claim to Classify\n\"patching of subspaces can lead to an illusory sense of interpretability\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Subspace activation patching",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The claim 'patching of subspaces can lead to an illusory sense of interpretability' is from the abstract and discusses a limitation of subspace activation patching, which is an interventional method (Rung 2). The claim asserts a mechanistic insight about interpretability illusions, implying a counterfactual understanding of how models can mislead researchers, which is a Rung 3 claim. The language 'illusory sense of interpretability' suggests a unique mechanism or underlying truth about interpretability methods, fitting Rung 3 markers like 'underlies' or 'mechanistic narrative'. There is no hedging present. Confidence is 4 due to some ambiguity in whether the claim is about a general phenomenon or a specific finding, but the abstract context supports a Rung 3 interpretation."
  },
  "metadata": {
    "model": "deepseek/deepseek-v3.2",
    "usage": {
      "prompt_tokens": 42523,
      "completion_tokens": 214,
      "total_tokens": 42737
    },
    "finish_reason": "stop"
  }
}