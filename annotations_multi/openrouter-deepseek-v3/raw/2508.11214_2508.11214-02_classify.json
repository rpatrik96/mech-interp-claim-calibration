{
  "timestamp": "2026-02-19T12:02:14.681009+00:00",
  "paper_id": "2508.11214",
  "claim_id": "2508.11214-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2508.11214\nTitle: How Causal Abstraction Underpins Computational Explanation\nFull text:\n             HOW CAUSAL ABSTRACTION UNDERPINS\n                   COMPUTATIONAL EXPLANATION\n\n\n                       ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n                 Abstract. Explanations of cognitive behavior often appeal to computations over represen-\n                       tations. What does it take for a system to implement a given computation over suitable\n                       representational vehicles within that system? We argue that the language of causality—and\n                         specifically the theory of causal abstraction—provides a fruitful lens on this topic. Drawing\n                  on current discussions in deep learning with artificial neural networks, we illustrate how\n                         classical themes in the philosophy of computation and cognition resurface in contemporary2025              machine learning. We offer an account of computational implementation grounded in causal\n                       abstraction, and examine the role for representation in the resulting picture. We argue that\n                      these issues are most profitably explored in connection with generalization and prediction.Aug\n15                                                1. Introduction\n\n             Imagine how the sciences of the mind would look if we had free, unconstrained observational\n          and experimental access to brains. Suppose we could record neural activity at arbitrarily\n             fine levels of fidelity, while an organism is performing any given task, and even intervene on\n           that activity with limitless precision. Impressive technical advances notwithstanding, such a[cs.LG]            scenario would of course be far from our present predicament. It is nonetheless worth asking:\n                  if we enjoyed such access, what work would remain before we could explain or understand\n           thought and behavior? Many authors over the decades have forcefully argued that even in\n           such a privileged epistemic position we would still likely be very far from good explanations\n          and deep understanding of human cognition (see Putnam 1975; Fodor 1975; Cummins 1977;\n          Marr 1982; Pylyshyn 1984; Jonas and Kording 2017, among many others).\n         An animating contention spurring the development of cognitive science over the past 70\n           years is that productive explanation of cognition—and especially of intelligent behavior in\n         humans—is often best targeted at higher levels of abstraction. Such explanations posit\n           computations over internal representations. While it is common across the sciences to model\n          phenomena computationally, e.g., simulating aspects of natural phenomena on computers, the\n           assumption here is that cognition can be usefully construed as itself a kind of computationalarXiv:2508.11214v1      or algorithmic process. This raises a distinctive question for the cognitive sciences: what\n          would it take for a physical system (such as a brain) to implement an algorithm? And what\n           does it take to identify the requisite representational vehicles in the system?\n             Early answers to this question ventured that there should be some mapping from states\n             of the physical system to states of a suitable computational model, such that transitions\n            in the model are “emulated” by transitions in the system (e.g., Putnam 1967). A series\n             of “triviality” arguments convinced many that stricter criteria were needed (see Sprevak\n           2018 for an overview). One compelling response, from Chalmers (1996, 2011a), characterizes\n           computational systems in terms of a formal object called a combinatorial state automaton\n          (CSA), essentially an automaton with vector-valued states. Implementation then requires\n            that the physical system be broken down into suitable parts, and that the causal relationships\n         among these parts be “isomorphic” to the transition structure of the corresponding CSA.\n                                                                1\n\n 2               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n  While Chalmers’ appeal to CSAs is highly suggestive, our aim here is to explore what\nhappens when details of the causal mapping are interrogated more closely, employing our\n best tools for reasoning about causality. We propose understanding the “isomorphism” or\n“emulation” relation by appeal to a precise notion of causal abstraction. Our first task is to\n articulate what this concept is and how it helps clarify the implementation relation.\n   Delving into these details raises pointed questions about which (causally and counterfactu-\n ally faithful) mappings between the physical system and the computational model should\nbe admissible. Our second task is to help clarify these questions. As we shall see, questions\nabout admissible mappings are intertwined with questions about the place of representation\n in computational explanation. We often want to think of internal vehicles taking on repre-\n sentational content when they occupy an appropriate causal role within a working system,\n including upstream causes of (tokenings of) the relevant representational vehicle, as well\n as downstream effects of it (e.g., Cummins 1975; Block 1986; Dretske 1988; Neander 2017;\nShea 2018). Representational content of a mental structure thus depends on what (if any)\n algorithmic procedures it implements, as the algorithmic (causally abstract) description helps\n carve the agent into causally relevant variables in the first place. Laxer mapping constraints\n for implementation may thereby admit an excess of representational claims. We shall explore\n this issue in some detail, drawing on current discussions in deep learning.\n  We intend our discussion to be general, applying to human cognition as well as to other\n intelligent agents and artifacts. However, much of our focus will be on deep neural network\n models, drawing upon and contributing to the emerging field of mechanistic1 interpretability.\nA significant strand of work has been grounded in theories of causal mediation and abstraction\n(Vig et al. 2020; Geiger et al. 2020, 2021, 2024; Finlayson et al. 2021; Chan et al. 2022; Meng\n et al. 2022; Wang et al. 2023; Geva et al. 2023; Hanna et al. 2023; Wu et al. 2023; see Mueller\n et al. 2024; Geiger et al. 2025 for overviews). A remarkable fact about artificial systems is that\nwe often do occupy the auspicious epistemic position with respect to these systems imagined\n in the opening paragraph: our observational and experimental access to them is limited only\nby the computational tractability of the tools we use to study them. For this reason, they\n provide a unique test bed for exploration of ideas about computational implementation and\n explanation (and indeed much else in cognitive science; cf. Frank and Goodman 2025), which\nwe believe will ultimately be portable to the study of humans and other animals.\n\n                                2. Computation and Causation\n\n  We follow a line of work in philosophy that understands computational explanation as a\n species of causal explanation.2 Meanwhile, we adopt a broadly interventionist approach to\n causal explanation, whereby explaining why A is a matter of identifying causal difference-\nmakers for A, answering “what-if-things-were-different” questions about A, and facilitating\nsome degree of (at least in-principle) manipulation and control of A (Woodward, 2003).\nThe distinctive feature of computational explanation is that the relevant causal structure is\n identified by a computational model. Computations, meanwhile, are commonly described\n at varying levels of detail (Marr, 1982). To take a popular example (e.g., Sprevak 2018),\nstochastic gradient descent is the name of a learning algorithm that can be implemented in any\n\n   1See Saphra and Wiegreffe (2024), who argue the term ‘mechanistic interpretability’ is polysemous,\n admitting multiple cultural and technical definitions.\n    2See, e.g., Putnam (1967); Chalmers (1996, 2011a); Scheutz (2001), among others. Not everyone endorses\n this assumption; e.g., Piccinini (2015); Rusanen and Lappi (2016) explicitly deny it.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION           3\n\nnumber of ways. The Adam algorithm commits to a particular parameter update schedule,\nwhile a specific implementation of Adam in Python will need to commit to even more details,\ne.g., a specific learning rate. And so on. This pervasive feature of computation suggests that\nour framework should accommodate descriptions at different levels of abstraction.\n  Another stance we adopt—not defended here—is that “computational model” should be\nunderstood as a prototype concept, without sharp boundaries or defining conditions. We\nhave lots of paradigmatic examples: automata, Turing machines, neural networks, Python\nprograms, pseudocode, and many more. There have been attempts to delineate precisely what\nan algorithm is (e.g., Moschovakis 2001; Thompson 2023); we leave the notion open-ended,\nallowing anything that has a computational or algorithmic “flavor.” We do, however, insist\nthat a computational model can be construed in causal terms. What exactly does this mean?\n  Most computational models already commit to some intuitive causal construal. Computer\nprogram analysis (debugging, etc.) regularly invokes cause-effect relationships among variables\nin programs (e.g., Zeller 2002). Turing’s (1936) landmark analysis of computability was\ncompelling partly because he provided a mechanical picture of how computation works, viz.\ntransitions caused by states of a system, reading and writing symbols on a tape, etc. A close\nformal correspondence between (e.g., Turing machine) programs and causal models can in\nfact be demonstrated (see Icard 2017; Ibeling and Icard 2019).\n  For our purposes3 a causal model M “ pV, Fq is a pair given by a set V of variables with\nsets of possible values ValpXq for each variable X P V, and a set of functional mechanisms\nFX : ValpYq Ñ ValpXq, one for each X P V, which produces a value for X as a function of\nvalues of some other set of “parent” variables Y Ď V.\n  As an elementary example of a computational model construed as a causal model, imagine\na binary circuit with four binary “input” variables A1, A2, A3, A4. Let the circuit include two\nXNOR (“biconditional”) gates, one for A1 and A2, and another for A3 and A4, the mechanisms\nof variables B1 and B2, respectively. Finally, another XNOR gate, the mechanism of C, takes\nB1 and B2 as input. Let FØ denote the binary function that returns 1 on inputs p0, 0q and\np1, 1q, and 0 otherwise.4 Then we have said that FB1 “ FB2 “ FC “ FØ. Meanwhile, let FAi\nfor i P t1, 2, 3, 4u be the (0-ary) constant function to value 0. This model looks as follows:\n\n\n\n                           C\n\n                          FØ\n                                B1       B2\n                    FØ           FØ\n\n\n                          A1     A2     A3     A4\n\n     Figure 1. A simple circuit construed as a causal model M. The arrows\n      denote functional dependence. For instance, there is an arrow from A4 to B2\n      because a change to A4 can (in some context) bring about a change to B2.\n\n  3The reader may consult Geiger et al. (2025) for many more details on the following. See also Pearl (2009);\nPeters et al. (2017) for more general treatments of causal models, especially as they feature in causal inference.\nFor simplicity, we omit any discussion of probability here, though much of the following generalizes.\n  4To simulate other inputs to the circuit, intervene on the parentless variables and fix them to a new value.\n\n 4               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n  Assuming—as we shall—that this dependence relation is acyclic, we can determine a\n“solution” to a model by reading off values of parentless variables and then iteratively\n determining values for subsequent variables along the dependence relation. When speaking\n of computational models, this is essentially a matter of “running the computation forward.”\n Let RunpMq be this (unique) solution of M. Note that RunpMq P ValpVq.\n  We can “intervene” on a model to simulate the effect of a manipulation to the system\n(Spirtes et al., 2000; Pearl, 2009). For instance, we might hold B2 fixed at 1. This involves\n overriding the mechanism FB2 with the constant function that always takes on value 1, in\n effect severing any dependence B2 had on other variables (in this case, A3 and A4):\n\n\n                            C\n\n                           FØ\n                                B1       B2\n                    FØ\n\n\n                           A1     A2     A3     A4\n\n\n Call an operation that replaces some mechanisms with constant functions a hard intervention.5\n  We could imagine other operations on models, e.g., overriding FB2 to make B2 now function\n as an AND gate instead of an XNOR gate. We could also make it depend on a different\n set of variables than it did initially; e.g., we might make B2 now depend on all four input\n variables, now computing a quaternary boolean function. More generally, if MV is the set\n of all causal models over these variables V, we will say an interventional is any function\n i : MV Ñ MV. Let Mi denote the manipulated model. As it is simply another model in MV,\nwe can “run” this manipulated model to obtain a solution RunpMiq just as before. Given a\n set I of interventionals, we will say that it forms an intervention algebra if it satisfies the\nsame algebraic laws as hard interventions. Roughly speaking, this means that interventionals\n in I are nicely modular: e.g., we can perform one intervention without perturbing others.6\n  On this characterization of causality, it should be clear that many familiar examples of\n computational models can be assimilated to causal models:  all we need is some way to\n carve the model into natural causal variables, together with a suitable notion of intervention\non the model. For Turing machines the variables can be taken as time-indexed tape cells,\nwhile interventions hold tape cells fixed to specified values, blocking rewrites to those cells.\n Chalmers’ CSAs also easily fit this mold, where state components play the role of variables.\n\n\n\n   5An important class of hard interventions are known as interchange interventions: those that fix variables to\n the value they would take if a different input were provided (Geiger et al., 2020; Vig et al., 2020). Interchange\n interventions are constrained to the space of values achieved for actual inputs, and recursive interchange\n interventions fix variables to the value they would take if a (recursive) interchange intervention had been\n performed (Geiger et al., 2025). Interchange interventions on neural networks sometimes go under the name\n activation patching (Wang et al., 2023; Zhang and Nanda, 2024) or resampling ablations (Chan et al., 2022).\nThey appear in causal mediation analysis to compute natural (in)direct effects (Pearl, 2001; Vig et al., 2020).\n   6See Geiger et al. (2025) for more precise definitions and motivation for the notion of intervention algebra,\n including a representation theorem with respect to composition of hard interventions.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION           5\n\nWe maintain that, when discussing computational explanation, we must always (if only\nimplicitly) commit to some causal construal of the computational model.7\n\n             3. Computational Implementation as Causal Abstraction\n\n  Suppose we have a candidate (“high-level”) computational model H, construed as a causal\nmodel. What does it take for a “low-level” system L to implement this computation? Here\nwe draw on a body of work exploring notions of causal abstraction (e.g., Chalupka et al. 2017;\nRubenstein et al. 2017; Beckers and Halpern 2019; Zennaro 2022). While this literature has\nnot explicitly dealt with the problem of computational implementation, we believe it provides\nthe right framework for theorizing about the topic (cf. Geiger et al. 2021, 2025).\n   Often, L and H will involve different sets of variables, say, VL and VH. Let IL and IH be\ndistinguished sets of interventionals on L and H that each form an intervention algebra. Then\nan exact transformation is a pair pτ, ωq of partial, surjective maps τ : ValpVLq Ñ ValpVHq\nand ω : IL Ñ IH such that:8\n\n                             τpRunpLiqq “  RunpHωpiqq,\nfor all low-level interventionals i P IL. In other words, up to the translation (τ, ω) from\nlow-level to high-level, the effects of interventionals in IL and IH are the same. They validate\nnot only all the same factual statements, but also all the same hypothetical statements (“were\nsuch and such to happen, this would be the result”). Exact transformation is one of the most\ngeneral definitions of abstraction that has been offered in the literature.\n\n3.1. Constructive Abstraction. A special case of exact transformation is motivated by\nthe idea that abstractions often arise by ignoring distinctions at the low-level (e.g., Simon\nand Ando 1961; Chalupka et al. 2017). Imagine that for each high-level variable X P VH\nthere is a set ΠX Ď VL of low-level variables that correspond to it, together with a partial,\nsurjective map πX from values of the variables in ΠX to values of X. Where IH is the set of\nall high-level hard interventions, and IL is the set of all low-level hard interventions targeting\n(sets of) partition cells, the maps πX induce (unique) partial functions ωπ : IL Ñ IH and\nτπ : ValpVLq Ñ ValpVHq.\n\n      Constructive Abstraction: A constructive abstraction9 of a model M is any\n      model that can be obtained by a partition of the variables in M together with\n       a family π of component maps, such that pτπ, ωπq is an exact transformation.\n\nIntuitively, a constructive abstraction of M is a model that ignores some distinctions made\nin M, in a way that is causally consistent (by virtue of being an exact transformation).10\n\n   7More abstract frameworks like recursive functions should not be understood as computational models in\nthe relevant sense, but rather as a mathematical means of extensionally capturing the computable functions.\n  8The notion of exact transformation is originally due to Rubenstein et al. (2017); the version here,\ngeneralized to interventionals, appears in Geiger et al. (2025). A further condition, elided here for ease of\nexposition, is that ω be monotone with respect to a natural ordering on interventionals.\n   9This formulation is inspired by Beckers and Halpern (2019); Beckers et al. (2019); cf. Geiger et al. (2025).\n   10Note the technical point that the intervention map ωπ is logically guaranteed to include all (recursive)\ninterchange interventions in its domain.  Simply observe that (1) the domain of ωπ contains all input\ninterventions because each map πX of an input variable X is surjective, and so (2) πX maps are defined under\nall values realized under input interventions, and so (3) the domain of ωπ contains interchange interventions.\nA similar argument holds for recursive interchange interventions. While exact transformation allows for\n\n6               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n  Consider the following example, a simplified instance of one given in (Geiger et al., 2025,\n§2.6). Suppose we have thirteen real-valued variables, functionally arranged as follows:\n\n\n                                    Y\n\n\n                   H21         H22         H23         H24\n\n\n\n                   H11         H12         H13         H14\n\n\n\n                    X1          X2          X3          X4\n\n\nFXi are all constantly 0 (i “ 1, 2, 3, 4), while FHi,j and FY are defined by a particular functional\nform involving matrix multiplication and a non-linear function ReLUpnq “ maxp0, nq:\n                                               `       ˘\n                         FH1i pxq “ ReLU rxW1si                                               `        ˘\n                        FH2i ph1q “ ReLU rh1W2si                                               `      ˘\n                        FY ph2q “ ReLU h2W3  ,\n\n where the relevant matrices are given as follows:\n         »                    fi      »                fi      »     fi\n                1  ´1  0   0              1  ´1  1  0              1\n            ´1  1   0   0              1  ´1  1  0              1     W1 “ ——–                                                                                ffiffifl  W2 “ ——–                                                                ffiffifl  W3 “ ——–                   ffiffifl                0   0   1  ´1          ´1  1  0  1              0.99\n                0   0  ´1  1          ´1  1  0  1              0.99\n\nNote that in the picture “negative” influences on a variable are drawn with a dotted arrow.\n  These functions fully define a causal model over the thirteen variables; call the model N.\nN has the general form of a simple, feedforward neural network. It turns out that M (the\nXNOR circuit in Fig. 1) is a constructive abstraction of N.\n  To see this, let each Ai correspond to Xi for i “ 1, 2, 3, 4, Y corresponds to C, and let\nhigh-level variable B1 correspond to tH11, H12u and B2 to tH13, H14u; that is, ΠAi “ tXiu,\nΠBj “ tH12j´1, H12ju, and ΠC “ tY u. There is no high-level variable corresponding to H2i\nfor i “ 1, 2, 3, 4 (the third “row” of N). The maps for Ai are all identity; the map for B1\nand B2 check if the relevant hidden units are equal; the map for C checks if the output is\nless than or equal to 0. In other words, πAipxiq “ xi for i “ 1, 2, 3, 4; for j “ 1, 2 we have\nπBjph2j´1, h2jq “ rh2j´1 “ h2js; finally, πCpyq “ ry ď 0s.11\n\n3.2. Translations. Another example of an exact transformation stems from a different\nintuition: we can carve the same system up into different sets of variables, depending on\nwhat set of primitive operations (interventionals) we admit (cf. Janzing and Mejia 2022). A\n\noff-distribution interventions, constructive abstraction requires interventions that are constrained by what\nactually happens in the model on the input space.\n   11Note that we are using notation rSs for the indicator function: equal to 1 if S holds, 0 otherwise.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION           7\n\n“recarving” of the variable space for a model M can understood as a bijective (one-to-one,\n onto) function τ to a new variable space. Such a function determines a canonical model\nτpMq in the second variable space and a canonical map ωτ from some set of interventionals\nI to the set of all hard interventions on τpMq (see Geiger et al. 2025 for details).\n\n       Translation is an Exact Transformation: Relative to some bijective τ,\n        the translation τpMq is an exact transformation of M under pτ, ωτq.12\n\nAs an example of translation, consider another casual model M˚, pictured as follows:\n\n\n                            C   id\n\n\n                              D1       D2\n                    FØ           Fô\n\n\n                           A1     A2     A3     A4\n\n\n Here, id is the identity function, FØ is as above in §2 (the XNOR function), and Fô is the\n quaternary function composed of XNORs: Fôpa1, a2, a3, a4q “ FØpFØpa1, a2q, FØpa3, a4qq.\n   It turns out M˚ can be translated into the model M from §2. Define the map τ from\n variable settings of M˚ to those of M in the following way:\n\n                   pa1, a2, a3, a4, d1, d2, cq   τÞÑ  pa1, a2, a3, a4, d1, FØpd1, d2q, cq.\n\n It is easy to verify that this is a bijective function. The model τpM˚q is simply M. Now\nwhich interventionals on M˚ simulate hard interventions on M? These can look relatively\n complex. For example, what corresponds to the hard intervention on M setting variable B1\n to value 0? This involves several different function replacements in M˚. First, we replace\nFD1 (which was FØ) with the constant function to value 1; second, we replace FD2 (which\nwas Fô) with FØ, now depending only on input variables A3 and A4:\n\n\n                            C   id\n\n\n                              D1       D2\n                                 FØ\n\n\n                           A1     A2     A3     A4\n\n\n\n\n   12See (Geiger et al., 2025, Theorem 30). Under these conditions, it follows that ω then becomes an\n isomorphism of intervention algebras under the operation of composition.\n\n 8               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nAs the example shows, though the variable space is slightly different, hard interventions on M\n can be emulated by suitable operations on M˚.13 This is a general feature of translations: hard\n interventions on the translation can always be “pulled back” to corresponding interventionals\non the model undergoing translation. Call this set of interventionals Iτ.\n  Equipped with this notion of translation, we can formulate a useful notion of abstraction-\nunder-translation, which captures a natural class of model transformations:\n\n      Abstraction-Under-Translation: We shall say that H is an abstraction-\n        under-translation of L if there is a translation τpLq of L such that H is a\n        constructive abstraction of τpLq.\n\n Informally, abstraction-under-translation is the composition of a translation and a constructive\n abstraction: we first allow carving up the low-level variable space in a different, but causally\n equivalent, way (the translation) and then we group the variables into “macrovariables” (the\n constructive abstraction). With this much we can formulate what we take to be a necessary\n condition on any claim of computational implementation:\n\n     “No Computation without Abstraction” Principle: Given a computa-\n        tional model H, another system L implements H only if H is an abstraction-\n        under-translation of L.\n\n In other words, we claim that to implement a computation, the (causal construal of that)\ncomputation must be a constructive abstraction of a translation of the system in question.\nThis, we believe, captures the spirit of claims in the literature that implementing is a matter\n of mirroring causal structure (Chalmers, 1996, 2011a; Scheutz, 2001; Godfrey-Smith, 2009).\n   Importantly, this principle states only a necessary condition. If one likes, it can be taken as\n a characterization of “syntactic” implementation, leaving open that there are other important\n“semantic” notions (cf. Chalmers 2011b; Sprevak 2018). For example, some have forcefully\nargued that proper implementation demands representational equivalence: computational\n explanations fundamentally cite representational properties of a system—“no computation\nwithout representation” (Fodor, 1975)—at least across many important cases in the cognitive\n sciences (Rescorla, 2013). We return to questions about the role of representation in §5.\n  A second important way in which the principle commits only to a necessary condition\n concerns the translation and the classes of low-level manipulations that we ought to allow.\nWithout any further constraints, we allow any bijective function from the original variable\n space whatsoever. This can result in arbitrarily complex sets Iτ of interventionals at the\n low-level.14 Would we want to say, for instance, that M˚ implements the XNOR algorithm\nM depicted in Fig. 1? To develop the line of concern further, we delve more deeply into\nsome concrete scientific applications of these ideas.\n\n\n   13In fact, even the example here underdescribes the relevant interventionals. For instance, if we had first\n intervened upon B2 this would result in a model where D2 no longer depends on A3 and A4. For this set of\n(already intervened upon) functions, the result would have D2 simply set to a constant value. These nuances\n matter for ensuring what the resulting set of interventionals forms an intervention algebra. It is important,\n for example, that one be able to perform these two interventionals in succession, in either order.\n   14As emphasized several times, abstraction-under-translation guarantees that the set Iτ of interventionals\n on the implementing system form an intervention algebra. We return to this important point below in §6.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION           9\n\n                       4. Neural Networks and Language Models\n\n  A typical example of a cognitive task that has long interested psychologists is to determine\nwhether two objects are “the same” in some relevant respect, such that this ability generalizes\nbeyond a specific set of previously seen objects (Wasserman and Young, 2010). This has\nbeen suggested to sit at the root of humans’ capacity for abstract, relational, and symbolic\nthought.15 A notable instance of this task was introduced by Premack (1983). Imagine being\npresented, not with just a pair of objects, but with two pairs of objects. For example:\n\n                       ♦ ♦\n\nThe task is to determine whether the relation (same vs. different) exhibited by the first pair\nis the same as that exhibited by the second. In this example, the answer would be positive.\nSimilarly, the answer would be positive for the following, since both pairs exhibit difference:\n\n                      ♦                     l\n\nThe response should then be negative for instances like this:\n\n                      ♦                     l\n\nBecause the task involves recognizing a relation between relations, it has been used as a\nlitmus test for abstract abilities in human infants and in non-human animals.16 Empirically,\nhumans and chimpanzees succeed at the task, while other primates and some other species\n(pigeons, etc.) have not shown success (Thompson et al., 2001). Infants as young as 2 years\nold succeed at the task, but only—similar to chimpanzees—if they are provided with some\nexplicit symbolic scaffolding (Christie and Gentner, 2014).\n A key question for the cognitive scientist is, what representations and operations over\nthem—that is, what computations—might underpin performance on this and related tasks\nfor those agents who succeed? Some have argued that tasks like these require a symbolic\narchitecture (Thompson et al., 2001), and specifically that neural network models would be\nincapable of generalizing in the right ways (e.g., Marcus 2001; see Alhama and Zuidema 2019\nfor an overview). Instead, one might imagine a computation something like that depicted in\nFig. 1 (the XNOR circuit): determine whether the first two are the same, determine whether\nthe second two are the same, and check that the relation exhibited by each pair is the same.\n  Geiger et al. (2023) recently showed that neural networks can in fact demonstrate the\nrequisite generalization behaviors on such tasks, using standard backpropagation learning,\nprovided inputs are associated with distributed representations. The natural follow-up\nquestion, inspired by earlier discussions of connectionism (see especially Smolensky 1986), is\nwhether such trained models have essentially induced the symbolic algorithm hypothesized\nby psychologists. Geiger et al. (2024) show that, indeed, the model in Fig. 1 is a causal\n\n   15As Wasserman and Young (2010) point out, Williams James asserted that, “sense of sameness is the\nvery keel and backbone of our thinking” (James, 1890, p. 459). Karl Lashley later wrote, “The use of symbols\ndepends upon the recognition of similarity, and not the reverse” (Lashley, 1948).\n   16The version of the task most commonly used in experimental work has the participant choose either a\nmatching pair or a non-matching pair; e.g., choose two smiley faces rather than the pair of a smiley face and\na frowney face (when presented with two diamonds). This task is usually called relational match-to-sample\n(e.g., Premack 1983; Christie and Gentner 2014).\n\n10               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nabstraction—and specifically an abstraction-under-translation—of the neural network shown\nto exhibit generalization on the hierarchical equality task.\n  For the neural network to implement this algorithm it is crucial that we allow translation\nbefore clustering. We should not expect to find the network taking on a shape like the hand-\ncrafted example in §3.1. This is one of the key lessons of connectionist research in the 1980s:\nneural networks trained by gradient descent tend to result in shared, distributed, overlapping\nencodings of information within intermediate layers (Hinton et al., 1986; Smolensky, 1986).17\nReflecting this, Geiger et al. (2024) were unable to find a direct constructive abstraction of\nthe network (that is, treating sets of intermediate nodes as causal variables). However, only a\nmodest translation is necessary: simply rotating the vector space spanned by a set of neurons\nin a given hidden layer is enough to reveal the causal structure. In other words, applying\nlinear functions to (the values of) sets of low-level variables suffices.\n  Smolensky (1986) explicated the idea that concepts are realized as linear structures in\nneural networks, and much more recent work has explored this insight under the name of\nthe linear representation hypothesis (Mikolov et al., 2013; Elhage et al., 2022; Park et al.,\n2023). A growing body of mechanistic interpretability research—largely on language models—\nassumes the linear representation hypothesis when identifying abstract causal variables in a\nrange of high-level tasks: arithmetic (Wu et al., 2023; Mueller et al., 2025), abstract reasoning\n(Geiger et al., 2024; Todd et al., 2024; Rodriguez et al., 2025), sentiment analysis (Tigges\net al., 2024), entity binding (Feng and Steinhardt, 2024; Dai et al., 2024; Prakash et al.,\n2025), factual recall (Huang et al., 2024), game playing (Nanda et al., 2023b), truthfulness\nevaluation (Marks and Tegmark, 2024), and syntactic processing (Guerner et al., 2023; Arora\net al., 2024). All of this work tacitly assumes that the relevant causal structure—if it is\npresent at all—can be identified via linear transformations.\n   Relatedly, the neuroscience community has commonly targeted linear mappings from neural\nstructures to features of predictive computational models. Researchers will typically find a\nsuitable mapping from brain to model by linear regression over neural data. In this context,\ntoo, the assumption has an air of plausibility:  if some information is linearly decodable,\nthat seems a decent heuristic for when that information can be easily “read out” by some\ndownstream system (see, e.g., Kriegeskorte 2011).\n  In both contexts—neuroscience and deep learning—the linearity assumption has been\ncalled into question. In neuroscience, some have suggested that arbitrary linear mappings\nmay be too permissive (e.g., Thobani et al. 2025), or perhaps unnecessarily restrictive (e.g.,\nIvanova et al. 2022). In machine learning, Csord´as et al. (2024) train small recurrent neural\nnetworks to repeat sequences of symbols and uncover “onion” representations that are patently\nnon-linear.18 Li et al. (2023) demonstrate that a GPT model trained on Othello games has a\nnon-linear representation of the board state (though see Nanda et al. 2023b). To date, there\nis no known example of a causal variable being realized by a non-linear representation in a\npretrained large language model, although Engels et al. (2025) show that pretrained language\nmodels have irreducibly multi-dimensional linear features.\n\n\n\n   17More recently, this phenomenon has been rediscovered and given the name “polysemanticity” (Olah\net al., 2020; Scherlis et al., 2022).\n   18Specifically, the patterns decompose into a sum of vectors where the direction stores the symbol identity,\nwhile the order of magnitude stores position. When the same symbol appears at multiple positions, a single\nline stores the value of both symbols. See Csord´as et al. (2024) for details.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          11\n\n  These discussions make salient the question of which translations we should allow in our\n account of implementation. When taking account of causality, this is bound up with the\n question of which operations on the low-level system we allow to witness a causal abstraction\n(i.e., which sets Iτ of interventionals can bare witness to a causal claim of implementation).\nBecause implementation plays into an account of computational explanation, our answer\n to these questions should be informed by what we want from a theory of computational\n explanation in cognitive science and in machine learning. As often emphasized, computational\n explanations often invoke representations; indeed, claims of implementation are often thought\n to be tightly bound up with representational claims. It will therefore be helpful to clarify\nwhat we take the role of representation in computational explanations to be.\n\n                               5. The Role of Representation\n\n  Computational models, in addition to realizing an abstract (causal) structure, are often\nassumed to be semantically laden: computation happens over some representational objects,\nwhich in turn allows us to speak of functions being computed over some values, and the like\n(Rescorla, 2015). For instance, a string of 1’s on the input or output tape of a Turing machine\n is standardly taken to represent a number in unary.\n  When the explanation of a system’s behavior cites implementation of a computation, we\n often seem to commit to representational states mirroring those in the computational model.\n If a 3-year-old succeeds at the hierarchical equality task, and we explain this in part by saying\n that she is performing the computation captured by M (Fig. 1), we are suggesting that there\n is some cognitive structure—e.g., corresponding to the abstract computational variable B2 in\n Fig. 1—whose content is something like, “those two faces have the same expression.” By\n virtue of what would it have this content? We can ask this question both about the abstract\n computational model and about the physical system implementing it.\n   In both cases, to simplify the discussion, let us assume that the relevant “inputs” and\n“outputs” are already associated with semantic content. For the computational model, these\n are the input nodes A1, A2, A3, A4—which we might stipulate represent, say, shapes and\n facial expressions—and the output node—which might represent affirmative/negative. For\n the 3-year-old human, we assume that stimuli are processed in a way that produces internal\n registration of the relevant objects (black diamond, smiley face, and so on), which become\n the inputs to further processing.  Meanwhile, the “output” might be an internal state\n encoding the recognition that (say) the two relations are the same, which in turn causes\n the corresponding behavioral response. Putting aside thorny questions about how these\n(“peripheral”) structures acquire their contents, a question remains about how structures\n mediating between the periphery—e.g., B1 and B2—come to mean something.\n  A common thread in the philosophy of representation is that internal vehicles will often\nhave meaning by virtue of the (causal) role they play in a larger system (Cummins, 1975;\n Block, 1986; Dretske, 1988; Neander, 2017; Shea, 2018).19 Following (informational) criteria\nsummarized in Harding (2023), to say that a representational vehicle R represents some\n property P in the context of a given task, the following three criteria should be satisfied:\n\n    (1) Information: R bears information about P.\n\n\n   19Conceptual role semantics suggests that this role (within, say, a brain or a network) may even exhaust\n representational content (Block, 1986; Piantadosi and Hill, 2022); we need not commit to any such claim here.\n\n12               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n    (2) Use: The information R bears about P is used by the system to perform the task.\n\n    (3) Misrepresentation: It should be possible for R to misrepresent P.\n\nIn our running example, where the task is hierarchical equality, P might be the (relational)\nproperty of “showing the same facial expressions.” In the computational model captured by\nM (Fig. 1), the variable B2 clearly bears information about P: again assuming that A3 and\nA4 represent the faces in question, under “normal” running of the system, the value of B2\ncorrelates perfectly with P. The system also uses this information to produce the correct\noutput value for C: for the system to function appropriately this value must be correct.\nFinally, it is possible for B2 to misrepresent P: for example, if both faces are smiling, and\nthus A3 and A4 are both equal to 1, but we intervene to set B2 equal to 0, then B2 assumes\nthe “incorrect” value, a fact that will be reflected in an incorrect output at Y (assuming that\nB1 is not misrepresenting the relation between shapes).\n  As the example indicates, at least in some cases, the content of an internal representational\nvehicle is nicely captured by its causal role, understood essentially as being a variable in a\nsuitable (abstract, computational) causal model. The place of B2 in the model M (together\nwith our assumptions about the contents of the inputs and outputs to the model) is what\nallows us to say that B2 represents “sameness” of the second two inputs.\n  What about a physical system implementing this computation, such as a neural network\nor, potentially, a child’s brain? A natural (“functionalist”) suggestion is that representational\ncontent will be inherited by the low-level vehicle abstracted by the high-level causal variable:\nall that is needed is for the low-level structure to stand in the right causal relations and this\nis precisely what is captured cleanly by the high-level model.20 To take the example in §3.1,\nthis means that the variables H13 and H14 together represent sameness of facial expression\n(again, assuming content can already be assigned to X3, X4, and Y ).\n  As highlighted in the previous section (§4), neural models trained to perform tasks will often\nbe abstracted by high-level algorithms, but only if we allow translation (e.g., rotation) first.\nThis means that the interventionals corresponding to the high-level content will generally\nbe less direct: not simply changing the activations of some neurons, but instead wholesale\nfunction replacement. As emphasized by Shea (2007), this makes the representational vehicles\npotentially quite abstract. They are not generally individual neurons or even sets of neurons in\nthe network. Shea suggests that they could be something like sets of points in the underlying\nstate space for some group of neurons. This makes sense in the setting of linear transformations,\nfor instance, where we might think of high-level variables encoded as linearly separable subsets\nof state space. But as the example above in §3.2 shows, abstraction-under-translation will\ngenerally produce even less concrete representational vehicles.\n  When would we want to say that variable B1 implicitly “takes on” value 1 in circuit\nM˚? What representational vehicle in M˚ can have content “the two shapes are the same”?\nIt cannot be simply a set of points in state space. Instead, we are compelled to think of\nvehicles in terms of the corresponding operations (viz. interventionals) on the system that\nessentially put it into the state of representing that content. There is then a sense in which\nthe circuit—when processing inputs A1 “ 1 and A2 “ 1—does represent the relation of\nsameness, insofar as performing the operation does not change the circuits behavior. Setting\nB1 to 0 in this case, by contrast, overrides the effect of inputs A1 and A2, producing the\n\n   20As Dennett (1978) articulated the idea, “The content of a particular vehicle of information, a particular\ninformation-bearing event or state, is and must be a function of its function in the system” (p. 213).\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          13\n\nexpected change in output as a function of A3 and A4. The relevant interventional induces\nthe desired representational state in an indirect, but causally meaningful, way. Indeed, all\nthree conditions above (Information, Use, Misrepresentation) are patently satisfied.\n\n                                    6. Triviality Revisited\n\n  Putnam (1988) and Searle (1992) famously introduced “triviality” arguments for naive\ntheories of computational implementation.21 Those arguments assumed models of computation\nwith monolithic sequences of states (e.g., finite state automata) and minimal requirements on\nmappings between models and physical systems. It has been presumed that placing more\ndemanding constraints on implementation—causal, counterfactual, etc.—would circumvent\ntriviality arguments. For instance, Chalmers’ (1996; 2011a) combinatorial state automata\ndecompose the monolithic computational states such that each component must be realized\nin the low-level system. Does this work, or could stronger triviality arguments arise? And\nwhat bearing, if any, would triviality have on the claims above about representation?\n  Drawing on Geiger et al. (2024, 2025) and related work, Sutter et al. (2025) consider the\npresent notion of abstraction-under-translation (they call it “input-restricted distributed\nabstraction”). Although they do not explicitly relate their work to the earlier philosophical\ndiscussion, the paper essentially presents an even stronger triviality argument to the effect\nthat, for any algorithm, and any network satisfying some minimal assumptions,22 there is\nsome translation and constructive abstraction that render the algorithm an abstraction-\nunder-translation of the network (see their Theorem 1). In other words, given our analysis\nof implementation (and plausibly all causal analysis suggested earlier in the philosophical\nliterature), under minimal conditions, every neural network implements every algorithm.\n  As one might expect, the translations needed to witness this existence claim look rather\ngerrymandered. However, when it comes to the notion of implementation on offer here,\nnothing rules them out. What are we to make of this continued specter of triviality? Should\nwe add further conditions to our set of necessary conditions on computational implementation?\nFor instance, maybe further restrictions on allowable vehicles would help rule out some of\nthese gerrymandered cases (see, e.g., Godfrey-Smith 2009)? Or perhaps an independently\nmotivated account of representational content—going beyond (and possibly even coming\napart from) the causal role of intermediate variables—could help narrow down the space of\npossible implementations (Shagrir, 2001; Sprevak, 2010; Rescorla, 2013)?\n   Triviality results like these have a strikingly similar flavor to a series of results on “internal\nmodel principles” (see, e.g., Conant and Ashby 1970; Richens and Everitt 2024; Piantadosi\nand Gallistel 2024 for three examples). Each formulated somewhat differently, these results\nare all offered in a positive light: as long as an agent demonstrates a particular sort of\nbehavior, we know without any further ado that there must be some corresponding internal\nstructure within the agent that facilitates the behavior. In a sense, they articulate how\nsuccessful task performance can itself narrow down the space of possible agent types that we\nneed to consider in our attempt to understand how a class of agents works.\n  We might interpret results like those of Sutter et al. (2025) in a similar manner. The proof\nof their result gives a recipe for furnishing the relevant manipulations to the neural network\n\n  21An even earlier version of the argument can be traced to Ian Hinckfuss; see Sprevak (2018).\n   22As a rough gloss, those assumptions say the network should solve the task—that is, display the right\ninput/output mapping—it should be large enough so that in principle the algorithm could “fit” in it, and the\nfunctional relationships between layers is (roughly) bijective.\n\n14               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nwhich carry out the respective high-level (algorithmic) interventions. In other words, the\nresult articulates precisely how the high-level model can guide a certain kind of manipulation\nand control of the low-level system. Moreover, this manipulation and control will typically\nbe at the level of intuitive notions and concepts (“same shape” etc.), at least in as far as the\nalgorithmic description itself is. The fact that the low-level manipulations (interventionals) are\nguaranteed to form an intervention algebra means that the type of manipulation and control\ninvolved will be suitably modular; e.g., we can intervene on one variable while leaving others\nas they were. According to a compelling interventionist account of causal explanation (e.g.,\nWoodward 2003), this variety of manipulation and control is the hallmark of good explanation.\nThe fact that it is always possible—whenever a network performs a task described by a\nsuitable algorithm—could be taken as a positive finding.\n  In some explanatory contexts, the low-level interventionals might be so complex that they\ntake us beyond what we could actually implement, either for practical reasons or because it\nis too hard to compute what they should be. In their experimental investigation of these\nphenomena, Sutter et al. (2025) found it strikingly difficult to identify the relevant space of\ninterventionals (even though they knew from first principles of their existence). For instance,\nthey could not even be extracted from multi-layered neural networks. This might make\none more reluctant to accept that the proof furnishes good causal explanations of network\nbehavior,23 even if, in principle, we can identify the relevant levers for controlling the network.\n\n                            7. Explanation and Generalization\n\n  Because abstraction-under-implementation facilitates manipulation and control in the ways\ncomputational (qua causal) explanations are supposed to, we are inclined to accept abstraction-\nunder-implementation not only as a necessary condition for computational implementation,\nbut also as a sufficient one. At the same time, there is a widely held intuition that, for some\nalgorithms and some neural networks, the network does not really implement the algorithm.\nIndeed, one might feel strongly that the system M˚ is not an implementation of the algorithm\ndescribed by M, even though the latter can be obtained from the former relative to a suitable\nclass of (rather complex) interventionals on M˚. If an agent came to solve the hierarchical\nequality task with method M˚, we might not want to say that the solution they found\ncoincides with the algorithm M, and we might not want to explain the agent’s success by\nclaiming that they represent (e.g.) whether the two faces exhibit the same expression. More\nneeds to be said about what might ground this kind of skeptical intuition.\n  As we have already emphasized, verdicts like this will not be reached by appeal to simplis-\ntic interventionist causal explanation: by construction, abstraction-under-implementation\nproduces answers to relevant “what-if-things-had-been-different” questions, identifying levers\nfor manipulation and control (Woodward, 2003; Ylikoski and Kuorikoski, 2010). To the\nextent that we are truly only interested in explaining one well-circumscribed behavior—say,\none specific, precisely delineated input-output mapping—it is hard to imagine how we could\ndemand more than this relatively flexible method of algorithmic decomposition.\n  However, when studying complex agents, we are rarely in this situation. Even in the study\nof language models where we have in-principle perfect (observational and interventional)\naccess to their internal structure, at least two further challenges present themselves:\n\n  23One is reminded of yet another slogan: “no causation without manipulation” (Holland, 1986), where\nmanipulation is to be understood in an in-practice sense, not merely conceivable. This is in contrast to\nWoodward (2003) and others, where manipulation is understood as in-principle manipulation.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          15\n\n    (1) As a practical matter, we only ever observe a small sample of agent behavior. Often\n      what we want to understand—and what we want to compress into succinct, manageable\n       descriptions—is what the agent would do in settings beyond those observed.\n    (2) A “task” typically implies a relatively open-ended behavioral profile, going far beyond\n       the instances we will have explicitly enumerated or studied. Furthermore, we often\n       have only a vague, partial characterization of what constitutes it.\nEach of these lays bear the familiar point that good explanations are appropriately tailored\nto the goals, interests, limitations, and background information of their consumers (see\nYlikoski and Kuorikoski 2010; Potochnik 2017; Harding et al. 2025, among many others).\nGood explanations of cognitively interesting behavior, in particular, should compactly and\neffectively describe the nature of the agent’s competence. This will be most helpful when it\nfacilitates accurate prediction of what the agent will do in new circumstances.24\n  To return again to the hierarchical equality task discussed in §4, suppose a child succeeds\nat the task as presented, involving shapes and faces. We might then hypothesize that they\ninternalized the algorithm M in Fig. 1, which involves combining two different relational\njudgments and recognizing a relation between those. Importantly, this algorithm is intended\nto be a general one, suitable for any instance of the hierarchical equality task. Suppose, for\nexample, that the child is also able to distinguish “up” ò from “down” ó. Then, on the basis\nof our hypothesis, we would expect the same algorithm M to engender appropriate behavior\non a new version of the task, involving shapes and arrows, e.g.:\n\n                       ♦ ♦                                    ò  ó\n\nAll the child has to do is assimilate the “up” and “down” to values of A3 and A4—so that\nthey can represent sameness of direction just as they did sameness of facial expression, via\nvariable B2—and the rest is as before. This is what it means for that algorithm to describe\nthe child’s competence; and were they to fail at one of these “generalization” tasks, we would\nconclude that the original test failed to probe the relevant competence. We would likewise\nconclude that the intermediate internal vehicle (B2) does not represent “sameness” in general,\nbut (at best) sameness within some limited domain.\n A closely analogous situation confronts researchers in machine learning.  Similarly to\nbehavioral psychological experiments with people, benchmarks are designed to test model\nabilities on tasks like text summarization, pronoun resolution, toxicity detection, part-\nof-speech tagging, and the like. No one, however, believes that any existing benchmark\ncomprehensively circumscribes the intended tasks, or even that the tasks are all perfectly\nwell-defined (see Hupkes et al. 2023; Harding and Sharadin 2025 for further discussion of this\npoint). It is notoriously easy to find “adversarial” examples that show benchmark success\ndoes not imply the intended competence (e.g., Jia and Liang 2017), and some have therefore\ncalled for a move toward more dynamic benchmarking methods (e.g., Kiela et al. 2021).\n  In mechanistic interpretability, we face an analogous problem when using supervised\nmachine learning, e.g., distributed alignment search (Geiger et al., 2024) or desiderata-based\nmasking (Davies et al., 2023), to localize causal variables within a neural network. What if\nthe causal variable is only realized on the narrow distribution of inputs used for localization?\nThis question of whether a causal analysis generalizes to “out-of-distribution” inputs becomes\n\n  24The tight connection between prediction and explanation was a central theme in early philosophical\nwork on explanation (Hempel and Oppenheim, 1948), and it has been stressed by philosophers of science\nmore recently as well (see, e.g., Longino 2002 and especially Douglas 2009).\n\n16               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nmore important as supervised localization tools become more powerful and therefore more\ncapable of overfitting (Wu et al., 2023; Huang et al., 2024; Sutter et al., 2025).\n   In cases where we can show that a model has internalized a concrete algorithm for solving\na task (or performing well on a benchmark), this might lend some confidence that the model\nwill generalize to instances of the task beyond what we have observed. Employing only linear\ntranslations, Huang et al. (2025) show that localization of causally meaningful variables helps\npredict model behavior on variants of a task that could flummox other prediction methods.25\nRoughly, models get it right in cases where they have correctly matched the new input to an\nexisting algorithmic template, just as in the hierarchical equality case imagined above.\n  For this connection between implementation and prediction to be robust, it does matter\nhow the putative algorithm is implemented. In particular, the system must implement the\nalgorithm in a way that enables it to assimilate an open-ended range of inputs to the same\nalgorithmic template. Concretely, when presented with ò and ó (or $ and ₤, and so on),\nthe pair must be mapped appropriately to the correct label (“same” or “different”), so that\nthe right vehicle (what corresponds to B2) takes on the right value. If the representational\nvehicles witnessing an implementation are sufficiently foreign to the way the system itself\nworks, there will be little hope of this. If, by contrast, the algorithm is encoded via vehicles\nnative to how the system works, we will likely have tapped into a productive guide to behavior\nover unobserved inputs. In the spirit of the linear representation hypothesis (discussed in §4),\na focus on linear mappings can perhaps be motivated in this way.\n  Some have argued that we are justified in making representational claims (and presumably\nalso claims of computational implementation) only when the putative representations play\na pivotal role in a model’s generalization behavior (e.g., Shea 2007).  This seems like a\nsensible restriction, at least when our explanatory interests center around prediction of unseen\ninstances on a wide-ranging (and possibly even vaguely defined) task.\n  Computational explanation, in particular, enjoys a privileged place in this effort. When\nwe hypothesize specific algorithms that a model or agent might be using to solve a problem,\nwe tend to focus on simple, compact, and general algorithms that solve the task efficiently.\nThis is not incidental: the same principles we use to describe a task succinctly may well\nbe similar to the ones an effective learning system will employ to solve a hard induction\n(i.e., generalization) task. Humans and artificial neural networks alike have been shown to\nlearn and generalize in a way consistent with “minimum description length” or “compression”\nprinciples (e.g., Feldman 2016; Deletang et al. 2024; Mingard et al. 2025); and compression is\ntightly related to presence of algorithmically meaningful causal variables (Wendong et al.,\n2025). There is even suggestive evidence that emergence of generalization abilities coincides\nwith (and is plausibly due to) the emergence of algorithmic structure in the hidden layers of\nneural networks, whereby particularized (“memorized”) solutions to problems are gradually\nreplaced with algorithmic methods that generalize in the right ways (Nanda et al., 2023a).\n  In this setting, it becomes essential to understand more of how the system in question\nworks, and specifically how the system learns. Computational implementation—as captured\nby abstraction-under-translation—is not enough. Following others in the literature (e.g.,\nChalmers 2011a,b), we maintain that it is still theoretically fruitful to have a notion of\nimplementation that does not depend on which learning method a system uses. But when we\n\n   25For example, if asking whether a price is between two numbers (Wu et al., 2023), the price could be\npresented in Canadian dollars or in Turkish lira (Huang et al., 2025). This is very obviously incidental to our\nunderstanding of the intended number comparison task.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          17\n\nwant our explanation to shine light on unobserved behavior, including further adaptation\nand learning in new contexts,26 not just any mapping from system to algorithm will do.\nFurthermore, it may well turn out that the appropriate mapping restrictions (e.g., linear),\nacross different classes of agents (e.g., biological versus artificial), will differ markedly.\n\n                                          8. Conclusion\n\n  Cognitive science and mechanistic interpretability research in machine learning face some\nof the same fundamental challenges. Chief among those is carving the target system into\nmeaningful parts, such that the most interesting behaviors can be understood in terms of\nthe (causal) interactions among those parts. A founding doctrine of cognitive science is\nthe idea that a helpful decomposition will be guided by computational and representational\nmotifs. Similar ideas are currently under exploration in mechanistic interpretability. We\nhave argued that a causal lens on this topic—and on the critical concept of implementation\nin particular—helps clarify what it would take to get computational explanation right, and\nwhat problems still remain to be solved. Causal abstraction already addresses a core target\nof causal explanation, by pinpointing variables ripe for controlling the system in high-level,\n(potentially more) interpretable terms. This is captured in our “no computation without\nabstraction” principle. Additional explanatory goals, like generalizing and predicting beyond\npreviously observed behavior, will require further refinement of this principle, including more\nstringent demands on allowable mappings from system to algorithm, which will plausibly be\nmore tailored to specifics of the family of systems under investigation.\n\n                                  9. Contribution Statement\n\n  This work is the culmination of years of thought and conversation between the authors.\nThomas Icard lead the paper writing process.\n\n                             References\n\nAlhama, R. G. and Zuidema, W. (2019). A review of computational models of basic rule\n   learning: The neural-symbolic debate and beyond.  Psychonomic Bulletin & Review,\n  26(4):1174–1194.\nArora, A., Jurafsky, D., and Potts, C. (2024). CausalGym: Benchmarking causal inter-\n   pretability methods on linguistic tasks. In Ku, L.-W., Martins, A., and Srikumar, V.,\n   editors, Proceedings of the 62nd Annual Meeting of the Association for Computational\n   Linguistics (Volume 1: Long Papers), pages 14638–14663, Bangkok, Thailand. Association\n   for Computational Linguistics.\nBeckers, S., Eberhardt, F., and Halpern, J. Y. (2019). Approximate causal abstractions. In\n  Proceedings of The 35th Uncertainty in Artificial Intelligence Conference.\nBeckers, S. and Halpern, J. (2019). Abstracting causal models. In AAAI Conference on\n   Artificial Intelligence.\nBlock, N. (1986). Advertisement for a semantics for psychology. Midwest Studies in Philosophy,\n  10:615–678.\nChalmers, D. (1996). Does a rock implement every finite-state automaton?   Synthese,\n  108:310–333.\n\n   26For adaptation, learning, and fine-tuning, we want causal explanations not just of the present behavioral\nrepertoire, but of the system’s cross-temporal behavior across those new tasks as well. This will generally\nrequire acknowledging (if only implicitly) the method by which this learning or adaptation is taking place.\n\n18               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nChalmers, D. (2011a). A computational foundation for the study of cognition. Journal of\n  Cognitive Science, 12(4):323–357.\nChalmers, D. (2011b). The varieties of computation: A reply. Journal of Cognitive Science,\n  13(3):211–248.\nChalupka, K., Eberhardt, F., and Perona, P. (2017). Causal feature learning: an overview.\n  Behaviormetrika, 44:137–164.\nChan,   L.,  Garriga-Alonso,   A.,  Goldwosky-Dill,   N.,  Greenblatt,   R.,   Nitishin-\n  skaya,  J.,  Radhakrishnan,  A.,  Shlegeris,  B., and Thomas,  N.  (2022).   Causal\n  scrubbing,  a  method  for  rigorously  testing  interpretability  hypotheses.    AI\n  Alignment Forum.   https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/\n  causal-scrubbing-a-method-for-rigorously-testing.\nChristie, S. and Gentner, D. (2014). Language helps children succeed on a classic analogy\n  task. Cognitive Science, 38(2):383–397.\nConant, R. C. and Ashby, W. R. (1970). Every good regulator of a system must be a model\n  of that system. International Journal of Systems Science, 1(2):89–97.\nCsord´as, R., Potts, C., Manning, C. D., and Geiger, A. (2024). Recurrent neural networks\n  learn to store and generate sequences using non-linear representations. In Belinkov, Y.,\n  Kim, N., Jumelet, J., Mohebbi, H., Mueller, A., and Chen, H., editors, Proceedings of the\n  7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages\n  248–262.\nCummins, R. (1975). Functional analysis. Journal of Philosophy, 72:741–765.\nCummins, R. (1977). Programs in the explanation of behavior.  Philosophy of Science,\n  44(2):269–287.\nDai, Q., Heinzerling, B., and Inui, K. (2024). Representational analysis of binding in language\n  models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Proceedings of the 2024\n  Conference on Empirical Methods in Natural Language Processing, pages 17468–17493,\n  Miami, Florida, USA. Association for Computational Linguistics.\nDavies, X., Nadeau, M., Prakash, N., Shaham, T. R., and Bau, D. (2023). Discovering\n  variable binding circuitry with desiderata. arXiv preprint arXiv:2307.03637.\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Genewein, T., Mattern, C., Grau-\n  Moya, J., Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., and Veness, J. (2024).\n  Language modeling is compression. In The Twelfth International Conference on Learning\n  Representations.\nDennett, D. C. (1978). Toward a cognitive theory of consciousness. In Savage, C. W., editor,\n  Minnesota Studies in the Philosophy of Science, Volume 9: Perception and Cognition,\n  Issues in the Foundation of Psychology, pages 201–228. University of Minnesota Press.\nDouglas, H. E. (2009). Reintroducing prediction to explanation. Philosophy of Science,\n  76(4):444–463.\nDretske, F. (1988). Explaining Behavior: Reasons in a World of Causes. MIT Press.\nElhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds,\n   Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D.,\n  Wattenberg, M., and Olah, C. (2022). Toy models of superposition. Transformer Circuits\n  Thread.\nEngels, J., Michaud, E. J., Liao, I., Gurnee, W., and Tegmark, M. (2025). Not all language\n  model features are one-dimensionally linear. In The Thirteenth International Conference\n  on Learning Representations.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          19\n\nFeldman, J. (2016). The simplicity principle in perception and cognition. WIREs Cognitive\n  Science, 7(5):330–340.\nFeng, J. and Steinhardt, J. (2024). How do language models bind entities in context? In The\n  Twelfth International Conference on Learning Representations.\nFinlayson, M., Mueller, A., Gehrmann, S., Shieber, S., Linzen, T., and Belinkov, Y. (2021).\n  Causal analysis of syntactic agreement mechanisms in neural language models. In Zong,\n  C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International Joint Conference\n  on Natural Language Processing (Volume 1: Long Papers), pages 1828–1843, Online.\n  Association for Computational Linguistics.\nFodor, J. A. (1975). The Language of Thought. Harvard University Press.\nFrank, M. C. and Goodman, N. D. (2025). Cognitive modeling using artificial intelligence.\n  Annual Review of Psychology. Forthcoming.\nGeiger, A., Carstensen, A., Frank, M. C., and Potts, C. (2023). Relational reasoning and\n  generalization using nonsymbolic neural networks. Psychological Review, 130(2):308–333.\nGeiger, A., Ibeling, D., Zur, A., Chaudhary, M., Chauhan, S., Huang, J., Arora, A., Wu, Z.,\n  Goodman, N., Potts, C., and Icard, T. (2025). Causal abstraction: A theoretical foundation\n   for mechanistic interpretability. Journal of Machine Learning Research, 26(83):1–64.\nGeiger, A., Lu, H., Icard, T. F., and Potts, C. (2021). Causal abstractions of neural networks.\n  In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in\n  Neural Information Processing Systems.\nGeiger, A., Richardson, K., and Potts, C. (2020). Neural natural language inference models\n  partially embed theories of lexical entailment and negation. In Alishahi, A., Belinkov, Y.,\n  Chrupa la, G., Hupkes, D., Pinter, Y., and Sajjad, H., editors, Proceedings of the Third\n  BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages\n  163–173, Online. Association for Computational Linguistics.\nGeiger, A., Wu, Z., Potts, C., Icard, T., and Goodman, N. D. (2024). Finding alignments\n  between interpretable causal variables and distributed neural representations. In Proceedings\n  of the 3rd Conference on Causal Learning and Reasoning (CLeaR), pages 160–187.\nGeva, M., Bastings, J., Filippova, K., and Globerson, A. (2023). Dissecting recall of factual\n  associations in auto-regressive language models. In The 2023 Conference on Empirical\n  Methods in Natural Language Processing.\nGodfrey-Smith, P. (2009). Triviality arguments against functionalism. Philosophical Studies,\n  145(2):273–295.\nGuerner, C., Svete, A., Liu, T., Warstadt, A., and Cotterell, R. (2023). A geometric notion\n  of causal probing. CoRR, abs/2307.15054.\nHanna, M., Liu, O., and Variengien, A. (2023). How does GPT-2 compute greater-than?:\n  Interpreting mathematical abilities in a pre-trained language model. In Thirty-seventh\n  Conference on Neural Information Processing Systems.\nHarding, J. (2023). Operationalising representation in natural language processing. The\n  British Journal for the Philosophy of Science.\nHarding, J., Gerstenberg, T., and Icard, T. (2025). A communication-first account of\n  explanation.\nHarding, J. and Sharadin, N. (2025). What is it for a machine learning model to have a\n  capability? British Journal for the Philosophy of Science.\n\n20               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nHempel, C. G. and Oppenheim, P. (1948). Studies in the Logic of Explanation. Philosophy\n  of Science, 15(2):135–175.\nHinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986). Distributed representations.\n  In Rumelhart, D. E., McClelland, J. L., and the PDP Research Group, editors, Parallel\n  Distributed Processing: Explorations in the Microstructure of Cognition: Psychological and\n  Biological Models, volume 1, pages 77–109. MIT Press.\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical\n  Association, 81(396):945–960.\nHuang, J., Tao, J., Icard, T., Yang, D., and Potts, C. (2025). Internal causal mechanisms\n  robustly predict language model out-of-distribution behaviors. In Forty-second International\n  Conference on Machine Learning.\nHuang, J., Wu, Z., Potts, C., Geva, M., and Geiger, A. (2024). RAVEL: Evaluating\n  interpretability methods on disentangling language model representations. In Ku, L.-W.,\n  Martins, A., and Srikumar, V., editors, Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages 8669–8687,\n  Bangkok, Thailand. Association for Computational Linguistics.\nHupkes,  D.,  Giulianelli,  M.,  Dankers,  V.,  Artetxe,  M.,  Elazar,  Y.,  Pimentel,  T.,\n  Christodoulopoulos, C., Lasri, K., Saphra, N., Sinclair, A., Ulmer, D., Schottmann,\n  F., Batsuren, K., Sun, K., Sinha, K., Khalatbari, L., Ryskina, M., Frieske, R., Cotterell,\n  R., and Jin, Z. (2023). A taxonomy and review of generalization research in NLP. Nature\n  Machine Intellignece, 5:1161–1174.\nIbeling, D. and Icard, T. F. (2019). On open-universe causal reasoning. In Proceedings of the\n  35th Conference on Uncertainty in Artificial Intelligence (UAI).\nIcard, T. F. (2017). From programs to causal models. In Proceedings of the 21st Amsterdam\n  Colloquium.\nIvanova, A. A., Schrimpf, M., Anzellotti, S., Zaslavsky, N., Fedorenko, E., and Isik, L. (2022).\n  Beyond linear regression: Mapping models in cognitive neuroscience should align with\n  research goals. Neurons, Behavior, Data analysis, and Theory, 1.\nJames, W. (1890). Principles of Psychology, volume 1. New York: Holt.\nJanzing, D. and Mejia, S. H. G. (2022). Phenomenological causality.\nJia, R. and Liang, P. (2017). Adversarial examples for evaluating reading comprehension\n  systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\n  Processing, pages 2021–2031.\nJonas, E. and Kording, K. P. (2017). Could a neuroscientist understand a microprocessor?\n  PloS Computational Biology, 13(1).\nKiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh,\n  A., Ringshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal,\n  M., Potts, C., and Williams, A. (2021). Dynabench: Rethinking benchmarking in NLP. In\n  Proceedings of the 2021 Conference of the North American Chapter of the Association for\n  Computational Linguistics: Human Language Technologies, page 4110–4124.\nKriegeskorte, N. (2011).   Pattern-information  analysis:  From stimulus decoding to\n  computational-model testing. NeuroImage, 56(2):411–421.\nLashley, K. S. (1929/1948).  Brain mechanisms and intelligence.  In Dennis, W., editor,\n  Readings in the History of Psychology, page 557–570. Appleton-Century-Crofts.\nLi, K., Hopkins, A. K., Bau, D., Vi´egas, F., Pfister, H., and Wattenberg, M. (2023). Emergent\n  world representations: Exploring a sequence model trained on a synthetic task. In The\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          21\n\n  Eleventh International Conference on Learning Representations.\nLongino, H. (2002). The Fate of Knowledge. Princeton University Press.\nMarcus, G. (2001). The Algebraic Mind: Integrating Connectionism and Cognitive Science.\n  MIT Press.\nMarks, S. and Tegmark, M. (2024). The geometry of truth: Emergent linear structure in large\n  language model representations of true/false datasets. In First Conference on Language\n  Modeling.\nMarr, D. (1982). Vision. W.H. Freeman and Company.\nMeng, K., Bau, D., Andonian, A., and Belinkov, Y. (2022). Locating and editing factual\n  associations in gpt. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and\n  Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages\n  17359–17372. Curran Associates, Inc.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013).  Distributed\n  representations of words and phrases and their compositionality. In Burges, C., Bottou, L.,\n  Welling, M., Ghahramani, Z., and Weinberger, K., editors, Advances in Neural Information\n  Processing Systems, volume 26. Curran Associates, Inc.\nMingard, C., Rees, H., Valle-P´erez, G., and Louis, A. A. (2025). Deep neural networks have\n  an inbuilt Occam’s razor. Nature Communications, 16(220):1–9.\nMoschovakis, Y. N. (2001). What is an algorithm? In Engquist, B. and Schmid, W., editors,\n  Mathematics Unlimited — 2001 and Beyond, page 919–936. Springer.\nMueller, A., Brinkmann, J., Li, M., Marks, S., Pal, K., Prakash, N., Rager, C., Sankara-\n  narayanan, A., Sharma, A. S., Sun, J., Todd, E., Bau, D., and Belinkov, Y. (2024).\n  The quest for the right mediator: A history, survey, and theoretical grounding of causal\n   interpretability.\nMueller, A., Geiger, A., Wiegreffe, S., Arad, D., Arcuschin, I., Belfki, A., Chan, Y. S.,\n  Fiotto-Kaufman, J. F., Haklay, T., Hanna, M., Huang, J., Gupta, R., Nikankin, Y., Orgad,\n  H., Prakash, N., Reusch, A., Sankaranarayanan, A., Shao, S., Stolfo, A., Tutek, M., Zur,\n  A., Bau, D., and Belinkov, Y. (2025). MIB: A mechanistic interpretability benchmark. In\n  Forty-second International Conference on Machine Learning.\nNanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. (2023a). Progress measures\n   for grokking via mechanistic interpretability. In The Eleventh International Conference on\n  Learning Representations.\nNanda, N., Lee, A., and Wattenberg, M. (2023b).  Emergent linear representations in\n  world models of self-supervised sequence models. In Belinkov, Y., Hao, S., Jumelet, J.,\n  Kim, N., McCarthy, A., and Mohebbi, H., editors, Proceedings of the 6th BlackboxNLP\n  Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP\n  2023, Singapore, December 7, 2023, pages 16–30. Association for Computational Linguistics.\nNeander, K. (2017). A Mark of the Mental: A Defence of Informational Teleosemantics. MIT\n  Press, Cambridge, MA.\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020). Zoom\n   in: An introduction to circuits. Distill. https://distill.pub/2020/circuits/zoom-in.\nPark, K., Choe, Y. J., and Veitch, V. (2023). The linear representation hypothesis and the\n  geometry of large language models. CoRR, abs/2311.03658.\nPearl, J. (2001). Direct and indirect effects. In Breese, J. S. and Koller, D., editors, UAI\n  ’01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University\n  of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 411–420. Morgan\n\n22               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\n  Kaufmann.\nPearl, J. (2009). Causality. Cambridge University Press.\nPeters, J., Janzing, D., and Sch¨olkopf, B. (2017). Elements of Causal Inference: Foundations\n  and Learning Algorithms. The MIT Press, Cambridge, MA.\nPiantadosi, S. T. and Gallistel, C. R. (2024). Formalising the role of behaviour in neuroscience.\n  European Journal of Neuroscience, 60(5):4756–4770.\nPiantadosi, S. T. and Hill, F. (2022). Meaning without reference in large language models.\nPiccinini, G. (2015). Physical Computation: A Mechanistic Account. Oxford University Press.\nPotochnik, A. (2017). Idealization and the Aims of Science. University of Chicago Press,\n  Chicago.\nPrakash, N., Shapira, N., Sharma, A. S., Riedl, C., Belinkov, Y., Shaham, T. R., Bau, D.,\n  and Geiger, A. (2025). Language models use lookbacks to track beliefs.\nPremack, D. (1983). The codes of man and beasts. Behavioral and Brain Sciences, 6(1):125–\n  136.\nPutnam, H. (1967). Psychological predicates. In Capitan, W. H. and Merrill, D. D., editors,\n  Art, Mind, and Religion. Pittsburgh University Press.\nPutnam, H. (1975).  Philosophy and our mental life.  In Mind, Language, and Reality.\n  Cambridge University Press.\nPutnam, H. (1988). Representation and Reality. MIT Press.\nPylyshyn, Z. W. (1984). Computation and Cognition. MIT Press.\nRescorla, M. (2013). Against structuralist theories of computational implementation. British\n  Journal for the Philosophy of Science, 64(4):681–707.\nRescorla, M. (2015). The representational foundations of computation. Philosophia Mathe-\n  matica, 23(3):338–366.\nRichens, J. and Everitt, T. (2024). Robust agents learn causal world models. In Kim, B.,\n  Yue, Y., Chaudhuri, S., Fragkiadaki, K., Khan, M., and Sun, Y., editors, International\n  Conference on Representation Learning, volume 2024, pages 15786–15817.\nRodriguez, J. D., Mueller, A., and Misra, K. (2025). Characterizing the role of similarity\n  in the property inferences of language models. In Chiruzzo, L., Ritter, A., and Wang, L.,\n   editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the\n  Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\n  Papers), pages 11515–11533, Albuquerque, New Mexico. Association for Computational\n  Linguistics.\nRubenstein, P. K., Weichwald, S., Bongers, S., Mooij, J. M., Janzing, D., Grosse-Wentrup, M.,\n  and Sch¨olkopf, B. (2017). Causal consistency of structural equation models. In Proceedings\n  of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI).\nRusanen, A.-M. and Lappi, O. (2016).  On computational explanations.   Synthese,\n  193:3931–3949.\nSaphra, N. and Wiegreffe, S. (2024). Mechanistic? In Belinkov, Y., Kim, N., Jumelet, J.,\n  Mohebbi, H., Mueller, A., and Chen, H., editors, Proceedings of the 7th BlackboxNLP\n  Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 480–498, Miami,\n  Florida, US. Association for Computational Linguistics.\nScherlis, A., Sachan, K., Jermyn, A. S., Benton, J., and Shlegeris, B. (2022). Polysemanticity\n  and capacity in neural networks. CoRR, abs/2210.01892.\nScheutz, M. (2001). Computational vs. causal complexity. Minds and Machines, 11(4):543–\n  566.\n\n       HOW CAUSAL ABSTRACTION UNDERPINS COMPUTATIONAL EXPLANATION          23\n\nSearle, J. R. (1992). The Rediscovery of the Mind. MIT Press.\nShagrir, O. (2001). Content, computation and externalism. Mind, 110(438):369–400.\nShea, N. (2007). Content and its vehicles in connectionist systems. Mind & Language,\n  22:246–269.\nShea, N. (2018). Representation in Cognitive Science. Oxford University Press.\nSimon, H. A. and Ando, A. (1961). Aggregation of variables in dynamic systems. Econometrica,\n  29(2):111–138.\nSmolensky, P. (1986). Neural and conceptual interpretation of PDP models. In McClelland,\n  J. L., Rumelhart, D. E., and the PDP Research Group, editors, Parallel Distributed\n  Processing: Explorations in the Microstructure of Cognition: Psychological and Biological\n  Models, volume 2, pages 390–431. MIT Press.\nSpirtes, P., Glymour, C., and Scheines, R. (2000). Causation, Prediction, and Search. MIT\n  Press.\nSprevak, M. (2010). Computation, individuation, and the received view on representation.\n  Studies in History and Philosophy of Science, 41:260–270.\nSprevak, M. (2018). Triviality arguments about computational implementation. In Sprevak,\n  M. and Colombo, M., editors, The Routledge Handbook of the Computational Mind, pages\n  175–191. Routledge.\nSutter, D., Minder, J., Hofmann, T., and Pimentel, T. (2025). The non-linear representation\n  dilemma: Is causal abstraction enough for mechanistic interpretability?\nThobani, I., Sagastuy-Brena, J., Nayebi, A., Prince, J. S., Cao, R., and Yamins, D. L. (2025).\n  Model-brain comparison using inter-animal transforms.  In 8th Annual Conference on\n  Cognitive Computational Neuroscience.\nThompson, D. (2023). Algorithms and Execution Traces. PhD thesis, Stanford University.\nThompson, R. K., Rattermann, M. J., and Oden, D. L. (2001). Perception and judgement of\n  abstract same-different relations by monkeys, apes and children: Do symbols make explicit\n  only that which is implicit? Croatian Review of Rehabilitation Research, 37(1):9–22.\nTigges, C., Hollinsworth, O. J., Geiger, A., and Nanda, N. (2024). Language models linearly\n  represent sentiment. In Belinkov, Y., Kim, N., Jumelet, J., Mohebbi, H., Mueller, A.,\n  and Chen, H., editors, Proceedings of the 7th BlackboxNLP Workshop: Analyzing and\n  Interpreting Neural Networks for NLP, pages 58–87, Miami, Florida, US. Association for\n  Computational Linguistics.\nTodd, E., Li, M., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. (2024). Function\n  vectors in large language models. In The Twelfth International Conference on Learning\n  Representations.\nTuring, A. M. (1936). On computable numbers, with an application to the Entscheidungsprob-\n  lem. Proceedings of the London Mathematical Society, s2-42:230–265.\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. (2020).\n  Causal mediation analysis for interpreting neural NLP: The case of gender bias.\nWang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. (2023). Inter-\n  pretability in the wild: a circuit for indirect object identification in GPT-2 small. In The\n  Eleventh International Conference on Learning Representations.\nWasserman, E. A. and Young, M. E. (2010). Same-different discrimination: the keel and\n  backbone of thought and reasoning. Journal of Experimental Psychology: Animal Behavior\n  Processes, 36(1):3–22.\n\n24               ATTICUS GEIGER, JACQUELINE HARDING, THOMAS ICARD\n\nWendong, L., Buchholz, S., and Sch¨olkopf, B. (2025). Algorithmic causal structure emerging\n  through compression. In Huang, B. and Drton, M., editors, Proceedings of the Fourth\n  Conference on Causal Learning and Reasoning, pages 201–242.\nWoodward, J. (2003). Making Things Happen: A Theory of Causal Explanation. Oxford\n  University Press.\nWu, Z., Geiger, A., Icard, T., Potts, C., and Goodman, N. (2023).  Interpretability at\n   scale: Identifying causal mechanisms in Alpaca. In Thirty-seventh Conference on Neural\n  Information Processing Systems.\nYlikoski, P. and Kuorikoski, J. (2010). Dissecting explanatory power. Philosophical Studies,\n  148(2):201–219.\nZeller, A. (2002). Isolating cause-effect chains from computer programs. In Proceedings of\n  the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering, SIGSOFT\n  ’02/FSE-10, page 1–10. Association for Computing Machinery.\nZennaro, F. M. (2022). Abstraction between structural causal models: A review of definitions\n  and properties. In UAI 2022 Workshop on Causal Representation Learning.\nZhang, F. and Nanda, N. (2024). Towards best practices of activation patching in language\n  models: Metrics and methods.  In The Twelfth International Conference on Learning\n  Representations.\n\n## Claim to Classify\n\"we offer an account of computational implementation grounded in causal abstraction\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Causal abstraction theory",
    "method_rung": 3,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The claim is a theoretical proposal about computational implementation, not an empirical claim about a specific model's internals. The method used is causal abstraction theory, which is a conceptual framework for establishing counterfactual mappings between computational models and physical systems. This aligns with Rung 3 methods that establish unique mechanisms or counterfactual relationships. The claim 'we offer an account of computational implementation grounded in causal abstraction' is a Rung 3 claim because it proposes a theoretical account of how computation is implemented, which is a mechanistic/counterfactual claim about the underpinnings of computational explanation. There is no hedge present. Confidence is 4 because while the claim is clearly theoretical and aligns with Rung 3, the paper is philosophical/theoretical rather than empirical ML research, but the codebook's rung definitions still apply to claims about methods and their implications."
  },
  "metadata": {
    "model": "deepseek/deepseek-v3.2",
    "usage": {
      "prompt_tokens": 25592,
      "completion_tokens": 227,
      "total_tokens": 25819
    },
    "finish_reason": "stop"
  }
}