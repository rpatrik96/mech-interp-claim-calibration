paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,"Path patching (activation patching variant) combined with attention pattern analysis, ablation, and other causal interventions",2,3,1,4,0,"The claim 'presenting an explanation for how GPT-2 small performs a natural language task' uses the verb 'performs' which is a functional attribution marker (R3). The paper uses path patching (a causal intervention method, R2) and other interventional techniques to discover a circuit. However, the claim implies a mechanistic explanation of how the model performs the task, which is a R3 claim. The gap is +1 (R2 method to R3 claim), consistent with the calibration anchor pattern from the IOI paper. There is no hedging language in this claim."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,"Path patching (activation patching variant), attention pattern analysis, ablation",2,3,1,4,0,"The claim 'Our explanation encompasses 26 attention heads grouped into 7 main classes' is from the abstract. The paper uses path patching (a causal intervention method, Rung 2), attention pattern analysis (Rung 1), and ablation (Rung 2). The claim uses 'explanation' and 'encompasses' to describe a circuit that implements the IOI task, implying a mechanistic understanding of how the model performs the task. This aligns with Rung 3 linguistic markers like 'the circuit' (uniqueness) and functional attribution ('performs' implied). The claim is stated as fact without hedging. However, there is minor ambiguity because 'explanation' could be interpreted as a causal account (Rung 2), but the context of grouping heads into functional classes suggests a mechanistic narrative (Rung 3)."
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,Path patching (activation patching variant) and other causal interventions,2,3,1,4,0,"The claim 'this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior' uses the term 'reverse-engineering,' which implies uncovering the underlying mechanism or algorithm (Rung 3). The paper uses path patching (a causal intervention method) and other interventional techniques like knockouts, which are Rung 2 methods. However, 'reverse-engineering' suggests a mechanistic understanding beyond just causal effects, aligning with Rung 3 claims. The claim is stated as a fact without hedging. Confidence is 4 due to minor ambiguity in whether 'reverse-engineering' is being used in a strictly mechanistic sense, but the context supports a Rung 3 interpretation."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,Activation patching / path patching,2,3,1,4,0,"The claim 'Name Mover Heads... move the name from the subject to the end position' uses the functional verb 'move' which is a Rung 3 marker (mechanistic claim). The method used is activation patching/path patching (causal intervention), which is Rung 2. The claim asserts a specific function ('move the name'), implying a mechanistic role beyond just correlation or causal effect. There is no hedging language. However, the paper provides interventional evidence supporting the head's role, so while the claim is Rung 3, the method is Rung 2, indicating a +1 gap. Confidence is 4 due to minor ambiguity in whether 'move' is purely mechanistic or could be interpreted as causal sufficiency."
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,Activation patching / path patching,2,2,0,4,0,"The claim 'S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token' describes a causal effect established through interventional methods. The paper uses path patching (a variant of activation patching) to show that S-Inhibition Heads directly influence Name Mover Heads' queries, causing a drop in logit difference and changing attention patterns. This is a Rung 2 method (interventional) supporting a Rung 2 claim (causal effect: 'inhibit' implies causal influence). The claim is stated as a fact without hedging. Confidence is high but not maximal due to some ambiguity in whether 'inhibit' implies a unique mechanism (R3) or just causal sufficiency; however, the context and evidence support a causal claim without uniqueness assertions."
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,"Path patching, ablation, and quantitative criteria evaluation",2,3,1,4,0,"The claim 'the circuit is faithful complete and minimal' is a mechanistic claim about the circuit's properties (faithfulness, completeness, minimality) as defined in the paper. The methods used include path patching (Rung 2 interventional) and ablation (Rung 2), along with quantitative criteria evaluation. The claim uses definitive language ('is') to assert that the circuit possesses these properties, implying a mechanistic understanding and uniqueness (it is THE circuit). This aligns with Rung 3 linguistic markers like 'THE circuit' and functional attribution. However, the paper notes limitations (e.g., gaps in understanding, compensation effects), which slightly reduces confidence. No hedging words are present."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,Causal Tracing (activation patching) and ROME (weight editing),2,3,1,4,0,"The claim 'factual associations correspond to localized directly-editable computations' uses mechanistic language ('correspond to', 'computations') implying a specific mechanism. The paper uses causal tracing (activation patching, Rung 2) and ROME editing (weight intervention, Rung 2) to support this. However, the claim suggests a unique, localized computational mechanism (Rung 3) based on interventional evidence. This matches the overclaim pattern 'Patching → THE circuit' (R2→R3). No hedging words present."
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,Causal Tracing (activation patching variant),2,2,0,4,0,"The claim 'mediate factual predictions' uses the linguistic marker 'mediate,' which is explicitly listed as a Rung 2 (Causal) marker in the codebook. The method used is Causal Tracing, which involves corrupting inputs and restoring activations to measure causal effects—this is an interventional method (Rung 2). The claim does not assert uniqueness or a complete mechanism (e.g., 'the circuit' or 'encodes'), so it is appropriately Rung 2. There is no hedging language (e.g., 'may' or 'suggests'), so hedge_flag is 0. Confidence is 4 because the method and claim alignment is clear, though there is minor ambiguity in whether 'mediate' could be interpreted as a stronger mechanistic claim, but the codebook explicitly categorizes it as Rung 2."
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal tracing (activation patching) and ROME editing,2,3,1,4,0,"The claim 'mid-layer feed-forward modules... storing factual associations' uses the term 'storing', which is a Rung 3 mechanistic term implying a memory mechanism. According to the codebook's decision tree for 'encodes/represents/stores', if the paper provides interventional evidence (yes), we must check if the claim is about the intervention's result or the underlying mechanism. The paper uses causal tracing (Rung 2 interventional method) and ROME editing (Rung 2 interventional method) to support this claim. The claim is about the underlying storage mechanism, not just the result of the intervention, so it should be coded as Rung 3. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'storing' is meant mechanistically or as a result of intervention, but the context suggests a mechanistic claim."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,ROME (Rank-One Model Editing),2,2,0,4,0,"The claim 'ROME is effective on a standard zero-shot relation extraction model-editing task' is an empirical claim about the performance of an interventional method. ROME involves modifying model weights (specifically MLP weights via a rank-one update) to edit factual associations, which is an intervention on the model's parameters to change its behavior. This fits Rung 2 (Interventional) methods, as it establishes causal effects under specific interventions (editing weights and measuring changes in output). The claim itself states that the method 'is effective' on a task, which is a causal claim about the method's ability to produce desired changes (improved performance on the editing task). This matches Rung 2 linguistic markers like 'is effective' (implying causal sufficiency) and aligns with the intervention's purpose. There is no hedge present (e.g., 'may be effective'), so hedge_flag is 0. Confidence is 4 because while the method and claim are clearly interventional, there is minor ambiguity in whether 'effective' strictly implies causality, but the context supports it."
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,"Weight analysis, activation analysis, Fourier transform analysis, ablation in Fourier space",2,3,1,4,0,"The claim 'fully reverse engineer the algorithm learned by these networks' is a Rung 3 (counterfactual/mechanistic) claim because it asserts a complete understanding of the unique algorithm implemented by the model, implying uniqueness and full mechanistic explanation. The methods used include observational (R1: weight/activation analysis, Fourier transforms) and interventional (R2: ablation in Fourier space). The highest-rung method directly supporting the claim is ablation (R2), which establishes causal effects but not necessarily uniqueness. However, the claim uses 'fully reverse engineer' which implies a complete mechanistic understanding beyond just causal sufficiency, making it R3. There is no hedging language. Confidence is 4 because while the claim is strong, the paper provides extensive evidence across multiple methods converging on the algorithm, though 'fully' is an absolute term."
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,"Weight analysis, activation analysis, Fourier transform analysis, ablation in Fourier space",2,3,1,4,0,"The claim 'uses discrete Fourier transforms and trigonometric identities to convert addition to rotation' is from the abstract and describes the algorithm the model implements. The paper uses multiple methods: analysis of weights and activations (Rung 1 observational), but also ablation in Fourier space (Rung 2 interventional) to confirm the algorithm. The claim uses functional language 'uses' and describes a specific computational mechanism ('convert addition to rotation'), which is a mechanistic claim (Rung 3). The paper provides interventional evidence (ablation) supporting the algorithm, but the claim itself is about what the model 'uses' (mechanistic function), not just causal effects. Following the decision tree for 'encodes/represents/stores', since interventional evidence is provided and the claim is about the underlying mechanism (how it works), it should be coded as Rung 3. There is no hedging language in the claim."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,Ablation in Fourier space,2,3,1,4,0,"The claim 'grokking arises from the gradual amplification of structured mechanisms encoded in the weights' is from the abstract. The paper uses ablation in Fourier space (Rung 2 interventional method) to identify key frequencies and show their causal role. The claim uses 'mechanisms encoded in the weights' which implies a mechanistic understanding (Rung 3). Following the decision tree for 'encodes/represents/stores': the paper provides interventional evidence (ablation), but the claim is about the underlying mechanism ('arises from...amplification of structured mechanisms'), not just the intervention result, so it should be coded as R3. There is no hedge ('arises' is declarative). Confidence is 4 due to some ambiguity in whether 'encoded' is being used in a mechanistic sense versus a decodability sense, but the context of 'structured mechanisms' suggests mechanistic."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,Interchange interventions with binary masking,2,2,0,4,0,"The claim 'SAEs struggle to reach the neuron baseline' is based on evaluation using interchange interventions (a causal intervention method) with differential binary masking to select features. This is an interventional method (Rung 2) that measures causal effects of patching features. The claim is about comparative performance ('struggle to reach'), which is an empirical outcome of the intervention, not a mechanistic claim about what the SAEs 'encode' or 'represent'. It fits Rung 2 linguistic markers like 'degrades' (implied by struggle) and is about the effect of interventions. There is no hedge present."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,Distributed Alignment Search (DAS) interchange intervention,2,2,0,4,0,"The claim 'sets of SAE features that separately mediate knowledge' uses the term 'mediate,' which is a Rung 2 linguistic marker for causal claims. The paper uses DAS interchange interventions (a Rung 2 method) to evaluate whether SAE features mediate knowledge. The claim is directly supported by the interventional evidence from the DAS-based evaluation, aligning with the method rung. There is no hedge present. Confidence is high but not maximal due to some ambiguity in whether 'mediate' implies causal sufficiency or a stronger mechanistic claim, but the context and codebook decision rules support R2."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,Linear probing,1,1,0,5,1,"The claim 'activation probes may be a promising misuse mitigation technique' is from the abstract. The paper's primary method is linear probing (and variants like attention probes) on frozen activations to classify inputs, which is a Rung 1 observational method (establishes correlational evidence only). The claim uses linguistic markers 'may be a promising... technique' which is a Rung 1 associational claim about utility/potential, not a causal or mechanistic claim. The hedge 'may' indicates uncertainty, so hedge_flag = 1. Confidence is high because the method and claim are clearly aligned with Rung 1."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,Linear probing,1,1,0,4,0,"The claim 'probes fail to generalize under important production distribution shifts' is an empirical claim about probe performance. The method used is linear probing (or activation probing), which is explicitly described as a primary method in the paper (e.g., 'activation probes may be a promising misuse mitigation technique'). Linear probing is a Rung 1 (Observational/Associational) method per the codebook, as it establishes correlational evidence only (train classifier on frozen activations). The claim rung is 1 because it describes a performance observation ('fail to generalize') without causal or mechanistic language; it is an associational claim about probe behavior under distribution shifts. There is no hedge (no 'may', 'suggests', etc.), so hedge_flag = 0. Confidence is 4 due to minor ambiguity about whether 'probes' refers specifically to linear probes or includes other architectures, but the context indicates linear probing is the primary method."
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Activation patching,2,3,1,4,0,"The claim 'reverse-engineered nontrivial behaviors of transformer models' appears in the abstract. The paper's primary method is activation patching (a Rung 2 interventional method) as part of the ACDC algorithm. The term 'reverse-engineered' implies discovering the underlying mechanism or algorithm, which is a Rung 3 (counterfactual/mechanistic) claim according to the codebook's linguistic markers ('reverse-engineered' suggests uncovering the actual computational mechanism). There is no hedging language. However, the paper focuses on automating circuit discovery rather than fully reverse-engineering behaviors, so there is some ambiguity about the strength of the claim, leading to confidence 4."
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,ACDC algorithm (activation patching variant),2,2,0,4,0,"The claim states that ACDC 'rediscovered 5/5 of the component types in a circuit.' The method used is ACDC, which is described as an algorithm that automates activation patching (a form of intervention) to find connections between abstract neural network units. According to the codebook, activation patching is a Rung 2 (interventional) method. The claim is about rediscovering component types that were previously identified, which implies causal sufficiency (the components are involved in the behavior) but does not assert uniqueness or a full mechanistic explanation. The language 'rediscovered' suggests replication of known causal components, aligning with Rung 2 causal claims (e.g., 'is sufficient for,' 'enables'). There is no hedge present. Confidence is 4 due to minor ambiguity about whether 'component types' implies full mechanistic understanding, but the context indicates it's about identifying components involved in the circuit."
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,Activation patching,2,2,0,4,0,"The claim 'researchers can understand the functionality of each component' appears in the abstract summarizing the paper's contribution. The paper's primary method is activation patching (a Rung 2 interventional method) as part of the ACDC algorithm for automated circuit discovery. The claim uses 'functionality' which suggests understanding causal roles of components, aligning with Rung 2 causal claims ('mediates', 'influences', 'enables'). It does not use definitive uniqueness language ('THE circuit') or strong mechanistic verbs ('encodes', 'computes') that would indicate Rung 3. The claim is stated as a fact without hedging words like 'may' or 'suggests', so hedge_flag=0. Confidence is 4 because while the method is clearly Rung 2, the claim wording 'functionality' could be interpreted as slightly stronger than pure causal effect, but the context suggests it's about understanding causal roles through intervention."
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,Activation patching (ACDC algorithm),2,2,0,4,0,"The claim 'finding the connections between abstract neural network units that form a circuit' describes an interventional method (activation patching via ACDC) that systematically tests causal effects by replacing activations with corrupted ones to identify important edges. This matches Rung 2 methods. The claim itself uses language like 'finding connections' and 'form a circuit' which describes causal sufficiency/mediation rather than unique mechanistic attribution, aligning with Rung 2 claims. No uniqueness or mechanistic language (e.g., 'the circuit', 'encodes', 'computes') is present. The claim is stated as a factual description of what the algorithm does."
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,"Activation patching (path patching), resample ablation, edge attribution patching (EAP), ACDC",2,3,1,4,0,"The claim 'partially reverse-engineer the circuit responsible for the Indirect Object Identification task' appears in the abstract. The methods used include activation patching (path patching), resample ablation, edge attribution patching (EAP), and ACDC, which are interventional methods (Rung 2). The claim uses the phrase 'the circuit responsible for,' which implies a unique, mechanistic explanation (Rung 3) according to the codebook's decision tree for 'the circuit' (definite article implies uniqueness unless qualified). The hedge 'partially' does not reduce the claim rung; it indicates incompleteness but still asserts a circuit is responsible. The gap is +1 (R2→R3), consistent with the calibration example from the IOI circuit paper. Confidence is 4 due to minor ambiguity in 'partially' but clear R3 markers."
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,"Resample ablation, layer removal, minimal cross-talk circuit analysis",2,2,0,4,0,"The claim 'Layer 39 is a key bottleneck' appears in the abstract. The paper uses multiple interventional methods to support this: resample ablation (Section 3.1.4, Figure 4), zero-ablation of layer outputs (Section 4.1.2, Figure 5), and constructing minimal cross-talk circuits (Section 4.1.3, Figure 6). These are Rung 2 methods (interventional) as they involve causal interventions like ablation and patching. The claim uses 'bottleneck' which implies a causal role in information flow, aligning with Rung 2 causal claims (e.g., 'mediates', 'influences'). It does not use Rung 3 markers like 'encodes' or 'the circuit', nor does it claim uniqueness or a full mechanism. No hedging words are present."
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,Activation patching (resample ablation) and cosine similarity analysis,2,3,1,4,0,"The claim 'Convolutions in layer 39 shift names one position forward' is from the abstract. The paper uses resample ablation (a causal intervention) on conv slices and cosine similarity analysis of hidden states to support this claim. The method involves patching conv slices and observing effects on model behavior, which is interventional (Rung 2). The claim uses the functional verb 'shift' and implies a specific computational mechanism performed by the convolution, which is a mechanistic claim (Rung 3). There is no hedge present. The gap is +1 (R2→R3), consistent with the overclaim pattern of using interventional evidence to support a functional/mechanistic claim. Confidence is 4 due to minor ambiguity in whether 'shift' is explicitly causal or mechanistic, but the context suggests a mechanistic interpretation."
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,Activation patching (resample ablation) and representation modification via averaging/subtraction,2,3,1,4,0,"The claim 'The name entities are stored linearly in Layer 39's SSM' uses the term 'stored' which, per the codebook's decision tree for 'encodes/represents/stores', defaults to R3 (mechanistic) unless context indicates a decodability sense. The paper provides interventional evidence: resample ablation (R2) shows layer 39 is a bottleneck, and representation modification experiments (subtract/add method) demonstrate linear manipulation of SSM inputs changes outputs. However, the evidence establishes causal effects of interventions (R2), not uniqueness or full mechanistic explanation of storage. The claim implies a specific storage mechanism ('linearly stored'), which is a R3 mechanistic claim. There is no hedge ('are stored' is definitive). Confidence is 4 due to some ambiguity: the paper's evidence supports linear manipulability but not necessarily storage as a unique mechanism."
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,Benchmarking/comparison of steering methods,2,2,0,4,0,"The claim 'prompting outperforms all existing methods followed by finetuning' is an empirical comparison of model control techniques (steering methods) based on evaluation results from AXBENCH. The methods compared (prompting, finetuning, representation-based steering like SAEs, ReFT-r1, etc.) involve interventions (e.g., adding vectors to activations, finetuning) to assess causal effects on model outputs, which aligns with Rung 2 (Interventional). The claim itself is a comparative performance statement about causal efficacy ('outperforms'), which is a Rung 2 claim about causal effects under specific interventions. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'outperforms' implies a causal claim, but the context of benchmarking steering methods supports this classification."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,Benchmark comparison of steering methods (including SAEs) using concept detection and model steering evaluations,2,2,0,4,0,"The claim 'SAEs are not competitive' is an empirical comparison based on the AXBENCH evaluation, which uses interventional methods (steering vectors, ReFT-r1, etc.) to assess causal efficacy in model steering and concept detection. The paper's methods involve interventions (e.g., adding vectors to activations) and performance metrics (AUROC, steering scores), which are Rung 2 (causal effects under interventions). The claim is about comparative performance (SAEs lag behind other methods), which is a causal efficacy claim (Rung 2), not a mechanistic explanation (Rung 3). No hedging words like 'may' or 'suggests' are present, so hedge_flag = 0. Confidence is 4 due to clear method classification but some ambiguity in whether the claim is purely Rung 2 (it's about causal performance, not mechanism)."
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,Difference-in-means,1,1,0,4,0,"The claim 'representation-based methods such as difference-in-means perform the best' is from the abstract. The method 'difference-in-means' is an observational/associational method (Rung 1) that computes the difference between averaged representations from two classes of inputs, establishing correlational evidence without intervention. The claim uses the phrase 'perform the best,' which is an empirical performance claim about concept detection (C) and model steering (S) tasks, not a causal or mechanistic claim. It does not use markers like 'causally affects' (R2) or 'encodes' (R3). The claim is stated as a factual finding without hedging. Confidence is 4 because the method is clearly R1, and the claim is about performance, but there is minor ambiguity about whether 'perform' could imply functional attribution; however, in context, it refers to benchmark results."
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,causal tracing or interchange interventions,2,2,0,5,0,"The claim states that 'specific components within middle layers show strong causal effects at the last token of the subject.' The method used is explicitly stated as 'causal tracing or interchange interventions' in the abstract, which are interventional methods (Rung 2). The claim uses the phrase 'causal effects,' which is a direct linguistic marker for Rung 2 claims (causal claims). There is no hedge present, and the claim is stated as an established finding. The method and claim rungs align appropriately, with no overclaiming."
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,ROME (Rank One Model Editing),2,2,0,4,0,"The claim 'rank-one model editing methods can successfully insert facts at specific locations' appears in the abstract and is supported by the paper's experiments applying ROME to Mamba. The method used is ROME, which involves making rank-one changes to model parameters to alter factual associations—this is an interventional method (editing weights and observing output changes), placing it at Rung 2. The claim states that these methods 'can successfully insert facts,' which is a causal claim about the effect of an intervention (editing changes model behavior), matching Rung 2 linguistic markers like 'can produce' or 'enables.' There is no hedge (e.g., 'may' or 'suggests'), and the claim is presented as an established finding. The confidence is 4 because the method and claim alignment is clear, but there is minor ambiguity in whether 'successfully' implies a mechanistic claim (Rung 3), but the context indicates it's about causal efficacy."
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,Linearity of relation embedding (LRE) analysis,1,1,0,4,0,"The claim 'linearity of Mamba's representations of factual relations' refers to the LRE analysis method described in Section 5, which involves computing Jacobians and using linear approximations to test faithfulness of linear decodability. This is an observational/associational method (Rung 1) that establishes correlational evidence about whether factual relations can be linearly decoded from representations. The claim itself uses the term 'representations' which could be ambiguous, but in context it refers to the linear decodability analysis (Rung 1 linguistic markers like 'linearity' and 'representations' in the associational sense). There is no intervention mentioned, and the paper's LRE analysis measures how well linear approximations match model outputs, which is correlational. No hedging language is present in this specific claim phrase."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,Causal mediation analysis and interchange interventions,2,3,1,4,0,"The claim 'LM binds each character-object-state triple together by co-locating their reference information' appears in the abstract and describes a mechanistic process ('binds', 'co-locating'). The paper uses causal mediation analysis and interchange interventions (activation patching) to support this claim, which are Rung 2 (interventional) methods. The claim uses functional/mechanistic language ('binds', 'co-locating') implying a specific computational mechanism, which is Rung 3. There is no hedging language. However, the claim is about a discovered mechanism rather than a uniqueness claim ('the mechanism'), so while it's Rung 3, it's not making an absolute uniqueness claim. Confidence is 4 because while the methods are clearly interventional, the claim language is mechanistic but not explicitly uniqueness-claiming."
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,Causal mediation analysis and interchange interventions,2,2,0,4,0,"The claim 'lookback mechanism which enables the LM to recall important information' appears in the abstract. The paper's primary method is causal mediation analysis using interchange interventions (activation patching), which is a Rung 2 (interventional) method. The claim uses the term 'enables,' which suggests a causal sufficiency claim (the mechanism enables recall). According to the codebook, 'enables' is a Rung 2 linguistic marker (causal claim). The claim does not assert uniqueness or a complete mechanistic description (e.g., 'THE mechanism' or 'computes'), so it is appropriately Rung 2. There is no hedging language ('may,' 'suggests'), so hedge_flag = 0. Confidence is 4 because the claim is clear, but there is minor ambiguity in whether 'enables' could be interpreted as a stronger mechanistic claim; however, given the context and the codebook's guidance on 'enables' as Rung 2, this is appropriate."
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,Causal mediation analysis with interchange interventions (activation patching),2,3,1,4,0,"The claim 'the binding lookback retrieves the correct state OI' appears in the abstract. The paper's primary method is causal mediation analysis using interchange interventions (activation patching), which is a Rung 2 (interventional) method. The claim uses the verb 'retrieves,' which implies a functional mechanism (the lookback performs an operation). According to the codebook's decision tree for polysemous terms, functional verbs like 'retrieves' typically indicate Rung 3 (mechanistic) claims, as they describe what a component does rather than just establishing causal effects. The paper provides interventional evidence (activation patching) to support this, but the claim itself is mechanistic (describing the algorithm's operation). There is no hedging language (e.g., 'may' or 'suggests'), so hedge_flag = 0. Confidence is 4 because while the claim is clearly mechanistic, the method is interventional (Rung 2), leading to a potential overclaim gap, but the context strongly supports a Rung 3 interpretation."
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,Causal mediation analysis and interchange interventions (activation patching),2,3,1,4,0,"The claim 'reverse-engineering ToM reasoning in LMs' appears in the abstract. The paper uses causal mediation analysis and interchange interventions (activation patching), which are Rung 2 (interventional) methods. The phrase 'reverse-engineering' implies uncovering the underlying mechanism or algorithm, which is a Rung 3 (counterfactual/mechanistic) claim according to the codebook's linguistic markers ('reverse-engineering the algorithm' is listed as a Rung 3 marker in the calibration examples). The claim does not contain explicit hedges like 'may' or 'suggests', so hedge_flag = 0. Confidence is 4 because while 'reverse-engineering' strongly suggests a mechanistic claim, the abstract phrasing is somewhat broad."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,Interchange interventions (activation patching variant),2,3,1,4,0,"The claim 'LMs implement such retrieval via a positional mechanism' is a mechanistic claim about how LMs work ('implement', 'mechanism'), which is Rung 3. The paper uses interchange interventions (a form of activation patching) to establish causal effects, which is a Rung 2 method. The claim overreaches the method's rung by asserting a specific implementation mechanism rather than just causal sufficiency. There is no hedging language. Confidence is 4 because the claim is explicit, but the method is interventional (R2) while the claim is mechanistic (R3), creating a clear overclaim pattern."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,Interchange interventions (activation patching),2,2,0,4,0,"The claim 'LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism' is from the abstract. The paper uses interchange interventions (a form of activation patching) as the primary method, which is a Rung 2 (interventional) method. The claim describes causal supplementation of mechanisms ('supplement'), which is a causal claim about how mechanisms interact, fitting Rung 2 linguistic markers like 'influences' or 'enables'. It does not assert uniqueness or full mechanistic explanation (Rung 3), nor is it purely correlational (Rung 1). There is no hedging language. Confidence is 4 due to minor ambiguity in whether 'supplement' implies causal sufficiency or mechanistic role, but the context supports Rung 2."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,Interchange interventions (causal tracing/activation patching),2,2,0,4,0,"The paper uses interchange interventions (a Rung 2 interventional method) to establish causal effects of different mechanisms (positional, lexical, reflexive). The claim states they 'develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement' - this is a claim about causal sufficiency ('estimates next token distributions') based on interventional evidence, matching Rung 2 linguistic markers like 'causal model' and 'estimates' (predictive causal effect). There's no uniqueness claim or mechanistic explanation of how the model computes these mechanisms, just that they combine to predict behavior. The claim is stated as an empirical result without hedging."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,interchange interventions (activation patching variant),2,3,1,4,0,"The claim 'how LMs bind and retrieve entities in-context' is a mechanistic claim about the internal processes of language models, implying an understanding of the underlying mechanisms. The paper uses interchange interventions (a form of activation patching) which is a Rung 2 interventional method that establishes causal effects but not unique necessity or counterfactual proof. The claim uses functional language ('bind and retrieve') which suggests a mechanistic explanation (Rung 3). There is no hedge present. The method is interventional (Rung 2) but the claim is mechanistic (Rung 3), indicating a +1 gap. Confidence is 4 due to some ambiguity in whether 'bind and retrieve' is explicitly a uniqueness claim, but the overall narrative strongly implies mechanistic understanding."
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,Activation patching (path patching) and circuit analysis,2,2,0,4,1,"The paper uses interventional methods (activation patching/path patching) to discover and analyze circuits, which is Rung 2. The claim states 'circuits within LLMs may be more flexible and general than previously recognized' - this is a causal claim about circuit properties (flexibility/generality) based on intervention results, but uses 'may' as a hedge. The claim does not assert a unique mechanism (R3) but rather a causal property discovered through intervention."
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,activation patching (path patching),2,3,1,4,0,"The paper uses activation patching (path patching) to discover and evaluate circuits, which is a Rung 2 (interventional) method. The claim 'the circuit generalizes surprisingly well reusing all of its components and mechanisms' uses the definite article 'the circuit' implying uniqueness and the functional term 'reusing all of its components and mechanisms' suggests a mechanistic narrative about how the circuit operates across variants. According to the codebook's decision tree for 'the circuit', without explicit uniqueness tests, the definite article typically indicates a Rung 3 claim (mechanistic/uniqueness). The paper does not prove uniqueness but treats the circuit as the identified mechanism. No hedging words are present."
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,Activation patching (path patching) and knockout/ablation analysis,2,3,1,4,0,"The paper uses path patching (a Rung 2 interventional method) to discover circuits and analyze causal effects. The claim 'we discover a mechanism that explains this which we term S2 Hacking' uses the term 'mechanism' which is a Rung 3 linguistic marker indicating a mechanistic explanation. The claim implies an understanding of how the circuit works (explaining generalization), not just causal effects. However, the paper provides interventional evidence (patching, ablation) to support the mechanism, so it's not purely observational. Following the decision tree for 'mechanism': the paper provides interventional evidence (YES), and the claim is about the underlying mechanism (how it works) not just the intervention result, so Rung 3. Confidence is 4 because while 'mechanism' strongly suggests Rung 3, the abstract doesn't provide full details, but the paper's methods support a mechanistic claim."
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,circuit discovery via activation patching,2,3,1,4,0,"The claim 'implement algorithms responsible for performing specific tasks' appears in the abstract, describing the goal of mechanistic interpretability. The paper's primary method is activation patching (path patching) to discover circuits, which is a Rung 2 interventional method. The claim uses the verb 'implement' which is a Rung 3 linguistic marker indicating a mechanistic claim about what the circuit does. There is no hedge present. The confidence is 4 because while the claim is clearly Rung 3, the method is Rung 2, but the paper builds on established circuit discovery methods (like Wang et al. 2023) which use patching, so the method classification is clear."
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Multi-task Distributed Alignment Search (MDAS) with interchange interventions,2,2,0,4,0,"The method used is MDAS, which builds on Distributed Alignment Search (DAS) and uses interchange interventions (activation patching) to find distributed features that satisfy multiple causal criteria. This is an interventional method (Rung 2). The claim states that MDAS 'achieves state-of-the-art results on RAVEL' and demonstrates 'the importance of going beyond neuron-level analyses to identify features distributed across activations.' This is an empirical claim about causal effectiveness (performance on a benchmark) and the utility of distributed features, not a mechanistic claim about unique underlying mechanisms. The language 'demonstrates the importance' indicates causal sufficiency/utility, not uniqueness or mechanistic explanation. There is no hedge present. Confidence is 4 due to minor ambiguity about whether 'identify features' implies mechanistic identification, but the context suggests it's about causal localization."
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,Interchange intervention (activation patching),2,3,1,4,0,"The method used is interchange intervention (a form of activation patching), which is a Rung 2 interventional method. The claim uses the term 'encodes' in the context of providing evidence from an intervention. Following the decision tree for 'encodes': the paper provides interventional evidence (YES), and the claim is about the intervention's result (what changed) rather than the underlying mechanism. This suggests Rung 2. However, the phrase 'encodes the attribute' is a strong mechanistic claim that typically implies Rung 3. The context indicates the authors are interpreting the intervention result as evidence that the feature represents the attribute, which leans toward a mechanistic claim. Given the default mechanistic reading of 'encodes' and lack of explicit hedging, I classify as Rung 3. Confidence is 4 due to some ambiguity between R2 and R3."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,Interchange interventions (activation patching),2,2,0,4,0,"The claim 'Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle' is from the results section. The method used is interchange interventions (activation patching), which is an interventional method (Rung 2). The claim compares performance of methods with counterfactual supervision (like DBM, DAS) versus unsupervised featurizers (like PCA, SAE). The linguistic markers 'achieve strong results' and 'struggle' describe empirical outcomes from interventions, not mechanistic explanations. This is a causal claim about method effectiveness based on intervention evidence, fitting Rung 2. No hedging terms are present. Confidence is 4 due to minor ambiguity in whether 'counterfactual supervision' implies Rung 3, but context clarifies it refers to intervention-based training objectives."
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,Multi-task Distributed Alignment Search (MDAS) with interchange interventions,2,2,0,4,0,"The claim 'The representations of different attributes gradually disentangle as we move towards later layers' is based on results from MDAS, which uses interchange interventions (a form of activation patching) to measure causal effects. This is an interventional method (Rung 2). The claim uses the term 'disentangle' in the context of the paper's evaluation metric (Disentangle score), which combines Cause and Iso scores from interventions. It describes an observed pattern from interventions ('gradually disentangle'), not a unique mechanistic explanation. The claim is about causal sufficiency of features across layers, not uniqueness or underlying mechanisms, so it fits Rung 2 (causal claims). There is no hedge present."
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,Multi-task Distributed Alignment Search (MDAS) with interchange interventions,2,2,0,4,0,"The claim is based on results from MDAS, which uses interchange interventions (a form of activation patching) to measure causal effects of features on attribute predictions. The claim states that changing one entangled attribute has 'unavoidable ripple effects' on others, which is a causal claim about intervention effects ('changing...has...effects'). This matches Rung 2 linguistic markers ('causally affects', 'intervening on X changes Y'). The claim does not assert uniqueness or mechanism (Rung 3) but describes observed causal relationships from interventions."
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,"Linear probing, activation logging, SAE feature attribution, attention visualization, PCA/SVD, correlation analysis (observational/associational methods)",1,3,2,4,0,"The claim 'representations encode rich semantic information' uses the term 'encodes,' which according to the codebook's decision tree for polysemous terms defaults to Rung 3 (mechanistic claim) unless context indicates it means 'linearly decodable from.' The cited 'prior interpretability work' likely includes methods like linear probing and activation analysis (Rung 1 observational methods). No interventional evidence is provided in this claim, and the context does not clarify it as decodability. Thus, claim_rung is 3, method_rung is 1, indicating a +2 gap (overclaiming). There is no hedging language."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,"Interventional interpretability (DAS, subspace interventions)",2,3,1,4,0,"The claim 'human-interpretable concepts are encoded linearly' appears in the Related Work section summarizing interventional interpretability methods like DAS (distributed alignment search) and subspace interventions, which are Rung 2 (causal interventions). However, the term 'encoded' triggers the polysemous decision tree: the paper provides interventional evidence (YES), but the claim is about the underlying mechanism (how concepts are represented) rather than just the intervention's result, so it defaults to Rung 3. There is no hedging language ('encodes' is stated as fact). Confidence is high but not maximum due to some ambiguity in whether 'encoded' is meant in a purely linear decodability sense (R1) or mechanistic sense (R3), but the context of interventional evidence suggests a stronger claim."
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,Distributed Alignment Search (DAS),2,2,0,4,0,"The claim references DAS, which is described in the paper as a method that 'finds the subspace that maximises the probability of the expected counterfactual output after intervention' and is used for 'localizing concepts within model representations.' DAS involves interventions (specifically distributed interchange interventions) to establish causal effects, placing it at Rung 2 (Interventional). The claim states DAS 'can effectively localize concepts within model representations.' 'Localize' here implies causal localization through intervention, aligning with Rung 2 causal claims (e.g., 'mediates,' 'has causal effect on'). There is no hedge present. Confidence is 4 due to minor ambiguity in interpreting 'localize,' but the context strongly supports a causal interpretation based on the method's description."
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,LoReFT (Low-rank Linear Subspace ReFT) interventions,2,3,1,4,0,"The claim 'a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks' is from the discussion section, interpreting results from LoReFT experiments. The method used is LoReFT, which involves training interventions on hidden representations via learned low-rank projections (eq. 2). This is an interventional method (Rung 2) because it modifies activations during inference to steer model behavior, establishing causal effects under specific interventions. The claim uses mechanistic language ('can achieve generalised control') and implies a functional mechanism about how the model works, which is characteristic of Rung 3 claims. There is no hedge present. However, the claim is somewhat abstract and not directly about a specific circuit, so confidence is 4 due to minor ambiguity in whether it's a direct mechanistic claim or a broader implication."
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,Low-rank Linear Subspace ReFT (LoReFT) interventions on hidden representations,2,2,0,4,0,"The method used is LoReFT, which involves learned interventions on hidden representations (adding learned vectors to specific positions/layers). This is an interventional method (Rung 2) because it modifies activations to steer model behavior. The claim states 'can induce a base LM to follow instructions' – this describes a causal effect (steering behavior through intervention), which matches Rung 2 linguistic markers like 'can produce' or 'enables'. There is no hedge present. The claim does not assert a unique mechanism or representational content (Rung 3), nor is it purely observational (Rung 1). Confidence is 4 due to minor ambiguity: the claim could be interpreted as a strong causal claim, but it's appropriately matched to the interventional method."
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,SAE feature attribution,1,3,2,4,0,"The claim 'stored in the parameters' uses storage language ('stored'), which is a Rung 3 linguistic marker implying a mechanistic claim about how knowledge is represented. The paper's primary method for analyzing knowledge involves editing via hyper-networks (Rung 2 interventional), but the claim itself is about knowledge being stored, which is a mechanistic claim. However, the paper also uses observational methods like analyzing updates and gradients, which are Rung 1. Since the claim is about storage without direct interventional evidence for that specific claim, and following the decision tree for 'stores', which defaults to Rung 3 when no interventional evidence is provided, the claim rung is 3. The method used to support this claim is primarily observational (analyzing where updates occur), so method rung is 1. There is no hedge in the claim. Confidence is 4 due to some ambiguity in whether the claim is supported by interventional evidence elsewhere, but the claim text itself is strongly mechanistic."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,Hyper-network weight update analysis,2,2,0,4,0,"The method involves analyzing which components (parameters) need to be changed via hyper-network predictions to edit factual knowledge, which is an interventional approach (changing weights to observe effects). The claim uses 'revealing which components need to be changed to manipulate factual knowledge,' which is a causal claim about identifying components that causally affect knowledge manipulation, fitting Rung 2 linguistic markers like 'manipulate' (interventional). There is no hedge, and the claim is stated as fact. Confidence is high but not maximum due to some ambiguity in whether 'revealing' implies a unique mechanism (R3), but the context suggests it's about identifying causal components rather than proving uniqueness."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,Activation analysis of weight updates,1,1,0,4,0,"The claim 'the updates tend to be concentrated on a small subset of components' is based on analyzing the weight updates predicted by the hyper-network (e.g., visualizing average magnitude across layers). This is an observational/associational analysis (Rung 1 method) that identifies patterns in the learned updates without intervention. The claim uses language 'tend to be concentrated' which is associational, not causal or mechanistic. No intervention or causal evidence is presented for this specific claim in the abstract. The method is observational analysis of update patterns, fitting Rung 1. The claim is also Rung 1 as it describes an observed pattern."
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,Hyper-network weight update analysis,2,3,1,4,0,"The method involves analyzing which components of the network are updated by the hyper-network to manipulate factual knowledge, which is an interventional method (Rung 2) as it involves modifying weights and observing effects. The claim uses the phrase 'causal mediation mechanisms,' which implies a mechanistic understanding of how knowledge is stored and manipulated, moving beyond mere causal effect to describe the underlying mechanism. This is a Rung 3 claim because it suggests the hyper-network reveals the specific mechanisms (causal mediation) by which knowledge is encoded and edited, which is a stronger claim than just establishing causal sufficiency. There is no hedging language present."
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Analysis of weight updates predicted by hyper-network (gradient analysis),1,2,1,3,1,"The method involves analyzing which parameters receive updates from the hyper-network (gradient analysis), which is observational/associational (Rung 1) as it examines correlations between updates and model components without intervention. The claim uses 'seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,' which suggests a causal influence ('achieved by modifying') but is hedged ('seems to'), making it a Rung 2 causal claim rather than a Rung 3 mechanistic claim about unique encoding or computation. The hedge flag is 1 due to 'seems to.' Confidence is moderate due to ambiguity in interpreting 'achieved by' as causal vs. correlational."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,Conceptual analysis and argumentation,1,1,0,4,0,"The claim is about a conceptual difficulty in evaluating deception detectors, not an empirical finding from an interventional or counterfactual method. The paper uses conceptual arguments, analysis of existing works, and case studies to support this claim, which are observational/associational methods (Rung 1). The claim itself states that distinguishing strategic deception 'requires making claims about a model's internal beliefs and goals'—this is an associational claim about what is needed, not a causal or mechanistic claim. There is no intervention or counterfactual evidence presented for this specific claim; it's a conceptual point about evaluation challenges. The claim is stated as a fact without hedging."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,Conceptual analysis and reasoning about necessary conditions,1,3,2,3,0,"The claim is a conceptual question about what must be true internally for a deception detector (like an activation probe) to work. The paper does not present empirical evidence from applying a specific method (like linear probing or activation patching) to support this claim. Instead, it's a theoretical discussion about necessary conditions for deception detection to be tractable. According to the codebook, Rung 3 claims involve mechanistic narratives about how the model works ('what must be true about the internal state'). The claim implicitly suggests a unique or necessary internal structure ('consistent internal mechanisms') for deception detection to be possible, which aligns with Rung 3 linguistic markers about mechanisms. However, the method used is not empirical experimentation but conceptual reasoning, which is observational/associational (Rung 1). There is no hedge in the claim. Confidence is moderate (3) because while the claim is clearly about mechanisms (R3), the method classification is ambiguous as it's not a standard empirical method listed in the codebook, but fits R1 as observational analysis."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,Conceptual analysis and empirical observation,1,3,2,4,0,"The claim 'Model beliefs are not stable and are far more context dependent than animal or human beliefs' is a mechanistic claim about the nature of model representations (Rung 3). It uses the term 'beliefs' in a representational/mechanistic sense, implying how models internally encode and process information. The paper supports this through conceptual arguments and analysis of empirical works (e.g., roleplaying examples, belief modification studies), but does not use interventional methods to establish causal mechanisms or counterfactual uniqueness. The methods are observational/associational (Rung 1), analyzing patterns and context sensitivity without interventions. There is no hedge in the claim. Confidence is 4 due to some ambiguity in interpreting 'beliefs' as a mechanistic claim versus a correlational observation, but the context suggests a strong mechanistic interpretation."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,Correlation analysis,1,1,0,4,0,"The claim 'We find very low agreement between a full-transcript autorater and the MASK labels' is an empirical observation about the correlation (or lack thereof) between two labeling methods. The method used is comparing labels from an autorater with MASK labels, which is a form of correlation analysis (observational/associational evidence). The claim uses language like 'low agreement,' which indicates a correlational relationship (Rung 1). There is no intervention on the model; it's purely observational comparison of labels. The claim is stated as a factual finding without hedging. Confidence is 4 because while the method and claim are clearly Rung 1, the specific method 'correlation analysis' is inferred from context (comparing labels), but it fits the Rung 1 description of statistical associations."
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,Empirical observation from existing literature,1,1,0,4,1,"The claim states 'It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive.' This is an associational claim about a correlation between deceptive behavior and consistent mechanisms, based on observed patterns in current models. The method used is observational analysis of existing works (referencing studies on verbalized deceptive intent), which fits Rung 1 (observational/associational). The claim uses language like 'have a consistent mechanism' which describes a correlation, not a causal or mechanistic attribution. The hedge 'mostly true today' indicates uncertainty, so hedge_flag = 1. Confidence is 4 due to minor ambiguity in interpreting 'consistent mechanism' but overall clear as an associational claim."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,Distributed interchange interventions (DAS variant) with hypernetwork-based token selection and subspace identification,2,2,0,4,0,"The method involves interchange interventions (patching features in hidden states) which are interventional (Rung 2). The claim states 'disentangling concepts in hidden states' via benchmark performance, which is a causal claim about intervention effects (Rung 2). No uniqueness or mechanistic language is used; it's about achieving SOTA performance on a benchmark measuring causal mediation."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,Distributed alignment search (DAS) / interchange interventions,2,2,0,4,0,"The claim 'features that mediate concepts and enable predictable manipulation' appears in the abstract, referencing the paper's method. The method used is DAS/interchange interventions, which involves patching features based on counterfactual data to measure causal effects, placing it at Rung 2 (interventional). The claim uses 'mediate' and 'enable predictable manipulation', which are linguistic markers for causal effects (Rung 2). There is no uniqueness or mechanistic language like 'encodes' or 'the circuit', so it does not rise to Rung 3. No hedging words are present. Confidence is high but not maximal due to some ambiguity in 'enable predictable manipulation', which could imply mechanistic control but is consistent with causal sufficiency."
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,HyperDAS (hypernetwork with cross-attention to model states and weighted interchange interventions),2,2,0,4,0,"The method uses a hypernetwork to select token positions and perform weighted interchange interventions (Equation 11), which is an interventional technique (Rung 2). The claim uses 'locates' which could be ambiguous, but the context specifies 'automatically locates...that a concept is realized in' implying causal localization via intervention, not just correlation. The paper frames this as identifying mediators through interventions, aligning with Rung 2 causal claims ('mediates', 'localizing'). No uniqueness or mechanistic language like 'encodes' or 'the circuit' is used here, so claim_rung is 2. Hedge_flag is 0 as it's stated as a fact."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,Interchange interventions,2,2,0,5,0,"The claim 'Interchange interventions identify neural representations that are causal mediators of high-level concepts' explicitly references 'interchange interventions' as the method. According to the codebook, interchange interventions are classified as Rung 2 (Interventional) methods, as they involve interventions that change features to counterfactual values to measure causal effects. The claim uses the phrase 'causal mediators', which is a Rung 2 linguistic marker (causal claims). There is no hedge present. The method and claim rungs align appropriately, with no overclaiming. Confidence is very high (5) as the method and claim are clearly stated and match the codebook definitions."
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,HyperDAS (interchange intervention with automated token selection),2,3,1,4,0,"The method involves interchange interventions (weighted patching) on hidden states using a learned subspace, which is an interventional technique (Rung 2). The claim uses 'store attributes', which under the decision tree for 'stores' implies a mechanistic claim (Rung 3) because: (1) The paper provides interventional evidence (yes), but (2) the claim is about the underlying mechanism (that positions 'store' attributes) rather than just the intervention's result. The claim asserts a discovery about where attributes are stored, implying a representation/storage mechanism. No explicit hedge is present."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,Steering vectors (Difference of Means extraction) + attribution patching,2,2,0,4,0,"The claim states 'mediated by linear directions' and 'can be controlled using steering vectors.' The method used is steering vectors extracted via Difference of Means (Section 2.2) and evaluated via attribution patching (Section 4.3), which are interventional techniques (Rung 2). The claim uses 'mediated by' which is a Rung 2 linguistic marker (causal mediation). It does not claim uniqueness or mechanistic explanation (Rung 3). No hedge present."
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,Steering vectors (activation addition/intervention),2,2,0,4,0,"The method used is steering vectors (activation addition/intervention), which involves adding or subtracting vectors to model activations at inference time to change behavior. This is an interventional method (Rung 2) as it directly intervenes on activations and measures output changes. The claim states that positive steering increases behaviors and negative steering suppresses them, confirming causal influence. The phrase 'causal influence' explicitly indicates a causal claim (Rung 2), matching the interventional method. There is no hedge present ('confirming' is assertive). Confidence is high (4) because the claim directly describes the intervention's effect, though there is minor ambiguity about whether 'confirming' implies stronger mechanistic claims."
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,Steering vectors via Difference of Means method,2,3,1,4,0,"The method used is steering vectors extracted via Difference of Means (Section 2.2) and evaluated via attribution patching (Section 4.3), which involves adding/subtracting vectors to activations at inference time to change behavior. This is an interventional method (Rung 2). The claim uses 'encode' in a mechanistic sense: 'Thinking LLMs encode these reasoning mechanisms as linear directions.' Following the decision tree for 'encodes': the paper provides interventional evidence (steering vectors change behavior), but the claim is about the underlying mechanism (how the model represents/encodes reasoning), not just the intervention result. Thus, claim_rung is 3. There is no hedge ('hypothesis' refers to the broader hypothesis being reinforced, not hedging this specific claim). Confidence is 4 due to minor ambiguity: 'encode' could be interpreted as R2 causal mediation, but the context suggests a mechanistic claim about internal representation."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,Steering vectors via Difference of Means method,2,2,0,4,0,"The method used is steering vectors extracted via Difference of Means (Section 2.2), which involves computing mean activation differences between datasets exhibiting vs. not exhibiting behaviors. This is an interventional method (Rung 2) because it involves adding/subtracting vectors to activations at inference time to change behavior, establishing causal effects. The claim states behaviors 'can be isolated to specific directions' and 'enabling precise control through steering vectors.' The language 'isolated to specific directions' suggests linear directions mediate behaviors, and 'enabling precise control' indicates causal sufficiency via intervention. This matches Rung 2 causal claims (e.g., 'can be controlled,' 'intervening on X changes Y'). There is no uniqueness or mechanistic language like 'THE mechanism' or 'encodes'; it's about causal control via linear directions, not proving a unique underlying mechanism. No hedge present."
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,Steering vectors via Difference of Means method,2,3,1,4,0,"The method used is steering vectors extracted via Difference of Means and evaluated via attribution patching (a causal intervention), placing it at Rung 2 (interventional). The claim uses 'distinct mechanisms' which implies specific computational structures or algorithms underlying reasoning, a Rung 3 (mechanistic/counterfactual) claim. The paper does not provide uniqueness proofs or necessity tests to establish these as 'the' mechanisms, but 'mechanisms' is a strong mechanistic term. There is no hedge ('indicates' is assertive). Confidence is 4 due to some ambiguity: 'mechanisms' could be interpreted as causal pathways (R2), but the phrasing 'distinct mechanisms to achieve their reasoning process' suggests a functional/mechanistic attribution beyond mere causal sufficiency."
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,HyperSteer hypernetwork training and evaluation,2,2,0,4,0,"The claim is about performance comparison between HyperSteer and other activation steering methods (ReFT-r1, SAE, etc.) based on empirical evaluation results (steering scores). The method involves training hypernetworks to generate steering vectors and evaluating them via interventions (adding vectors to activations) on base LMs, which is interventional (Rung 2). The claim states that scaling HyperSteer 'exceeds the performance' of state-of-the-art methods, which is a comparative empirical claim about causal efficacy of steering interventions, not a mechanistic claim about unique internal mechanisms. It fits Rung 2 linguistic markers like 'exceeds performance' (causal effect on output). No hedge present."
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,HyperSteer hypernetwork-based activation steering,2,2,0,4,0,"The claim 'HYPERSTEER performs on par with steering-via-prompting' is an empirical performance comparison based on intervention (adding steering vectors to activations). The method involves training hypernetworks to generate steering vectors added to base LM activations, which is an interventional technique (Rung 2). The claim uses 'performs on par with' to indicate comparable efficacy, which is a causal sufficiency claim about steering effectiveness relative to prompting, fitting Rung 2 linguistic markers like 'is sufficient for' or 'can produce'. There is no hedge, and the claim is stated as fact. Confidence is 4 due to minor ambiguity in whether 'performs on par' implies causal effect or just correlational comparison, but the interventional method supports a Rung 2 interpretation."
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,HyperSteer (cross-attention variant) trained and evaluated on AXBENCH dataset,2,2,0,4,0,"The method involves training a hypernetwork to generate steering vectors that are added to model activations (activation steering), which is an interventional technique (Rung 2). The claim states that this variant 'performs better on unseen steering prompts than every supervised activation steering baseline,' which is a comparative performance claim about causal efficacy under intervention, not a mechanistic claim about unique internal mechanisms. The language 'performs better' indicates a causal effect claim (R2) rather than a mechanistic 'encodes' or 'computes' claim (R3). There is no hedge present. Confidence is high but not maximum due to some ambiguity in whether 'performs better' could be interpreted as a stronger mechanistic claim, but the context suggests it's about intervention outcomes."
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,Empirical evaluation of compute efficiency scaling,2,2,0,4,0,"The claim 'as training data increases HYPERSTEER becomes much more economical than supervised activation steering' is based on empirical results comparing compute efficiency (TFLOPS) between HYPERSTEER and ReFT-r1 across different dataset scales (Figure 3). The method involves measuring performance (loss) relative to compute cost, which is an interventional evaluation of method efficacy under varying conditions. This fits Rung 2 (interventional) because it compares outcomes under different training data interventions. The claim is about relative efficiency ('more economical'), which is a causal/comparative claim about method performance, appropriate for Rung 2. There is no hedge present. Confidence is 4 due to clear empirical backing but some ambiguity in whether 'economical' implies a mechanistic claim (it does not)."
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,Pairwise cosine similarity analysis of steering vectors,1,1,0,4,0,"The claim 'cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance' is based on analyzing pairwise cosine similarities of steering vectors (Figure 6a). This is an observational/correlational method (Rung 1) that measures associations between vectors without intervention. The claim uses language like 'similarity is weakened' and 'not at the cost of steering performance,' which describes patterns and relationships without asserting causal or mechanistic explanations. There are no causal markers (R2) or mechanistic markers (R3). The claim is stated as an empirical observation from the analysis, with no hedging terms present."
2510.01070,2510.01070-01,Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor,abstract,3,Logit lens and sparse autoencoders (SAEs),1,2,1,4,0,"The methods used are logit lens and SAE feature attribution, which are observational/associational (Rung 1) as they involve inspecting activations without intervention. The claim states these techniques 'increase the success rate of the LLM auditor,' implying a causal effect (they improve performance). This is a causal claim (Rung 2) because it asserts that applying these methods leads to a change in an outcome (auditor success). There is no hedge. Confidence is 4 due to minor ambiguity: the claim is about effectiveness, not just correlation, but the methods are R1."
2510.01070,2510.01070-02,secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly,results,1,Sparse autoencoders (SAEs) and logit lens,1,1,0,4,0,"The claim states that secret knowledge 'can be successfully extracted from the model's internal states' using white-box methods like SAEs and logit lens. SAE feature attribution and logit lens are observational methods (Rung 1) that identify correlations between activations and concepts. The claim uses linguistic markers 'extracted from' and 'internal states' which are associational (Rung 1), not causal or mechanistic. It does not claim the model 'encodes' or 'represents' the knowledge (which would be Rung 3), but rather that it can be extracted, which aligns with Rung 1 observational evidence. No intervention is mentioned for this specific claim. The claim is stated as an empirical finding without hedging."
2510.01070,2510.01070-03,Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal,body,1,Logit lens and SAE feature attribution,1,1,0,4,0,"The claim is about the logit lens and SAE feature attribution methods, which are observational/associational (Rung 1). The claim states that these methods 'reveal strong signals corresponding to the female gender' during a refusal, which is a correlational observation (information is present/decodable). The linguistic markers 'reveal' and 'signals corresponding to' indicate Rung 1 associational claims. There is no intervention mentioned, and the claim does not assert causal effect or unique mechanism. No hedging language is present."
2510.01070,2510.01070-04,Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it,results,2,Fine-tuning evaluation via downstream tasks,2,2,0,4,0,"The method used is fine-tuning models on synthetic data and evaluating their performance on downstream tasks requiring application of the secret knowledge (e.g., generating hints, following encrypted instructions, answering gender-dependent questions). This involves interventions: the models are trained to apply knowledge while denying it, and their performance is measured on tasks that require using that knowledge. This is interventional (Rung 2) because it involves creating models with specific knowledge and testing their behavior. The claim states the models 'internalized' and 'are aware of' secret knowledge, which is a causal claim about the effect of fine-tuning on model behavior (they apply knowledge downstream). It does not use mechanistic terms like 'encodes' or 'the circuit' that would imply Rung 3. The claim is about successful training causing the model to possess and use knowledge, which matches Rung 2 causal language ('internalized' in this context means the model has learned to apply it, supported by intervention evidence). No hedge words are present."
2510.01070,2510.01070-05,Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques,body,1,Sparse autoencoders (SAEs) and logit lens,1,3,2,4,0,"The claim states: 'Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques.' The methods explicitly mentioned in the surrounding context are SAEs and logit lens, which are observational/associational methods (Rung 1). The claim uses the phrase 'extract it through mechanistic interpretability techniques,' which implies a mechanistic understanding (Rung 3) because 'extract' suggests uncovering the underlying representation, not just correlational evidence. There is no hedge present. The gap is +2 (R1 to R3), which matches the overclaim pattern 'SAE → represents' from the codebook. Confidence is 4 due to minor ambiguity in whether 'extract' is being used in a strictly R3 sense, but the context suggests a mechanistic claim."
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,Eigendecomposition of bilinear MLP weights,1,1,0,4,0,"The method used is eigendecomposition of weights, which is an observational/associational analysis (Rung 1) as it involves analyzing weight matrices without intervening on the model. The claim states 'reveals interpretable low-rank structure' which is an associational claim about what the decomposition shows, not a causal or mechanistic claim. The language 'reveals' and 'structure' fits Rung 1 linguistic markers. There is no hedge present. Confidence is 4 due to minor ambiguity about whether 'interpretable' implies mechanistic understanding, but the abstract context suggests it's about revealing patterns."
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,Eigendecomposition of bilinear MLP weights,1,1,0,4,0,"The method used is eigendecomposition of the bilinear tensor B, which is a weight-based analysis technique that extracts eigenvectors from model weights without intervention on activations or inputs. This is an observational/associational method (Rung 1) as it involves analyzing static weight structures. The claim states that eigenvectors 'represent' curve segments and 'function as' edge detectors. Following the decision tree for 'represents'/'functions as', since no interventional evidence is provided (the paper shows visualizations and correlations but no causal tests), and the context suggests the authors mean 'are linearly decodable from' or 'correlate with' these patterns, this is a Rung 1 associational claim. There is no hedge present. Confidence is 4 due to minor ambiguity in interpreting 'functions as' but the overall context supports an observational claim."
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,Eigendecomposition of bilinear tensor weights,1,2,1,4,0,"The method used is eigendecomposition of the bilinear tensor weights to extract eigenvectors (Section 3.2). This is an observational/associational analysis of model weights without intervention on the model's computations, placing it at Rung 1. The claim states that adversarial masks constructed from these eigenvectors 'cause misclassification demonstrating causal importance of extracted features.' The phrase 'cause misclassification' and 'causal importance' are linguistic markers for Rung 2 (causal claims), as they imply an intervention (applying masks) changes model behavior. However, the method itself is Rung 1, so there is a +1 gap. The claim is stated as a fact without hedging (e.g., 'may' or 'suggests'), so hedge_flag=0. Confidence is 4 because while the claim is clearly causal, the method is purely analytical/observational of weights, but the paper does construct and apply masks as an intervention, though the method description focuses on decomposition."
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,Eigendecomposition of bilinear MLP weights and SAE feature attribution,1,3,2,4,0,"The claim 'computes not-good and not-bad features via AND-gate-like interactions' uses the functional verb 'computes' and describes a specific circuit mechanism ('AND-gate-like interactions'), which are Rung 3 (mechanistic) linguistic markers. The method used is eigendecomposition of bilinear MLP weights (a form of weight analysis) combined with SAE feature attribution, which are observational/associational methods (Rung 1) as they involve analyzing weights and feature correlations without intervention. There is no interventional evidence (e.g., patching, ablation) provided in the claim's context to support the mechanistic claim. This creates a method-claim gap of +2 (R1 → R3). The claim is stated as a fact without hedging."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,Eigendecomposition of bilinear tensor interaction matrices,2,1,0,4,0,"The method involves decomposing the bilinear tensor (derived from model weights) into eigenvectors and eigenvalues, then using these to approximate SAE output feature activations. This is an interventional method because it constructs approximations from the weight structure and tests their correlation with observed activations, which involves a form of intervention (truncation/approximation) and comparison. The claim states 'well-correlated with low-rank eigenvector approximations' which is an associational claim about correlation between approximations and activations, particularly at large values. It does not assert causality or a unique mechanism. The linguistic marker 'correlated with' is explicitly Rung 1. No hedge words are present."
2406.11779,2406.11779-01,The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit,body,1,SVD analysis of EQKE matrix and qualitative examination of EVOU matrix,1,3,2,4,0,"The claim describes a mechanistic narrative: the model 'outputs the largest logit... by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit.' This uses functional verbs ('attending,' 'copying') and describes how the model works, which is characteristic of Rung 3 (mechanistic/counterfactual) claims. The methods used are SVD (observational, Rung 1) and qualitative examination of matrices (observational, Rung 1), which are correlational. There is no interventional evidence (e.g., patching, ablation) provided for this specific claim in the context. Thus, there is a method-claim gap of +2 (R1 → R3). The claim is stated as fact without hedging."
2406.11779,2406.11779-02,EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component,body,1,Singular-value decomposition (SVD),1,1,0,5,0,"The method used is SVD, which is an observational/associational technique (Rung 1) that identifies linear components without intervention. The claim describes a statistical property (singular value ratio) without asserting causal or mechanistic roles. It uses associational language ('contains', 'larger than') typical of Rung 1 claims. No hedging terms are present."
2406.11779,2406.11779-03,Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning,body,1,Zero ablation,2,2,0,5,0,"The method used is zero ablation (zeroing out components), which is explicitly listed as a Rung 2 (Interventional) method in the codebook. The claim states that ablating EQKP changes model accuracy from 0.9992 to 0.9993, which is a causal effect measurement ('changes model accuracy'). The language 'is unimportant to model functioning' is a causal sufficiency claim (Rung 2) rather than a uniqueness/mechanistic claim (Rung 3), as it's based on the intervention's result showing minimal impact, not claiming it's the unique or underlying mechanism. No hedging terms are present. Confidence is high because the method and claim align clearly with Rung 2 definitions."
2406.11779,2406.11779-04,Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds,abstract,3,Quantitative analysis of proof strategies and unexplained dimensionality metric,2,2,0,4,1,"The claim is from the abstract: 'Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds.' The paper uses multiple proof strategies (cubic, subcubic) with varying degrees of mechanistic interpretation (e.g., using SVD, mean+diff trick, max row-diff trick) and quantifies 'mechanistic understanding' via unexplained dimensionality (Section 5.1). The methods involve interventions on model components (e.g., approximations, bounding error terms) to derive proofs, which fits Rung 2 (interventional). The claim uses 'seem to' (hedge) and discusses causal relationships ('require and provide,' 'leads to') based on empirical correlations from their experiments, which is Rung 2 causal language ('influences,' 'affects') rather than Rung 3 mechanistic uniqueness claims. The hedge flag is 1 due to 'seem to.' Confidence is 4 due to some ambiguity in interpreting 'mechanistic understanding' but clear interventional evidence."
2406.11779,2406.11779-05,Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices,body,1,SVD decomposition and error analysis of matrix approximations,1,2,1,4,0,"The method involves analyzing singular value decomposition (SVD) of matrices like EQKE and examining error terms when making rank-1 approximations, which is an observational/associational method (Rung 1) that establishes correlations between matrix structure and approximation errors. The claim states that compounding structureless errors 'are a key challenge' - this is a causal claim about how these errors affect proof compactness and bound tightness, implying a causal effect (Rung 2). The language 'are a key challenge' suggests these errors causally impact the ability to generate compact proofs, which goes beyond mere correlation. However, it doesn't claim uniqueness or full mechanistic understanding (Rung 3)."
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,Relevance Patching (RelP) and Attribution Patching (AtP) comparison,2,2,0,5,0,"The claim is from the abstract: 'RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs.' The method used is Relevance Patching (RelP), which is an interventional method (Rung 2) as it involves gradient-based attribution with LRP propagation coefficients to approximate activation patching effects. The claim states that RelP 'more accurately approximates activation patching'—this is a comparative claim about causal effect approximation accuracy, not a mechanistic claim about unique circuits or internal computations. It fits Rung 2 linguistic markers: it's about the method's ability to approximate causal effects (activation patching is Rung 2). There is no hedge present ('more accurately' is a direct comparative statement). Confidence is high because the claim is explicit and matches the method's interventional nature."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,Pearson correlation comparison between attribution patching and RelP,1,1,0,5,0,"The claim reports a Pearson correlation coefficient (0.006 vs. 0.956) between two methods (attribution patching and RelP) and activation patching. This is a correlational/observational comparison of method performance, not an intervention on the model internals. The claim uses language like 'achieves' and 'reaches' to describe empirical results, which is associational (R1). No causal or mechanistic language is present. The method used is statistical correlation analysis, which is explicitly listed as a Rung 1 method in the codebook. There is no hedge."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,Relevance Patching (RelP) with LRP propagation coefficients,2,2,0,4,0,"The method RelP is an interventional technique that modifies attribution patching by using LRP propagation coefficients to approximate activation patching effects. It involves interventions (patching) to estimate causal contributions, placing it at Rung 2. The claim states that RelP achieves 'comparable faithfulness to Integrated Gradients in identifying sparse feature circuits'—this is a comparative claim about the method's performance (faithfulness) relative to another interventional method (IG). It does not assert a unique mechanism or counterfactual necessity; it focuses on causal sufficiency and approximation accuracy, which aligns with Rung 2 causal claims. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'identifying circuits' implies mechanistic discovery, but the context emphasizes empirical comparison."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,Relevance Patching (RelP) with sparse autoencoder features,2,2,0,4,0,"The claim is from the paper's Section 5.2, which describes using RelP (a gradient-based interventional method) to identify sparse feature circuits from SAE features. The method involves estimating contributions of features and their interactions via RelP, which approximates activation patching (Rung 2). The claim 'small feature circuits explain most of the model's behavior' uses 'explain' in a causal sufficiency sense (they account for performance), which aligns with Rung 2 causal claims ('is sufficient for', 'accounts for'). It does not assert uniqueness or a complete mechanistic description (Rung 3). No hedge words are present."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,Relevance Patching (RelP),2,2,0,4,0,"The claim 'RelP enables more faithful localization of influential components in large models' is from the abstract. The method used is Relevance Patching (RelP), which is introduced as an enhancement over attribution patching (AtP). RelP builds on Layer-wise Relevance Propagation (LRP) to propagate relevance scores backward through the network. The paper compares RelP to activation patching (ground truth) and attribution patching, evaluating via correlation metrics. This involves interventions (patching activations) to estimate causal effects, placing it at Rung 2 (Interventional). The claim 'faithful localization of influential components' suggests identifying components that causally affect outputs, which aligns with Rung 2 causal claims (e.g., 'mediates', 'influences'). There is no uniqueness or mechanistic language (e.g., 'encodes', 'the circuit'), so it is not Rung 3. The claim is stated as a fact without hedging words like 'may' or 'suggests', so hedge_flag is 0. Confidence is 4 because the method is clearly interventional, and the claim is causal but not mechanistic; minor ambiguity exists in whether 'faithful localization' implies causal sufficiency or just correlation, but context supports causal interpretation."
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,Sparse attention post-training with constrained-loss optimization,2,2,0,4,0,"The method involves an intervention (sparsity regularization under constrained-loss objective) that modifies attention patterns, which is interventional (Rung 2). The claim states that connectivity can be reduced while retaining loss, which is a causal effect claim about the intervention's outcome ('reduces', 'retains'), matching Rung 2 linguistic markers. There is no hedge. Confidence is high but not maximum due to some ambiguity in whether 'retaining loss' is purely observational, but the intervention context supports Rung 2."
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,Activation patching,2,2,0,4,0,"The claim is based on activation patching results shown in Figure 4, which measures the effect of patching/ablating heads on logit differences (causal effect). The method involves intervention (patching/ablation) to measure causal effect, placing it at Rung 2. The claim states that sparse models require fewer heads to 'recover' a certain percentage of the 'clean-model effect', which is a causal sufficiency claim ('requires fewer heads to recover effect') without asserting uniqueness or mechanism. It uses language like 'requires...to recover' which indicates causal sufficiency (R2) rather than mechanistic uniqueness (R3). No hedging words present."
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,Activation patching,2,2,0,4,0,"The claim is based on activation patching experiments (Figure 5, Section 4.2), which is an interventional method (Rung 2). The claim states that sparse models require fewer edges to explain model behavior, which is a causal sufficiency claim ('to reach 90% of the cumulative single-instance effect') matching Rung 2 language ('explain model behavior'). It does not assert uniqueness or mechanism (Rung 3). No hedging words present."
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,Activation patching (circuit discovery),2,2,0,4,0,"The claim is from the abstract and describes a result from activation patching experiments (a Rung 2 interventional method). The claim states that sparsity leads to circuits involving 'far fewer components' and 'up to 100x fewer edges', which is a causal claim about the effect of an intervention (sparsity regularization) on circuit complexity. The language 'involve' and quantitative comparison ('100x fewer') indicates a causal effect, not a mechanistic uniqueness claim. No hedge words are present. Confidence is 4 because while the method is clearly Rung 2, the claim's rung is borderline between R2 (causal effect) and R3 (mechanistic simplification), but the phrasing focuses on the outcome of the intervention rather than describing the underlying mechanism."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,Sparse attention post-training (sparsity regularization under constrained-loss objective),2,2,0,4,0,"The method involves an intervention (post-training with sparsity regularization) that changes the model's attention patterns, establishing a causal effect on information flow. The claim uses causal language ('consolidates information flow') and describes the result of the intervention, which aligns with Rung 2 causal claims. There is no hedge, and the claim is supported by the paper's experimental results showing reduced attention edges. Confidence is 4 due to minor ambiguity in whether 'consolidates' implies a mechanistic narrative, but the context suggests it's about the intervention's effect."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,Sparse Autoencoders (SAEs) with linear probing and steering interventions,2,2,0,4,0,"The claim states that SAEs can reveal latent features (R1 observational) but that high correlation does not guarantee causal control over generation. The paper uses SAE feature attribution (R1) combined with steering interventions (R2) to test causal control. The claim explicitly distinguishes between correlation (R1) and causal control (R2), making it a R2 claim about causal sufficiency/insufficiency. No hedge words present. Confidence is 4 due to minor ambiguity in whether 'reveal' implies R1 or R2, but the contrast with 'causal control' clarifies the claim is about causal evidence."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Ordered SAEs (sparse autoencoders with hierarchical structure),1,2,1,4,0,"The method used is Ordered SAEs, which are a type of sparse autoencoder that imposes hierarchical structure on latent features. According to the codebook, SAE feature attribution is classified as Rung 1 (observational/associational) because it identifies which SAE features activate without intervention on the model. The claim states that Ordered SAEs 'reliably identifies steerable features' – 'steerable' implies causal control through intervention (steering vectors are Rung 2 methods), and 'identifies' suggests the SAE can find features that are causally effective. However, the SAE itself is an observational method; the steering intervention is separate. The claim is about identifying features that are steerable (causally effective), which is a causal claim (Rung 2) because it implies the features have causal effects when used for steering. There is no hedge present. Confidence is 4 due to minor ambiguity: the SAE method is R1, but the claim about 'steerable features' references causal utility, which might be supported by subsequent steering experiments (R2 interventions) in the paper, though the abstract claim itself links the SAE's hierarchical structure to reliable identification of such features."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,SAE feature attribution,1,3,2,4,0,"The method used is SAE feature attribution (observational analysis of which latents activate), which is a Rung 1 method. The claim uses the term 'represent,' which according to the codebook's decision tree for polysemous terms defaults to Rung 3 (mechanistic) unless context indicates a decodability sense. The context does not indicate 'linearly decodable from'; the claim is a mechanistic statement about what the latents do ('collectively represent antibody information'). This is a +2 gap (R1 method → R3 claim), a common overclaim pattern noted in the codebook (SAE → 'represents'). The claim is stated as fact without hedging. Confidence is 4 due to minor ambiguity in whether 'represent' could be interpreted associatively, but the default reading and codebook guidance point to R3."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE feature attribution and linear probing,1,3,2,4,0,"The claim 'top latents encoded contextual information of the preceding residues' uses the term 'encoded' in a mechanistic sense (R3 linguistic marker). The paper uses SAE feature attribution (R1 observational method) and linear probing (R1) to identify latents correlated with gene identity. No interventional evidence (like patching or steering) is provided to support that latents 'encode' contextual information; the evidence is correlational (activation patterns and predictive accuracy). According to the decision tree for 'encodes', since there is no interventional evidence and the context does not clearly indicate 'linearly decodable from', the default mechanistic reading applies, making this a R3 claim. The gap is +2 (R1 method → R3 claim). Confidence is 4 due to some ambiguity in whether 'encoded' might be intended as 'decodable', but the narrative suggests a mechanistic interpretation."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering vectors,2,2,0,5,0,"The method used is steering vectors (adding a decoder vector direction to hidden states), which is explicitly listed as a Rung 2 (Interventional) method in the codebook. The claim states that intervening by steering on latent 12 changes the output (increases IGHJ4 proportion), which is a causal effect claim ('increased IGHJ4 proportion'). This matches Rung 2 linguistic markers ('intervening on X changes Y'). There is no uniqueness or mechanistic language (like 'encodes' or 'the circuit'), and the claim is about the result of an intervention, not the underlying mechanism. No hedging is present."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,Sparse autoencoder feature extraction and steering (activation clamping),2,2,0,5,0,"The claim states they reproduced 'basic feature extraction and steering capabilities' from Anthropic's work. The methods used are SAE feature extraction (Rung 1 observational) and feature steering via activation clamping (Rung 2 interventional). The steering intervention (clamping features to influence outputs) is the key method supporting the claim about steering capabilities, making method_rung = 2. The claim is about reproducing capabilities to extract features and steer model behavior - this is a causal claim about what interventions can do (steering changes outputs), matching Rung 2 linguistic markers like 'can be used to steer' from the codebook. No hedge present. High confidence as this is a clear empirical claim about intervention results."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,Feature steering experiments (clamping SAE features),2,2,0,5,0,"The claim is about the causal effect of steering interventions (clamping SAE features) on model output, specifically that steering effects are fragile and sensitive to parameters like layer, magnitude, and context. This is a causal claim about the effect of an intervention (steering) on model behavior, fitting Rung 2 linguistic markers like 'sensitivity to' (indicating causal influence). The method used is feature steering via clamping, which is an interventional method (Rung 2). The claim does not assert a unique mechanism or representational property (Rung 3), nor is it purely correlational (Rung 1). It is stated as an empirical finding without hedging."
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,Sparse Autoencoder (SAE) feature attribution and activation analysis,1,1,0,4,0,"The claim 'We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another' is from the paper's abstract. The method used is SAE feature attribution and activation analysis (observing which SAE features activate on given inputs), which is a Rung 1 (Observational/Associational) method per the codebook (SAE feature attribution listed under Rung 1). The claim describes observing activation behavior and demonstrating difficulty in distinguishing features, which are associational statements about feature correlations and patterns. The linguistic markers 'observe' and 'demonstrate' indicate reporting empirical observations without causal or mechanistic language. There is no intervention mentioned, and the claim does not assert causal effects or unique mechanisms. The claim is stated as a fact (no hedge). Confidence is 4 because while the method and claim rungs are clear, the phrase 'non-standard activation behavior' has some ambiguity, but the overall context aligns with Rung 1 observational claims."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,Sparse autoencoder feature extraction and steering evaluation,2,2,0,4,0,"The claim is from a paper that evaluates SAE-based feature extraction and steering (interventional methods: Rung 2). The claim states that current methods 'fall short of systematic reliability required for safety-critical applications'—this is an empirical evaluation of the methods' reliability/causal effectiveness, not a mechanistic claim about what features encode or compute. It matches Rung 2 linguistic markers: evaluates sufficiency/reliability of interventions for practical outcomes. No hedge present. Confidence is 4 due to minor ambiguity about whether 'reliability' implies causal sufficiency (R2) or mechanistic understanding (R3), but context suggests it's about practical steering reliability."
2507.08802,2507.08802-01,any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative,abstract,3,"Theoretical proof under assumptions (input-injectivity, output-surjectivity, matchable partial-orderings, DNN solves task)",3,3,0,4,0,"The claim 'any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative' is a theoretical claim about the existence of a perfect alignment map under certain assumptions (Assumptions 1-5 in the paper). The method used is a theoretical proof (Theorem 1) that establishes a counterfactual scenario: under mild conditions, any algorithm can be perfectly aligned with any DNN via arbitrarily complex alignment maps, making causal abstraction vacuous. This is a Rung 3 method because it proves a general existence result about what would happen (counterfactual) with unrestricted alignment maps. The claim is Rung 3 because it makes a strong mechanistic claim about the fundamental limitations of causal abstraction as a framework ('trivial and uninformative'), implying a unique, necessary property of the abstraction method itself. There is no hedging language; it's stated as a definitive conclusion. Confidence is 4 because while the claim is clearly Rung 3 and the method is theoretical proof (Rung 3), there is minor ambiguity in whether 'trivial and uninformative' is an empirical claim about the method's utility (Rung 2) or a mechanistic claim about its inherent properties (Rung 3). The context (title: 'Is Causal Abstraction Enough for Mechanistic Interpretability?') suggests it's a mechanistic claim about the framework's adequacy."
2507.08802,2507.08802-02,it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task,abstract,3,Distributed alignment search (DAS) with non-linear alignment maps,2,2,0,4,0,"The claim is from the abstract and describes an empirical finding from the paper's experiments. The method used is DAS, which involves training alignment maps (including non-linear ones) and performing interchange interventions to measure alignment accuracy (IIA). This is an interventional method (Rung 2) because it involves swapping representations between inputs and measuring causal effects on outputs. The claim states that models can be 'perfectly mapped' to algorithms even when models cannot solve the task, which is a claim about causal sufficiency of the intervention (the map enables the model to produce algorithm-consistent behavior under interventions). This matches Rung 2 linguistic markers like 'can be mapped' (implying sufficiency) and is about the result of an intervention. The claim does not assert a unique mechanism or that the algorithm is 'the' mechanism, so it is not Rung 3. There is no hedge present."
2507.08802,2507.08802-03,randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task,abstract,3,Distributed alignment search (DAS) with non-linear alignment maps,2,2,0,5,0,"The method used is distributed alignment search (DAS), which involves training an alignment map (ϕ) to map neural network activations to algorithm nodes and performing interchange interventions. This is an interventional method (Rung 2) because it involves swapping activations based on counterfactual inputs and measuring the effect on output accuracy (IIA). The claim states that alignment maps achieve 100% interchange-intervention accuracy (IIA) on the IOI task with randomly initialized models. This is a causal claim about the effect of the intervention (achieving perfect alignment accuracy), not a mechanistic claim about unique underlying circuits. The language 'reach 100% interchange-intervention accuracy' describes an empirical outcome of the intervention, fitting Rung 2 linguistic markers ('intervening on X changes Y'). There is no hedge present ('reach' is a factual statement). Confidence is high because the method and claim are clearly described in the paper's experiments section."
2507.08802,2507.08802-04,causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information,abstract,3,Theoretical analysis and empirical demonstration of causal abstraction with arbitrary alignment maps,2,3,1,4,0,"The paper uses a combination of theoretical proof (Theorem 1) and empirical experiments (DAS with non-linear alignment maps) to show that causal abstraction becomes vacuous without constraints on alignment maps. The method involves interventions (distributed alignment search) and theoretical analysis of causal abstraction frameworks, which fits Rung 2 (Interventional). The claim 'causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information' makes a strong statement about the insufficiency of causal abstraction for mechanistic interpretability, implying a mechanistic limitation (Rung 3). The claim is stated definitively without hedging ('is not enough', 'becomes vacuous'). Confidence is high but not maximal due to some ambiguity in classifying the theoretical component's rung, though the empirical DAS experiments are clearly Rung 2."
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,Subspace activation patching (DAS),2,2,0,4,0,"The claim is about the effect of subspace activation patching, which is an interventional method (Rung 2). The claim states that the intervention can make the model's output behave as if a feature value changed, but this may be achieved via a dormant pathway. This is a causal claim about the mechanism of the intervention ('may be achieved by activating'), not a mechanistic uniqueness claim (Rung 3). It fits Rung 2 linguistic markers: 'intervening on X changes Y' and describes a causal effect under specific interventions. No hedging terms like 'may' are present in the quoted segment (the 'may' refers to possibility of alternative mechanism, not hedging the claim itself). The method is clearly subspace activation patching as described in the paper."
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,Subspace activation patching,2,3,1,4,0,"The claim 'patching of subspaces can lead to an illusory sense of interpretability' is from the abstract and discusses a limitation of subspace activation patching, which is an interventional method (Rung 2). The claim asserts a mechanistic insight about interpretability illusions, implying a counterfactual understanding of how models can mislead researchers, which is a Rung 3 claim. The language 'illusory sense of interpretability' suggests a unique mechanism or underlying truth about interpretability methods, fitting Rung 3 markers like 'underlies' or 'mechanistic narrative'. There is no hedging present. Confidence is 4 due to some ambiguity in whether the claim is about a general phenomenon or a specific finding, but the abstract context supports a Rung 3 interpretation."
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,Distilled mathematical example and empirical demonstration in two real-world domains,2,2,0,4,0,"The claim states 'we demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice.' The method involves constructing a mathematical example (theoretical) and empirical demonstrations in real-world tasks (IOI and factual recall) using interventions like subspace activation patching and DAS. This is interventional (Rung 2) as it involves patching interventions to show causal effects. The claim is about demonstrating a phenomenon (the illusion) which is a causal effect under specific interventions, matching Rung 2 linguistic markers like 'demonstrate' (implies causal effect). There is no hedge ('demonstrate' is assertive). Confidence is 4 due to some ambiguity in whether the demonstration includes counterfactual elements, but the overall context suggests interventional evidence."
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,Causal tracing (activation patching variant),2,2,0,4,0,"The claim 'there is an inconsistency between fact editing performance and fact localization' is an empirical claim about a discrepancy observed between two methods: fact editing (likely ROME/MEMIT, Rung 2 interventional) and fact localization (likely causal tracing/activation patching, Rung 2 interventional). The paper uses activation patching (specifically subspace activation patching) as a method to demonstrate this inconsistency, which is an interventional technique (Rung 2). The claim itself is about an observed inconsistency between two interventional techniques, making it a Rung 2 claim (causal effect under intervention). There is no hedge present; it's stated as an established finding. Confidence is 4 because while the method and claim are clearly interventional, the specific phrasing 'inconsistency' could be interpreted as a comparative empirical observation rather than a direct causal claim, but the context supports Rung 2."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,Activation patching,2,3,1,4,0,"The claim describes activation patching as a 'mechanistic interpretability technique.' According to the codebook, 'mechanistic' is a linguistic marker for Rung 3 (mechanistic/counterfactual claims). The method used is activation patching, which is classified as Rung 2 (interventional). The claim is an implicit assertion that activation patching reveals mechanisms, which is a Rung 3 claim. There is no hedge present. Confidence is 4 because while the claim is clearly Rung 3, the paper is a tutorial/advice paper rather than presenting new empirical results, but the classification is still clear."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching (general discussion),2,2,0,4,1,"The claim is from a paper about activation patching, which is an interventional method (Rung 2). The claim states that varying hyperparameters (like choice of corrupted prompts, patching direction, metrics) could lead to disparate interpretability results. This is a causal claim about how methodological choices affect outcomes, using language like 'could lead to' which implies a causal relationship but is hedged ('could'). It does not make mechanistic claims about unique circuits or representations (Rung 3), nor is it purely associational (Rung 1). The hedge flag is 1 due to 'could'."
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,Activation patching (causal tracing/interchange intervention),2,2,0,4,0,"The paper's primary method is activation patching (also called causal tracing or interchange intervention), which involves interventions on model activations to measure causal effects. This is a Rung 2 (Interventional) method according to the codebook. The claim 'systematically examine the impact of methodological details in activation patching' is about evaluating variations of this interventional technique itself, not making mechanistic claims about model internals. It fits Rung 2 linguistic markers like 'examine impact' and focuses on methodological evaluation rather than claiming unique mechanisms. No hedging language is present. Confidence is 4 due to minor ambiguity about whether methodological evaluation claims have a distinct rung, but the codebook treats claims about methods based on the evidence level they provide."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching (causal tracing/interchange intervention),2,2,0,4,0,"The claim 'varying these hyperparameters could lead to disparate interpretability results' is an empirical claim about the causal effect of methodological choices (corruption methods, metrics) on localization outcomes in activation patching. The method used is activation patching (causal tracing/interchange intervention), which is an interventional technique (Rung 2). The claim asserts that changing hyperparameters causally influences interpretability results ('could lead to'), which is a causal claim about the effect of interventions on outcomes, matching Rung 2 linguistic markers like 'influences' or 'changes'. There is no hedge ('could' indicates possibility but not uncertainty about the claim itself). Confidence is 4 due to minor ambiguity in whether 'disparate interpretability results' refers strictly to causal effects or includes observational correlations, but the context supports a causal interpretation."
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,Activation patching,2,3,1,4,0,"The method used is activation patching (systematic activation patching across all 12 layers), which is an interventional technique (Rung 2). The claim uses the verb 'act as' (functional attribution) and 'encoding' (storage/representation). Following the decision tree for 'encodes': the paper provides interventional evidence (activation patching), but the claim is about the underlying mechanism ('act as lexical sentiment detectors encoding...') rather than just the intervention result. This makes it a Rung 3 mechanistic claim. There is no hedge ('act as' is definitive). The gap is +1 (R2 method → R3 claim), consistent with the overclaim pattern 'Patching → functional attribution' from the codebook."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,Activation patching,2,2,0,4,0,"The claim is from the abstract and states that contextual phenomena 'are integrated primarily in late layers (8-11).' The paper's primary method is systematic activation patching across all 12 layers, which is a Rung 2 (interventional) method. The claim uses the verb 'integrated,' which, following the codebook's decision tree for polysemous terms, is not explicitly listed but implies a causal or functional role. However, the claim is about where integration primarily occurs based on the intervention (patching) results, not a uniqueness or mechanistic claim about how it works. It matches Rung 2 linguistic markers like 'mediates' or 'influences' in spirit, as it describes a causal effect location from intervention. There is no hedge ('primarily' indicates a comparative finding, not uncertainty). Confidence is 4 due to minor ambiguity in 'integrated' but clear method-match."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,Activation patching,2,2,0,4,0,"The claim 'GPT-2's sentiment computation differs from the predicted hierarchical pattern' is from the abstract. The paper's primary method is systematic activation patching across all 12 layers, which is an interventional technique (Rung 2). The claim states that the computation 'differs from the predicted hierarchical pattern,' which is a causal claim about how the model's processing works based on intervention results. It uses 'differs from' to indicate a causal effect of the model's architecture on processing, aligning with Rung 2 linguistic markers like causal effects from interventions. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'differs' implies a mechanistic claim, but context supports it as causal."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,Causal tracing (activation patching) with Gaussian noise perturbations,2,2,0,4,0,"The method used is causal tracing (activation patching) with controlled Gaussian noise perturbations on input images, which involves interventions (restoring specific activations) to measure causal effects. This is a Rung 2 interventional method. The claim states that MHSAs 'play a critical role in aggregating cross-modal information,' which uses the phrase 'play a critical role in'—this is interpreted as a causal sufficiency claim (they are important for the aggregation function), aligning with Rung 2 linguistic markers like 'influences' or 'has causal effect on.' It does not use definitive uniqueness language (e.g., 'THE mechanism') or strong mechanistic verbs like 'encodes' or 'computes' that would push it to Rung 3. The claim is stated as an established fact without hedging. Confidence is 4 due to minor ambiguity in whether 'play a critical role' implies causal sufficiency (R2) or a stronger mechanistic claim (R3), but the context of causal tracing supports R2."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,Intermediate Representation Injection (IRI) - inference-time intervention on activations,2,2,0,4,0,"The method IRI involves intervening on internal activations (MHSA and MLP outputs) by injecting stored representations into later layers during inference, which is an interventional technique (Rung 2). The claim states it 'reinforces visual object information flow,' which is a causal effect claim (reinforcing/strengthening flow via intervention) without uniqueness or mechanistic language like 'encodes' or 'the mechanism.' It aligns with Rung 2 causal sufficiency claims (intervening changes behavior). No hedge present. Confidence is 4 due to minor ambiguity in 'reinforces' but clear interventional context."
2601.05679,2601.05679-01,many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens,abstract,3,Causal token injection (activation patching variant),2,2,0,5,0,"The claim is from the abstract: 'many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens'. The method used is causal token injection, which involves inserting tokens into non-reasoning text and measuring activation changes. This is an intervention on the model (replacing/adding tokens) and measures causal effect on feature activation, fitting Rung 2 (Interventional). The claim describes a causal effect: injecting tokens changes activation. It uses language like 'sensitive to token-level interventions' and 'activating after injecting', which are causal markers (intervening on X changes Y). There is no uniqueness or mechanistic language; it's about sufficiency of tokens to elicit activation. No hedge words present. Confidence is high because the method and claim are clearly described and align with Rung 2 definitions."
2601.05679,2601.05679-02,LLM-guided falsification produces targeted non-reasoning inputs that trigger activation,abstract,3,LLM-guided falsification (counterexample construction),2,2,0,4,0,"The method 'LLM-guided falsification' involves constructing targeted non-reasoning inputs (interventions) to test whether SAE features activate on them, which is an interventional method (Rung 2). The claim states that this method 'produces targeted non-reasoning inputs that trigger activation'—this describes a causal effect (the intervention changes activation), which is a Rung 2 claim (causal effect). There is no hedge. The claim does not assert uniqueness or mechanism (Rung 3)."
2601.05679,2601.05679-03,sparse decompositions can favor low-dimensional correlates that co-occur with reasoning,abstract,3,SAE feature attribution + causal token injection + LLM-guided falsification,2,2,0,4,0,"The claim 'sparse decompositions can favor low-dimensional correlates that co-occur with reasoning' is from the abstract. The paper uses SAE feature attribution (Rung 1 observational method) combined with causal token injection (intervention on model inputs) and LLM-guided falsification (testing counterexamples). The primary interventional method is causal token injection, which involves inserting tokens into non-reasoning text to test if lexical cues alone elicit activation - this is an intervention on the model's input to measure causal effects on feature activation, fitting Rung 2. The claim uses language 'can favor' which is appropriately causal ('can produce'/'enables') and doesn't make stronger mechanistic claims about encoding or representation. It's about what sparse decompositions 'can favor' - a causal sufficiency claim about methodological bias, not a uniqueness or mechanistic claim. No hedge words present. Confidence is 4 due to some ambiguity in method classification but clear claim classification."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,Steering vectors (additive perturbation to residual stream) + logit-lens projection,2,2,0,4,0,"The claim 'the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token' is from the paper's results section. The method used is steering vectors (trained with RL objective) added to the residual stream, which is an interventional method (Rung 2). The claim describes a causal effect: the vector 'acts like' a bias that changes token probabilities, specifically boosting certain tokens at the first position. This is a causal claim about the vector's effect, not a mechanistic claim about how the model inherently represents or computes. The language 'acts like' indicates a functional analogy based on observed intervention effects, not a uniqueness or mechanistic claim. No hedge words are present. Confidence is 4 because while the method and claim align clearly, there is minor ambiguity in 'acts like' but it's clearly describing the intervention's effect."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,Steering vectors (activation engineering) with path patching/ablation analysis,2,2,0,4,0,"The claim 'the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words' is from a section analyzing penultimate-layer steering vectors using path patching (steering specific projections) and ablation (Skip-Attn, Skip-Layer). The methods involve interventions (patching vectors into specific components) to measure causal effects on performance, which is Rung 2 (Interventional). The claim uses language like 'operates through' and 'preferentially up-weighting' which describes a causal mechanism (how the vector affects the model's behavior via specific components). This is a causal claim about the vector's effect, not a uniqueness or full mechanistic explanation claim (Rung 3). There is no hedge present. Confidence is 4 because the claim is clearly causal but there is minor ambiguity about whether 'operates through' implies a full mechanistic explanation; however, the context suggests it's describing the causal pathway identified via interventions."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,Steering vectors (trained with RL objective) and transfer experiments,2,2,0,4,0,"The paper uses trained steering vectors (an interventional method) and tests their transfer to other models by inserting them and measuring performance changes. The claim 'steering vectors transfer to other models' is a causal claim about the effect of the intervention (steering vector insertion) on model behavior across different models. It does not make a uniqueness or mechanistic claim about how the vectors work internally, nor does it use Rung 3 markers like 'encodes' or 'the mechanism'. The claim is about the causal effect of the intervention (transferability), which aligns with Rung 2 causal claims like 'intervening on X changes Y'. The paper provides empirical results showing performance changes when vectors are transferred, supporting a causal effect claim. No hedging language is present in the claim as stated."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,Contrastive Activation Addition (CAA) steering vector intervention,2,2,0,5,0,"The method used is Contrastive Activation Addition (CAA), a steering vector method where a learned bias is added to activations during inference. This is an interventional method (Rung 2) as it involves modifying activations to observe changes in output. The claim states that prompt types 'produce a net positive steering effect' and 'exhibit high variance across samples', which describes the causal effect of the intervention (steering changes model behavior) and its variability. This matches Rung 2 linguistic markers for causal effects ('produces', 'effect'). No uniqueness or mechanistic language is used, and no hedging terms are present."
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,Correlation analysis between cosine similarity of activation differences and steering effect size,1,1,0,5,0,"The claim states that higher cosine similarity 'predicts' more effective steering. The word 'predicts' is a Rung 1 linguistic marker for correlational/associational claims. The method used is observational: they compute cosine similarities between activation differences (from steering vector training) and correlate this with steering effect size (an empirical outcome). No intervention on the model's internal mechanisms is described; it's a statistical association between two measured quantities. The claim does not use causal or mechanistic language (e.g., 'causally affects', 'encodes', 'controls'), and there is no hedge present. This is a clear Rung 1 claim supported by Rung 1 methods."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Contrastive Activation Addition (CAA) steering vector analysis,2,2,0,4,0,"The paper uses CAA (steering vectors), an interventional method (Rung 2) where a learned bias is added to activations at inference time to measure causal effects on model behavior. The claim 'vector steering is unreliable when the target behavior is not represented by a coherent direction' is a causal claim about when steering fails, based on observed correlations between directional agreement/separability and steering effectiveness. It uses terms like 'unreliable' and 'represented by a coherent direction' which describe causal sufficiency/effectiveness of the intervention, fitting Rung 2 markers ('predicts', 'is represented by'). It does not claim unique mechanisms (Rung 3) like 'encodes' or 'the circuit'. No hedge present."
2301.04709,2301.04709-01,Causal abstraction provides a theoretical foundation for mechanistic interpretability,abstract,3,Theoretical framework development,3,3,0,5,0,"The claim 'Causal abstraction provides a theoretical foundation for mechanistic interpretability' is a theoretical claim about establishing a framework for understanding mechanisms. The paper develops a formal theory of causal abstraction, which involves defining exact transformations, intervention algebras, and interchange interventions to establish counterfactual reasoning about model internals. This aligns with Rung 3 methods (counterfactual/mechanistic) as it provides a theoretical foundation for analyzing unique mechanisms. The claim uses definitive language ('provides a theoretical foundation') without hedging, indicating a strong mechanistic claim. The method is theoretical development, not empirical intervention, but the codebook's method classification focuses on empirical methods; however, the paper's overall approach is to formalize methods like interchange interventions (Rung 2) and abstraction (Rung 3). Given the claim is about providing a theoretical foundation, it inherently involves counterfactual reasoning about mechanisms, justifying Rung 3 for both method and claim."
2301.04709,2301.04709-02,generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation,abstract,3,Theoretical framework extension,1,1,0,4,0,"The claim 'generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation' is from the abstract of a theoretical paper that extends a mathematical framework. The paper does not involve empirical methods like interventions on actual models; it is a theoretical contribution proposing a generalization of existing causal abstraction theory. According to the codebook, theoretical contributions without empirical intervention are observational/associational (Rung 1). The claim describes a theoretical extension ('generalizing'), which is an associational claim about the framework's scope, not a causal or mechanistic claim about model internals. There is no hedge present. Confidence is 4 because while the claim is clearly theoretical, the codebook focuses on empirical claims about model internals, and this is a meta-level claim about the paper's contribution."
2301.04709,2301.04709-03,unifying a variety of mechanistic interpretability methods in the common language of causal abstraction,abstract,3,Theoretical framework development,1,1,0,4,0,"The claim is from the abstract of a theoretical paper that proposes causal abstraction as a unifying framework for mechanistic interpretability methods. The paper does not present empirical results from applying these methods to models; it develops a theoretical framework and categorizes existing methods within it. Therefore, the method used is theoretical framework development, which is observational/associational (Rung 1) as it involves categorizing and relating methods without intervention. The claim 'unifying a variety of mechanistic interpretability methods' is an associational claim about providing a common language, not a causal or mechanistic claim about model internals. It describes a conceptual synthesis, which aligns with Rung 1 linguistic markers like 'unifying' (associational). There is no hedge present. Confidence is 4 due to minor ambiguity in classifying theoretical work, but the claim is clearly about categorization rather than empirical demonstration of causal effects or mechanisms."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,Library implementation and demonstration,2,2,0,4,0,"The claim is about the library's capabilities, not about model internals. The method used is implementing and demonstrating intervention techniques (like activation patching, interchange interventions) which are Rung 2 interventional methods. The claim 'supports customizable interventions' matches the Rung 2 level of describing what the tool can do (enable causal interventions). There is no overclaim to Rung 3 (mechanistic) as it's not claiming to uncover unique mechanisms. No hedging present."
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,Library implementation supporting multiple intervention types,2,2,0,4,0,"The claim is about pyvene providing a framework for performing interventions, which are interventional methods (Rung 2). The claim uses 'performing interventions' which aligns with Rung 2 causal language ('intervening on X changes Y'). No uniqueness or mechanistic claims are made (not Rung 3), and it's not purely observational (not Rung 1). The method rung is 2 because pyvene supports interventional techniques like activation patching, DAS, etc. The claim rung is 2 because it's about enabling interventions (causal methods). There's no hedge. Confidence is 4 due to minor ambiguity about whether 'unified and extensible framework' implies something beyond Rung 2, but the core claim is about performing interventions."
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,causal abstraction and knowledge localization,2,2,0,4,0,"The claim states the library is illustrated via 'interpretability analyses using causal abstraction and knowledge localization.' Causal abstraction (e.g., DAS) is an interventional method (Rung 2), and knowledge localization (e.g., as in ROME) typically involves causal tracing/interventions (Rung 2). The claim is about demonstrating the library's power through these methods, which aligns with Rung 2 causal claims ('illustrate...via interpretability analyses'). There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'knowledge localization' might imply Rung 3 storage claims, but the context suggests it's about performing such analyses with the library."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,Sparse Autoencoder (SAE) feature attribution,1,3,2,4,0,"The method used is SAE feature attribution, which is an observational/associational method (Rung 1) that identifies which SAE features activate without intervention. The claim uses the verb 'decompose' and states that hidden states are transformed into a 'disentangled feature space,' implying a mechanistic transformation or representation. According to the codebook's decision tree for 'encodes/represents/stores,' since no interventional evidence is provided for this claim in the abstract, and the context does not clearly indicate a decodability sense, the default mechanistic reading applies, making it a Rung 3 claim. This is an overclaim pattern: SAE attribution (R1) → 'decompose' implying mechanistic representation (R3), gap +2. No hedging language is present."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE-Steering (two-stage pipeline: logit estimation + intervention evaluation),2,2,0,4,0,"The method SAE-Steering involves: Stage 1 uses logit estimation (observational, R1) to recall features that amplify strategy keywords; Stage 2 evaluates control effectiveness via intervention experiments (steering vectors applied to model activations, measuring effect on output). This intervention (steering) establishes causal effects, making the method Rung 2. The claim 'identifies strategy-specific features' is about selecting features based on their capacity to steer target strategy generation, which is a causal sufficiency claim (features can produce the strategy). It uses language like 'identifies' and 'strategy-specific' in context of control effectiveness, aligning with Rung 2 causal claims (e.g., 'can produce', 'enables'). No uniqueness or mechanistic language (e.g., 'encodes', 'the circuit') is present, so claim is R2. No hedge present."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,SAE-Steering (activation patching/intervention),2,2,0,4,0,"The claim 'SAE-Steering outperforms existing methods by over 15% in control effectiveness' is an empirical performance comparison based on intervention experiments. The method SAE-Steering involves modifying hidden states via identified SAE features (Eq. 5), which is an interventional technique (Rung 2). The claim describes a causal effect ('outperforms') on model behavior measured via control effectiveness (Eq. 7), which is a Rung 2 causal sufficiency claim. No uniqueness or mechanistic language (e.g., 'encodes', 'the circuit') is present. The claim is stated as a fact without hedging."
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,SAE-Steering (activation-based steering with SAE features),2,2,0,4,0,"The method used is SAE-Steering, which involves intervening on hidden states using identified SAE features as control vectors to steer reasoning strategies. This is an interventional method (Rung 2) as it modifies activations to measure causal effects on output. The claim 'controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones' uses language like 'redirect' which implies causal influence (changing output via intervention), aligning with Rung 2 causal claims. It does not assert uniqueness or mechanistic details (Rung 3), and is supported by experimental results in the paper showing accuracy improvements via steering. No hedging is present. Confidence is 4 due to minor ambiguity in whether 'redirect' might imply stronger mechanistic control, but context confirms it's based on interventional evidence."
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,"Observational/Associational (Linear probing, activation logging, SAE feature attribution, PCA/SVD, correlation analysis)",1,3,2,4,0,"The claim 'neural networks represent meaningful concepts as directions in their representation spaces' uses the term 'represent,' which is a Rung 3 linguistic marker (mechanistic claim). According to the decision tree for 'encodes/represents/stores,' since the paper does not provide interventional evidence for this specific claim in the abstract (it's a general statement about neural networks), and the context does not clearly indicate it means 'is linearly decodable from,' the default mechanistic reading applies, making it R3. The method used to support such a claim in the broader literature (e.g., linear probing, SAE attribution) is typically R1 (observational), leading to a method rung of 1. There is no hedge in the claim. Confidence is 4 due to some ambiguity in interpreting 'represent' without full paper context, but the abstract's phrasing aligns with mechanistic claims."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,Theoretical analysis and mathematical framework development,1,1,0,4,0,"The claim 'we develop the first unified theoretical framework considering SDL as one optimization problem' is from the abstract. The method used is theoretical analysis and mathematical framework development, which involves formalizing SDL methods as a single optimization problem. This is an observational/associational method (Rung 1) as it involves creating a theoretical model without intervening on actual models. The claim is about developing a framework, which is a Rung 1 claim as it describes a theoretical contribution without making causal or mechanistic claims about model internals. There is no hedge present. Confidence is 4 due to minor ambiguity in whether theoretical analysis fits neatly into the codebook's method categories, but it clearly aligns with Rung 1."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,Theoretical analysis of optimization landscape,3,3,0,4,0,"The claim is from the abstract: 'we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons'. The method used is theoretical analysis of the SDL optimization landscape, which involves proving theorems about global minima, spurious partial minima, and feature absorption patterns (Theorems 3.3, 3.4, 3.7, 3.10). This constitutes a counterfactual/mechanistic method (Rung 3) as it establishes what would happen under certain conditions and demonstrates unique mechanisms (e.g., hierarchical concept structures naturally induce feature absorption). The claim rung is 3 because it uses mechanistic language ('theoretical explanations for phenomena') and makes a uniqueness claim ('novel'). There is no hedging language ('provide' is definitive). Confidence is 4 because while the method is clearly Rung 3 (theoretical proofs about mechanisms), the claim phrasing 'theoretical explanations' could be interpreted as Rung 2 causal claims, but the context of proving theorems about optimization landscapes supports Rung 3."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,Sparse autoencoder feature attribution,1,3,2,4,0,"The claim 'neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions' appears in the abstract. The paper's primary method is using sparse autoencoders (SAEs) to extract features and measure superposition via entropy of activations, which is an observational/associational method (Rung 1). The claim uses the term 'encoding' in a mechanistic sense (describing how networks represent features) without interventional evidence for that encoding mechanism. Following the decision tree for 'encodes/represents/stores': there is no interventional evidence provided for this claim in the abstract (the paper later uses interventions for other claims), and context does not indicate 'linearly decodable from', so default mechanistic reading applies → Rung 3. The claim is stated as fact without hedging. Confidence is 4 due to some ambiguity about whether 'encoding' in the abstract refers to the general phenomenon (which might be supported by toy model interventions elsewhere) versus the specific measurement method."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,Information-theoretic framework applied to sparse autoencoder activations,1,1,0,4,0,"The claim is from the abstract and describes a measurement framework. The method involves applying Shannon entropy to SAE activations to compute effective degrees of freedom, which is an observational/associational analysis (Rung 1) as it quantifies statistical properties without intervention. The claim uses 'measuring,' which is a Rung 1 marker for observational quantification. There is no intervention or causal language, and no uniqueness or mechanistic claim. The claim is stated as a fact without hedging."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,Correlation analysis,1,1,0,5,0,"The claim 'our metric strongly correlates with ground truth in toy models' uses the phrase 'correlates with', which is a clear Rung 1 linguistic marker for associational claims. The method implied is correlation analysis between the proposed metric and ground truth in toy models, which is an observational method establishing correlation only, placing it at Rung 1. There is no intervention or causal language, and no uniqueness or mechanistic claims. The claim is stated as a fact without hedging words like 'may' or 'suggests', so hedge_flag is 0. Confidence is very high (5) because the claim text directly matches Rung 1 markers and the method is straightforwardly correlational."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,Sparse autoencoder feature attribution with entropy-based measurement,1,2,1,4,0,"The method used is SAE feature attribution combined with entropy calculation to measure 'effective features' (F = e^H(p)). This is an observational/associational method (Rung 1) that establishes correlations between SAE activations and adversarial training conditions. The claim states 'adversarial training can increase effective features while improving robustness' - this is a causal claim about the effect of an intervention (adversarial training) on feature count and robustness, which matches Rung 2 linguistic markers ('can increase', 'improving'). The claim does not make uniqueness or mechanistic claims about how this works internally, so it's not Rung 3. There's no hedging language in the claim text."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,Sparse autoencoder (SAE) feature attribution with group equivariance adaptation,1,2,1,4,0,"The method involves training SAEs with group symmetries (equivariant SAEs) and evaluating via probing tasks, which is primarily observational (Rung 1) as it uses SAE feature attribution and linear probing. The claim states that incorporating symmetries 'yields features more useful in downstream tasks,' which implies a causal effect (the incorporation leads to improved utility). This is a causal sufficiency claim (Rung 2) because it suggests the intervention (adding symmetries) produces better features. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'yields' is strictly causal, but the context of comparing methods supports a Rung 2 interpretation."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,Linear transformation matrix optimization (M) to fit activation transformations,2,2,0,4,0,"The method involves optimizing a matrix M to minimize the difference between transformed activations and predictions (Equation 3: LM := Ex∈X,p=1,...,|G| ∥ψ(gpx) −Mpψ(x)∥22). This is an interventional approach where M is learned to map canonical reconstructions to transformed activations, establishing a causal effect of the transformation matrix on explaining activation changes. The claim 'a single matrix can explain how their activations transform' uses 'explain' in a causal sufficiency sense (Rung 2 marker: 'can produce' / 'explains'), not a unique mechanistic sense. The paper shows M explains >98% variance (Result 1), which is a causal effect under the intervention of applying M. No uniqueness or counterfactual necessity is claimed here, so claim_rung is 2. Hedge_flag is 0 as it's stated as a fact. Confidence is 4 due to minor ambiguity in 'explain' but context supports causal interpretation."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,Sparse autoencoder feature attribution with probing evaluation,1,1,0,5,0,"The claim is from the abstract and describes SAEs discovering features that lead to superior probing performance. The method involves training SAEs (sparse autoencoders) and evaluating them via linear probing on downstream tasks, which is an observational/correlational method (Rung 1). The claim uses language like 'discover features that lead to superior probing performance,' which is associational (features lead to better performance in probing tasks). There is no intervention on the model or causal language, and no uniqueness or mechanistic claims. The claim is stated as an empirical finding without hedging."
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,4,0,"The method used is Contrastive Activation Addition (CAA), a steering vector technique that involves adding a learned bias to model activations during inference. This is an interventional method (Rung 2) because it modifies activations to observe changes in output. The claim 'steering effectively controls the targeted summary properties' uses the verb 'controls' in a causal sufficiency sense (intervening on X changes Y), which aligns with Rung 2 linguistic markers ('intervening on X changes Y'). The claim does not assert uniqueness or mechanistic explanation (Rung 3), and it is not purely associational (Rung 1). There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'controls' might be interpreted as stronger causal language, but the context of evaluating steering efficacy supports a Rung 2 interpretation."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,Contrastive Activation Addition (steering vectors),2,2,0,5,0,"The method used is Contrastive Activation Addition (CAA), a steering vector technique that involves adding a learned bias to model activations during inference. This is an interventional method (Rung 2) because it modifies activations and measures the effect on outputs. The claim states that high steering strengths 'consistently degrade both intrinsic and extrinsic text quality.' This is a causal claim about the effect of an intervention (increasing steering strength) on outcomes (text quality degradation), using language like 'degrade' which indicates a causal effect. It does not make mechanistic or uniqueness claims (Rung 3) about how the model works internally, nor is it purely associational (Rung 1). The claim is stated as an established fact without hedging. Confidence is high because the method and claim are clearly described in the paper."
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,Contrastive Activation Addition (steering vectors) combined with prompt engineering,2,2,0,4,0,"The method used is Contrastive Activation Addition (CAA), a steering vector technique that adds a learned bias to activations during inference. This is an interventional method (Rung 2) because it involves modifying activations to observe changes in output. The claim states 'combining steering and prompting yields the strongest control over text properties.' The phrase 'strongest control' implies causal sufficiency/comparative efficacy, not uniqueness or mechanism. It is a claim about the effectiveness of an intervention (steering + prompting) relative to other methods, fitting Rung 2 causal claims (e.g., 'can produce', 'enables'). There is no hedge. Confidence is 4 due to minor ambiguity: 'strongest control' could be interpreted as a mechanistic claim, but the context (evaluation of control efficacy in summarization) supports a causal interpretation."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,Causal abstraction theory,3,3,0,4,0,"The claim is from the abstract of a theoretical paper proposing a framework for computational implementation using causal abstraction. The method is theoretical analysis and framework development (causal abstraction theory), which establishes counterfactual reasoning about mechanisms (Rung 3). The claim uses 'provides a fruitful lens' which is a functional/mechanistic claim about explanatory utility, aligning with Rung 3 linguistic markers ('provides a lens' implies a framework for understanding mechanisms). There is no hedge present. Confidence is 4 because while the method is theoretical (not empirical), the paper's overall approach involves causal modeling and abstraction which fits Rung 3, but the claim itself is about providing a 'lens' rather than a specific empirical finding."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,Causal abstraction theory,3,3,0,4,0,"The claim is a theoretical proposal about computational implementation, not an empirical claim about a specific model's internals. The method used is causal abstraction theory, which is a conceptual framework for establishing counterfactual mappings between computational models and physical systems. This aligns with Rung 3 methods that establish unique mechanisms or counterfactual relationships. The claim 'we offer an account of computational implementation grounded in causal abstraction' is a Rung 3 claim because it proposes a theoretical account of how computation is implemented, which is a mechanistic/counterfactual claim about the underpinnings of computational explanation. There is no hedge present. Confidence is 4 because while the claim is clearly theoretical and aligns with Rung 3, the paper is philosophical/theoretical rather than empirical ML research, but the codebook's rung definitions still apply to claims about methods and their implications."
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,Activation patching,2,3,1,4,0,"The claim 'the output language is encoded in the latent at an earlier layer than the concept to be translated' uses the term 'encoded,' which typically indicates a mechanistic claim (Rung 3). According to the codebook's decision tree for 'encodes,' if the paper provides interventional evidence (activation patching, which is Rung 2), the claim is about the intervention's result (what changed) rather than the underlying mechanism, so it should be coded as R2. However, the claim implies a specific internal representation ('encoded in the latent') and a temporal ordering ('earlier layer than'), which suggests a mechanistic narrative about how the model processes information. The method used is activation patching (Rung 2), which establishes causal effects but not unique mechanisms. The claim goes beyond causal sufficiency to describe a specific internal encoding process, making it Rung 3. There is no hedging language. Confidence is 4 due to some ambiguity in interpreting 'encoded' in context, but the mechanistic implication leans toward R3."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,Activation patching,2,2,0,4,0,"The method used is activation patching, which is an interventional technique (Rung 2). The claim states that activation patching can independently manipulate concept and language, which is a causal sufficiency claim ('can change X without changing Y') without asserting uniqueness or mechanism. This matches Rung 2 linguistic markers ('can produce', 'intervening on X changes Y'). There is no hedge, and the claim is directly about the intervention's effect, not the underlying mechanism. Confidence is 4 due to minor ambiguity in whether 'change' implies causal sufficiency versus mechanistic control, but the context supports a causal interpretation."
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,Activation patching,2,2,0,4,0,"The claim is from the abstract: 'patching with the mean representation of a concept across different languages improves translation.' The method used is activation patching, which involves intervening on activations (specifically patching mean representations) and measuring effects on translation performance. This is an interventional method (Rung 2). The claim states that patching improves translation, which is a causal effect claim ('improves' indicates a change due to intervention). It does not assert uniqueness or a mechanistic explanation (e.g., 'encodes' or 'the circuit'), so it is Rung 2. There is no hedging language. Confidence is 4 due to minor ambiguity about whether 'improves' implies a causal effect, but the context supports this interpretation."
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,Activation patching,2,2,0,4,0,"The paper uses activation patching (an interventional method) to show that language and concept information can be manipulated independently and that averaging concept representations across languages improves translation performance. The claim 'provides evidence for the existence of language-agnostic concept representations' is a causal claim about the model's internal representations based on intervention results, matching Rung 2 linguistic markers like 'evidence for' and causal implications. It does not claim uniqueness or full mechanistic explanation (Rung 3). No hedging words are present."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,Activation patching (de-noising variant),2,3,1,4,0,"The method used is activation patching (de-noising variant), which is an interventional method (Rung 2). The claim uses the verb 'transform' and states a mechanistic narrative ('transform persona tokens into richer representations which are then used by middle MHA layers'), implying a specific computational process and functional role. This is characteristic of Rung 3 claims (mechanistic/counterfactual). There is no hedge present. The claim is explicit in the abstract. However, the evidence from activation patching shows causal effects but does not establish uniqueness or full mechanistic explanation, creating a +1 gap. Confidence is 4 due to clear linguistic markers but some ambiguity about whether 'transform' is being used in a strictly mechanistic sense."
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,Activation patching,2,2,0,4,0,"The method used is activation patching (CLAP), which involves replacing corrupted activations with clean ones and measuring recovery of logit difference. This is an interventional method (Rung 2). The claim states that patching recovered 56% of correct preference, demonstrating that associative knowledge is distributed. The phrase 'demonstrating that associative knowledge is distributed' is a causal claim about the effect of the intervention (patching the first feedforward layer partially restores performance, implying distributed representation). This matches Rung 2 linguistic markers like 'demonstrates' indicating causal effect evidence. There is no hedge present. Confidence is 4 because while the method and claim alignment is clear, the term 'distributed' could be interpreted as a stronger mechanistic claim, but the context supports a causal interpretation."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation patching,2,3,1,4,0,"The method used is activation patching (Causal Layer Attribution via Activation Patching/CLAP), which involves replacing corrupted activations with clean ones to measure recovery effects. This is an interventional method establishing causal effects, placing it at Rung 2. The claim states that patching 'completely restored accuracy indicating that definitional knowledge is localised.' The phrase 'definitional knowledge is localised' uses the term 'localised' in a mechanistic sense, implying that knowledge is stored/encoded in a specific location (the final output layer). According to the codebook's decision tree for 'encodes/represents/stores,' without interventional evidence for mechanism (only causal effect), and without context suggesting decodability, this defaults to a Rung 3 mechanistic claim. The claim asserts a specific mechanism (knowledge localization) based on interventional evidence of causal effect, which is a +1 gap (R2→R3). There is no explicit hedge (e.g., 'may' or 'suggests'), so hedge_flag=0. Confidence is 4 due to minor ambiguity in interpreting 'localised' but strong alignment with the overclaim pattern of patching → localization claims."
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Causal Layer Attribution via Activation Patching (CLAP),2,3,1,4,0,"The method used is activation patching (CLAP), which involves replacing corrupted activations with clean ones and measuring recovery of logit difference. This is an interventional method (Rung 2) that establishes causal effects but does not prove uniqueness or necessity. The claim 'factual knowledge is more localized and associative knowledge depends on distributed representations' uses mechanistic language ('localized', 'depends on distributed representations') that implies understanding of how knowledge is organized and processed in the model. While 'depends on' could be interpreted as causal (R2), the overall claim about knowledge representation being 'localized' vs 'distributed' suggests a mechanistic understanding of how knowledge is stored and used, which aligns with Rung 3 claims about representation and mechanism. The paper provides interventional evidence (patching shows different recovery patterns for different layers), but the claim goes beyond causal effects to make claims about the nature of knowledge representation. There is no hedge present in the claim."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,Sparse Autoencoders (SAEs) trained on activations from multiple models,1,1,0,4,0,"The method used is SAE feature attribution (observational/associational), which is a Rung 1 method. The claim states the USAE can 'reconstruct and interpret' internal activations. 'Interpret' here likely means identifying which concepts activate, which is associational evidence (R1). There is no intervention mentioned, and the claim does not use strong mechanistic language like 'encodes' or 'computes' in a R3 sense. The claim is about learning a concept space that can reconstruct activations, which aligns with R1 linguistic markers ('can reconstruct', 'interpret')."
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,Universal Sparse Autoencoders (USAEs),1,1,0,4,0,"The method used is Universal Sparse Autoencoders (USAEs), which involves training a sparse autoencoder on activations from multiple models to reconstruct them. This is an observational/associational method (Rung 1) because it analyzes activations without intervention. The claim states 'captures common factors of variation concepts across different tasks architectures and datasets,' which uses language like 'captures' and 'common factors of variation' that are associational markers (Rung 1). There is no causal or mechanistic language implying intervention or unique mechanisms. The claim is stated as a fact without hedging."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,Universal Sparse Autoencoders (USAEs) trained on activations of multiple models,1,3,2,4,0,"The method used is USAEs, which involve training a sparse autoencoder on activations from multiple models to reconstruct them and identify shared features. This is an observational/associational method (Rung 1) because it analyzes activations without intervention. The claim uses 'discover' and 'universal concepts' which implies uncovering underlying shared representations or mechanisms across models, which is a mechanistic claim (Rung 3). There is no explicit hedge. The gap is +2 (R1 method to R3 claim), consistent with the overclaim pattern 'SAE → represents' in the codebook."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,Sparse Autoencoders (SAEs) feature attribution,1,3,2,4,0,"The method used is SAE feature attribution, which is an observational/associational method (Rung 1) that identifies which SAE features activate on inputs. The claim states that SAEs 'facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features.' The phrase 'explaining single-meaning atomic features' implies a mechanistic understanding of how the model works, suggesting that SAEs reveal the underlying atomic features that the model uses. This is a Rung 3 claim because it uses mechanistic language ('explaining') and implies that SAEs can decompose the model's internal representations into interpretable, atomic components that have single meanings. There is no explicit hedge in the claim. The confidence is 4 because while the claim is clearly mechanistic, the paper's abstract does not provide interventional evidence for this claim, aligning with the overclaim pattern 'SAE → represents' (R1→R3)."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,Sparse Autoencoder (SAE) feature attribution with concept-specific interpretability evaluation,1,2,1,4,0,"The method involves training SAEs and evaluating them using metrics like L0,t and ICDF to assess differentiation of safety concepts, which is observational/correlational (Rung 1). The claim 'systematically identifies SAE with best concept-specific interpretability' implies a causal effect of selecting optimal SAEs for interpretability, which is a Rung 2 claim (causal sufficiency/intervention). However, the paper does not provide interventional evidence for uniqueness or mechanistic explanation, so it's not Rung 3. No hedging language is present."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,Sparse Autoencoders (SAEs) with automated interpretation,1,1,0,4,0,"The method used is SAE feature attribution and automated explanation generation, which involves training SAEs to decompose internal representations and then using an LLM to generate explanations for neurons. This is an observational/associational method (Rung 1) because it identifies features that activate on certain inputs without causal intervention. The claim states 'extract...features that effectively capture high-risk behaviors' – 'capture' here means detect or correlate with, not causally influence. The linguistic marker 'capture' aligns with R1 associational claims (like 'activates on' or 'correlates with'). There is no intervention evidence mentioned for this claim, and the paper's methods section confirms SAE training and interpretation are used to identify features, not to test causal effects. The claim is not hedged."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,CountScope (causal probing method) and attention analysis,2,3,1,4,0,"The claim 'latent counts are computed and stored in the final item representations of each part' uses mechanistic language ('computed and stored') which typically indicates Rung 3. The paper uses CountScope (a causal probing method involving activation patching) and attention analysis, which are interventional methods (Rung 2). According to the decision tree for 'encodes/represents/stores', since interventional evidence is provided (CountScope patches activations), but the claim is about the underlying mechanism (how counts are stored), it should be coded as Rung 3. There is no hedge in the claim. Confidence is 4 due to some ambiguity in whether 'stored' implies a unique storage mechanism versus causal mediation, but the mechanistic narrative suggests R3."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,attention analysis and causal mediation analysis,2,3,1,4,0,"The claim 'counts are transferred to intermediate steps via dedicated attention heads' uses the functional verb 'transferred' and implies a specific mechanistic role for attention heads. The paper uses attention analysis (Rung 1 observational) and causal mediation analysis including activation patching and attention knockout (Rung 2 interventional). However, the claim goes beyond establishing causal effects to describe a specific transfer mechanism ('via dedicated attention heads'), which is a mechanistic claim about how the model works. According to the decision tree for 'controls/is responsible for', since the evidence includes interventions (activation patching, attention knockout) but the claim does not assert uniqueness, it could be R2. However, the phrase 'dedicated attention heads' suggests a specific functional role, leaning toward R3. Given the calibration examples (IOI paper's 'performs' = R3) and the mechanistic narrative, I classify as R3. There is no hedge. Confidence is 4 due to some ambiguity between R2 and R3."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,"Observational and causal mediation analyses (activation patching, attention knockout, zero ablation)",2,2,0,4,0,"The claim 'enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting' is a causal claim about the effect of the System-2 strategy (intervention). The paper uses interventional methods like activation patching, attention knockout, and zero ablation (Rung 2) to support this. The claim does not assert uniqueness or a mechanistic explanation (Rung 3), but rather that the strategy causally enables improved performance. No hedging words like 'may' or 'suggests' are present, so hedge_flag=0. Confidence is 4 due to minor ambiguity in whether 'enables' implies sufficiency (R2) or a mechanistic role (R3), but context suggests causal effect."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,Theoretical analysis / analogy to machine learning,1,1,0,4,0,"The claim 'neuron identification can be viewed as the inverse process of machine learning' is an observational/associational analogy. It draws a structural parallel between two processes without intervening on the model. The method used is theoretical analysis and analogy, which is observational (Rung 1). The claim uses language like 'can be viewed as,' which is associational, not causal or mechanistic, fitting Rung 1. There is no hedge present. Confidence is 4 due to minor ambiguity in whether this analogy implies deeper mechanistic claims, but the phrasing is clearly associational."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,Theoretical analysis of generalization bounds for similarity metrics,1,1,0,4,0,"The claim is about deriving generalization bounds for similarity metrics used in neuron identification. The method is theoretical analysis (statistical learning theory) applied to observational methods like linear probing and similarity metrics (accuracy, AUROC, IoU), which are Rung 1 (observational/associational). The claim uses language 'guarantee faithfulness' which is about providing theoretical guarantees for correlational evidence, not causal or mechanistic claims. No intervention on the model is described; it's about bounding the performance of similarity metrics. The claim rung is 1 because it's about guarantees for observational metrics. There is no hedge in the claim."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,Bootstrap ensemble,1,2,1,4,0,"The method described is a bootstrap ensemble procedure applied to neuron identification algorithms (like CLIP-Dissect and NetDissect). Bootstrap ensemble is a statistical resampling technique that aggregates results from multiple resampled datasets to quantify uncertainty/variability. It does not involve intervening on the model's internals (e.g., patching, ablation) or establishing counterfactual mechanisms; it is an observational method that analyzes variability across datasets. Thus, method_rung = 1 (Observational/Associational). The claim is about quantifying stability and providing guaranteed coverage probability for concept identification. 'Quantifies stability' suggests measuring consistency (associational), but 'guaranteed coverage probability' implies a statistical guarantee about the procedure's output, which is a causal claim about the method's reliability under resampling (i.e., the procedure ensures coverage). However, since it's about the method's own performance guarantee (not a causal effect on model behavior), it aligns with Rung 2 (Interventional) as it involves a procedural intervention (resampling) to assess stability. The claim does not assert a unique mechanistic explanation (Rung 3). No hedging words present."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,Causal tracing (activation patching) with Gaussian noise perturbation,2,3,1,4,0,"The method used is causal tracing (activation patching) with Gaussian noise perturbation, which involves interventions (corrupting inputs and restoring activations) to measure causal effects, placing it at Rung 2 (Interventional). The claim uses 'storage and transfer' which are mechanistic terms implying encoding and processing mechanisms. Following the decision tree for 'encodes/represents/stores', since the paper provides interventional evidence (R2), but the claim is about the underlying mechanism (how FFNs handle representations) rather than just the intervention's result, it defaults to Rung 3. There is no hedge present. Confidence is 4 due to minor ambiguity in whether 'storage' is meant mechanistically, but the abstract context suggests a mechanistic claim."
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,Activation patching (de-noising variant),2,3,1,4,0,"The claim 'early MLP layers attend not only to the syntactic structure but also process its semantic content' is from the abstract. The method used is activation patching (de-noising variant), which is an interventional method (Rung 2). The claim uses functional language ('attend', 'process') and makes a mechanistic assertion about what the layers do, implying they perform semantic processing. According to the codebook, functional verbs like 'process' and claims about what components 'do' are Rung 3 markers. The claim does not merely state correlation or causal effect but describes the layers' functional role, making it a Rung 3 claim. There is no hedge. Confidence is 4 due to some ambiguity in whether 'process' is explicitly mechanistic, but the context (contrasting with prior syntactic-only findings) supports a mechanistic reading."
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,Activation patching (de-noising variant) with attention pattern analysis,2,2,0,4,0,"The claim 'we identify specific attention heads that disproportionately attend to racial and color-based identities' is from the abstract. The method used is activation patching (specifically the 'de-noising variant' described in Section 4.1), which is an interventional method (Rung 2). The claim uses the verb 'attend to,' which describes a pattern of attention allocation observed through analysis of attention patterns (a form of visualization/analysis of intervention results). The claim does not assert a unique mechanism (e.g., 'THE heads that control...'), nor does it use strong mechanistic verbs like 'encode,' 'represent,' or 'compute.' It states that heads 'disproportionately attend,' which is a causal/functional claim based on observed intervention effects (patching changes attention patterns). Therefore, the claim rung is 2. There is no hedge ('may,' 'suggests') in this specific claim. Confidence is 4 because while the method and claim alignment is clear, the exact boundary between Rung 2 and Rung 3 for 'attend' could be ambiguous, but the context suggests it's about identifying heads with a causal role (via patching) rather than claiming a unique representational mechanism."
