paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,Hyper-network weight update prediction (observational analysis of which parameters the learned editor modifies),1,3,2,4,0,"The claim 'stored in the parameters of Language Models' uses storage/memory language ('stored'), which per the codebook is a Rung 3 mechanistic marker implying a memory mechanism. The method is a hyper-network that learns to predict weight updates; the 'probe' interpretation comes from observing which components the hyper-network modifies, which is observational (R1) -- no causal intervention (patching, ablation) is performed on the LM itself to establish that these parameters store knowledge. The paper treats the update pattern as revealing storage locations, but this is an indirect inference from a learned editing procedure, not a causal test of storage. Gap: +2 (R1 method -> R3 claim)."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,Hyper-network weight update prediction (observational analysis of learned update patterns),1,2,1,4,0,"The claim states the hyper-network 'can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge.' The phrase 'need to be changed to manipulate' implies causal necessity -- that these specific components must be modified (R2 causal language). However, the method is observational: the hyper-network learns an update pattern, and the authors observe which components receive large updates. No ablation or patching experiment verifies that these components are causally necessary. The word 'probe' itself is R1 language, but the claim that it reveals what 'needs to be changed' implies causal sufficiency/necessity (R2). Gap: +1 (R1 method -> R2 claim)."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,Hyper-network weight update prediction (observational analysis of update magnitude distribution),1,1,0,5,0,"The claim 'the updates tend to be concentrated on a small subset of components' is a purely observational/descriptive statement about the distribution of learned weight updates. It describes a pattern (concentration/sparsity) without making causal or mechanistic claims about why or what these components do. The method (observing update magnitudes from the hyper-network) is R1, and the claim language is appropriately R1 -- it reports an empirical observation. No overclaim. Gap: 0."
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,Hyper-network weight update prediction (observational analysis of update patterns),1,3,2,4,0,The claim states the hyper-network 'can be regarded as a probe revealing the causal mediation mechanisms.' The phrase 'causal mediation mechanisms' is explicitly mechanistic (R3) -- it claims the update patterns reveal the underlying mechanisms by which factual knowledge is mediated in the model. Per the codebook decision tree for 'the mechanism': does the paper test uniqueness or provide interventional evidence? No -- the evidence is purely observational (which weights the hyper-network learns to update). The authors reference Vig et al. (2020) for 'causal mediation' but do not themselves perform causal mediation analysis. Using R3 mechanistic language based on R1 observational evidence. Gap: +2 (R1 -> R3).
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Hyper-network weight update prediction (observational analysis of update magnitude across parameter types),1,3,2,4,1,"The claim 'the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution' uses mechanistic language: 'achieved by' implies a functional mechanism (R3), describing how the model accomplishes knowledge manipulation through specific parameter types (attention key/query weights). The hedge 'seems to be' is noted (hedge_flag=1) but per codebook guidelines, the underlying claim is still coded as R3. The method is observational -- Section 6.4 analyzes the magnitude of updates across different parameter types (K, Q, V weights) without interventional validation that these parameters are causally responsible. Gap: +2 (R1 -> R3)."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,Causal tracing (activation patching with corruption and restoration) and ROME weight editing,2,3,1,5,0,"The claim 'factual associations correspond to localized directly-editable computations' uses 'correspond to' which is an identity/correspondence claim (R3) -- it asserts that factual associations ARE localized computations, not merely that interventions on these locations have causal effects. Per the calibration rationale for this paper, 'correspond to localized computations' is a +1 overclaim. The method (causal tracing) establishes that mid-layer MLPs have high causal effect on factual predictions (R2), and ROME demonstrates editability (R2). But 'correspond to' implies a structural identity between the abstract concept (factual association) and the computation, which is a mechanistic claim beyond what interventional evidence supports. Gap: +1 (R2 -> R3)."
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,Causal tracing (activation patching with corruption and restoration),2,2,0,5,0,"The claim identifies 'a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions.' The key verb is 'mediate,' which per the codebook is proper R2 (causal) language -- it describes causal mediation without implying a unique mechanism or storage. The method is causal tracing (corruption + restoration of activations), which is a causal intervention (R2). The claim-method alignment is appropriate. Per the calibration rationale, 'mediate factual predictions' is identified as an appropriate R2 claim with no overclaim. Gap: 0."
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal tracing (activation patching with corruption and restoration) and ROME weight editing,2,3,1,5,0,"The claim 'mid-layer feed-forward modules... storing factual associations' uses 'storing,' which per the codebook and calibration rationale is R3 mechanistic language implying a memory/storage mechanism. Per the calibration document: 'Storage/memory language (stores, encodes, contains) typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.' The causal tracing shows these modules have high causal effect (R2), and ROME shows they can be edited (R2), but neither establishes that the modules function as a storage mechanism. The claim goes beyond causal mediation to assert a specific functional role (storage). Gap: +1 (R2 -> R3)."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,ROME weight editing evaluated on zero-shot relation extraction benchmark,2,2,0,5,0,"The claim 'ROME is effective on a standard zero-shot relation extraction model-editing task' is an empirical performance claim about the editing method's effectiveness. It does not make mechanistic claims about what the model represents or how it works internally. The method (ROME editing + benchmark evaluation) is R2 interventional, and the claim is appropriately R2 -- it describes the outcome of an intervention without overclaiming about mechanisms. Per the calibration rationale, 'ROME is effective' is identified as an appropriate empirical claim matched to method. Gap: 0."
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,"Path patching (causal intervention), activation patching, mean ablation knockouts",2,3,1,5,0,"The claim 'presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification' uses 'performs' -- a functional verb that is a R3 mechanistic marker per the codebook. The claim asserts that the paper explains the mechanism by which the model performs the task, which is a mechanistic/functional attribution (R3). The methods used are path patching and activation patching (R2 interventional). Per the calibration anchor: 'performs IOI task' with 'performs' = functional is a +1 overclaim (R2 -> R3). The interventions establish causal sufficiency of specific heads but not that these heads uniquely perform the computation. Gap: +1 (R2 -> R3)."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,"Path patching (causal intervention), activation patching, attention pattern analysis, mean ablation knockouts",2,2,0,4,0,"The claim 'Our explanation encompasses 26 attention heads grouped into 7 main classes' is primarily a descriptive/structural claim about the discovered circuit's composition. It describes the result of the causal analysis (which heads were found to be causally important via path patching) without making strong mechanistic claims about what these heads 'do' or 'compute.' The grouping into classes is based on observed attention patterns and patching results. This is appropriately matched to R2 methods -- it summarizes the output of interventional analysis. Minor ambiguity: the word 'explanation' could imply mechanistic understanding (R3), but in context it refers to the circuit description rather than a uniqueness or functional claim. Coded as R2 with slight ambiguity. Gap: 0."
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,"Path patching (causal intervention), activation patching, mean ablation knockouts",2,3,1,5,0,"The claim 'this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior' uses 'reverse-engineering,' which per the calibration rationale is a R3 marker implying complete mechanism discovery. 'Reverse-engineering' implies recovering the full algorithm/mechanism (R3), not merely identifying causally important components (R2). The methods (path patching, ablation) are R2 interventional -- they identify which components have causal effects but do not establish that the discovered circuit is THE unique mechanism. Per the calibration anchor, 'reverse-engineering the algorithm' implies complete mechanism and is a +1 overclaim. Gap: +1 (R2 -> R3)."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,"Path patching (causal intervention), attention pattern analysis, copy score analysis (OV matrix)",2,3,1,5,0,"The claim 'Name Mover Heads... move the name from the subject to the end position' uses 'move' -- a functional/mechanistic verb (R3) that attributes a specific computational function to these attention heads. Per the calibration anchor: 'Name Movers move names' with 'move' = mechanistic is a +1 overclaim (R2 -> R3). The evidence comes from path patching (showing these heads have causal effect on logit difference, R2) and attention pattern/copy score analysis (showing correlation between attention and output direction, R1). Neither method establishes that the heads' function IS 'moving names' as a unique mechanism -- the patching shows causal importance and the copy scores show correlation, but 'move' implies a specific functional role. Gap: +1 (R2 -> R3)."
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,"Path patching (causal intervention on query inputs to Name Mover Heads), attention pattern analysis",2,3,1,5,0,"The claim 'S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token' uses 'inhibit' -- a functional verb (R3) that attributes a specific computational mechanism to these heads. Per the calibration anchor: 'S-Inhibition heads inhibit' with 'inhibit' = functional is a +1 overclaim (R2 -> R3). The evidence comes from path patching on queries of the Name Mover Heads (showing these heads causally affect the attention pattern of downstream heads, R2) and attention pattern analysis (R1). The claim attributes a specific function ('inhibit attention to S') that goes beyond demonstrating causal effect -- it implies a mechanistic understanding of what the computation does. Gap: +1 (R2 -> R3)."
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,"Faithfulness (circuit knockout comparison), completeness (subset knockout search), minimality (node removal impact) tests using mean ablation",2,3,1,4,0,"The claim 'the circuit is faithful complete and minimal' asserts three properties: faithfulness (circuit performs like full model), completeness (circuit contains all task-relevant nodes), and minimality (no unnecessary nodes). 'Complete' and 'minimal' together imply uniqueness -- that this is THE circuit for the task, with nothing missing and nothing extra. Per the codebook, 'THE circuit' with definite article = uniqueness is R3. The methods are ablation-based tests (R2), which evaluate causal importance of components. Importantly, the paper itself acknowledges the circuit fails the most challenging completeness tests (greedy optimization found high incompleteness scores), yet the abstract claim is stated without this qualification. The faithfulness/completeness/minimality framework attempts R3-level validation but the evidence only partially supports it. The claim as stated in the abstract is R3; the method provides R2-level evidence with partial R3 validation attempts. Gap: +1 (R2 -> R3)."
2301.04709,2301.04709-01,Causal abstraction provides a theoretical foundation for mechanistic interpretability,abstract,3,Mathematical formalization and theoretical unification of existing mechanistic interpretability methods under causal abstraction framework; no new empirical experiments,2,3,1,3,0,"This is a theoretical paper that formalizes causal abstraction as a foundation for MI. The claim 'provides a theoretical foundation for mechanistic interpretability' is a meta-level framing claim rather than a standard empirical claim about model internals. The paper unifies methods spanning R1-R3 (probing, patching, DAS, causal scrubbing) but does not itself run new experiments. The method_rung is coded as R2 because the paper's formalism centers on interchange interventions and causal abstraction, which are interventional (R2) frameworks. The claim_rung is R3 because 'theoretical foundation' implies a mechanistic/structural account of how interpretability methods relate -- it claims to provide THE foundational framework (uniqueness/completeness implication). However, confidence is moderate because this is a framework/theory paper, not a standard empirical claim about model internals, making rung classification somewhat ill-fitting."
2301.04709,2301.04709-02,generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation,abstract,3,Mathematical formalization extending causal abstraction theory from hard/soft interventions to arbitrary mechanism transformations (interventionals),2,2,0,4,0,The claim is about 'generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation.' This is a theoretical contribution about extending the formalism of interventions. The method is mathematical proof/formalization of interventionals that generalize hard and soft interventions (R2-level operations). The claim itself describes extending interventional (R2) tools -- it does not claim to have discovered a mechanism or made a uniqueness claim about model internals (R3). It is a precise description of a theoretical contribution: broadening the class of interventions that can be formalized. Both method and claim are well-matched at R2 (interventional framework). No overclaim.
2301.04709,2301.04709-03,unifying a variety of mechanistic interpretability methods in the common language of causal abstraction,abstract,3,"Theoretical analysis mapping existing MI methods (activation patching, path patching, causal mediation, causal scrubbing, circuit analysis, concept erasure, SAEs, DAS, steering) onto causal abstraction formalism",2,2,0,4,0,"The claim is about 'unifying a variety of mechanistic interpretability methods in the common language of causal abstraction.' The paper demonstrates that methods like activation patching (R2), causal scrubbing (R3), DAS (R2), SAEs (R1), and steering (R2) can all be expressed within the causal abstraction framework. The method is formal theoretical analysis showing these mappings. The claim is appropriately scoped: it says 'unifying' (a descriptive/organizational claim about the relationship between methods), not that the methods themselves are correct or that the framework reveals the true mechanism. This is a meta-level claim about the expressiveness of a formalism, well-matched to the theoretical approach. Method and claim both operate at the interventional (R2) level since the unifying language is built on interventions and causal models."
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,"Weight analysis and Fourier decomposition (R1), ablation of Fourier components (R2)",2,3,1,5,0,"The claim 'We fully reverse engineer the algorithm learned by these networks' is a strong R3 claim. 'Fully reverse engineer' implies completeness and uniqueness of the mechanistic account -- it claims to have identified THE algorithm (definite article implied), not just causally important components. 'Reverse engineer' is mechanistic language implying a complete mechanistic understanding. The primary methods are weight/activation analysis in Fourier space (R1, observational) and ablation of Fourier components (R2, interventional). The highest-rung method directly supporting the claim is ablation (R2). This is a classic R2->R3 overclaim pattern identified in the calibration set: 'fully reverse engineer' + 'the algorithm' from interventional evidence. Gap = +1. Per the calibration notes for this exact paper, this overclaim is less concerning because the setting is simple (synthetic task), mathematically constrained, and well-replicated."
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,"Weight analysis and Fourier decomposition of embedding/MLP/unembedding matrices (R1), ablation of key Fourier frequencies (R2)",2,3,1,5,0,"The claim states the network 'uses discrete Fourier transforms and trigonometric identities to convert addition to rotation.' The verb 'uses' is mechanistic language -- 'the model uses X to do Y' is a canonical R3 marker per the codebook. It attributes a specific computational algorithm to the model, not merely that certain components are causally important. The evidence comes from (1) observing periodic structure in weights/activations (R1), (2) decomposing the neuron-logit map into Fourier components (R1), and (3) ablating non-key frequencies and showing performance drops (R2). The highest method rung is R2 (ablation). The claim is R3 because it makes a functional attribution ('uses DFT to convert'), describing the mechanism the model implements. This matches the calibration pattern for this paper: +1 gap (R2->R3). The ablation shows key frequencies are causally necessary, but 'uses' implies a complete mechanistic account beyond what ablation alone establishes."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,"Training dynamics analysis using progress measures (restricted loss, excluded loss) derived from Fourier decomposition (R1), ablation in Fourier space (R2)",2,3,1,4,0,"The claim states 'grokking arises from the gradual amplification of structured mechanisms encoded in the weights.' This makes a mechanistic claim about the training dynamics: (1) 'arises from' implies a causal/mechanistic origin story, (2) 'structured mechanisms encoded in the weights' uses 'encoded' which per the codebook decision tree defaults to R3 unless clearly used in a decodability sense -- here it refers to mechanisms stored in weights, which is mechanistic (R3). The evidence comes from tracking progress measures (restricted/excluded loss) across training, which are derived from Fourier ablation (R2). These measures show continuous improvement before the grokking phase transition. The method is interventional (R2) since it relies on ablation-based progress measures. The claim goes beyond what these interventions establish: ablation shows causal importance of Fourier components at different training stages, but 'encoded in the weights' and 'arises from gradual amplification' posit a specific mechanistic narrative about the training process (R3). Gap = +1."
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Activation patching (R2) applied systematically via ACDC algorithm for automated circuit discovery,2,3,1,4,0,"The claim references prior work that has 'reverse-engineered nontrivial behaviors of transformer models.' 'Reverse-engineered' is strong mechanistic language (R3) -- it implies recovering the complete algorithm/mechanism, not merely identifying causally important components. The paper's own method (ACDC) automates activation patching to find circuit edges, which is interventional (R2). The paper is describing and systematizing what prior circuit-discovery papers did, and it adopts their framing language. 'Reverse-engineered' implies a full mechanistic understanding (R3), while the actual methods (activation patching, ablation) establish causal effects (R2). This is the canonical R2->R3 overclaim pattern via 'reverse-engineering' language, matching the IOI calibration anchor. Gap = +1. Confidence is 4 rather than 5 because the claim partially describes prior work's claims rather than the paper's own direct claim."
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,ACDC algorithm performing automated activation patching to prune computational graph edges (R2),2,2,0,5,0,"The claim states 'ACDC algorithm rediscovered 5/5 of the component types in a circuit.' This is a well-calibrated empirical claim about the performance of an automated method. 'Rediscovered' here means the algorithm's output matched previously identified components -- it is a claim about method accuracy/recall, not about what the circuit mechanistically does. The method is activation patching via ACDC (R2, interventional). The claim is about the interventional method's ability to recover known components, which is an R2-level claim (what components are causally involved). It does not claim that these components 'perform,' 'compute,' or 'encode' anything specific (which would be R3). No overclaim. Method and claim are well-matched at R2."
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,"Activation patching at varying granularity levels, combined with manual inspection of recovered circuit components (R2)",2,3,1,4,0,"The claim states 'researchers can understand the functionality of each component.' 'Understand the functionality' is R3 language -- it implies a mechanistic understanding of what each component does (functional attribution), not merely that each component is causally important. Per the codebook, functional language like 'performs,' 'computes,' or understanding 'functionality' implies mechanistic claims (R3). The method described in the workflow is activation patching with varying datasets and metrics (R2), which identifies which components are involved. But identifying causal involvement (R2) does not by itself establish what each component functionally does (R3). The gap between 'this component is causally involved' and 'we understand its functionality' is the R2->R3 overclaim. Gap = +1. Confidence is 4 because the claim is somewhat implicit -- it describes the workflow's goal rather than a specific empirical result."
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,ACDC algorithm performing iterative activation patching to prune edges in the computational graph (R2),2,2,0,5,0,"The claim is about 'finding the connections between abstract neural network units that form a circuit.' This is a precise description of what ACDC does: it identifies which edges (connections) in the computational graph are causally important for a given behavior. 'Finding connections' is well-matched to the activation patching method (R2) -- it describes the output of an interventional procedure (which edges matter) rather than making mechanistic claims about what those connections compute or represent. The language is appropriately interventional: 'connections that form a circuit' describes causal structure, not functional mechanism. No overclaim. Method and claim are both R2."
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,"Systematic comparison of activation patching variants: Gaussian noising vs. symmetric token replacement corruption methods, probability vs. logit difference metrics, single-layer vs. sliding window patching (R2)",2,2,0,5,0,"The claim is about 'systematically examining the impact of methodological details in activation patching.' This is a meta-methodological claim: the paper studies how different hyperparameter choices in an R2 method (activation patching) affect results. The method is running activation patching experiments with varying corruption methods, metrics, and patching strategies -- all interventional (R2). The claim itself is about the sensitivity of these interventional procedures, which is an R2-level claim about the behavior of R2 methods. No mechanistic or functional attribution is made. The claim is well-calibrated to the evidence. No overclaim."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,"Empirical comparison of activation patching with different corruption methods (GN vs STR), metrics (probability vs logit difference), and window sizes across multiple tasks (factual recall, IOI, greater-than, docstring) (R2)",2,2,0,5,1,"The claim states 'varying these hyperparameters could lead to disparate interpretability results.' The verb 'could' is an explicit hedge -- it signals possibility rather than certainty, making hedge_flag = 1. The claim is about the sensitivity of activation patching (R2) to methodological choices, supported by empirical comparisons showing that different corruption methods and metrics yield different localization and circuit discovery outcomes. This is an R2 claim about R2 methods: it describes what happens when you vary the parameters of interventional experiments. No mechanistic or functional attribution is made. The claim is well-calibrated: the paper demonstrates empirically that GN and STR produce inconsistent results, and that probability and logit difference metrics diverge. No overclaim."
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,Subspace activation patching (DAS) with mathematical analysis of dormant/disconnected subspace decomposition,2,2,0,4,1,"Claim: 'even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway'. The method is subspace activation patching (interchange intervention), which is R2 interventional. The claim itself describes a causal effect of interventions -- that patching 'may' activate a dormant pathway -- staying at the interventional level (R2). The claim does not assert a unique mechanism or counterfactual necessity; it describes what interventions can produce. The 'may' is an explicit hedge. Claim-method alignment: no overclaim."
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,Subspace activation patching (DAS) on MLP hidden activations with nullspace/rowspace decomposition analysis,2,2,0,5,0,"Claim: 'patching of subspaces can lead to an illusory sense of interpretability'. The method is subspace activation patching (R2 interventional). The claim describes a failure mode of an interventional method -- that it 'can lead to' misleading results. This is a claim about the behavior of interventions, staying at R2. No mechanistic/uniqueness language; no overclaim. 'Can lead to' is not a hedge in the codebook sense (it describes a possibility demonstrated empirically, not uncertainty about the claim itself). Stated as established finding."
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,Toy linear model construction plus subspace activation patching (DAS) on GPT-2 Small (IOI task and factual recall),2,2,0,4,0,"Claim: 'we demonstrate this phenomenon in a distilled mathematical example in two real-world domains'. The methods are: (1) a constructed toy linear network (mathematical proof, not empirical R1/R2/R3 per se, but used to illustrate the interventional illusion), and (2) subspace activation patching via DAS on GPT-2 Small for IOI and factual recall tasks (R2 interventional). The claim is empirical demonstration of an interventional phenomenon -- staying at R2. 'Demonstrate' here refers to showing experimental results, not claiming a unique mechanism. No overclaim."
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,Subspace activation patching with rank-1 model editing equivalence analysis (connection between 1D activation patches and ROME-style rank-1 weight edits),2,2,0,4,0,"Claim: 'there is an inconsistency between fact editing performance and fact localization'. The method involves subspace activation patching and rank-1 model editing on GPT2-XL for factual recall, both R2 interventional methods. The claim reports an empirical observation about interventional results -- that editing succeeds even when localization evidence is absent. This stays at R2: it describes what interventions show, not a unique mechanism. The paper frames this as 'providing a mechanistic explanation' in the abstract, but the specific claim quoted is about the inconsistency itself (an empirical observation about intervention outcomes). No overclaim."
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Multi-task Distributed Alignment Search (MDAS) evaluated via interchange interventions on Llama2-7B,2,2,0,4,0,"Claim: 'MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations'. The method is MDAS, which uses interchange interventions (activation patching) to learn and evaluate distributed subspace features -- R2 interventional. The claim reports empirical performance on a benchmark ('achieves state-of-the-art') and draws a methodological conclusion ('importance of going beyond neuron-level'). The language stays at the interventional level: 'identify features' in context means finding subspaces with causal effects via intervention, not claiming a unique mechanism. No R3 mechanistic language. No overclaim."
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,Interchange intervention on distributed feature F in Llama2-7B residual stream,2,3,1,4,0,"Claim: 'If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent'. The method is interchange intervention (activation patching), which is R2. The claim uses 'encodes' -- per the codebook decision tree for 'encodes': does the paper provide interventional evidence? Yes. Is the claim about the intervention result or the underlying mechanism? The phrase 'feature F encodes the attribute continent' makes a mechanistic claim about what the feature represents, not merely that intervening changes output. Per the decision tree, 'encodes' used in the mechanistic sense (not 'is decodable from') = R3. Overclaim: +1 (R2 method, R3 claim). The word 'evidence' softens but is not a standard hedge per codebook."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,"Interchange interventions evaluating multiple interpretability methods (PCA, SAE, RLAP, DBM, DAS, MDAS) on RAVEL benchmark with Llama2-7B",2,2,0,5,0,"Claim: 'Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle'. The methods evaluated span R1 (PCA, SAE) and R2 (DAS, MDAS, DBM). The claim itself is a comparative empirical observation about method performance on a benchmark -- purely descriptive of intervention-based evaluation results. No mechanistic language, no uniqueness claims. Stays at R2. No overclaim."
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,Multi-task Distributed Alignment Search (MDAS) evaluated via interchange interventions across layers of Llama2-7B,2,2,0,3,0,"Claim: 'The representations of different attributes gradually disentangle as we move towards later layers'. The method is MDAS with interchange interventions across model layers (R2). The claim uses 'disentangle' which describes an empirical pattern observed in intervention-based evaluation scores (Cause, Iso, Disentangle) across layers. In context, 'disentangle' refers to the measured ability of interventions to isolate attributes, which is an R2-level observation. However, there is mild ambiguity: 'representations disentangle' could be read as a mechanistic claim about what the model does internally (R3). In context of the paper's careful framework, this describes intervention outcomes. Coding as R2 with confidence 3 due to this ambiguity."
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,Multi-task Distributed Alignment Search (MDAS) and DAS evaluated via interchange interventions on Llama2-7B city attributes,2,2,0,4,0,"Claim: 'Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects'. The method is interchange interventions via MDAS/DAS (R2). The claim describes empirical results: certain attribute pairs (country-language, latitude-longitude) resist disentanglement under all tested intervention methods. 'Changing one of these entangled attributes has seemingly unavoidable ripple effects' describes what happens when you intervene -- an R2-level observation about intervention outcomes. 'Seemingly unavoidable' is cautious language but refers to the observed pattern across methods, not uncertainty about the claim. No mechanistic or uniqueness language. No overclaim."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,"Software library implementation supporting interchange interventions, addition interventions, ablation, activation collection, and trainable interventions (DAS) across PyTorch model architectures",2,2,0,4,0,"Claim: 'pyvene supports customizable interventions on a range of different PyTorch modules'. This is a tooling/capability claim about the library, not an empirical claim about model internals. The 'method' is the library itself, which implements interventional techniques (R2). The claim asserts the library's functionality -- it supports interventions (R2-level operations) on various architectures. No mechanistic or uniqueness claims about model internals. This is a software engineering claim verified by demonstration. Coding as R2 since the library enables R2 interventions. No overclaim."
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,Software library implementation with unified API for static and trainable interventions on neural models,2,2,0,4,0,"Claim: 'pyvene provides a unified and extensible framework for performing interventions on neural models'. This is a software capability claim about the library's design. The library implements interventional methods (interchange interventions, ablations, steering -- all R2). The claim is about the framework's properties (unified, extensible), not about model mechanisms. No mechanistic language. Coding as R2 since the claim is about enabling R2-level operations. No overclaim."
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,Causal tracing reproduction (Meng et al. 2022) on GPT2-XL and DAS intervention + linear probe training on Pythia-6.9B for gender localization,2,2,0,4,0,"Claim: 'we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization'. The methods demonstrated are causal tracing (activation patching with noise + restoration, R2) and DAS (trainable interchange intervention, R2), plus linear probing (R1). The claim uses 'illustrate the power' which is about demonstrating library capabilities, not making mechanistic claims about models. 'Causal abstraction' and 'knowledge localization' describe the intervention-based analysis paradigms used (R2). No uniqueness or mechanistic language about what the model computes. No overclaim."
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,Linear probing and representation analysis (cited prior work),1,3,2,4,0,"Claim: 'representations encode rich semantic information'. 'encode' defaults to R3 per decision tree (mechanistic reading). The method cited is 'prior interpretability work' which includes probing (R1). The paper uses this as motivation, not as a direct finding."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,Distributed Alignment Search (DAS) / interchange interventions,2,3,1,4,0,"Claim: 'human-interpretable concepts are encoded linearly'. 'encoded' defaults to R3 (mechanistic reading). The evidence comes from interventions on linear subspaces (DAS, interchange interventions), which are R2. The paper presents this as established finding from prior intervention work."
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,Distributed Alignment Search (DAS) / interchange interventions,2,2,0,4,0,Claim: 'DAS is highly expressive and can effectively localize concepts within model representations'. 'localize' and 'can effectively' are R2 language about causal sufficiency of the method. DAS uses interchange interventions (R2). Claim matches method level.
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,LoReFT / low-rank linear subspace interventions,2,2,0,4,0,Claim: 'a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks'. 'achieve control' is R2 language about causal sufficiency. LoReFT intervenes on representations (R2). Claim-method alignment is appropriate.
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,LoReFT / low-rank linear subspace interventions,2,2,0,5,0,Claim: 'training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions'. 'can induce' is causal sufficiency language (R2). The method is LoReFT which intervenes on representations (R2). Perfect method-claim alignment.
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,Activation patching / causal tracing,2,2,0,5,0,Claim: 'specific components within middle layers show strong causal effects at the last token of the subject'. 'causal effects' is R2 language. Method is activation patching / causal tracing (R2). Perfect alignment.
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,ROME (Rank-One Model Editing),2,2,0,5,0,Claim: 'rank-one model editing methods can successfully insert facts at specific locations'. 'can successfully insert' describes a causal intervention result (R2). ROME editing is R2. Perfect alignment.
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,Jacobian analysis of relation representations,1,1,0,4,0,"Claim: 'linearity of Mamba's representations of factual relations'. This is an observational claim about the structure of representations, based on Jacobian analysis (measuring linearity without intervention). Method is R1 (observational). Claim is R1 (descriptive/associational about representation structure)."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,Literature review / methodological analysis,2,1,0,3,0,"Claim: 'activation patching is a popular mechanistic interpretability technique but has many subtleties'. This is a meta-claim about the method itself, not an empirical finding about model internals. It's a descriptive/associational observation about the field (R1). The paper discusses activation patching (R2) but this claim doesn't make a causal or mechanistic assertion."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching with varied hyperparameters,2,2,0,4,1,Claim: 'varying these hyperparameters could lead to disparate interpretability results'. 'could lead to' is hedged causal language (R2). The method is activation patching experiments with varying setups (R2). The hedge 'could' is present. Claim-method aligned at R2.
2406.11779,2406.11779-01,The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit,body,1,Weight analysis (SVD of QK/OV circuits) and attention pattern analysis,1,3,2,4,0,"Claim: 'The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit'. 'outputs by attending... and copying' is a mechanistic narrative ('the model uses X to do Y') = R3. The primary evidence is weight analysis and SVD decomposition (R1), with some ablation support (R2). The claim is a full mechanistic explanation."
2406.11779,2406.11779-02,EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component,body,1,Singular Value Decomposition (SVD) of weight matrices,1,1,0,5,0,Claim: 'EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component'. This is a purely observational/descriptive claim about the mathematical structure of a weight matrix. SVD is R1 (observational analysis). Claim is R1 (descriptive).
2406.11779,2406.11779-03,Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning,body,1,Zero ablation,2,2,0,5,0,Claim: 'Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning'. 'zero ablating changes' is interventional language (R2). The method is ablation (R2). 'unimportant to model functioning' is a causal sufficiency claim. Perfect alignment.
2406.11779,2406.11779-04,Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds,abstract,3,Quantitative analysis of proof strategies with varying mechanistic understanding,1,1,0,3,1,"Claim: 'Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds'. 'seem to require' is hedged (R1 association). 'leads to' could be R2, but the evidence is correlational analysis across proof strategies. This is a meta-finding about the relationship between proof length and mechanistic understanding, based on observational metrics. Hedge present in 'seem'."
2406.11779,2406.11779-05,Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices,body,1,Analysis of rank-1 approximation errors across weight matrices,1,1,0,4,0,Claim: 'Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices'. This is a descriptive/observational claim about error accumulation in mathematical approximations. Method is weight analysis (R1). Claim is observational (R1).
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,Activation patching / Edge Attribution Patching (EAP) / resample ablation,2,3,1,4,0,Claim: 'partially reverse-engineer the circuit responsible for the Indirect Object Identification task'. 'reverse-engineer' implies complete mechanism discovery (R3). 'the circuit responsible for' uses definite article + 'responsible for' = R3. Method is activation patching and ablation (R2). Note: 'partially' is a qualification but not a hedge on the rung level - they still claim circuit discovery.
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,Resample ablation and layer removal (zero ablation),2,2,0,4,0,Claim: 'Layer 39 is a key bottleneck'. 'key bottleneck' describes causal importance established through ablation. The evidence comes from resample ablation and layer removal experiments (R2). This is a causal sufficiency/necessity claim appropriate for R2.
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,Resample ablation on conv slices,2,3,1,4,0,Claim: 'Convolutions in layer 39 shift names one position forward'. 'shift names' is a functional/mechanistic verb describing what the component does (R3 - 'this component performs X'). Evidence is from resample ablation on conv slices (R2) and cosine similarity analysis (R1). The claim makes a mechanistic attribution about the function of the convolutions.
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,Activation steering / representation substitution on SSM inputs,2,3,1,4,0,"Claim: 'The name entities are stored linearly in Layer 39's SSM'. 'stored linearly' = R3 per decision tree ('stores' = memory/mechanism claim). Evidence is from subtract-and-add representation editing on SSM inputs (R2), where replacing representations with averages changes outputs. This demonstrates causal effect but 'stored' implies a mechanistic storage claim."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,Interchange interventions with learned binary masks,2,2,0,5,0,"Claim: 'SAEs struggle to reach the neuron baseline'. This is an empirical performance comparison claim. The method is interchange interventions with binary feature masks (R2). The claim describes relative performance of methods at a causal task, appropriate for R2."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,Interchange interventions with learned binary masks,2,2,0,5,0,Claim: 'sets of SAE features that separately mediate knowledge of which country a city is in'. 'mediate' is proper R2 language for interchange intervention evidence. Method is interchange interventions (R2). Perfect alignment - this is the calibration paper from the codebook.
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,Eigendecomposition of bilinear MLP weight matrices,1,1,0,4,0,Claim: 'Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling'. 'reveals' is observational language. Eigendecomposition is a weight analysis method (R1). The claim is about what the analysis shows (R1 - descriptive observation).
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,Eigendecomposition of bilinear MLP weight matrices,1,3,2,4,0,Claim: 'top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors'. 'represent' and 'function as' are R3 mechanistic language (functional attribution). Method is weight eigendecomposition (R1). R1 method supporting R3 claim.
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,Adversarial mask construction from eigenvectors,2,2,0,4,0,Claim: 'Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features'. 'cause misclassification' and 'causal importance' are R2 language. Adversarial masks are a form of input-level intervention that demonstrates causal effect (R2). Claim-method aligned.
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,Eigendecomposition with SAE feature dictionaries,1,3,2,4,0,"Claim: 'A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions'. 'circuit computes... via AND-gate-like interactions' is full mechanistic narrative (R3). 'the circuit' = R3 per decision tree. Method is eigendecomposition of bilinear tensor with SAE features (R1 - weight-based analysis, no interventions). R1 method supporting R3 claim."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,Correlation analysis between SAE features and eigenvector approximations,1,1,0,5,0,Claim: 'Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values'. 'well-correlated' is explicitly R1 associational language. Method is correlation analysis (R1). Perfect alignment.
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,Activation patching (residual stream patching across layers),2,3,1,4,0,"Claim: 'the output language is encoded in the latent at an earlier layer than the concept to be translated'. 'encoded' defaults to R3 per decision tree. The paper provides interventional evidence (activation patching, R2), and the claim is about the mechanism (how information is structured across layers). Per the decision tree: interventional evidence + mechanism claim = R3."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,Activation patching (swapping residual stream activations),2,2,0,5,0,Claim: 'we can change the concept without changing the language and vice versa through activation patching alone'. 'can change... without changing' is interventional/causal sufficiency language (R2). Method is activation patching (R2). Perfect alignment - describes what happens under intervention.
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,Activation patching with mean representations across languages,2,2,0,5,0,Claim: 'patching with the mean representation of a concept across different languages improves translation'. 'patching... improves' is R2 language describing the result of an intervention. Method is activation patching (R2). Perfect alignment.
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,Activation patching (multiple patching experiments),2,3,1,4,0,Claim: 'results provide evidence for the existence of language-agnostic concept representations'. 'existence of language-agnostic concept representations' is a R3 mechanistic claim about what the model represents internally. Method is activation patching (R2). The claim goes beyond causal effects to assert a structural property of representations.
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,Activation patching and ablation on IOI circuit,2,2,0,4,1,Claim: 'circuits within LLMs may be more flexible and general than previously recognized'. 'may be' is an explicit hedge. The claim is about causal properties of circuits discovered through patching (R2). 'flexible and general' describes behavioral properties observed under intervention variations. Hedged R2 claim.
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,Activation patching and logit difference analysis on prompt variants,2,3,1,4,0,Claim: 'the circuit generalizes surprisingly well reusing all of its components and mechanisms'. 'the circuit' = R3 per decision tree (definite article implies uniqueness). 'its components and mechanisms' is mechanistic language. 'reusing' implies understanding of how the model works internally. Method is activation patching (R2).
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,Activation patching and attention pattern analysis,2,3,1,4,0,Claim: 'we discover a mechanism that explains this which we term S2 Hacking'. 'discover a mechanism' and 'explains' are R3 language (mechanistic discovery). 'the mechanism' with naming = R3. Method is activation patching and attention analysis (R2). The claim asserts understanding of the underlying mechanism.
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,Circuit discovery via activation patching and ablation,2,3,1,5,0,Claim: 'implement algorithms responsible for performing specific tasks'. 'implement algorithms' is R3 (functional attribution - 'performs'). 'responsible for' = R3 per decision tree. Method is activation patching and circuit analysis (R2). Classic R2->R3 overclaim pattern.
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,"Benchmark comparison of steering methods (prompting, finetuning, representation-based)",2,2,0,5,0,"Claim: 'prompting outperforms all existing methods followed by finetuning'. This is an empirical performance comparison on a steering benchmark. Steering involves interventions on model behavior (R2). The claim describes relative performance, appropriate for R2."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,Benchmark evaluation of SAE steering and concept detection,2,2,0,5,0,Claim: 'SAEs are not competitive'. This is an empirical finding from benchmarking SAEs on both concept detection (R1) and steering (R2) tasks. The claim describes performance relative to other methods. R2 is appropriate as it includes steering evaluation.
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,Concept detection evaluation (probing / classification),1,1,0,5,0,"Claim: 'representation-based methods such as difference-in-means perform the best'. This specifically refers to concept detection performance, which is a classification/probing task (R1). DiffMean is a representation analysis method. The claim is about detection accuracy, an observational metric."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,Sparse Autoencoder (SAE) feature attribution with cross-model reconstruction,1,3,2,4,0,Method is SAE-based dictionary learning applied to frozen model activations (R1 observational). Claim uses 'interpret the internal activations' which implies mechanistic understanding of what models represent (R3). No interventions are performed on the models themselves.
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,Sparse Autoencoder (SAE) feature attribution with cross-model reconstruction,1,1,0,4,0,"Method is SAE dictionary learning (R1). Claim that 'the learned dictionary captures common factors of variation' is a correlational/associational claim about what the SAE discovers. 'Captures' here describes the SAE's learned representation, not a mechanistic claim about the models. This is appropriately R1."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,Sparse Autoencoder (SAE) feature attribution with cross-model reconstruction,1,1,0,4,0,"Method is SAE dictionary learning (R1). 'Discover semantically coherent and important universal concepts' describes what the SAE finds via observational analysis. 'Discover' here refers to identifying patterns in activations, which is associational. No mechanistic claim about what models do or how they work."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,HyperDAS (hypernetwork-based distributed interchange intervention),2,2,0,5,0,"Method is interchange intervention (R2). Claim is about performance on a benchmark for 'disentangling concepts in hidden states' - this is a causal claim about manipulating representations, matched to the interventional method. Appropriately R2."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,HyperDAS (hypernetwork-based distributed interchange intervention),2,2,0,4,0,Method is interchange intervention (R2). 'Features that mediate concepts and enable predictable manipulation' - 'mediate' is proper R2 causal language. 'Enable predictable manipulation' describes the interventional capability. Well-calibrated to method.
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,HyperDAS (hypernetwork-based distributed interchange intervention),2,3,1,4,0,"Method is interchange intervention (R2). 'Automatically locates the token-positions of the residual stream that a concept is realized in' - 'is realized in' implies the concept has a definite location/encoding in the model, which is a mechanistic R3 claim about how the model represents information. Goes beyond causal mediation to imply specific representational structure."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,Interchange interventions (DAS),2,2,0,5,0,Method is interchange interventions (R2). 'Identify neural representations that are causal mediators of high-level concepts' - 'causal mediators' is textbook R2 language. Perfectly calibrated.
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,HyperDAS (hypernetwork-based distributed interchange intervention),2,3,1,4,0,"Method is interchange intervention (R2). 'Previously unknown to store attributes' - 'store' is mechanistic R3 language per the codebook decision tree (storage/memory language implies R3). The intervention shows causal mediation, not storage mechanism."
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,Activation patching (CLAP - Causal Layer Attribution via Activation Patching),2,2,0,4,0,"Method is activation patching (R2). 'Patching the first feedforward layer recovered 56%... demonstrating that associative knowledge is distributed' - 'distributed' describes how the causal effect is spread, which is an interventional finding. However, 'knowledge is distributed' borders on R3 (how knowledge is organized), but the specific phrasing ties to the patching result. Coding as R2 since the claim directly reports an interventional result."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation patching (CLAP - Causal Layer Attribution via Activation Patching),2,2,0,4,0,Method is activation patching (R2). 'Patching the final output layer completely restored accuracy indicating that definitional knowledge is localised' - reports an interventional result. 'Localised' describes the causal effect pattern. Still tied to the patching evidence.
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Activation patching (CLAP - Causal Layer Attribution via Activation Patching),2,3,1,4,0,"Method is activation patching (R2). 'Factual knowledge is more localized and associative knowledge depends on distributed representations' - this is a general mechanistic claim about how knowledge IS organized in the model, going beyond describing interventional results. Per the codebook, claims about how the model stores/organizes knowledge are R3. This generalizes from specific patching results to a theory of knowledge representation."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,Interchange interventions and causal abstraction (DAS-based desiderata patching),2,3,1,5,0,Method is interchange interventions / causal abstraction (R2). 'LM binds each character-object-state triple together by co-locating their reference information' - 'binds' and 'co-locating' are mechanistic functional verbs describing what the model DOES (R3). This is a mechanistic narrative about the model's algorithm.
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,Interchange interventions and causal abstraction (DAS-based desiderata patching),2,3,1,5,0,Method is interchange interventions (R2). 'Lookback mechanism which enables the LM to recall important information' - 'mechanism' with functional description of what it does (recall) is R3. 'The LM to recall' is a mechanistic narrative about how the model works.
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,Interchange interventions and causal abstraction (DAS-based desiderata patching),2,3,1,5,0,Method is interchange interventions (R2). 'The binding lookback retrieves the correct state OI' - functional attribution with 'retrieves' describing what a specific mechanism does. This is a mechanistic claim (R3) about the model's internal algorithm.
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,Interchange interventions and causal abstraction (DAS-based desiderata patching),2,3,1,5,0,"Method is interchange interventions (R2). 'Reverse-engineering ToM reasoning in LMs' - 'reverse-engineering' implies complete mechanistic understanding, a canonical R3 claim per calibration (cf. grokking paper). Claims to have uncovered the algorithm the model uses."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,5,0,Method is steering vectors (R2 intervention). 'All seven prompt types produce a net positive steering effect but exhibit high variance' - reports the result of an intervention. Appropriately R2 causal language about the effect of steering.
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,Contrastive Activation Addition (CAA) steering vectors with cosine similarity analysis,1,1,0,4,0,"The specific claim about cosine similarity predicting effectiveness is based on correlational analysis of activation geometry (R1). 'Predicts' is R1 language per the codebook. The underlying steering method is R2, but this particular claim is about the correlation between a geometric property and steering success."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Contrastive Activation Addition (CAA) steering vectors with activation geometry analysis,2,3,1,3,0,Method involves steering (R2) and geometric analysis. 'Vector steering is unreliable when the target behavior is not represented by a coherent direction' - 'is not represented by' makes a claim about how behaviors ARE represented in the model (R3 per 'represents' decision tree). This goes beyond the interventional result to claim something about the model's representational structure.
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,5,0,"Method is steering vectors (R2). 'Steering effectively controls the targeted summary properties' - 'controls' with interventional evidence. Per the codebook decision tree for 'controls': evidence is from intervention, and the paper does NOT claim unique control. Thus R2 (causal sufficiency, not uniqueness)."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,5,0,Method is steering vectors (R2). 'High steering strengths consistently degrade both intrinsic and extrinsic text quality' - reports the empirical effect of an intervention. Appropriately R2.
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors combined with prompting,2,2,0,5,0,Method is steering vectors + prompting (R2). 'Combining steering and prompting yields the strongest control' - empirical comparison of intervention strategies. Appropriately R2.
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,HYPERSTEER hypernetwork-based activation steering,2,2,0,5,0,Method is activation steering via hypernetwork (R2). 'Scaling HYPERSTEER... exceeds the performance of state-of-the-art activation steering methods' - empirical performance comparison of intervention methods. Appropriately R2.
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,HYPERSTEER hypernetwork-based activation steering,2,2,0,5,0,Method is activation steering via hypernetwork (R2). 'HYPERSTEER performs on par with steering-via-prompting' - empirical performance comparison. Appropriately R2.
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,HYPERSTEER cross-attention hypernetwork-based activation steering,2,2,0,5,0,Method is activation steering (R2). 'Cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline' - empirical performance comparison of intervention methods. Appropriately R2.
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,HYPERSTEER hypernetwork-based activation steering,2,2,0,5,0,Method is activation steering (R2). 'As training data increases HYPERSTEER becomes much more economical' - empirical observation about scaling properties of the intervention method. Appropriately R2.
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,HYPERSTEER cross-attention hypernetwork with geometric analysis of steering vectors,1,1,0,4,0,"This specific claim about 'residual inter-concept similarity' being weakened by additional conditioning is about the geometric properties of the generated steering vectors, analyzed via cosine similarity (R1 observational). The claim describes correlational properties of the learned representations rather than an intervention result."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,Steering vectors (Difference of Means) with attribution patching,2,3,1,4,0,"Method is steering vectors (R2). 'These behaviors are mediated by linear directions' is R2 ('mediated'). But 'can be controlled using steering vectors' with 'controlled' - per codebook decision tree for 'controls': evidence is from intervention, paper doesn't claim unique control, so that part is R2. However, the full claim combines mediation with the representational claim that behaviors exist as 'linear directions in the model's activation space', which is a claim about how the model represents these behaviors (R3). The representational claim goes beyond causal mediation."
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,Steering vectors (Difference of Means),2,2,0,4,0,Method is steering (R2). 'Positive steering increases behaviors... negative steering suppresses them confirming the causal influence' - 'causal influence' is R2 language. Reports intervention results directly. Appropriately R2.
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,Steering vectors (Difference of Means),2,3,1,4,0,"Method is steering (R2). 'Thinking LLMs encode these reasoning mechanisms as linear directions' - 'encode' defaults to R3 per the codebook decision tree (not in decodability context, and the interventional evidence supports mediation not encoding mechanism). 'Reasoning mechanisms' further implies mechanistic understanding."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,Steering vectors (Difference of Means) with attribution patching,2,3,1,4,0,Method is steering + attribution patching (R2). 'Reasoning behaviors... can be isolated to specific directions in the model's activation space enabling precise control' - 'isolated to specific directions' is a representational/mechanistic claim about where behaviors live in the model (R3). Goes beyond showing causal effect to claiming specific representational structure.
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,Steering vectors (Difference of Means) with attribution patching,2,3,1,4,0,Method is steering (R2). 'DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process' - 'distinct mechanisms' is a strong R3 mechanistic claim about the model's internal workings. Steering vectors show causal effects but don't establish the existence of distinct mechanisms.
2507.08802,2507.08802-01,any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative,abstract,3,Distributed Alignment Search (DAS) with non-linear alignment maps (interchange interventions),2,2,0,4,0,Method is interchange interventions with non-linear maps (R2). 'Any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial' - this is a methodological/theoretical claim about the limits of the interventional framework itself. It describes what interchange interventions can achieve (R2 scope). Not a mechanistic claim about a specific model.
2507.08802,2507.08802-02,it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task,abstract,3,Distributed Alignment Search (DAS) with non-linear alignment maps (interchange interventions),2,2,0,4,0,Method is interchange interventions with non-linear maps (R2). 'It is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task' - empirical finding about the intervention methodology. Reports what can be achieved with interchange interventions. R2.
2507.08802,2507.08802-03,randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task,abstract,3,Distributed Alignment Search (DAS) with non-linear alignment maps on randomly initialised models,2,2,0,5,0,Method is interchange interventions (R2). 'Randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the IOI task' - directly reports the result of interchange interventions. Perfectly R2.
2507.08802,2507.08802-04,causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information,abstract,3,Distributed Alignment Search (DAS) with non-linear alignment maps (theoretical analysis + empirical),2,2,0,4,0,Method is interchange interventions + theoretical analysis (R2). 'Causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information' - this is a meta-claim about the limitations of the R2 methodology. It's a conclusion drawn from interventional experiments about the intervention framework itself. R2.
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,Activation patching (de-noising variant / causal mediation analysis),2,3,1,4,0,"Method is activation patching (R2). 'Early MLP layers attend not only to the syntactic structure but also process its semantic content' - 'process' is a functional verb (R3) attributing a specific computational role to the MLP layers. This is a mechanistic claim about what these layers DO, going beyond the causal mediation result."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,Activation patching (de-noising variant / causal mediation analysis),2,3,1,4,0,Method is activation patching (R2). 'These layers transform persona tokens into richer representations which are then used by middle MHA layers' - 'transform' and 'used by' are functional/mechanistic verbs describing a computational pipeline (R3). This is a mechanistic narrative about how the model processes information: 'the model uses X to do Y' pattern.
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,Activation patching with attention pattern analysis,2,1,0,4,0,"The claim about identifying attention heads 'that disproportionately attend to racial and color-based identities' is primarily based on attention pattern analysis (R1 observational), even though the broader paper uses activation patching. 'Attend to' describes attention weight patterns, which is observational. 'Identify' here means finding them via observation. The method directly supporting this specific claim is attention visualization (R1), and the claim language ('attend to') is R1."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,Theoretical/philosophical analysis (causal abstraction framework),1,2,1,2,0,"This is a theoretical/philosophical paper with no empirical experiments on model internals. The method is philosophical argumentation grounded in formal causal theory (R1 - observational/theoretical). The claim that causal abstraction 'provides a fruitful lens' uses sufficiency language (R2), though in a meta-theoretical rather than empirical sense. Low confidence because the rung framework is designed for empirical MI claims, not philosophical arguments."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,Theoretical/philosophical analysis (causal abstraction framework),1,1,0,2,0,Theoretical paper. 'Offer an account' is descriptive/presentational language (R1). The method is philosophical analysis (R1). No overclaim since claim_rung equals method_rung. Low confidence because rung framework targets empirical MI claims.
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,Correlation analysis (Pearson correlation between RelP/LRP-based attribution scores and activation patching scores),1,1,0,4,0,"RelP is a propagation-based attribution method that approximates activation patching without actually performing interventions on the model. The evidence is Pearson correlation coefficients between RelP scores and activation patching scores (e.g., Table 5). The claim uses 'more accurately approximates' which is a comparative empirical claim about approximation quality, not a causal or mechanistic claim about model internals. The linguistic markers ('approximates', 'Pearson correlation') are associational. Method rung is R1 because RelP itself is a gradient/propagation-based attribution method (no intervention on the model); the paper uses correlation analysis to benchmark it. Claim rung is R1 because the claim is about statistical alignment between two methods, not about causal effects or mechanisms. No overclaim (gap = 0). Minor ambiguity: one could argue the ground truth (activation patching) is R2, but the claim itself is about approximation fidelity, not about causal effects."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,Correlation analysis (Pearson correlation between attribution/relevance patching scores and activation patching scores on GPT-2 Large MLP outputs),1,1,0,5,0,"This is a specific empirical data point from Table 5: AtP achieves PCC=0.006 and RelP achieves PCC=0.956 for MLP outputs in GPT-2 Large. The method is correlation analysis comparing two attribution methods against the activation patching ground truth. The claim is purely quantitative and associational - it reports Pearson correlation values. No causal or mechanistic language is used. Method rung is R1 (correlation analysis, no intervention). Claim rung is R1 (reporting statistical association metrics). No overclaim (gap = 0). Very clear case."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,"Faithfulness and completeness evaluation of sparse feature circuits identified by RelP vs. Integrated Gradients, using mean ablation as baseline",2,1,0,4,0,"The claim compares RelP and IG for identifying sparse feature circuits, evaluated using faithfulness (which involves mean ablation - an interventional method, R2) and completeness metrics. The faithfulness metric is defined as L(C)-L(empty)/L(M)-L(empty), where L(empty) is the fully mean-ablated model - this is an interventional evaluation. However, the claim itself is a comparative methodological claim: 'achieves comparable faithfulness... without the extra computational cost.' This is about method performance comparison, not about model internals. The linguistic markers are comparative/empirical ('achieves comparable', 'without the extra computational cost'). Method rung is R2 because the evaluation involves ablation (mean ablation for faithfulness measurement). Claim rung is R1 because the claim is a comparative statement about two methods' performance metrics, not a causal claim about model internals. No overclaim (gap = 0, since claim rung <= method rung). Minor ambiguity about whether this is fundamentally a method comparison claim or a claim about model internals."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,Sparse feature circuit discovery via RelP/IG with faithfulness evaluation using mean ablation,2,2,0,4,0,"The claim states 'small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance.' The method involves discovering sparse feature circuits using attribution methods (RelP/IG) and evaluating them with faithfulness metrics that use mean ablation (R2). The faithfulness metric measures how much of the model's original performance is captured by the circuit when only the circuit is active and everything else is mean-ablated. This is interventional evidence (R2). The claim uses 'explain most of the model's behavior' and 'account for the majority of performance' - this is causal language about sufficiency ('is sufficient for'), which maps to R2. The claim does not use uniqueness language ('THE circuit') or mechanistic functional verbs ('performs', 'computes'). It says features 'account for' performance, which is closer to 'is sufficient for' (R2) than to 'performs' or 'encodes' (R3). No overclaim (gap = 0). Note: this claim replicates findings from Marks et al. (2025), not a novel finding of this paper."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,Relevance Patching (LRP-based propagation coefficients replacing gradients in attribution patching),1,2,1,3,0,"The claim states 'RelP enables more faithful localization of influential components in large models.' RelP itself is a propagation-based attribution method (R1) - it does not intervene on the model but computes attribution scores via a modified backward pass. The claim uses 'localization of influential components' which implies identifying causally relevant components. 'Influential' suggests causal influence (R2 language), and 'localization' in the mechanistic interpretability context typically means identifying which components causally contribute to behavior. However, RelP does not itself establish causal influence - it approximates activation patching scores without running the actual interventions. The claim bridges from R1 method to R2 claim language: 'influential' implies causal effect, and 'faithful localization' implies the attributions reliably identify causally important components. This represents a +1 overclaim. The method (R1: propagation-based attribution) is used to make a claim about causal influence (R2: 'influential components'). Confidence is moderate because 'influential' could be read as simply meaning 'high-attribution' rather than 'causally important', though the paper's framing clearly positions this as approximating causal (activation patching) results."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,"Steering vectors (trained via RL), logit-lens projection, token-level probability analysis",2,3,1,4,0,"The claim states 'the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token.' The method involves steering vectors (R2: additive interventions on the residual stream) combined with logit-lens analysis (R1: projecting through unembedding matrix) and behavioral evaluation (prefixing tokens to test causality). Steering vectors are interventional (R2) - they add a vector to the residual stream and measure output changes. The claim uses mechanistic/functional language: 'acts like a token-substitution bias' is a mechanistic narrative describing HOW the vector operates (not just THAT it has an effect). 'Acts like' assigns a functional role ('token-substitution bias') to the component, which is R3 language per the codebook ('this head DOES X' = functional attribution, 'the model uses X to do Y' = mechanistic narrative). The evidence shows the vector boosts 'To'/'Step' tokens at the first position, and prefixing these tokens recovers ~75% of the gain. This is interventional evidence (R2) supporting a mechanistic interpretation (R3). Overclaim gap: +1 (R2 method, R3 claim). This parallels the IOI calibration anchor where functional verbs ('moves', 'performs') on top of patching evidence yield +1 gaps."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,"Steering vectors with circuit-style ablation analysis (Skip-Attn, Skip-Layer, Steer-Q/K/V projection patching)",2,3,1,4,0,"The claim states 'the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words.' The methods include steering vectors (R2) and circuit-style ablation analysis where the authors systematically insert/omit the steering vector at specific points (Skip-Attn, Skip-Layer, Steer-Q/K/V projections) to localize which submodules process the steering signal. This is interventional (R2). The claim uses mechanistic language: 'operates through the MLP and unembedding' describes the pathway/mechanism by which the vector achieves its effect. 'Preferentially up-weighting process words' assigns a functional description to the mechanism. Per the codebook, 'the model uses X to do Y' is R3 mechanistic narrative. The claim describes not just that the vector has an effect, but HOW it achieves its effect (through specific submodules). The ablation evidence (R2) supports identifying which components mediate the effect, but the claim goes further to describe the mechanism ('operates through', 'up-weighting'). This parallels the 'controls'/'is responsible for' decision tree: evidence from intervention (yes) + claim about the mechanism (not just the result) = R3. Overclaim gap: +1 (R2 method, R3 claim)."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,Cross-model transfer of steering vectors with performance evaluation,2,2,0,4,0,"The claim states 'steering vectors transfer to other models.' The method involves training steering vectors (additive interventions, R2) on a donor model and inserting them into a recipient model from the same family, then measuring performance gains (Table 1). This is interventional evidence: the vectors are applied to new models and their causal effect on performance is measured. The claim uses 'transfer' which describes an empirical observation about the causal effect of the intervention generalizing across models. This is appropriate R2 language - it describes what happens when you intervene (apply vectors to a new model), not a mechanistic claim about why or how the transfer works. The paper does note that 'the directions associated with improved math performance are largely preserved under fine-tuning and instruction tuning' which adds a mild mechanistic flavor, but the core claim 'transfer to other models' is an empirical causal claim. No overclaim (gap = 0). The claim is well-calibrated: it describes an interventional result using interventional language."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,SAE feature attribution (TopKReLU sparse autoencoders trained on Qwen2.5-3B-Instruct MLP outputs),1,3,2,4,0,"The claim states 'SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features.' The method is SAE feature attribution (R1) - training sparse autoencoders and observing which features activate on what inputs. No interventions are performed on the model. The claim uses 'explaining... atomic features' and 'clarify model behavior' which implies that the SAE features provide mechanistic understanding of what the model is doing internally. 'Explaining' and 'clarify model behavior' are mechanistic claims about understanding internal representations. Per the codebook, SAE feature attribution is R1, and claims about what features 'explain' or 'represent' default to R3 unless the context clearly means 'is decodable from' (R1). Here, 'explaining single-meaning atomic features' implies the features have genuine semantic meaning inside the model (a representational claim), not merely that they correlate with inputs. This is R3 language. Overclaim gap: +2 (R1 method, R3 claim). This parallels the SAE -> 'represents' pattern in the codebook overclaim table."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,"Concept contrastive query pairs with delta frequency metrics (L0,t and ICDF) for SAE configuration selection",1,1,0,4,0,"The claim states 'Safe-SAIL systematically identifies SAE with best concept-specific interpretability.' The method involves training SAEs with different sparsity levels and evaluating them using concept contrastive query pairs - pairs of queries that differ only in the presence/absence of a safety concept. The metrics (L0,t, ICDF) measure how many neurons differentiate between concept and de-concept queries. This is observational/correlational (R1) - it measures activation patterns without intervening on the model. The claim is a methodological claim about the framework's ability to select optimal SAE configurations. 'Systematically identifies SAE with best concept-specific interpretability' describes a method evaluation/selection process, not a claim about model internals. The linguistic markers are empirical/evaluative ('systematically identifies', 'best'), not causal or mechanistic. Method rung is R1 (activation pattern analysis). Claim rung is R1 (empirical method evaluation claim). No overclaim (gap = 0)."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,"SAE feature attribution with concept contrastive query pairs, neuron activation analysis, and automated explanation generation",1,3,2,4,0,"The claim states 'we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors.' The method is SAE feature attribution (R1) - identifying which SAE neurons activate differentially on safety-related content using concept contrastive query pairs. No interventions on the model are performed. The claim uses 'features that effectively capture high-risk behaviors.' The word 'capture' implies the features represent or encode these behaviors internally - this is R3 mechanistic language per the codebook ('represents' decision tree: no interventional evidence, and the context does not clearly mean 'is decodable from', so default to R3). Additionally, 'safety-relevant features' implies these are genuine internal representations of safety concepts, not merely correlational patterns. The paper's framing throughout (Section 4: 'how LLMs encode specific real-world risk entities') reinforces the mechanistic reading. Overclaim gap: +2 (R1 method, R3 claim). This parallels the SAE -> 'represents' overclaim pattern in the codebook. The features are identified by observing activation patterns (R1) but claimed to 'capture' (represent/encode) behaviors (R3)."
2510.01070,2510.01070-01,Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor,abstract,3,Logit lens and sparse autoencoders applied to model activations,1,2,1,4,0,"Claim: 'Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor.' The white-box methods (logit lens, SAE feature attribution, activation-token embedding similarity) are observational/associational (R1): they inspect internal activations without intervening on the model. The claim uses 'increase the success rate', which is an empirical performance claim about the methods' effectiveness in an auditing pipeline. While 'increase' could be read as purely empirical (R1), the framing implies a causal relationship between providing white-box information and improved auditor performance, placing the claim at R2. The paper does run controlled comparisons (base model vs. fine-tuned model), but the core white-box methods themselves are R1. Gap: +1."
2510.01070,2510.01070-02,secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly,results,1,Logit lens and SAE feature attribution on model activations,1,3,2,4,0,"Claim: 'secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly.' The methods are logit lens and SAE feature attribution (R1), which are observational inspections of activations. The claim uses 'extracted from the model's internal states', which implies the secret knowledge is stored/encoded in these states (R3 mechanistic claim about representation). 'Extracted' goes beyond observing correlations; it implies the internal states contain and represent the secret, a mechanistic attribution. The paper does show controlled experiments where base model activations lack the signal, strengthening the claim, but the method itself remains R1. Gap: +2."
2510.01070,2510.01070-03,Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal,body,1,Logit lens and SAE feature attribution on model activations,1,3,2,5,0,"Claim: 'Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal.' The methods are logit lens tokens and SAE feature descriptions/tokens (R1), which are purely observational inspections of activations at intermediate layers. The claim uses 'reveal' twice, implying the model internally represents the secret knowledge even while externally concealing it. This is a mechanistic claim (R3) about what the model internally represents and how it manages information flow (concealing externally while representing internally). The language 'reveal strong signals corresponding to the female gender' implies the model encodes gender information in its activations. Gap: +2."
2510.01070,2510.01070-04,Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it,results,2,Downstream task evaluation comparing fine-tuned models to baselines,1,3,2,4,0,"Claim: 'Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it.' The method is behavioral evaluation: comparing fine-tuned model performance on downstream tasks against base model and in-context baselines (R1 observational). The claim uses 'internalize' and 'are aware of it', both strong mechanistic terms (R3). 'Internalize' implies the knowledge is stored in model parameters in a way that constitutes genuine internal representation. 'Aware of it' attributes mental-state-like properties to the model. The evidence (downstream task performance matching in-context baselines) establishes correlation between training and behavior, not the mechanistic claim that knowledge is 'internalized' in any specific representational sense. Gap: +2."
2510.01070,2510.01070-05,Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques,body,1,"Logit lens and SAE feature attribution (motivating premise, not directly evidenced)",1,3,2,4,0,"Claim: 'Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques.' This is a theoretical premise motivating the white-box approach. The claim asserts that models 'must internally represent' secret knowledge (R3 mechanistic necessity claim) and that this representation is extractable via interpretability tools. The methods used to support this are logit lens and SAE attribution (R1). The 'must internally represent' language asserts a mechanistic fact about how the model processes information, and 'extract' implies the information is encoded in a recoverable form. This is a strong R3 claim grounded in a reasonable but unproven assumption about model internals. Gap: +2."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,Interchange interventions (counterfactual patching) on residual stream activations,2,3,1,4,0,"Claim: 'LMs implement such retrieval via a positional mechanism.' The method is interchange interventions (R2): swapping residual stream activations between original and counterfactual inputs and measuring effects on next-token predictions. The claim uses 'implement' (R3 functional/mechanistic verb) asserting that LMs use a specific computational mechanism for entity retrieval. 'Implement' implies the model performs a specific algorithm, going beyond what interventional evidence alone establishes. Interchange interventions show causal effects of patched representations on outputs, but 'implement' implies a complete mechanistic account of how the computation works. Gap: +1."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,Interchange interventions (counterfactual patching) with causal abstraction modeling,2,3,1,4,0,"Claim: 'LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism.' The methods are interchange interventions and causal abstraction (R2): the paper designs counterfactual input pairs that distinguish three mechanisms and measures intervention effects. The claim uses 'supplement' in a mechanistic narrative about how three distinct mechanisms interact inside the model. The language implies the model has identifiable, named computational strategies that it deploys in combination. This is a mechanistic account (R3) of internal computation, not merely a description of causal effects. The interventional evidence supports that different causal pathways exist, but naming them as distinct 'mechanisms' that the model 'supplements' involves mechanistic interpretation beyond the interventional data. Gap: +1."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,Causal abstraction model trained on interchange intervention data,2,2,0,5,0,"Claim: 'causal model combining all three mechanisms that estimates next token distributions with 95% agreement.' The method is a learned causal model (Equation 2) trained on data from interchange interventions (R2). The claim reports the empirical fit of this causal model to the LM's behavior under intervention, stated as '95% agreement' (Jensen-Shannon Similarity). This is an appropriate R2 claim: it describes the predictive accuracy of a causal model under interventional conditions, without overclaiming about the underlying mechanism being the unique or true computation. The language 'estimates... with 95% agreement' is properly calibrated to the method. No gap."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,Interchange interventions (counterfactual patching) with causal abstraction,2,3,1,4,0,"Claim: 'how LMs bind and retrieve entities in-context.' This is an abstract-level framing claim asserting the paper explains the mechanism by which LMs perform entity binding and retrieval. The method is interchange interventions with causal abstraction (R2). The claim language 'how LMs bind and retrieve' implies a mechanistic explanation (R3): it asserts the paper reveals the computational process the model uses, not merely causal effects of interventions. 'Bind' and 'retrieve' are functional verbs attributing specific computational operations to the model. The paper does qualify with 'a more complete picture' in the full abstract, which slightly hedges the completeness claim, but the core framing is mechanistic. Gap: +1."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,Causal tracing (activation patching with Gaussian noise corruption and selective restoration),2,3,1,4,0,"Method: FCCT uses causal tracing (corrupt input with Gaussian noise, selectively restore activations, measure Recovery Rate). This is activation patching = Rung 2. Claim: 'play a critical role in aggregating cross-modal information' uses functional attribution language ('aggregating') assigning a specific mechanistic role to MHSAs. Per the codebook, 'this head DOES X' functional attribution is R3. The claim describes what MHSAs DO (aggregate cross-modal information), not merely that they have a causal effect. Stated as established fact with no hedge. Gap: +1 (R2 method, R3 claim). Parallels the ROME calibration case where causal tracing supports causal mediation claims but not mechanistic role assignments."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,Causal tracing (activation patching with Gaussian noise corruption and selective restoration),2,3,1,5,0,"Method: Same FCCT causal tracing framework (Rung 2). Claim: 'storage and transfer of visual object representations' uses 'storage' which is a Rung 3 mechanistic term per the codebook decision tree for 'stores/encodes'. The paper provides interventional evidence, but the claim is about the underlying mechanism (how FFNs store and transfer representations), not just about the intervention result. Per the decision tree: interventional evidence + mechanism claim = R3. Additionally 'hierarchical progression' describes a structured mechanistic narrative about how FFNs function across layers. No hedge present. Gap: +1 (R2 method, R3 claim). Directly parallels the ROME calibration anchor where 'storing factual associations' was coded R3 despite causal tracing (R2) evidence."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,Activation injection (intermediate representation injection guided by causal tracing),2,3,1,4,0,"Method: IRI injects mid-layer activations into later layers, scaled by Recovery Rates from causal tracing. This is an interventional technique (steering/injection) = Rung 2. Claim: 'reinforces visual object information flow' uses mechanistic narrative language. 'Information flow' implies a specific mechanistic model of how the network processes data, and 'reinforces' attributes a functional role to the intervention. Per the codebook, 'the model uses X to do Y' mechanistic narrative patterns are R3. The claim implies the model has an information flow that IRI reinforces, which is a mechanistic claim about how the model works rather than just stating an intervention changed outputs. No hedge. Gap: +1 (R2 method, R3 claim)."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,Sparse Autoencoder (SAE) feature attribution with downstream probing evaluation,1,1,0,5,0,"Method: The paper trains equivariant SAEs on synthetic image autoencoder activations and evaluates features via linear probing accuracy on downstream tasks. SAE feature extraction + probing = observational (R1). No interventions on model internals. Claim: 'incorporating group symmetries into the SAEs yields features more useful in downstream tasks' is an empirical comparative performance claim. 'More useful' is measured by probing accuracy, which is a correlational/predictive metric. No mechanistic language (no 'encodes', 'represents', 'controls'). This parallels the SAE Eval and Gemini Probes calibration anchors where probing-based claims are R1. No hedge. Gap: 0 (R1 method, R1 claim). Well-calibrated."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,Linear regression fitting (variance explained / R-squared) on activation transformations,1,1,0,4,0,"Method: The paper fits a single linear transformation matrix to predict how activations change under image rotation, measuring explained variance (R^2). This is linear regression / correlation analysis on observed activations with no intervention = R1. Claim: 'a single matrix can explain how their activations transform as the images are rotated' uses 'explain' in the statistical variance-explained sense (R^2), not in a mechanistic 'this IS the mechanism' sense. The claim is about predictive fit of a linear model to observed activation patterns, which is correlational/associational = R1. Per the codebook decision tree for polysemous terms: 'explain' here means 'predicts/accounts for variance' rather than 'this is the causal mechanism'. No hedge. Gap: 0 (R1 method, R1 claim). Confidence 4 not 5 because 'explain' is somewhat polysemous, but context strongly favors the statistical reading."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,Linear probing comparison between adaptive and regular SAE features,1,1,0,5,0,"Method: Linear probing on SAE features to compare downstream task accuracy between adaptive (equivariant) and regular SAEs. Probing = observational measurement of information in representations = R1. Claim: 'adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs' is an empirical comparative claim about probing accuracy. 'Discover features' describes SAE training output, and 'superior probing performance' is a measurable metric. No mechanistic attribution (no 'encodes', 'represents mechanism', 'controls'). The claim stays within what the evidence supports: features extracted by one method probe better than those of another. Parallels the SAE Eval calibration anchor where performance comparisons are R1. No hedge. Gap: 0 (R1 method, R1 claim). Well-calibrated."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,Conceptual analysis with illustrative case studies (behavioral experiments on model responses),1,3,2,3,0,"Claim: 'The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals.' The methods are conceptual analysis supplemented by behavioral case studies: prompting models, analyzing rollouts, comparing plausibility ratings (R1 observational). The claim asserts that distinguishing deception types 'requires making claims about a model's internal beliefs and goals', which is a claim about internal mental/computational states (R3). The language 'internal beliefs and goals' attributes representational states to the model and claims these are necessary for classification. This is a conceptual/theoretical claim rather than an empirical one, making confidence somewhat lower. The paper provides compelling arguments but the claim itself is about model internals. Gap: +2. Confidence reduced to 3 because this is a conceptual argument, not a direct empirical claim about model internals."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,Conceptual analysis of activation probes and internal classifiers,1,3,2,3,0,"Claim: 'What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance.' This is a conceptual/analytical claim about the necessary internal structure of models for deception detection to work. The paper analyzes this question through conceptual arguments about consistent mechanisms, not through direct empirical intervention. The claim references 'internal state of the language model when it is lying or deceiving', attributing internal representational states (R3). The method is conceptual analysis of prior work on probing (R1 methods). The claim frames the question in terms of what internal mechanisms must exist, which is R3 reasoning about model internals. Gap: +2. Confidence 3 because this is framed as a question/analysis rather than a definitive empirical finding."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,"Behavioral experiments: prompting models across contexts, plausibility ratings, roleplay breakout tests",1,3,2,4,0,"Claim: 'Model beliefs are not stable and are far more context dependent than animal or human beliefs.' The methods are behavioral experiments: prompting models in different contexts, testing whether models break character, analyzing plausibility ratings (R1 observational). The claim uses 'beliefs' (R3) attributing internal mental states to models, and makes a comparative claim about the nature of these beliefs relative to animals/humans. 'Beliefs are not stable' and 'context dependent' are claims about the internal representational structure of models. The behavioral evidence (models readily changing responses across contexts) supports this at an observational level, but the leap to characterizing 'beliefs' as internal states that are 'context dependent' is a mechanistic/representational claim. Gap: +2."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,LLM autorater compared against MASK ground-truth labels,1,1,0,5,0,"Claim: 'We find very low agreement between a full-transcript autorater and the MASK labels.' The method is comparing an LLM autorater's classifications against the MASK benchmark labels (R1 observational/correlational analysis). The claim reports an empirical finding about inter-rater agreement, using appropriate R1 language: 'find very low agreement' is a correlation/association claim about the relationship between two label sets. No mechanistic or causal language is used. The claim is well-calibrated to the method. No gap."
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,Analysis of current model capabilities and chain-of-thought monitoring,1,3,2,3,1,"Claim: 'It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive.' The method is analysis of existing empirical work on chain-of-thought monitoring and model deception capabilities (R1 observational review). The claim uses 'consistent mechanism' (R3): it asserts that strategically deceptive models share a common internal computational mechanism (verbalizing deceptive intent in CoT). 'Mechanism' is explicit R3 language about internal computation. The hedge 'mostly true today' qualifies the temporal scope and certainty, warranting hedge_flag=1. The claim is supported by citing that current models reliably verbalize deceptive reasoning, but framing this as a 'consistent mechanism' is mechanistic. Gap: +2. Confidence 3 due to the hedged and somewhat speculative nature of the claim."
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,Mathematical analysis of SDL optimization landscape under the Linear Representation Hypothesis; synthetic benchmarks (Linear Representation Bench) and SAE training on CLIP embeddings and Llama 3.1 activations,1,3,2,4,0,"The claim that 'neural networks represent meaningful concepts as directions in their representation spaces' uses 'represent' -- a Rung 3 mechanistic term per the codebook decision tree ('encodes'/'represents' defaults to R3 unless context makes clear the author means decodability). The paper treats this as an established assumption (the Linear Representation Hypothesis) rather than something they test with interventions. Their methods are observational/theoretical: they analyze SDL optimization mathematically and run SAE training (R1 feature attribution). No intervention on the model is performed to establish that concepts ARE directions. The claim is stated as fact without hedge. Method-claim gap: +2 (R1 to R3). Minor ambiguity: this claim summarizes prior literature rather than being a novel finding of this paper, but as stated in the abstract it adopts the mechanistic framing without qualification."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,"Mathematical proofs unifying SAEs, transcoders, and crosscoders as instances of a single piecewise biconvex optimization problem",1,1,0,5,0,"The claim is about developing 'the first unified theoretical framework considering SDL as one optimization problem.' This is a methodological/theoretical contribution claim -- it describes what the authors built, not a mechanistic claim about model internals. The method is mathematical analysis (observational/theoretical, R1 -- no intervention on any model). The claim itself is about the framework's existence and scope, which is appropriately R1: it does not claim causal effects or mechanistic roles. No overclaim. No hedge present."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,Mathematical proofs characterizing SDL optimization landscape: Theorem 3.7 proves prevalence of spurious partial minima exhibiting polysemanticity; Theorem 3.10 proves hierarchical concept structures induce feature absorption patterns,1,1,0,4,0,"The claim states the paper provides 'novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons.' The method is mathematical proof and analysis of the SDL optimization landscape (R1 -- purely theoretical, no intervention on any model). The claim itself describes providing explanations for observed phenomena, which is an associational/analytical claim (R1). The word 'explanations' here refers to mathematical characterization of optimization properties, not mechanistic claims about what neural networks do internally. The claim is well-calibrated to the method. Minor ambiguity: 'explanations' could be read as mechanistic, but in context it clearly means 'mathematical characterization of why these optimization artifacts occur,' which is R1."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,SAE feature attribution + Steering vectors,2,2,0,4,0,"The first half ('reveal biologically meaningful latent features') is supported by SAE feature attribution (R1) and linear probing (R1). The second half ('does not guarantee causal control over generation') is supported by failed steering experiments (R2). The claim language is appropriately cautious - 'reveal' for the observational part, and the negative result about causal control is well-matched to the interventional evidence. 'Causal control' is R2 language (interventional). The method supporting the overall claim is steering (R2), and the claim is R2. No overclaim."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Steering vectors (Ordered SAE latent steering),2,2,0,4,0,"The claim is about steerability - 'identifies steerable features' - which is demonstrated by successful steering experiments with Ordered SAE latents (R2 method). 'Steerable features' is interventional language (R2), describing features that can causally influence generation. The method (steering) matches the claim level. No overclaim."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,Linear probing (logistic regression on SAE latents),1,3,2,4,0,"The word 'represent' is a polysemous term. Applying the decision tree: Does the paper provide interventional evidence for this specific claim? No - this claim is supported by linear probing (logistic regression accuracy of 0.99 for CDR identity, F1=0.93 for gene identity). Does context make clear the author means 'is linearly decodable from'? Partially, but the word 'represent' defaults to R3 under the codebook. The evidence is purely observational (probe accuracy). Method R1, claim R3 = +2 overclaim."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE feature attribution (activation pattern analysis),1,3,2,4,0,"'Encoded' is a polysemous term. Applying decision tree: Does the paper provide interventional evidence for this claim? No - the evidence is observational (examining which IMGT positions the latents activate on). Does context make clear the author means 'is linearly decodable from'? No - the context discusses what specific residues the latents activate on, not decodability. Default mechanistic reading: R3. Method is SAE feature attribution (R1). Gap = +2."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering vectors (Ordered SAE latent steering),2,2,0,5,0,This is a straightforward interventional claim. The method is steering (R2) - adding scaled decoder vector to hidden state and measuring output change. The claim language 'steering on X increased Y' is proper R2 interventional language ('intervening on X changes Y'). Clear match between method and claim. No overclaim.
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,Post-training sparsification with L0-regularization under constrained-loss objective; attention edges gated via learned binary masks,2,2,0,5,0,"The method intervenes on attention connectivity by introducing learnable binary gates that zero out edges (interventional, R2). The claim states that connectivity 'can be reduced to ~0.3%' while retaining loss -- this describes the outcome of an intervention (sparsification) on a measurable quantity (pretraining loss). No counterfactual or causal mechanism language is used. The claim is well-calibrated: interventional method, interventional claim."
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,Activation patching for circuit discovery; heads progressively removed and effect on logit difference measured,2,2,0,5,0,Activation patching is interventional (R2): individual attention heads are 'switched off' by patching activations and the effect on task performance (logit difference) is measured. The claim reports that sparse models 'require roughly three times fewer heads to recover 90% of the effect' -- this directly describes the outcome of interventional circuit discovery. No counterfactual or mechanism language. Well-calibrated R2/R2.
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,Edge-level activation patching for circuit discovery; edges progressively included and cumulative effect measured,2,2,0,5,0,Edge-level activation patching is interventional (R2): edges between model components are patched (set to zero or mean) and the effect on task output measured. The claim reports that sparse models need '50-100x fewer edges to reach 90% of the cumulative single-instance effect' -- a direct report of interventional circuit discovery results. No counterfactual language. Well-calibrated R2/R2.
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,Post-training sparsification combined with activation patching circuit discovery,2,3,1,4,0,"The method combines interventional sparsification with activation patching for circuit discovery (R2). The claim uses 'cascades into' which is causal mechanism language -- asserting that local sparsity causes global circuit simplification. Per codebook, causal mechanism claims ('X causes/leads to Y') map to R3. The evidence shows correlation between local sparsity and simpler global circuits via interventional methods, but the 'cascades into' phrasing implies a mechanistic causal relationship beyond what the intervention directly demonstrates. This represents a +1 overclaim (R2 method, R3 claim)."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,Attention pattern analysis and activation patching comparing dense and sparse models,2,2,0,4,0,"The method compares attention patterns and circuit structure between dense and sparse models via interventional analysis (activation patching, R2). The claim describes the empirical observation that dense models have 'diffused' information flow while sparse post-training 'consolidates' it. While 'information flow' is a mechanistic concept, in this context it refers to attention edge weights -- a directly observable quantity. The claim describes the effect of the intervention (sparsification) on an observable property. No counterfactual language. Well-calibrated R2/R2."
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,Activation patching across GPT-2 layers; linear probing for sentiment classification; position specificity and context independence tests,2,3,1,4,0,"The method is activation patching (interventional, R2): activations are swapped between contrasting sentiment inputs at each layer to measure causal contribution. The claim states layers 'act as lexical sentiment detectors encoding stable position specific polarity signals.' Per codebook, 'encoding' defaults to R3 (representation/mechanism language) unless clear decodability context. While a linear probe is used (R1), and activation patching (R2) supports the 'act as' functional role claim, the word 'encoding' elevates the claim to R3 per polysemous term rules. This creates a +1 overclaim (R2 method, R3 claim)."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,"Activation patching with contextual test cases (negation, sarcasm, domain shifts) across GPT-2 layers",2,3,1,3,0,"The method is activation patching (interventional, R2): activations are replaced at each layer while testing contextual phenomena. The claim states these phenomena 'are integrated primarily in late layers (8-11).' The word 'integrated' implies a computational process -- that late layers perform the act of integrating contextual information into the representation. This goes beyond the interventional finding (which shows late layers are causally important) to claim a specific computational mechanism. Per codebook, mechanism/process language maps to R3. However, this is borderline: 'integrated in' could be read as a localization claim (R2). Confidence is 3 reflecting this ambiguity."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,Activation patching across all 12 GPT-2 layers testing hierarchical sentiment processing hypotheses,2,2,0,4,0,"The method is activation patching (interventional, R2): systematically testing which layers causally contribute to sentiment processing. The claim states GPT-2's 'sentiment computation differs from the predicted hierarchical pattern.' While 'sentiment computation' uses mechanism language, the actual claim is about the empirical result of interventions -- that the observed pattern of causal effects across layers does not match the predicted hierarchical pattern. This is a falsification claim based directly on interventional evidence. The claim describes what the interventions show (R2), not a specific mechanistic explanation (R3). Well-calibrated R2/R2."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,Information-theoretic framework applied to SAE activations; entropy-based measurement of effective features; validated on toy models with known ground truth,1,3,2,4,0,"The claim states 'neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions.' The phrase 'through superposition' asserts a mechanistic account -- the network achieves performance BY MEANS OF superposition. This is a R3 mechanistic claim: it attributes a functional role ('achieve performance') to a specific internal mechanism ('superposition encoding multiple features as overlapping directions'). The method is observational: SAE feature extraction and entropy computation on activations (R1). No intervention establishes that superposition is the mechanism enabling performance. The claim is stated as fact without hedge. Method-claim gap: +2 (R1 to R3). Note: like 2512.05534-01, this claim summarizes a widely held assumption in the mech-interp community, but it is framed as established mechanistic fact."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,"Shannon entropy applied to SAE activation magnitude distributions to compute effective features as exp(H(p)), yielding a superposition measure psi = F/N",1,1,0,5,0,"The claim is 'we present an information-theoretic framework measuring a neural representation's effective degrees of freedom.' This is a methodological contribution claim -- it describes what the authors built (a measurement framework), not a mechanistic claim about model internals. The method is observational: computing entropy statistics on SAE activations (R1). The claim is about the framework's existence and what it measures, which is appropriately R1. No overclaim. No hedge present."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,Validation on Elhage et al. (2022) toy models where ground-truth superposition is known; SAE extraction pipeline applied to toy model activations; Pearson correlation between entropy-based measure and ground-truth Frobenius norm measure,1,1,0,5,0,"The claim states 'our metric strongly correlates with ground truth in toy models.' The key linguistic marker is 'correlates with' -- explicitly R1 associational language per the codebook. The method is observational: computing correlations between two measures on toy model outputs (R1). The claim is about correlation, not causation or mechanism. This is well-calibrated: R1 method, R1 claim. The paper reports r=0.94 correlation through the SAE extraction pipeline. No overclaim. No hedge."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,"PGD adversarial training across architectures (MLPs, CNNs, ResNet-18) and datasets (MNIST, Fashion-MNIST, CIFAR-10); SAE-based entropy measurement of effective features before and after adversarial training; statistical analysis with normalized slopes and hypothesis testing",2,2,0,4,0,"The claim states 'adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability.' Adversarial training is an intervention on the training process (R2) -- it modifies the model and the paper measures the resulting change in effective features. The claim language is interventional: 'adversarial training can increase effective features' describes the effect of an intervention on a measurable quantity. 'Contradicting the hypothesis that superposition causes vulnerability' is a causal claim about what does NOT hold, supported by the intervention showing the predicted effect (universal reduction) does not occur. This is appropriately R2: interventional method, interventional claim. The word 'can' provides slight hedging on universality but the overall claim is stated assertively. I code hedge_flag=0 because 'can' here indicates possibility/capacity rather than uncertainty about the finding. Minor ambiguity: 'causes vulnerability' is R2 causal language about the hypothesis being contradicted, not about the paper's own positive claim."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,"Formal mathematical analogy between neuron identification (Eq. 3) and supervised learning (Eq. 4), showing structural correspondence between the two optimization problems",1,1,0,4,0,"The claim states 'neuron identification can be viewed as the inverse process of machine learning.' This is a conceptual/theoretical insight about the mathematical structure of neuron identification. The method is theoretical analysis: the authors demonstrate a formal analogy between neuron identification (searching for concepts matching neurons) and supervised learning (searching for models matching labels). No intervention is performed. The claim uses 'can be viewed as' which frames this as a perspective/framing rather than a hard mechanistic claim. It is R1 because it describes an associational/structural relationship between two mathematical formulations. No overclaim. Minor ambiguity: 'inverse process' could suggest a mechanistic relationship, but in context it clearly means structural/mathematical analogy. Hedge_flag=0 because 'can be viewed as' is a framing device, not uncertainty language -- the authors are confident in the analogy and present it as their key insight."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,"Generalization theory adapted to neuron identification setting; Theorem 3.1 provides high-probability bounds on generalization gap for similarity metrics; convergence rates derived for accuracy, AUROC, IoU, recall, precision",1,1,0,5,0,"The claim states 'we derive generalization bounds for widely used similarity metrics to guarantee faithfulness.' This is a theoretical contribution claim about mathematical guarantees. The method is statistical learning theory: deriving convergence rates and generalization bounds (R1 -- purely theoretical, no intervention). The claim is about the bounds' existence and their implications for faithfulness guarantees, which is R1. 'Guarantee faithfulness' describes a property of the theoretical result (high-probability bounds ensure identified concepts are approximately optimal), not a mechanistic claim about model internals. Well-calibrated: R1 method, R1 claim. No overclaim. No hedge."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,Bootstrap ensemble over probing datasets for neuron identification; Theorem 4.1 provides probabilistic guarantee on concept prediction set coverage; applied to Network Dissection and CLIP-Dissect on ResNet-50,1,1,0,5,0,"The claim states 'we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability.' This is a methodological contribution claim about a statistical procedure and its theoretical guarantees. The method is bootstrap resampling with formal probabilistic analysis (R1 -- observational/statistical, no intervention on model internals). The claim describes the procedure's existence and its guaranteed properties, which is R1. 'Guaranteed coverage probability' refers to a statistical guarantee (Theorem 4.1), not a mechanistic claim. Well-calibrated: R1 method, R1 claim. No overclaim. No hedge."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,"Causal mediation analysis (activation patching, zero ablation, CountScope causal probing, cross-context activation patching)",2,3,1,4,0,"Claim: 'latent counts are computed and stored in the final item representations of each part.' The verbs 'computed' and 'stored' are mechanistic/functional attributions (R3 markers per codebook: 'computes', 'stores'). These claim the model internally performs computation and maintains memory in specific locations, implying a mechanistic understanding beyond what interventions alone establish. The methods used are activation patching, zero ablation, attention knockout, and cross-context activation patching -- all interventional (R2). The causal mediation experiments show that ablating final item tokens drops counting accuracy and that patching activations between contexts transfers numerical information. These establish causal mediation (R2) but not that the model 'computes and stores' in a mechanistic sense (R3). The gap is +1 (R2 method -> R3 claim). This follows the ROME calibration pattern where 'stored' language implies R3 mechanistic claims from R2 interventional evidence."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,Attention analysis (attention weight visualization) supplemented by attention knockout (causal intervention),2,3,1,4,0,"Claim: 'counts are transferred to intermediate steps via dedicated attention heads.' The word 'transferred' implies a mechanistic process -- that information flows through a specific pathway. 'Dedicated' implies these heads have a specific functional role (R3 marker: functional attribution, 'THE mechanism'). The evidence comes from attention pattern analysis (R1) and attention knockout experiments (R2) showing that knocking out specific heads in layer 22 drops counting accuracy. Attention knockout is interventional (R2), establishing causal necessity of those heads. However, claiming they 'transfer' counts as 'dedicated' heads attributes a specific functional mechanism (R3). The gap is +1. Per the codebook decision tree for 'controls/is responsible for': the evidence is from intervention, but the claim attributes a specific mechanistic role ('dedicated' heads that 'transfer'), pushing it to R3."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,Behavioral evaluation (accuracy measurement across structured/unstructured inputs with/without CoT steps),1,2,1,4,0,"Claim: 'this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting.' The verb 'enables' is a causal sufficiency marker (R2: 'is sufficient for', 'can produce'). The claim asserts that the System-2 strategy causally produces the improvement. The primary evidence is behavioral: Tables 1-2 show that structured input + intermediate reasoning steps yields high accuracy where other configurations fail. This is observational/behavioral evaluation (R1) -- no internal model intervention is performed for this specific claim. However, the paper frames this as a causal claim about the strategy enabling the behavior. The behavioral results show the strategy is associated with better performance (R1 evidence), but the claim uses causal language ('enables', 'surpass limitations'). The gap is +1. Confidence is 4 because while 'enables' leans causal, the behavioral evidence is strong and consistent, and one could argue this is close to an empirical performance claim (which would be R1)."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,"SAE feature extraction and activation analysis (R1), feature steering via clamping (R2)",2,2,0,5,0,"Claim: 'We successfully reproduce basic feature extraction and steering capabilities.' This is an empirical replication claim about method performance. 'Feature extraction' (R1) and 'steering capabilities' (R2) are described at the level of what the methods achieve, not what the model internally computes. The paper demonstrates that SAE features activate on expected contexts (R1) and that clamping features changes model output in expected directions (R2 -- interventional). The claim is well-matched to the evidence: they show feature extraction works (coffee feature activates on coffee text) and steering works (clamping changes output). No overclaim -- the language is appropriate for the methods used. This follows the SAE Evaluation calibration pattern where evaluation papers tend to have good claim-method alignment."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,Feature steering experiments across layers and steering coefficients (R2),2,2,0,5,0,"Claim: 'feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context.' This is an empirical observation about the behavior of an interventional method (steering). The paper systematically varies the layer where SAE steering is applied, the steering coefficient magnitude, and the input context, finding that steering quality is highly sensitive to all three. This is appropriate R2 language: it describes what happens when you intervene (steer) under different conditions. The claim does not attribute internal mechanisms; it characterizes the practical reliability of the intervention. No overclaim -- the claim rung matches the method rung."
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,"SAE feature activation analysis across contexts (R1), feature similarity comparison (R1)",1,1,0,4,0,"Claim: 'We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another.' This claim has two parts. First, 'non-standard activation behavior' refers to observing that features activate on unexpected contexts (e.g., the coffee feature activating on 'coffins' -- as suggested by the title). This is purely observational (R1). Second, 'difficulty to distinguish thematically similar features' refers to analyzing activation patterns of similar features and finding overlap. This is also observational/correlational (R1). Both parts describe patterns observed in activation data without intervention. The claim language ('observe', 'demonstrate the difficulty') is appropriately descriptive. No overclaim. Confidence is 4 rather than 5 because the 'demonstrate' verb could be read as slightly stronger than the purely observational evidence warrants, but in context it refers to showing empirical examples rather than making a causal claim."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,"Combined feature extraction (R1), activation analysis (R1), and steering experiments (R2)",2,2,0,4,0,"Claim: 'current methods often fall short of the systematic reliability required for safety-critical applications.' This is a meta-level evaluative claim about the reliability of SAE-based interpretability methods. The evidence combines observational findings (non-standard activations, feature similarity issues) and interventional findings (steering fragility, sensitivity to parameters). The claim language is appropriately scoped: 'often fall short' describes empirical observations about method reliability rather than making mechanistic claims about model internals. The word 'often' is a frequency qualifier rather than a hedge about certainty. The claim is well-calibrated to the evidence presented. No overclaim. This follows the evaluation paper pattern from the calibration set where papers testing methods tend to have appropriate claim-method alignment."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,Sparse Autoencoder (SAE) training on LRM hidden states to decompose activations into sparse feature space (R1),1,3,2,4,0,"Claim: 'SAEs decompose strategy-entangled hidden states into a disentangled feature space.' The verb 'decompose' combined with the claim that hidden states are 'strategy-entangled' and the SAE produces a 'disentangled feature space' makes strong mechanistic assertions. 'Disentangled' implies that the SAE has successfully separated distinct strategies into individual features -- a claim about what the representation contains and how it is structured (R3: 'represents'). The method is SAE training (R1 -- observational dictionary learning), which establishes that sparse features can be extracted but does not causally validate that these features correspond to individual strategies. The paper's Stage 2 evaluation uses steering (R2) to test control effectiveness, but this specific claim is about the decomposition property of the SAE itself, not the steering results. The SAE decomposition is fundamentally an observational/correlational method (R1). The gap is +2 (R1 -> R3). This follows the 'SAE -> represents' overclaim pattern from the codebook."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE-Steering two-stage pipeline: logit lens estimation (R1) followed by steering effectiveness ranking on validation set (R2),2,2,0,4,0,"Claim: 'SAE-Steering identifies strategy-specific features from the vast pool of SAE features.' The verb 'identifies' here refers to the method's ability to select features associated with specific strategies. The two-stage pipeline first uses logit lens (R1) to recall candidate features, then ranks them by empirical steering effectiveness on a validation set (R2 -- interventional evaluation). The claim that these are 'strategy-specific' is supported by the interventional evidence: steering with the selected features produces the target strategy more often than baseline (measured by LLM judge). The claim language is appropriate for the method: 'identifies' in this context means 'selects features that are effective for steering', which is empirically validated through intervention. No overclaim -- the claim matches the highest-rung method (R2) directly supporting it."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,"Comparative steering experiments with LLM-judge evaluation against baselines (Logit Boosting, Think Intervention, Vector Steering) (R2)",2,2,0,5,0,"Claim: 'SAE-Steering outperforms existing methods by over 15% in control effectiveness.' This is a comparative empirical performance claim about an interventional method. The evidence comes from Table 1 in the paper, which reports control effectiveness (success rate of steering the model toward target strategy) across multiple strategies, models, and datasets, evaluated by majority vote of three LLM judges. SAE-Steering consistently outperforms baselines by the reported margin. The claim is purely about relative intervention effectiveness -- no mechanistic claims about model internals. The language ('outperforms', 'control effectiveness') is appropriate for R2 interventional evidence. No overclaim."
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,Steering intervention on erroneous reasoning paths with accuracy measurement (R2),2,2,0,4,0,"Claim: 'controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones.' The verb 'redirect' describes a causal effect of the intervention (steering) on model behavior. The evidence comes from error correction experiments where SAE-Steering is applied mid-reasoning when a flaw is detected, producing a 7% absolute accuracy improvement. 'Controlling' and 'redirect' are appropriate R2 language -- they describe what the intervention achieves ('intervening on X changes Y'). The claim does not assert a mechanistic explanation of why the redirect works or that the features represent the reasoning strategies (which would be R3). It stays at the level of 'this intervention produces this behavioral change.' No overclaim. Confidence is 4 because the word 'can' slightly hedges the claim, though I code hedge_flag as 0 because 'can' here indicates capability (possibility) rather than uncertainty about whether it works."
2601.05679,2601.05679-01,many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens,abstract,3,Causal token injection: inserting top-activating tokens into non-reasoning text and measuring activation changes (R2),2,2,0,5,0,Claim: 'many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens.' This describes the empirical result of a causal intervention: injecting tokens into non-reasoning text and observing activation increases. The language is precisely matched to the method. 'Sensitive to token-level interventions' correctly describes the causal test performed. The percentages (45-90%) are directly from the intervention results across 22 configurations (Table 2). No mechanistic claims about what the features represent -- the claim stays at the level of what happens when you intervene. No overclaim.
2601.05679,2601.05679-02,LLM-guided falsification produces targeted non-reasoning inputs that trigger activation,abstract,3,"LLM-guided falsification: hypothesis generation, counterexample construction (false positives and false negatives), activation measurement (R2)",2,2,0,4,0,"Claim: 'LLM-guided falsification produces targeted non-reasoning inputs that trigger activation.' This describes the result of the adversarial falsification procedure. The method constructs non-reasoning text designed to activate features (false positives) and paraphrases of reasoning text designed to suppress activation (false negatives). Measuring whether these constructed inputs change feature activation is an interventional test (R2). The claim language ('produces targeted inputs that trigger activation') accurately describes the intervention's outcome without attributing internal mechanisms. The claim is well-calibrated to the evidence. No overclaim. Confidence is 4 because the LLM-guided nature of the counterexample generation introduces some subjectivity in what counts as 'non-reasoning', but the paper addresses this with clear operational definitions."
2601.05679,2601.05679-03,sparse decompositions can favor low-dimensional correlates that co-occur with reasoning,abstract,3,"Theoretical analysis (stylized model of sparsity-regularized decoding) supplemented by empirical falsification experiments (R1 theory, R2 experiments)",2,2,0,3,1,"Claim: 'sparse decompositions can favor low-dimensional correlates that co-occur with reasoning.' The hedge 'can' marks this as a possibility claim rather than a definitive statement (hedge_flag=1). The claim combines theoretical motivation (Theorem 3.1 showing sparsity suppresses high-dimensional components) with empirical evidence (token injection and falsification results showing features track lexical cues rather than reasoning). The theoretical analysis is mathematical/observational (R1), while the empirical experiments are interventional (R2). The claim language 'can favor' is appropriate for R2 -- it describes a tendency observed through experiments and motivated by theory. The claim does not assert a mechanism for how the model reasons internally; it makes a methodological critique about SAE feature selection. No overclaim. Confidence is 3 because the claim bridges theory (stylized model) and empirical findings, and the word 'can' introduces inherent ambiguity about when this failure mode applies. The theoretical model is intentionally simplified (as the authors acknowledge)."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,"Linear probing, MLP probes, EMA probes, attention probes trained on model hidden states (R1)",1,1,0,5,1,"Claim: 'activation probes may be a promising misuse mitigation technique.' The hedge 'may be' explicitly signals uncertainty (hedge_flag=1). The claim is about probe performance -- whether probes trained on hidden states can classify harmful inputs. Probing is purely observational (R1): a classifier is trained on frozen activations without modifying the model. The claim language is well-calibrated: 'may be promising' is appropriately tentative for R1 evidence. The paper shows probes achieve comparable performance to LLM classifiers at much lower cost (Figure 1), and have been deployed in production. But the claim stays at the level of empirical utility, not mechanistic understanding of model internals. This matches the Gemini Probes calibration case exactly -- production/applied papers focused on probe performance with appropriate hedging. No overclaim."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,"Linear probing and novel probe architectures evaluated across distribution shifts (short-to-long context, multi-turn, adaptive red teaming) (R1)",1,1,0,5,0,"Claim: 'probes fail to generalize under important production distribution shifts.' This is an empirical observation about probe performance degradation when the test distribution differs from training (R1 claim language: observational finding). The evidence comes from evaluating probes trained on short-context data on long-context inputs, multi-turn conversations, and adversarial jailbreaks, finding significant performance drops. The claim language ('fail to generalize') is a direct empirical observation. No mechanistic claims about why this happens internally -- the paper focuses on the practical consequence. The method is probing (R1), and the claim is about probe behavior under distribution shift (R1). No overclaim. This again matches the Gemini Probes calibration case: production-focused, empirical claim matched to observational evidence."
