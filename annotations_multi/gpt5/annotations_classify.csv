paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,Circuit analysis using causal interventions (path patching / activation patching / knockouts via mean ablation),2,3,1,4,0,"Method: The paper’s core evidence for its explanation relies on causal interventions (path patching, activation patching, knockouts/ablations), which are Rung 2 interventional methods. Claim: “presenting an explanation for how GPT-2 small performs [IOI]” is a mechanistic/functional attribution about how the model performs the task (an end-to-end explanatory mechanism), which corresponds to a Rung 3 mechanistic claim rather than a narrower causal-effect claim. No explicit hedge language is present in the quoted text."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,Path patching / activation patching / knockouts (causal interventions) used to discover and validate the set of heads and their grouping,2,1,0,4,0,"The statement is primarily a descriptive/quantitative inventory claim about what their proposed explanation contains (a set of 26 heads grouped into 7 classes), not a causal or mechanistic attribution about what those heads do. It does not assert that these heads are responsible for IOI or implement an algorithm; it just counts/components-in-explanation. The supporting methodology in the paper for identifying these heads relies on interventional causal techniques (path patching, ablations/knockouts), which are R2."
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,Combination of interpretability approaches relying on causal interventions (path patching / activation patching / knockouts/ablations),2,3,1,4,0,"Method: the paper’s core evidence is interventional (path patching/activation patching/knockouts), which is Rung 2. Claim: calling the work an “end-to-end attempt at reverse-engineering a natural behavior” is a mechanistic/completeness framing (reverse-engineering implies identifying the underlying algorithm/mechanism, beyond merely showing causal effects of specific components), so it maps to Rung 3. It is also a superlative/meta-claim about scope (“largest”), but the key empirical/mechanistic assertion is the reverse-engineering framing. No explicit hedge in the quoted text."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,Activation/path patching (path patching / causal tracing-style intervention on attention head activations) plus knockout/ablation-style interventions; supported by attention pattern analysis and OV/copy-score analysis,2,3,1,5,0,"Method: The paper supports Name Mover Heads with interventional evidence (path patching showing direct causal effect on logit difference; knockouts/mean ablations) → method_rung 2. Claim: “move the name from the subject to the end position” is a functional/mechanistic attribution (a head *does* an operation: moving/copying a name to END), not merely that intervening changes outputs. This is R3 per codebook (functional verb ‘move’ / mechanistic narrative). No explicit hedge in the quoted claim."
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,Activation/path patching (causal intervention on internal activations; plus attention-pattern comparison before/after patching),2,2,0,5,0,"The paper supports this with interventional evidence: they path-patch heads 7.3, 7.9, 8.6, 8.10 into Name Mover Heads’ query inputs and show a drop in logit difference (Fig. 4b), and they directly compare Name Mover attention distributions before vs after patching these heads (Fig. 4c), showing increased attention to S1 when the S-Inhibition heads are patched from pABC. The claim itself is causal (“inhibit attention”), describing an interventionally-supported effect on attention, not a full mechanistic/unique-function attribution beyond that causal role. No explicit hedge in the quoted claim."
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,"Quantitative circuit validation via knockouts/mean ablation and performance metrics (faithfulness, completeness, minimality)",2,3,1,4,0,"Method: The paper evaluates faithfulness/completeness/minimality by intervening on the model (knocking out/mean-ablating sets of heads; comparing F(C), F(M), and F(·\K) under knockouts). This is interventional evidence (R2), not a uniqueness/counterfactual-proof method. Claim: Stating “the circuit is faithful, complete and minimal” is a strong mechanistic adequacy claim that the identified circuit constitutes the (sufficient) explanation of the behavior and that it contains all and only necessary components—this goes beyond a simple causal effect statement and functions as a ‘mechanism fully captured’ claim (R3). No explicit hedge in the quoted text."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,Causal tracing / causal mediation analysis via activation patching (corrupted-with-restoration) plus ROME rank-one weight editing (interventional weight update),2,3,1,4,0,"Method: The abstract’s support comes from (i) causal tracing that restores internal activations to measure indirect causal effects, and (ii) ROME weight edits that change outputs—both are interventions establishing causal influence under specific manipulations (R2). Claim: “factual associations correspond to localized, directly-editable computations” is an identity/mechanistic attribution: it asserts the associations *are* localized computations and that these computations are directly editable, which goes beyond a pure causal-effect statement (“intervening on X changes Y”) into a mechanistic framing about what the internal computation is (R3 per codebook: ‘correspond to localized computations’ is a known ROME overclaim pattern). No explicit hedge like “may/suggests” appears in the quoted claim, despite nearby abstract language (“finding evidence that…”)."
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,Causal tracing / causal mediation analysis via activation patching (corrupted run + restoration of internal activations; measuring indirect effects),2,2,0,5,0,"The supporting evidence is an interventional activation-restoration setup measuring (average) indirect causal effects of internal states, which is Rung 2. The claim says these middle-layer MLP modules ‘mediate factual predictions,’ which is explicitly a causal mediation/causal contribution claim (Rung 2), not a full mechanistic ‘stores/encodes/implements’ attribution (Rung 3). No explicit hedge terms (e.g., may/suggests) appear in the quoted fragment."
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal tracing / causal mediation analysis via activation patching (corrupted run + restoration of internal activations) plus weight-edit intervention (ROME rank-one weight update),2,3,1,5,0,"The quoted claim attributes a mechanistic function to a specific component: that mid-layer feed-forward (MLP) modules are *storing* factual associations. Per the codebook, “stores” is storage/memory language that defaults to a mechanistic/representational attribution (R3), not merely causal mediation. The paper’s main supporting evidence for localization comes from interventional methods (causal tracing/activation restoration; and ROME weight edits), which are R2 because they establish causal effects of interventions on outputs but do not establish counterfactual uniqueness/necessity of the storage mechanism. The abstract sentence is stated as a fact (“confirm an important role… in storing”), with no explicit hedge like “may/suggest,” so hedge_flag=0."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,ROME/MEMIT-style weight editing intervention evaluated on zsRE benchmark (interventional weight modification + behavioral evaluation),2,1,0,5,0,"The supporting evidence is an explicit intervention on model weights (ROME rank-one update) followed by measuring downstream task performance on zsRE, which is Rung 2. However, the claim itself is only an empirical effectiveness/performance statement (“ROME is effective on… task”) and does not assert a causal/mechanistic account of internals; it’s essentially a benchmark result claim, so it fits Rung 1 (observational outcome) rather than R2/R3 mechanistic language. No hedging present."
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,"Mechanistic interpretability via weight/activation analysis (Fourier/DFT structure, approximations) plus ablations in Fourier space",2,3,1,5,0,"The paper supports its main story with (i) observational analyses of weights/activations in a Fourier basis (R1) and (ii) explicit interventions/ablations on frequencies and components showing large performance changes (R2). The claim text—“fully reverse engineer the algorithm learned by these networks”—is a strong completeness/mechanistic attribution (“the algorithm”, “fully reverse engineer”), which is rung-3 per the codebook (mechanistic/unique/completeness framing), not merely causal effect. No explicit hedge is present. Thus method evidence is at best interventional (R2), while the claim is mechanistic/completeness (R3), implying a +1 overclaim gap."
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,Activation/weight analysis with Fourier transforms (DFT) + ablations in Fourier space (frequency ablation) used to confirm interpretation,2,3,1,5,0,"The claim attributes a specific internal algorithm/function to the network—""uses discrete Fourier transforms and trigonometric identities to convert addition to rotation""—which is a mechanistic/functional description (R3: 'uses X to do Y', 'algorithm learned'). The paper supports this with both observational analyses (Fourier structure in weights/activations) and interventional evidence via ablations in Fourier space showing performance drops when key frequencies are removed, which is R2. Since the strongest directly supporting method includes ablation, method_rung=2 while the claim remains R3. No explicit hedging in the quoted claim."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,Activation/weight analysis (Fourier structure in weights/activations; periodicity; rank/Fourier decomposition) plus ablations in Fourier space (frequency ablation / restricted-excluded loss interventions),2,3,1,4,0,"The paper supports its grokking-dynamics story using (i) observational analyses of weights/activations in a Fourier basis (R1) and crucially (ii) interventional ablations in Fourier space showing performance changes when key frequencies are removed/kept (R2). The claim itself—""grokking arises from the gradual amplification of structured mechanisms encoded in the weights""—attributes a mechanistic explanation of what grokking *is* (arises from amplification of mechanisms) and uses mechanistic language (""mechanisms"", ""encoded in the weights"") rather than just causal effect under a specific intervention, so it is R3. No explicit hedge words (e.g., may/suggests) appear in the quoted claim, so hedge_flag=0. Confidence is high but not maximal because the paper has substantial evidence, yet the specific phrasing ""arises from"" + ""mechanisms encoded"" goes beyond the strict scope of the ablation evidence (which is causal but not a uniqueness/complete-mechanism proof)."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,Interchange interventions (feature patching via DBM-selected SAE features) evaluated on RAVEL; neurons as baseline,2,1,0,5,0,"The evidence comes from interchange interventions/patching on selected features, which is an interventional (R2) method. However, the claim itself is a comparative performance statement (“struggle to reach the neuron baseline”) about evaluation outcomes, not a causal/mechanistic attribution about internals. It does not assert that any component mediates/controls/encodes; it just reports that SAE-based feature spaces underperform a neuron baseline. Thus the claim is rung 1 (empirical association/performance comparison) despite being supported by an R2 method. No explicit hedging language is present."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,Interchange interventions (features fixed to values from a source prompt; RAVEL evaluation with DBM-selected SAE features),2,2,0,4,0,"The supporting evidence described in the abstract/methods is interchange intervention/patching-style manipulation of selected SAE features, which is an interventional method establishing causal effects under intervention (R2). The claim uses the verb “mediate,” which in this codebook corresponds to a causal/mediational claim about changing those features affecting the model’s expressed knowledge (R2), not a full mechanistic attribution/uniqueness claim (R3) and not mere decodability (R1). No explicit hedge terms (e.g., may/suggests) appear in the quoted claim."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,Activation probing / linear probing on frozen activations (observational probe training and evaluation; no intervention on the base model),1,1,0,4,1,"The quoted claim is a high-level empirical takeaway about probe utility: that activation probes are ""promising"" as a misuse mitigation. In this paper, probes are trained on hidden states and evaluated by predictive performance under distribution shifts; this is observational with respect to model internals (no causal intervention like ablation/patching/steering on the base model), so method_rung=1. The claim itself is not mechanistic (no ""encodes/represents/computes""), and it does not assert a causal effect of intervening on internals; it’s an associational/utility claim about performance, explicitly hedged by ""may,"" so claim_rung=1 with hedge_flag=1. Minor ambiguity: one could read “mitigation technique” as causal in deployment, but the paper’s evidence for “promising” is still correlational performance evaluation rather than causal internal claims."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,"Activation probing / probe performance evaluation under distribution shift (train on short-context, test on long-context/multi-turn/adaptive red teaming); observational evaluation without intervening on model internals",1,1,0,5,0,"The claim is about empirical generalization performance of activation probes across production distribution shifts (e.g., short-context→long-context). The supporting method is standard probe training/evaluation and comparing error rates across datasets/splits, which is observational/associational (R1) rather than an intervention on the base model’s internals. The claim itself is not mechanistic or causal about internals; it asserts a performance failure (“fail to generalize”) under distribution shift, which is an empirical association about probe behavior, so claim_rung=1. No explicit hedge words (may/suggests) appear."
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Unspecified / narrative reference to prior work (no concrete method tied to this claim in the quoted fragment),1,3,2,4,0,"The phrase ""reverse-engineered nontrivial behaviors"" is a mechanistic/completeness claim: it asserts that prior works have recovered the underlying algorithmic explanation for behaviors, not merely correlated components with behavior. In the codebook, ""reverse-engineering"" and similar language is a common R2→R3 overclaim marker, and here it is directly a R3-style mechanistic attribution. The quoted fragment does not specify any empirical method used to support the claim (it’s a high-level abstract framing about the literature), so the most appropriate method rung is effectively observational/unspecified (R1) rather than interventional (R2) or counterfactual (R3). No explicit hedge terms (e.g., may/suggests) appear."
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,Activation patching / causal tracing style interventions used by ACDC to prune edges and evaluate recovery against prior circuits (ROC/AUC comparison to previous mechanistic interpretability results),2,1,0,4,0,"The supporting method is interventional: ACDC runs many activation patching (interchange/zero) experiments and measures effects on metrics, so method_rung=2. The claim itself—“rediscovered 5/5 of the component types in a circuit”—is an empirical recovery/overlap statement about matching previously identified component categories, not a causal/mechanistic functional attribution (no ‘computes/implements/uses’ language). Thus it is best coded as an observational performance/association claim about agreement with a reference circuit (claim_rung=1). No explicit hedge is present."
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,Activation patching / extensive iterative patching experiments (incl. interchange interventions) to identify involved units and connections; plus qualitative interpretation of roles,2,3,1,4,0,"The claim says researchers ""can understand the functionality of each component,"" which is a functional/mechanistic attribution about what components do (functionality), matching claim rung 3 per codebook (functional verbs/attribution). The paper’s described evidential basis in the abstract is activation patching to find involved units and then varying datasets/metrics/units—an interventional approach establishing causal effects of components under interventions, i.e., method rung 2. It does not establish uniqueness/complete mechanistic proof for each component, so the method does not reach rung 3. No explicit hedge (e.g., may/suggests) appears."
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,Activation patching / recursive activation patching (ACDC; interchange interventions / corrupted activations) used to identify important edges between abstract units,2,1,0,4,0,"The quoted abstract phrase describes the paper’s automated step as *finding connections* (edges) between abstract units that *form a circuit*. This is primarily a circuit/edge identification claim (which edges are included), not a functional/mechanistic attribution about what the circuit computes or how the model uses it. Thus the claim is best treated as an empirical recovery/selection statement (R1: associational/identification) rather than a causal or mechanistic claim. The supporting method is activation patching / interchange interventions (an intervention on activations to test edge importance), which is R2. Minor ambiguity: the phrase “form a circuit” could be read as implying a mechanistic subgraph, but without functional verbs (compute/implement/underlies) it stays closer to ‘we can identify the subgraph structure’ (R1)."
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,"Circuit-based mechanistic interpretability using resample/zero ablation, activation/edge patching (EAP/positional EAP), and ACDC-style automated circuit discovery on Mamba IOI",2,3,1,4,1,"Method: the paper’s core evidence for the IOI circuit comes from interventions (resample ablation / activation patching, zero ablation, layer removal, EAP attribution patching, ACDC pruning), which are Rung 2 causal methods. Claim: “reverse-engineer the circuit responsible for” is a mechanistic/functional attribution (“the circuit” + “responsible for”) implying an underlying mechanism rather than just causal effect, so it is Rung 3 per codebook (definite-article circuit + responsibility language). Hedge: “partially” explicitly hedges completeness, so hedge_flag=1, but it remains a R3-style mechanistic claim supported by R2 methods (R2→R3 overclaim gap of +1)."
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,Resample ablation / layer removal (zero-ablation) / token cross-talk removal via patching (interventional causal tracing-style interventions) used as evidence for layer-importance/bottleneck claims,2,3,1,4,0,"The claim text “Layer 39 is a key bottleneck” is a mechanistic/structural attribution about the model’s internal information flow (a ‘bottleneck’ role), which goes beyond a pure causal-effect statement like ‘intervening on layer 39 changes IOI performance’ and instead asserts a functional position in the computation (R3). The paper’s supporting evidence for this abstract claim is primarily interventional (resample ablation, zero/resample ablation, minimal cross-talk circuits, EAP/ACDC), which is method rung 2. No explicit hedge appears in the claim as stated. Minor ambiguity: ‘bottleneck’ could be read as a causal-importance summary (R2), but in interpretability context it typically implies a mechanistic information-routing role, so I code it as R3."
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,Resample ablation / activation patching on individual conv slices in layer 39 (plus supporting non-causal cosine-sim visualization),2,3,1,4,0,"The key evidence the paper cites for this statement is an intervention: resample ablation/patching on specific convolution ‘slices’ in layer 39 to distinguish whether shifting happens in layer 39 vs earlier layers (Section 4.2, Fig. 8). That is a Rung 2 interventional method. The claim text uses a functional/mechanistic verb (‘shift’) attributing a specific computation to a component (‘Convolutions in layer 39 shift names…’), which is a mechanistic/functional attribution rather than merely ‘affects’ or ‘is associated with’, so it is Claim Rung 3. No explicit hedge words appear in the claim as stated (even though the body contains some caution elsewhere), so hedge_flag=0."
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,"Activation replacement/averaging intervention on Layer 39 SSM input (""Replace"" and ""Subtract and Add"" methods; akin to activation patching/steering) plus supporting resample ablation/patching evidence",2,3,1,4,0,"The paper supports the statement using interventions: they average Layer 39 SSM-input activations for names and then substitute (or subtract+add) these activations to overwrite the model’s output name >95% of the time (Section 4.3), and they also use resample ablation/patching around Layer 39 as bottleneck evidence. These are Rung-2 interventional methods (they change internal activations and measure output effects). The claim text, however, uses mechanistic/storage language—“entities are stored linearly”—which attributes a representational mechanism (storage + linear representation) rather than merely reporting a causal effect of an intervention, so it is Claim Rung 3 per the codebook’s ‘stores/encodes’ guidance. No explicit hedge appears in the quoted claim (even though nearby text sometimes says ‘suggest’/‘hypothesize’)."
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,Benchmark evaluation comparing prompting vs. representation-based steering methods and finetuning baselines using an LLM judge on AXBENCH (generation under interventions + judged scores/winrates),2,1,0,4,0,"The claim is an empirical performance comparison: prompting achieves higher steering scores than other evaluated methods, with finetuning next. This is not a claim about model internals/mechanism; it’s an outcome/utility ranking on the benchmark. The supporting method is an interventional evaluation (they intervene via prompting/finetuning/activation additions and measure downstream outputs with an LLM judge), which is R2. The claim itself is best treated as an observational comparative result (R1) rather than a causal/mechanistic statement about internals."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,Benchmark evaluation experiments comparing steering/concept-detection methods on AXBENCH (LLM-judge scoring for steering; labeled synthetic data metrics like AUROC/F1 for concept detection),1,1,0,4,0,"The abstract claim “SAEs are not competitive” is an empirical performance comparison statement about outcomes on the benchmark, not a claim about internal mechanisms (no ‘encodes/represents/computes’ language). It is supported by observational evaluation metrics (AUROC/F1, judge scores, winrates) rather than a targeted causal intervention to establish a specific internal causal role; although some evaluated methods involve interventions (steering), the claim itself is about comparative benchmark performance, which fits R1 associational/empirical reporting. No explicit hedge terms appear in the quoted claim."
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,Benchmark evaluation comparing multiple concept-detection methods (incl. DiffMean) using labeled synthetic data; observational scoring of concept detection via AUROC/F1 on held-out set (no intervention required for this claim),1,1,0,4,0,"The claim is an empirical performance comparison: “perform the best” refers to higher concept-detection metrics (AUROC/F1) on AXBENCH. This is an associational/observational claim about evaluation results, not a causal/mechanistic statement about internals. The supporting method is standard supervised evaluation on labeled data (R1). Minor ambiguity because the paper also studies steering (interventions), but this specific abstract clause is about concept detection performance, which is evaluated without intervening on the model."
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,Activation patching / causal tracing / interchange interventions (interventional localization via patching),2,2,0,5,0,"The claim asserts that intervening on specific internal components produces strong causal effects on factual recall at a particular token position (""show strong causal effects""), which is a causal/mediation statement rather than a mechanistic functional attribution. The supporting methods named in the abstract (causal tracing / interchange interventions, i.e., activation patching) are rung-2 interventional techniques, matching a rung-2 causal claim. No explicit hedge terms (e.g., may/suggests) appear in the quoted claim."
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,Rank-one model editing (ROME-style weight edits) evaluated on COUNTERFACT with efficacy/generalization/specificity metrics,2,2,0,4,0,"The supporting evidence is an intervention on model parameters (rank-one weight update) and measuring downstream behavioral change, which is Method Rung 2. The claim says the method can 'successfully insert facts at specific locations'—i.e., that intervening at certain parameter locations produces the desired factual output change. This is a causal/sufficiency claim about the effect of an edit at a location, not a mechanistic attribution about how the model stores/represents facts, so Claim Rung 2. No explicit hedge terms (e.g., may/suggests) are present."
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,Linearity of Relation Embedding (LRE) via first-order Taylor/Jacobian analysis (collect Jacobians; evaluate faithfulness of affine approximation),1,1,0,4,0,"The quoted phrase is a descriptive/associational claim about a property of internal representations (their degree of linearity), supported by observational analysis (Jacobian/Taylor approximation and faithfulness metrics) without intervening on the model’s internals to change behavior. It does not attribute a specific mechanism (‘the model uses/encodes’) or causal role; it’s closer to ‘we examine/measure linearity’ than a mechanistic claim."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,Interchange interventions / activation patching (causal mediation analysis; causal abstraction with targeted interchange interventions on residual stream and localized subspaces via DCM/SVD),2,3,1,4,0,"The paper’s core evidence for this abstract claim comes from interventional methods: interchange interventions/activation patching on residual stream vectors and on learned subspaces (IIA-based causal abstraction), which are Rung 2. The claim itself uses mechanistic functional language—“binds … triple together” and specifies an internal implementation detail (“by co-locating their reference information”)—which attributes a particular computation/representation strategy to the LM, matching Claim Rung 3 (mechanistic narrative), not merely that information is decodable or that interventions change outputs. No explicit hedge appears in the quoted claim text. Confidence is high but not maximal because ‘co-locating’ could be read as a descriptive localization result; however in abstract framing it functions as a mechanistic explanation of binding."
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,Interchange interventions / activation patching framed as causal mediation analysis and causal abstraction (IIA),2,2,0,4,0,"The paper’s core supporting evidence for “lookback” comes from interventional methods (interchange interventions / activation patching on residual stream and subspaces, evaluated with IIA), which are Rung 2. The claim text says the mechanism “enables the LM to recall important information,” which is a causal/sufficiency-style statement about an internal mechanism affecting behavior (recall), but it does not explicitly assert a full mechanistic computation/unique algorithm in this snippet. Thus claim_rung=2 (causal effect of a mechanism). No explicit hedge words (e.g., may/suggest) appear in the quoted claim, so hedge_flag=0."
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,"Interchange interventions / activation patching (causal mediation / causal abstraction with IIA) on residual stream and localized subspaces to test hypothesized variables (OIs, pointers/addresses/payloads)",2,3,1,4,0,"Method: The paper supports the binding-lookback story using interchange interventions (activation patching) and causal abstraction alignment measured by IIA, which is rung 2 interventional evidence. Claim: “the binding lookback retrieves the correct state OI” is a functional/mechanistic attribution (retrieves; describes what the mechanism does), not merely that intervening changes outputs, so it is rung 3. It is stated in the abstract without hedging. Slight ambiguity because it could be read as a summary of an intervention result (R2), but the phrasing attributes an internal computation (‘binding lookback’ as a mechanism) performing retrieval, which defaults to R3 per codebook."
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,Interchange interventions / activation patching (causal mediation analysis) and causal abstraction alignment via targeted interchange interventions; plus subspace localization via Desiderata-based Component Masking (DCM),2,3,1,4,0,"The paper’s core evidence comes from interventional causal methods (interchange interventions / activation patching; IIA-based causal mediation; causal abstraction tests), which are Rung 2 because they establish causal effects of intervening on internal activations but do not prove uniqueness/complete mechanism in the strong counterfactual/necessity sense required for Rung 3 methods. The claim phrase “reverse-engineering ToM reasoning in LMs” is a mechanistic/completeness framing: it implies uncovering the underlying algorithm/mechanism of ToM reasoning (a Rung 3-style mechanistic attribution, akin to ‘reverse-engineering the algorithm’ in the grokking calibration). There is no explicit hedge in the quoted fragment (the abstract elsewhere says ‘taking a step toward’, but that hedge is not present in this exact claim text), so hedge_flag=0."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,Interchange interventions / activation patching on paired original vs counterfactual inputs (TargetRebind; causal model evaluation via interchange interventions),2,3,1,4,0,"The claim text attributes a specific internal algorithm/functional mechanism—""implement such retrieval via a positional mechanism""—which is a mechanistic/functional attribution (R3) rather than merely stating an intervention effect (R2) or decodability (R1). The paper’s supporting evidence (as described) relies primarily on interchange interventions/patching and ablations to show causal influence of positional-index information on outputs, which are interventional methods (R2). No explicit uniqueness/necessity proof is implied by this sentence alone, but ""implement ... via"" still reads as a mechanism claim. Hence method_rung=2, claim_rung=3. Confidence slightly reduced because the sentence appears in a background/prior-work framing, but as written it is an unhedged mechanistic claim."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,Interchange interventions / counterfactual patching (activation patching on paired original vs counterfactual inputs; plus ablation/knockout supporting experiments),2,3,1,4,0,"Method: The paper’s core evidence for multiple mechanisms comes from interchange interventions that patch internal states between original and counterfactual runs and measure changes in next-token distributions—an interventional causal method (R2). Claim: “LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism” attributes an internal functional decomposition/mechanistic story about what the model is doing (naming distinct mechanisms the model uses), not merely that interventions on certain variables affect outputs. This is a mechanistic/functional attribution (R3) rather than a pure causal-effect statement (R2). No explicit hedge (e.g., ‘may/suggests’) in the quoted sentence."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,Interchange interventions / counterfactual patching-style activation replacement to generate data and evaluate a causal model’s predicted next-token distribution (JSS≈0.95),2,2,0,4,0,"The supporting evidence described in the paper is interventional: they run paired original/counterfactual examples and perform interchange interventions (activation replacement) and then evaluate distributional agreement (e.g., JSS 0.95) between their causal model M and the LM. This is Rung 2 because it establishes causal effects under specific interventions and evaluates a causal abstraction’s predictive faithfulness, not a unique/complete mechanism. The claim itself is about predictive accuracy of a constructed causal model (“estimates next token distributions with 95% agreement”), which is an empirical performance/faithfulness claim rather than a mechanistic functional attribution like “the model uses/implements X” (R3). No explicit hedge terms appear in the quoted claim."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,Interchange interventions / counterfactual patching (activation patching on paired original vs. counterfactual inputs; plus ablations/attention knockout mentioned as supporting),2,3,1,3,0,"The quoted phrase is a high-level mechanistic framing (“how LMs bind and retrieve entities in-context”), which is a functional/mechanistic attribution about the internal process the model uses, so it maps to claim_rung=3. The paper’s primary evidence for these internal-process claims comes from interchange interventions / patching-style causal interventions on activations (R2). Although the paper uses the term “counterfactual patching,” it is still an interventional method establishing causal effects under interventions rather than uniqueness/complete counterfactual mechanism identification per the codebook’s R3 methods. The phrase itself contains no explicit hedge."
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,Path patching / activation patching (circuit discovery and evaluation on prompt variants; plus mean-ablation circuit evaluation and attention-pattern comparisons),2,3,1,4,1,"The paper’s core empirical support comes from interventional circuit methods (path patching / causal effects; mean-ablation knockout evaluation), which are Rung 2. The claim that “circuits within LLMs may be more flexible and general than previously recognized” is a broad mechanistic/generalization assertion about what circuits are like (flexible/general), going beyond a specific causal effect under a particular intervention; this fits Rung 3 (mechanistic narrative about circuit properties). The wording is explicitly hedged (“may”), so hedge_flag=1. Confidence is high but not maximal because the claim is somewhat high-level/interpretive rather than a crisp mechanistic attribution to a specific component."
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,Path patching / activation patching (circuit discovery + overlap/faithfulness evaluation across prompt variants),2,3,1,4,0,"Method: The paper’s evidence for generalization/reuse comes from interventional circuit methods (path patching/mean ablation/causal effect measurements) and overlap comparisons, which are Rung 2 because they intervene on activations/paths to measure causal effects but do not establish uniqueness/complete mechanistic necessity. Claim: Saying the circuit ‘reuses all of its components and mechanisms’ attributes a mechanistic story about what the circuit is doing across variants (‘mechanisms’ reuse), which is a Rung 3 functional/mechanistic attribution rather than a purely causal-effect statement. ‘Surprisingly well’ is emphasis, not a hedge; no explicit modal language (may/suggests), so hedge_flag=0."
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,Path patching / activation patching-style causal tracing (interventional circuit analysis; plus mean-ablation knockout evaluation),2,3,1,4,0,"Method: The paper’s evidence for S2 Hacking comes from interventional circuit methodology (path patching / direct causal effect measurements and circuit evaluation via mean-ablation knockouts), which is Rung 2 because it intervenes on activations/paths and measures output effects but does not establish uniqueness/complete counterfactual necessity. Claim: “discover a mechanism that explains this” is a mechanistic/functional attribution (they posit an internal mechanism responsible for the observed generalization behavior), which is Rung 3 per the codebook’s ‘mechanism/underlies/explains how the model does X’ markers. No explicit hedge (e.g., ‘may/suggests’) appears in the quoted text."
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,Circuit discovery/analysis via path patching / causal effect estimation (activation patching) as referenced in abstract framing of mechanistic interpretability,2,3,1,4,0,"The phrase ""circuits, or minimal subgraphs within the model that implement algorithms responsible for performing specific tasks"" is a functional/mechanistic attribution: it asserts that internal subgraphs *implement* an *algorithm* that is *responsible for* task performance. Under the codebook, this is a Rung 3 mechanistic claim (functional verbs: implement/responsible). The paper’s empirical toolkit for circuits (and the IOI lineage it builds on) is primarily interventional—path patching/causal effect measurements—supporting causal mediation/sufficiency but not establishing full mechanistic implementation/uniqueness, so method_rung is R2. No explicit hedge (e.g., may/suggests) appears in the quoted fragment."
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Interchange interventions / activation patching-based evaluation on RAVEL (II; used to compute Cause/Iso/Disentangle scores) comparing MDAS to baselines,2,3,1,4,0,"The supporting evidence for “achieves state-of-the-art results on RAVEL” comes from intervention-based metrics (interchange interventions) and benchmark comparisons, which are Rung 2 methods. However, the clause “demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations” is a mechanistic/general takeaway about how interpretability should work (implying distributed features are the right explanatory level), not merely that MDAS causally affects outputs under interventions. This is closer to a Rung 3 mechanistic/narrative claim (‘importance of’ distributed features) rather than a narrow causal-effect claim. No hedge language is present."
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,Interchange intervention / activation patching on a learned feature F (bijective featurizer) and observing output change,2,3,1,4,0,"Method: they intervene on internal states via interchange interventions (a form of activation patching), which is Rung 2 interventional evidence. Claim: “feature F encodes the attribute” uses the polysemous term “encodes”; in this context it is not explicitly framed as mere decodability but as a representational/mechanistic attribution supported by an intervention, so per the codebook default it is Rung 3 (mechanistic/representation claim) rather than R2. The phrase “we have evidence that” is not an explicit hedge like ‘may/suggests’, so hedge_flag=0. Minor ambiguity because they could mean ‘causally mediates’ (R2), but the specific ‘encodes’ wording pushes to R3."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,Benchmark evaluation on RAVEL using interchange interventions (activation patching) with counterfactual labels; comparison across methods (PCA/SAE/RLAP/DBM/DAS/MDAS) via Cause/Iso/Disentangle metrics,2,1,0,4,0,"The supporting evidence is an intervention-based benchmark: interchange interventions/activation patching are used to compute causal metrics (Cause/Iso), so the underlying evaluation method is Rung 2. However, the claim itself is comparative performance language (“achieve strong results”, “struggle”) about benchmark scores, not a mechanistic or causal attribution about model internals; it’s essentially an empirical association between supervision type (counterfactual vs unsupervised featurizers) and observed benchmark performance. Thus claim_rung is R1. No explicit hedging terms are present."
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,"Interchange interventions / activation patching-based metrics (Cause/Iso/Disentangle) applied across layers, using MDAS-identified features (and related intervention-based methods)",2,3,1,4,0,"The evidence described in the Results section for this statement is based on interchange interventions (a form of activation patching) measuring how intervening on an attribute feature changes the model’s outputs (Cause) without changing other attributes (Iso) across layers (e.g., Fig. 3b). That is an interventional/causal evaluation method (R2). The claim, however, is phrased as a mechanistic narrative about internal organization: that attribute representations themselves ‘gradually disentangle’ across layers. This goes beyond a direct causal-effect statement (‘interventions have higher Iso/lower cross-attribute effects in later layers’) and attributes a structural property to the representations, so it is best coded as R3. No explicit hedge terms (may/suggests) are present."
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,Interchange interventions / activation patching-based evaluation (Cause/Iso metrics on RAVEL; comparisons across DAS/MDAS etc.),2,3,1,4,1,"The evidence comes from interchange interventions that manipulate internal features and measure output changes (Cause/Iso), which is an interventional/causal method (R2). The claim goes beyond reporting a causal effect and attributes a mechanistic property to the model’s representations: that certain attribute pairs are intrinsically entangled and that changing one has (seemingly) unavoidable ripple effects. “Unavoidable ripple effects” implies a strong mechanistic/structural constraint (close to necessity/inevitability), which fits claim rung 3 rather than a bounded causal-effect statement. The phrase “seemingly” is an explicit hedge, so hedge_flag=1."
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,Unspecified prior interpretability work; implied observational evidence such as probing/activation analysis (no specific intervention cited in this sentence),1,3,2,4,0,"The claim uses the mechanistic verb ""encode"" and asserts a general property of internal representations (""encode rich semantic information""), which defaults to a Rung 3 mechanistic/representational claim under the codebook unless clearly framed as mere decodability. Here it is stated as established fact (""has shown"") with no explicit decodability qualifier. The supporting method is not specified beyond vague reference to prior work; absent explicit interventional evidence, we code the implied evidence as observational/associational (R1)."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,Interchange interventions / distributed interchange intervention (DII) / distributed alignment search (DAS) on linear subspaces (as cited in interventional interpretability prior work),2,3,1,4,0,"The claim attributes a representational property—“human-interpretable concepts are encoded linearly”—which is mechanistic/representational language (encode) and thus maps to claim_rung 3 by the codebook’s polysemy rule (default mechanistic reading unless clearly decodability-only). The supporting evidence referenced is ‘interventions on linear subspaces’ (interchange interventions/DII), which are causal interventions establishing causal role/mediation under intervention, i.e., method_rung 2. The sentence frames this as ‘increasing evidence’ rather than a strict proof, but it does not use explicit hedge markers like ‘may/suggests’; so hedge_flag=0. Minor ambiguity: some authors use ‘encoded linearly’ to mean ‘linearly decodable,’ but here it is explicitly tied to interventional work, making R3 appropriate."
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,Distributed Alignment Search (DAS) / distributed interchange intervention (DII) subspace interventions (interventional causal abstraction framework),2,3,1,4,0,"Method: DAS learns a subspace to maximize expected counterfactual output under distributed interchange interventions, i.e., it relies on interventions on hidden representations and measuring behavioral effects, which is Rung 2. Claim: saying DAS can ""effectively localize concepts within model representations"" is a mechanistic/functional attribution about internal organization (that concepts are localized in representations), going beyond a pure causal-effect statement like ""intervening on the DAS subspace changes outputs""; this aligns with Rung 3 language (representation-structure claim). No explicit hedge terms (e.g., may/suggest) appear."
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,Benchmark performance of ReFT/LoReFT (representation interventions trained via gradient descent; no direct causal tracing/patching evidence for the specific ‘linear subspace’ generalised-control interpretation),1,3,2,4,0,"The quoted statement attributes a functional/mechanistic role to an internal structure: ‘a linear subspace … can achieve generalised control over a vast number of tasks.’ This is a mechanistic/functional attribution about what a subspace *does* (R3 markers: ‘can achieve control’, ‘generalised control’), not merely that performance improved. In the paper, the direct evidence for this statement appears to be downstream task performance across many benchmarks after training LoReFT interventions; that is an outcome/associational evaluation and does not isolate or test the causal role of a specific subspace as the mechanism (no ablation/patching/necessity tests tied to this claim). Thus method_rung is best coded as R1 for this claim, while the claim itself is R3. No explicit hedge terms (‘may’, ‘suggests’) in the quoted text."
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,Representation finetuning via learned low-rank interventions on hidden representations (LoReFT/ReFT; trained interventions applied to residual stream activations; evaluated on instruction-following benchmark),2,2,0,4,0,"Method involves explicit interventions on internal representations (learned low-rank edits applied to residual stream activations during the forward pass) and measures downstream behavioral change (instruction-following performance), which is rung-2 interventional evidence. The claim says these interventions ""can induce"" instruction-following behavior in a base LM—i.e., intervening/training on X changes Y—so it is a causal/enablement claim (R2), not a unique-mechanism or counterfactual/""the model uses"" mechanistic attribution (R3). No explicit hedge terms are present."
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,"No direct empirical method for this statement; it is background framing in the abstract (at most supported by general prompt-based recall / observational evidence in the broader literature, i.e., R1).",1,3,2,4,0,"Claim text asserts a mechanistic/storage interpretation: factual knowledge is ""stored in the parameters"" (storage/representation language → default R3 per codebook). In the abstract this is presented as a general fact with no hedge. The paper’s core empirical methods are weight-editing via a hypernetwork and constrained optimization (interventional, R2), but this specific statement is not established by a targeted intervention demonstrating storage; it functions as background motivation. Thus method rung for this claim is best coded as R1/none (choose R1 per schema), while the claim itself is R3."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,Hyper-network weight-editing trained with constrained optimization (CKL) using gradient information; analysis of resulting weight updates/which components are updated (update magnitude visualization),2,3,1,4,1,"Method: the paper intervenes on model parameters via a learned hyper-network that outputs weight updates and evaluates behavioral changes (success/retain/equiv), which is interventional (R2). Claim: saying the hyper-network is a 'probe revealing which components need to be changed to manipulate factual knowledge' is a mechanistic/functional attribution about identifying the components responsible for manipulation (""need to be changed"" / ""manipulate factual knowledge""), going beyond a pure causal-effect statement into mechanism language (R3). The phrase 'can be regarded as' is an explicit hedge (hedge_flag=1)."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,"Analysis/visualization of weight updates (e.g., magnitude/heatmaps of parameter changes across components/layers) after applying the editing method; observational logging of where updates land",1,1,0,5,1,"The claim reports an empirical pattern in the learned/predicted parameter updates—i.e., that update mass is concentrated in a small subset of components. This is an associational/descriptive statement about observed update distribution, not a causal or mechanistic attribution (no claim that those components are responsible for knowledge, only that they receive larger updates). The supporting method is observational analysis/visualization of updates (no intervention to test necessity/sufficiency of those components for behavior), so method_rung=1. The phrase “tend to be” is an explicit hedge."
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,Hyper-network weight-editing trained with constrained optimization (CKL) using gradient information; analysis based on inspecting/attributing where weight updates concentrate (update magnitude heatmaps/cosine similarities),1,3,2,4,0,"The quoted claim functionally attributes a mechanistic role: the hyper-network is said to reveal the model’s “causal mediation mechanisms,” which is a mechanistic/causal-explanatory framing (R3) rather than a narrow causal-effect statement about a specific intervention outcome. The evidence directly supporting the ‘revealing mechanisms’ part in the paper is primarily observational analysis of the learned updates (e.g., updates concentrated in certain components; comparisons to gradients), which is associational/attributional (R1). Although the overall system performs interventions (editing weights) and measures behavioral effects (R2), that supports efficacy of editing, not that the update patterns uniquely identify or reveal the underlying causal mediation mechanism. No explicit hedge words appear in the claim."
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Analysis of model updates via visualization/inspection of average normalized magnitude of weight updates across layers and matrices (Figure 4); observational comparison to gradients/fine-tuning update patterns,1,2,1,4,1,"The evidence cited for this statement is an observational analysis/visualization of where the learned editor’s weight updates are largest (e.g., attention Q/K vs V matrices), i.e., associational/diagnostic inspection rather than an intervention that selectively changes those parameters to test causal impact. Thus the supporting method is R1. The claim itself attributes the achieved knowledge manipulation to primarily modifying a particular parameter subset (those shaping the attention distribution), which is a causal/production claim about what changes are doing the work (“achieved by primarily modifying…”), so it is at least R2 rather than mere correlation. It is explicitly hedged (“seems to be”), so hedge_flag=1."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,Conceptual arguments / analysis of existing empirical works and illustrative case studies (no direct intervention on model internals for this specific claim),1,3,2,4,0,"The quoted statement asserts a mechanistic/intentional-stance requirement about model internals: to distinguish strategic deception from simpler behaviors, one must make claims about the model’s internal beliefs and goals. This is not merely correlational (R1) or an intervention effect (R2); it is a strong framing claim about what is required to disambiguate deception, invoking internal mental states (beliefs/goals) as explanatory constructs, which fits R3 (mechanistic/functional attribution). The support in the paper for this point is primarily conceptual argumentation and discussion of prior work/case studies rather than a specific causal intervention on internals tied to this claim, so method_rung is best coded as R1 per the codebook’s observational/non-interventional category. No explicit hedge terms (e.g., may/suggests) appear in the claim text."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,Conceptual/theoretical argument (no empirical method; no intervention or observational analysis reported for this statement),1,1,0,4,0,"The quoted text is posed as a framing question about conditions required for an activation probe to work well; it does not assert a specific mechanistic fact about model internals (e.g., that a particular feature/circuit exists or causes deception). It’s not an empirical causal/mechanistic claim, but a conceptual setup about probe performance requirements. Under the codebook, since no empirical method is used here, we map method_rung to the lowest rung (R1) as ‘no intervention/observational evidence’. The claim itself is also not making a causal or mechanistic attribution; it’s essentially an associational/performance framing about what would be needed for classification, so claim_rung=R1. No explicit hedges like “may/suggests” appear in the snippet (it’s a direct question), so hedge_flag=0."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,Behavioral prompt manipulation / observational analysis of model outputs across contexts (no direct internal intervention),1,2,1,4,0,"The claim asserts a comparative causal property about LMs’ internal beliefs—i.e., that changing context leads to changes in attributed beliefs more than in humans/animals (“not stable”, “context dependent”). In the paper, this is supported by examples where prompts/system instructions are varied and the authors infer belief changes from resulting behavior (e.g., breaking roleplay, identity prompts, plausibility judgments). This is not an intervention on model internals (so method_rung=1), but it is an intervention on inputs; under the codebook’s method ladder, that still counts as observational/associational with respect to internals. The claim itself goes beyond mere correlation/decodability and makes a causal generalization about how context affects beliefs, so claim_rung=2. No explicit hedge terms like “may/suggests” appear in the quoted claim."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,Comparison of labels from a full-transcript LLM autorater vs. MASK’s labeling procedure (observational agreement analysis / confusion matrix),1,1,0,5,0,"The evidence is an observational/associational analysis: they compute/inspect agreement (and show confusion matrices) between two labeling sources without intervening on model internals or behavior. The claim itself is purely correlational/descriptive (“very low agreement”), not causal or mechanistic, so claim_rung=1. No explicit hedging terms are present."
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,Conceptual argument / literature-based discussion (no direct new intervention or observational measurement supporting this specific claim in the quoted sentence),1,1,0,3,1,"The sentence is a general empirical-sounding statement about current models: that strategically deceptive behavior typically involves a consistent mechanism. In context, the authors immediately motivate this by pointing to verbalized deceptive intent/chain-of-thought as a shared pattern, but the quoted claim itself is not backed by a specific reported experiment, probe, patching, or ablation result in this paper; it functions as a synthesis/associational regularity claim. The wording is hedged (“mostly true today”), so hedge_flag=1. Claim rung: it asserts existence of a consistent mechanism (a regularity) rather than a specific causal effect or detailed functional mechanism (‘X computes Y’), so R1 fits best; it’s not merely decodability, but also not a mechanistic attribution to a particular component/circuit, so I avoid R3. Moderate ambiguity because “mechanism” can read mechanistically (R3) in other contexts, but here it’s used as a broad regularity claim about current models’ deception often being accompanied by verbalized intent."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,RAVEL benchmark evaluation via distributed interchange interventions / activation patching-style interventions (DAS/MDAS/HyperDAS interventions on residual stream subspaces),2,1,0,5,0,"The supporting evidence is benchmark performance measured under explicit internal interventions (interchange interventions/patching), which is an interventional method (R2). However, the claim itself is not a mechanistic attribution about what a component computes; it is a comparative performance claim (“state-of-the-art performance on RAVEL”) about an evaluation metric. Under the codebook’s claim rungs (which target claims about model internals), this is best treated as a non-mechanistic/associational outcome claim and mapped to the lowest rung (R1) rather than R2/R3. No hedging language is present."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,"Interchange interventions / Distributed Alignment Search (DAS)-style causal interventions (patching hidden-state features using counterfactual data), as described in the abstract/background",2,2,0,4,0,"The claim asserts that certain internal features *mediate* concepts and *enable predictable manipulation*. “Mediate” and “enable manipulation” are causal-effect language (R2): intervening on the feature changes downstream behavior in a controlled way. It does not clearly assert a unique mechanism/circuit or functional computation beyond causal mediation, so it stays at claim_rung 2. The paper’s core evidence involves interchange interventions/patching learned subspaces, which is interventional (method_rung 2). Minor ambiguity: “features” could be read mechanistically, but the phrasing is standard causal-mediation framing rather than ‘the model uses/represents/computes’."
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,HyperDAS / DAS-style distributed interchange interventions with learned token-position selection (interventional patching of residual-stream subspace using counterfactual activations; evaluated on RAVEL via causal effect on outputs),2,3,1,4,0,"The supporting evidence for HyperDAS’s localization is based on interchange interventions/patching: selecting token positions and a subspace, then intervening by replacing/patching those features with counterfactual values and measuring output changes on RAVEL. That is an interventional causal method (R2). The claim language—“locates the token-positions … that a concept is realized in”—is a mechanistic/representational attribution about where the concept is realized in the model, not merely that intervening there affects behavior. Per the codebook, ‘realized in’ functions like ‘represented/encoded’ and defaults to a mechanistic reading (R3) rather than a purely causal-effect statement (R2). No explicit hedge is present."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,Interchange interventions (interchange intervention experiments / causal mediation-style interventions on internal states),2,2,0,5,0,"The claim is a general statement about what interchange interventions do: they “identify neural representations that are causal mediators” of concepts. This is a causal mediation claim (R2) rather than a mechanistic/functional attribution (R3), and interchange interventions are explicitly interventional (they set internal features to counterfactual values and measure output changes), which is method rung 2. No explicit hedging language is present."
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,Interventional analysis via HyperDAS token-position selection + interchange intervention/patching across layers (examining which tokens are selected for intervention at different layers; Fig. 4),2,3,1,4,0,"The supporting evidence is based on interventions (interchange intervention/patching) and analyzing the learned intervention locations across layers, which is Rung 2. The claim, however, uses mechanistic/storage language: tokens at deep layers were “previously unknown to store attributes,” which attributes a representational/storage role to those positions (a mechanistic interpretation rather than just ‘intervening there changes outputs’). There is no explicit hedge in the quoted text. While the paper’s evidence shows HyperDAS selects those positions and interventions can work there (causal effect), ‘store attributes’ goes beyond the demonstrated causal effect into a representation/storage claim, so R3."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,Steering vectors / activation addition intervention at inference time (add/subtract direction in residual stream) with evaluation of behavioral change; supported by attribution patching to select causally relevant layers,2,2,0,4,0,"The supporting evidence is interventional: they add/subtract extracted steering vectors to activations and measure changes in reasoning-behavior metrics (and use attribution patching as a causal relevance test). The claim says the behaviors are ""mediated"" by linear directions and ""can be controlled"" via steering vectors—both are causal-effect statements about interventions changing behavior, which fits claim rung 2. It does not clearly assert a full mechanistic account/unique circuit (R3), though ""mediated"" could be read mechanistically; here it is tied directly to demonstrated control under intervention, so R2 is the best match."
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,Steering vectors / activation addition and subtraction at inference time (intervention on residual stream activations; evaluated by changes in annotated behavior fractions),2,2,0,5,0,"The evidence described is an explicit intervention (adding vs. subtracting a steering vector) and measuring downstream behavioral changes, which is Rung 2. The claim asserts that steering increases/suppresses behaviors and that this confirms a causal influence—i.e., an intervention changes an outcome—so it is a causal claim (Rung 2), not a full mechanistic/functional attribution (Rung 3). No explicit hedging terms are present."
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,Steering vector intervention (activation addition/subtraction at inference time) with evaluation of behavioral changes; supported by attribution patching for layer selection,2,3,1,4,0,"The direct evidence comes from interventions (adding/subtracting steering vectors and measuring changes in annotated behaviors across models), which is Rung 2. The claim, however, goes beyond causal effect to a mechanistic representational assertion: that thinking LLMs ""encode"" reasoning mechanisms ""as linear directions"" in activation space. Per the codebook, ""encode"" defaults to a mechanistic/representation claim (R3) unless clearly used in a decodability sense; here it is tied to a hypothesis about internal representation structure, not merely behavioral controllability. The phrase ""reinforcing the hypothesis"" is not an explicit hedge marker (no may/might/suggests), so hedge_flag=0."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,"Steering vectors extracted via Difference-of-Means on residual stream activations, with interventional evaluation by adding/subtracting vectors at inference time (steering) and layer selection supported by attribution patching (gradient-based approximation to activation patching)",2,3,1,4,0,"Method: The paper intervenes on activations by adding/subtracting steering vectors and measures behavioral changes (and uses attribution patching to assess causal relevance), which is Rung 2 interventional evidence. Claim: Saying behaviors “can be isolated to specific directions in the model’s activation space” plus “enabling precise control” goes beyond a pure causal-effect statement and attributes a mechanistic representational structure (that the behavior corresponds to a specific direction), which is Rung 3 per the codebook’s ‘encodes/represents/uses/is isolated to a direction’ style mechanistic language. No explicit hedge terms appear in the quoted claim."
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,Steering vectors (activation addition/subtraction) with attribution patching (gradient-based approximation to activation patching) to select causally relevant layers; behavioral effects measured under intervention,2,3,1,4,0,"The supporting evidence described in the results section involves interventions on activations (adding/subtracting steering vectors; attribution patching to assess causal relevance), which is Method Rung 2. The claim asserts that the models have “distinct mechanisms” for reasoning—i.e., a mechanistic/functional attribution about how the model achieves reasoning, not merely that interventions change behavior. This is a Rung 3 mechanistic claim (mechanisms, distinctness). The phrase is not explicitly hedged (no ‘may/suggests’), so hedge_flag=0. Minor ambiguity: they might mean ‘distinct linear directions’ (which could be framed as R2 causal mediation), but ‘distinct mechanisms’ defaults to mechanistic reading per codebook."
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,Supervised activation steering via hypernetwork-generated steering vectors; evaluation on AxBench with activation interventions (adding steering vector to residual stream at a fixed layer) and measuring downstream steering performance (LM outputs judged by gpt-4o-mini).,2,1,0,4,0,"The supporting evidence is an interventional steering setup: they add a learned steering vector to the model’s residual stream and measure changes in behavior/performance, which is Rung 2 methodologically. However, the claim itself is a comparative performance statement (“exceeds the performance of… methods”) rather than a causal/mechanistic attribution about internals. Under the codebook, such outcome/benchmark claims are best treated as associational/empirical performance reporting (R1 claim rung), not a claim that a specific internal component causes/implements a mechanism (R2/R3). No explicit hedge words are present."
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,Activation steering via hypernetwork-generated steering vectors added to the base LM residual stream (interventional activation addition; evaluated on AxBench with LLM-judge scores),2,1,0,4,0,"The supporting evidence is an explicit intervention on model activations (adding a steering vector at a specified layer), so the method is Rung 2. The claim itself is a comparative performance statement about an outcome metric (""performs on par"" with prompting), not a claim about internal representations/mechanisms or causal roles of specific components. Under this codebook, such outcome/benchmark claims are best treated as non-mechanistic/associational (R1) rather than causal/mechanistic (R2/R3), even though the method is interventional."
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,Interventional activation steering (adding a hypernetwork-generated steering vector to the base LM residual stream at a specified layer; evaluated by output changes on held-out steering prompts),2,1,0,5,0,"The supporting evidence is an intervention on the model’s activations (activation addition/steering), so method_rung=2. However, the claim itself is a comparative performance statement about generalization to unseen steering prompts (i.e., an empirical outcome/association: higher steering score than baselines), not a causal/mechanistic attribution about internals. It does not use causal/mechanistic language like ‘mediates’, ‘is responsible for’, ‘encodes’, etc., so claim_rung=1."
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,Compute-efficiency analysis comparing TFLOPs required to reach a target eval loss as dataset size increases (training-time measurement + curve fit; no mechanistic intervention on internals beyond standard training),1,1,0,4,0,"The claim is about an empirical scaling relationship in compute cost (“more economical”) as training data increases, supported by measuring TFLOPs/steps to hit a fixed loss (Fig. 3; App. A.3.2) and comparing to a baseline. This is observational/associational evidence about efficiency, not a causal/mechanistic claim about model internals. Although it involves training runs, the evidence is a performance/compute comparison rather than an intervention isolating causal contributions of internal components, so method_rung=1. The claim itself is also rung 1: it asserts a comparative trend in compute efficiency, not that a specific internal component causes behavior or that the model ‘uses/encodes’ something."
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,Pairwise cosine similarity analysis of steering vectors (observational) plus comparison of steering performance metric across model variants (evaluation; not an intervention targeted to this specific claim),1,2,1,4,0,"The claim asserts an effect of an architectural change/conditioning (“additional conditioning”) on two outcomes: (i) it *weakens* residual inter-concept similarity and (ii) it does so *without* reducing steering performance. This is causal language about the impact of conditioning (R2), not merely an association. However, the evidence described in the surrounding text is primarily observational: they compute cosine-similarity heatmaps and report performance scores for variants, without a controlled intervention/ablation isolating conditioning while holding all else fixed for this specific statement. Thus the supporting method is best runged as R1, while the claim itself is R2 (implying a causal effect of conditioning). No explicit hedge terms appear in the quoted claim."
2510.01070,2510.01070-01,Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor,abstract,3,White-box mechanistic interpretability readouts using logit lens and sparse autoencoders (SAE feature attribution / SAE token readouts) evaluated by measuring auditor success rate improvement vs baseline,1,2,1,4,0,"The supporting techniques named (logit lens; SAEs used to extract tokens/features/descriptions from activations) are primarily observational readouts of internal states without intervening on the model’s computation, so method_rung=1. The claim asserts an effect on an outcome metric—these techniques “increase the success rate” of the auditor—i.e., using the technique causes higher auditor success in their evaluation, which is a causal/impact claim about an intervention in the auditing setup (providing white-box info), so claim_rung=2. No explicit hedging (e.g., may/suggests) appears."
2510.01070,2510.01070-02,secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly,results,1,White-box mechanistic interpretability readouts (logit lens; activation–token embedding similarity; SAE feature/token readouts) evaluated by downstream auditor success-rate improvement vs I/O baseline (Fig. 5; control using base model readouts),1,2,1,4,0,"The supporting evidence is that providing internal readouts (tokens/features/descriptions) to an auditor increases secret-guessing success rate relative to an I/O baseline, and does not help when extracted from the untuned base model. These are observational readouts of internal activations (logit lens/SAE attribution) rather than interventions on the secret-keeper’s internals, so method_rung=1. The claim says the secret knowledge can be 'successfully extracted' from internal states even when not verbalized; this is an empirical effectiveness claim about a procedure producing usable information, not a mechanistic 'the model represents/encodes X' attribution. It implies that access to internal states enables recovery of the secret (a causal/efficacy claim about using the method), so claim_rung=2. No explicit hedging ('can be' here is capability, not uncertainty), so hedge_flag=0."
2510.01070,2510.01070-03,Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal,body,1,White-box mechanistic interpretability readouts: logit lens + SAE feature attribution/inspection (and activation–token embedding similarity) on internal activations; presented as evidence via example Figure 4 (cherry-picked).,1,2,1,4,0,"The cited evidence is primarily observational readout/inspection of internal states (logit lens token distributions; SAE features/tokens/descriptions) without an intervention demonstrating causal effect on behavior, so method_rung=R1. The claim says these methods “reveal strong signals corresponding to the female gender during this refusal,” i.e., that gender information is present/decodable in internals even when the output refuses. That is an associational/decodability-style claim (R1) in many contexts, but here it is framed as successful extraction of the secret from internals during refusal; it does not assert a full mechanism (“the model uses X to do Y”), but it goes beyond mere correlation by implying the internal signal tracks the secret state in a way usable for elicitation. I therefore code the claim as R2 (causal/elicitation effectiveness framing) rather than R1, though it’s close; no explicit hedge words are used."
2510.01070,2510.01070-04,Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it,results,2,Downstream behavioral evaluation tasks comparing base vs fine-tuned vs in-context-secret baseline (accuracy / GPT-4-judged scores on tasks requiring use of the secret); no direct internal activation intervention,1,3,2,4,0,"The evidence described for ‘internalize’ is performance on downstream tasks (Taboo: chooses secret word; SSC: follows held-out Base64 constraints; User Gender: answers gender-dependent binary questions) and comparisons to baselines. This is observational/associational with respect to internals (no causal intervention on internal states), so method_rung=1. The claim ‘successfully internalize their secret knowledge’ and especially ‘are aware of it’ attributes an internal state/representation/awareness to the model (mechanistic/mentalistic), which is rung 3 per the codebook’s guidance that internal-representation language goes beyond decodability/association. No explicit hedge is present."
2510.01070,2510.01070-05,Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques,body,1,"No direct empirical method; this is a motivating premise/theoretical assertion (at most implicitly supported by later white-box readouts like logit lens/SAE attribution, which are observational).",1,3,2,4,1,"The sentence asserts a mechanistic necessity: to use secret knowledge, the model ""must internally represent"" it, and therefore it should be extractable via interpretability. This is a functional/mechanistic claim about internal representation (R3), not merely decodability/correlation. It is also explicitly hedged with ""should be able to,"" so hedge_flag=1. No intervention/counterfactual test is referenced in the claim itself; the relevant techniques mentioned (mechanistic interpretability readouts like logit lens/SAEs) are primarily observational extraction methods (R1) rather than causal tests of necessity/sufficiency, so method_rung=1."
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,Weight-based eigendecomposition / spectral analysis of bilinear MLP weight-derived interaction matrices (linear algebra decomposition; no intervention),1,1,0,4,0,"The claim reports an empirical observation from analyzing weights via eigendecomposition: that the resulting spectra/structure are (i) low-rank and (ii) interpretable across several domains. This is based on observational analysis of weights (no ablation/patching/steering), so method_rung=1. The wording “reveals interpretable low-rank structure” is primarily descriptive/associational (the decomposition output appears interpretable and low-rank), not a causal or mechanistic attribution about what the model computes or uses; thus claim_rung=1. Minor ambiguity: ‘interpretable’ can sometimes be read as a stronger mechanistic assertion, but here it’s framed as qualitative property of the decomposition results rather than ‘the model uses/encodes X’, so R1."
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,Eigendecomposition / spectral analysis of bilinear MLP weight-derived interaction matrices (visualization of top eigenvectors; no intervention),1,3,2,4,0,"Method: They compute interaction matrices Q=u·out B from weights and perform eigendecomposition, then qualitatively inspect/visualize the resulting eigenvectors (Figure 2B). This is observational/associational analysis of weights/derived features with no causal intervention on activations/weights for this specific statement → method_rung=1. Claim: Saying eigenvectors 'represent curve segments' and 'function as localized edge detectors' is a functional/mechanistic attribution (what the eigenvectors do/are), not merely that they correlate with or are decodable from data → claim_rung=3. No explicit hedge language ('appear' is used elsewhere in the section header, but the provided claim text itself is stated as fact) → hedge_flag=0."
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,"Weight-based eigendecomposition of bilinear MLP weights to obtain eigenvectors, then construct and apply adversarial input masks (input-level intervention) and measure misclassification vs random-mask baseline",2,2,0,4,0,"The key evidence is an intervention: adding an adversarial mask to the input (constructed from the extracted eigenvectors) and observing a change in model output (misclassification), compared against a random-mask baseline (Fig. 7). This supports a causal claim that the eigenvector-derived directions are causally efficacious for steering/misclassification under that intervention (R2). The claim does not assert a unique mechanism or that the model ‘uses’ these features as the underlying computation in a counterfactual/complete-mechanism sense; it stays at ‘demonstrating causal importance’ via an intervention, so claim_rung=2. Method is also interventional (R2) because it perturbs inputs and measures behavioral effect."
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,"Weight-based analysis of bilinear MLP weights via interaction matrices + eigendecomposition, using SAE-derived input/output features; supported by observational statistics (top interactions, cosine-similarity projections, activation correlation with low-rank approximation). No direct causal intervention/ablation/patching reported for this specific circuit claim.",1,3,2,4,0,"The evidence described for the ‘sentiment negation circuit’ is based on analyzing weights (interaction submatrix), projecting SAE features onto eigenvectors, and reporting correlations between a truncated eigenvector approximation and feature activations—these are observational/associational analyses without interventions on the model’s internals, so method_rung=1. The claim text uses mechanistic/functional language (‘circuit’, ‘computes’, ‘via AND-gate-like interactions’) attributing a specific computation to an internal structure, which is a rung-3 mechanistic claim rather than a mere correlation/decodability statement. The sentence is not explicitly hedged (no “may/suggests”), so hedge_flag=0."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,"Eigenvector decomposition of bilinear MLP weights to form truncated low-rank approximations of SAE output feature activations, then compute (active-only / tail) correlations between true activations and approximations (Section 5.2, Fig. 9)",1,1,0,5,0,"The supporting evidence is purely observational/associational: they compute correlations between SAE feature activations and low-rank approximations derived from weight eigendecomposition, without intervening on the model (no ablation/patching/steering). The claim itself is explicitly correlational (“well-correlated”) and restricted to a regime (“particularly at large activation values”), so it is a Rung 1 associational claim. No causal/mechanistic language (e.g., ‘causes’, ‘responsible for’, ‘computes’) is asserted here. No explicit hedge terms like ‘may/suggests’ appear."
2406.11779,2406.11779-01,The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit,body,1,Qualitative examination of QK/OV components + SVD of EQKE + (zero) ablation of EQKP (and likely similar for PVOU/EU) to argue components are unimportant; observational matrix/heatmap inspection for EVOU copying claim,2,3,1,4,0,"The claim uses functional/mechanistic language: the model achieves Max-of-K ""by attending"" (QK circuit) and ""copying"" (OV circuit), which is an attribution of what computations the circuits perform (R3). The supporting evidence described in Sec. 3.1 includes (i) qualitative inspection and SVD showing EQKE is dominated by a rank-1 component correlated with token size (R1), and (ii) an explicit intervention: zero ablation of EQKP with negligible accuracy change, used to argue EQKP is unimportant (R2). Because at least part of the evidence is interventional, the best method rung directly supporting the claim is R2. However, the overall statement goes beyond causal effect of ablating a component and asserts the mechanism/algorithm the model uses (attend-to-large + copy-attended), so claim_rung is R3. No explicit hedging terms (e.g., may/suggest) appear in the quoted sentence."
2406.11779,2406.11779-02,EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component,body,1,Singular-value decomposition (SVD) of the EQKE matrix (linear algebra / dimensionality reduction),1,1,0,5,0,"The evidence is purely observational: they compute an SVD and report the relative magnitudes of singular values/components, with no intervention on the model (Method R1). The claim itself is a descriptive statistical property of an internal matrix (“contains a single large rank-one component… 620× larger than the second”), i.e., an associational/measurement claim about structure revealed by SVD rather than a causal or mechanistic functional attribution (Claim R1). No hedging terms like “may/suggests” appear in the quoted claim."
2406.11779,2406.11779-03,Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning,body,1,Zero ablation of EQKP (position-dependent QK component) and measuring accuracy change,2,2,0,5,0,"Method: they intervene on the model by zero-ablating EQKP and observe the effect on accuracy, which is an interventional/causal test (R2). Claim: 'confirming EQKP is unimportant to model functioning' is a causal-importance/necessity-style statement supported by the ablation result (i.e., ablating EQKP does not change accuracy), so it is a rung-2 causal claim about (lack of) effect on performance, not a mechanistic functional attribution (R3). No explicit hedging language."
2406.11779,2406.11779-04,Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds,abstract,3,"Quantitative correlational analysis across proof strategies/models using metrics (proof length via estimated FLOPs/trace length; mechanistic understanding via unexplained dimensionality; faithfulness via singular value ratio; bound tightness via certified accuracy lower bound), plus qualitative examination of a subset of proofs",1,1,0,4,1,"The claim asserts relationships of the form “seem to require/provide” and “leads to tighter bounds” between measured variables (proof length, mechanistic understanding, faithfulness, bound tightness). In the paper these are supported by empirical plots/correlations (e.g., Fig. 5 correlation between FLOPs and unexplained dimensionality; Fig. 4 correlation between σ1/σ2 and normalized bound) and qualitative inspection, without interventions on model internals to establish causal effects. Thus the supporting method is observational/associational (R1). The language is partly causal (“leads to”) but is framed as correlational/suggestive (“seem to”, “suggestive evidence”), so per codebook it is best coded as an associational claim (R1) with hedging present."
2406.11779,2406.11779-05,Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices,body,1,Analytical/theoretical argument supported by empirical observation of approximation error growth under pessimization (rank-1/SVD approximations + worst-case error composition discussion; no direct causal intervention on model components),1,2,1,4,0,"The claim asserts a causal/constraint relationship: that compounding (worst-case) structureless error terms are a key challenge when approximating constituent matrices with rank-1 components—i.e., the approximation procedure leads to error growth that makes bounds vacuous. This is more than a mere association (R1) and is framed as an obstacle caused by error composition, so claim_rung=2. The support in the paper is primarily an argument plus empirical magnitude comparisons of actual vs pessimistic error bounds (e.g., Appendix G.2.5 discussion), not an intervention like ablation/patching on model internals to establish causal effects on behavior; thus method_rung=1."
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,Pearson correlation benchmarking of RelP vs AtP against activation patching (AP) on IOI prompts (compute PCC between attribution scores and AP scores); qualitative alignment plots,1,1,0,5,0,"The evidence described for this abstract claim is an observational comparison: they compute alignment (PCC) between RelP/AtP attribution scores and activation patching results across prompts/components. This is correlational/associational evaluation of approximation quality, not an intervention establishing new causal effects in the model. The claim itself is also about approximation accuracy (“more accurately approximates”) rather than a mechanistic/causal statement about model internals, so it is R1."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,Pearson correlation evaluation comparing RelP vs attribution patching against activation patching (benchmarking approximation quality; activation logging/statistical correlation),1,1,0,5,0,"The quoted statement is a purely quantitative associational result (a Pearson correlation value) about alignment between two methods’ outputs on MLP outputs in GPT-2 Large. No causal/mechanistic language (e.g., ‘responsible’, ‘computes’, ‘mediates’) is present; it reports a correlation coefficient. The supporting method is computing correlations over prompts, which is observational/statistical (R1)."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,"Relevance Patching (RelP) circuit discovery using LRP-based attribution patching; evaluated with faithfulness/completeness via mean-ablation-style circuit evaluation and compared against Integrated Gradients (IG) (Section 5.2, Fig. 3).",2,1,0,4,0,"The evidence is based on interventions used to evaluate circuits (keeping only circuit active / mean-ablating complement) and comparing resulting task metric—this is interventional (R2) rather than purely observational. However, the claim itself is not a mechanistic attribution about what components compute; it is a performance/benchmarking statement that RelP’s discovered circuits have similar faithfulness scores to IG while being cheaper. This is best treated as an associational/empirical comparison claim (R1) about measured faithfulness and compute cost, not a causal/mechanistic claim about internals. No explicit hedging (""achieves"" / ""without"")."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,Circuit discovery using SAE features with RelP/IG contribution scoring and evaluation via faithfulness/completeness using mean-ablation (circuit-only active vs mean-ablated baseline),2,2,0,4,0,"The supporting evidence is interventional: they evaluate circuits by activating only the selected feature set and comparing performance to a mean-ablated baseline (faithfulness), which is an ablation-style causal test (R2). The claim that ~100 features ‘explain most of the model’s behavior’ is a causal sufficiency/mediation statement about capturing performance under intervention, not a mechanistic functional attribution of what the features compute (so R2 rather than R3). No explicit hedge is present."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,"Relevance Patching (RelP), a gradient/LRP-based attribution method benchmarked via correlation to activation patching (AP)",1,3,2,4,0,"Method: RelP itself is an attribution-style, gradient/LRP propagation coefficient approach that does not directly intervene on model internals to measure output changes; it produces attribution scores and is evaluated by alignment (e.g., Pearson correlation) with activation patching, so the direct supporting evidence is observational/associational (R1). Claim: “enables more faithful localization of influential components” asserts that the method can localize the components that are influential/causally relevant, which is a mechanistic/functional attribution about internal responsibility rather than merely ‘correlates with’ or ‘better matches AP scores’; this reads as a stronger mechanistic claim (R3) rather than a purely correlational performance claim. No explicit hedge words present."
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,"Sparse attention post-training with constrained-loss objective (sparsity regularization + GECO), evaluated by measuring pretraining cross-entropy loss and fraction of active attention edges (no mechanistic intervention/patching needed for this claim)",1,2,1,4,0,"The evidence for this claim is an empirical before/after training result: they post-train to induce sparsity and then report (i) achieved attention-edge sparsity (~0.3%) and (ii) that the final pretraining loss matches the original/target loss. This is not a causal/mechanistic claim about internal components producing behavior; it’s a performance-retention claim about an intervention (post-training) causing loss to be retained while sparsity increases, so the claim is causal in the sense of an intervention changing properties (R2). The method used to support it, however, is essentially observational evaluation of metrics after training (reporting loss and sparsity), not an interpretability intervention like patching/ablation; hence method_rung=R1. No explicit hedging in the quoted claim."
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,Activation patching / logit attribution circuit discovery by keeping top‑k attention heads (intervening by patching/freezing other heads; Figures 4 and related text in §4.2),2,1,0,4,0,"The supporting evidence is an interventional circuit-discovery setup using activation patching/logit attribution where components are intervened on (kept vs patched/frozen) and the resulting change in logit difference/effect explained is measured, so method_rung=2. The claim itself is a quantitative comparative statement about how many heads are needed to reach a threshold (90% of clean-model effect) in sparse vs standard models; it does not attribute a specific computation/mechanism to the heads, nor claim responsibility/encoding, so it is an empirical performance/size comparison rather than a mechanistic functional attribution. Under the codebook, that is best treated as an associational/measurement claim (R1) even though it is established via an intervention."
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,Activation patching / logit attribution circuit discovery with per-sentence (single-instance) component importance scores; edges kept vs fraction of logit-difference effect explained (Fig. 5).,2,1,0,4,0,"The supporting evidence is interventional: they patch/replace activations (freeze/ablate components by patching to corrupted/zero/mean) and measure change in logit difference, so method_rung=2. However, the specific claim is a quantitative comparison about circuit *size* needed to reach a threshold of explained effect (""50–100× fewer edges to reach 90% of cumulative single-instance effect""), which is an empirical measurement/association about the outcome of their circuit-discovery procedure rather than a mechanistic functional attribution (no 'encodes/implements/is responsible for' language). Thus claim_rung=1 (descriptive/associational result about measured edge counts at a threshold), not a causal/mechanistic claim about what the edges do."
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,Activation patching / logit attribution circuit discovery by freezing/patching components and measuring explained logit difference; plus attention-edge sparsity measurement (counting non-zero edges),2,2,0,4,0,"The claim is that inducing local attention sparsity leads to smaller task-specific circuits (fewer heads/MLPs/edges), quantified as “up to 100× fewer edges.” In the paper, circuit size is operationalized via interventional circuit discovery: ranking components by importance scores and then intervening (patching/freezing/ablating) to find a minimal set explaining ~90% of the logit difference (Sec. 4.2; Figs. 4–5). That is Rung 2 evidence because it measures causal effect of removing/patching components on behavior. The claim itself is primarily about a causal consequence of the intervention (“sparsity cascades into simplification” and “circuits involve fewer components/edges”), not a unique mechanism of how the model computes the task, so it fits Rung 2 rather than Rung 3. No explicit hedge words appear in the quoted claim."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,"Sparse attention post-training (hard attention gating with L0-style sparsity regularization) plus activation/attention-edge logging to quantify % active edges (e.g., Table 1)",1,1,0,4,0,"The claim is a comparative statement about how information flow is distributed across attention edges in dense vs sparsified models. In the paper, this is primarily supported by measuring/visualizing attention connectivity (fraction of non-zero/active attention edges, attention patterns), which is observational/associational rather than a targeted causal intervention on specific components to show necessity/sufficiency for a behavior. The language (“diffused across many attention edges” vs “consolidates … into a small number of edges”) describes a structural distributional property of attention connectivity, not a mechanistic ‘does X’ circuit claim, so claim_rung=1. Method_rung is coded as 1 because the direct evidence for this specific statement is attention-edge sparsity measurement/logging; although the paper also uses interventional methods elsewhere (activation patching), those are not the direct support for this particular diffusion/consolidation claim."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,Sparse autoencoder (TopK SAE) feature attribution / feature–concept correlation analysis (observational) plus steering intervention experiments on SAE latents (decoder-vector addition) to test control over generation,2,2,0,4,0,"The claim has two parts: (1) “TopK SAEs can reveal biologically meaningful latent features” is supported by SAE feature–concept correlation/activation-pattern interpretability, which is primarily observational (R1). (2) “high feature–concept correlation does not guarantee causal control over generation” is a causal/negative-sufficiency statement about interventions: even when a latent correlates with a concept, intervening (steering) on it may not change generation as desired. Because the paper performs steering interventions (adding scaled decoder vectors) and measures changes in generated outputs, the relevant supporting method is interventional (R2). The claim itself is about (lack of) causal control under intervention, so claim_rung is R2 rather than R3 (it does not assert a specific mechanism, only that correlation is insufficient for control). No explicit hedging terms appear in the quoted sentence."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Ordered SAE feature identification + steering intervention on SAE latents (add scaled decoder vector to hidden state; measure change in IGHJ4 proportion/correlations),2,2,0,4,0,"The supporting evidence in the paper for “steerable features” comes from an explicit intervention (steering by adding α·decoder vector to the hidden state and observing systematic changes in generated gene proportions), which is a Rung 2 method. The claim says Ordered SAEs “reliably identifies steerable features,” i.e., that the method yields features that can be used to causally control generation under steering; this is a causal/utility claim about intervention effects (R2), not a full mechanistic attribution/uniqueness claim (R3). No explicit hedge terms (e.g., may/suggests) appear in the quoted claim."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,TopK/Ordered SAE latent activation analysis + linear probe performance (logistic regressor on SAE latents) comparing to hidden neurons; no intervention for this specific claim,1,3,2,4,0,"The supporting evidence in the surrounding text is observational: they train a logistic regressor on frozen SAE latents and report high accuracy/F1, plus qualitative activation-pattern inspection. That is Rung 1 (associational/decodability-style evidence) because it does not intervene on the model to test causal influence. The claim text uses mechanistic/representational language—“latents collectively represent antibody information”—which defaults to a Rung 3 mechanistic attribution per the codebook’s ‘represents/encodes’ decision rule (not explicitly framed as ‘decodable’). No explicit hedge words appear in the quoted claim."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE latent activation analysis / feature attribution (TopK SAE) with observational inspection of which residues/positions latents activate on (plus correlational selection via probe/correlation weights),1,3,2,4,0,"The evidence described for this statement is observational: they inspect which SAE latents activate on which aligned residue identities/positions and infer a pattern (no intervention/patching/ablation needed to support the specific ‘encoded’ statement). That places the supporting method at R1. The claim uses mechanistic/representational language—“latents encoded contextual information”—which, per the codebook’s polysemy rule for ‘encodes’, defaults to a mechanistic representation attribution (R3) unless clearly framed as mere decodability; here it is tied to an interpretation about context dependence of activations rather than a probe result, so R3. No explicit hedge terms (e.g., may/suggests) appear in the quoted claim."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering via adding a scaled SAE decoder vector to the hidden state (Eq. 6); measure change in generated-sequence IGHJ4 proportion; report Pearson correlation across steering factors,2,2,0,5,0,"The supporting method is an explicit intervention on the model’s activations (steering), which is Rung 2. The claim reports an observed causal effect of that intervention on an output metric (IGHJ4 proportion increases under positive steering; quantified by Pearson R=0.939), without asserting a unique mechanism or that the latent ‘represents/encodes’ IGHJ4. Therefore the claim is a Rung 2 causal-effect claim, well-matched to the method."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,Replication of SAE feature extraction (activation logging/SAE feature attribution) and feature steering interventions (clamping/steering coefficients) on Llama 3.1 using open-source SAEs,2,1,0,4,0,"The quoted claim reports an empirical replication outcome: that they can reproduce the *capabilities* of (i) feature extraction and (ii) steering. The evidence described in the paper for steering involves intervening on the model by clamping/adding a steering coefficient to SAE features, which is an interventional method (R2). However, the claim itself is not phrased as a causal/mechanistic attribution about internals (e.g., not ‘feature X controls Y’); it is a performance/observational success statement (‘we reproduce capabilities’), so it fits best as a rung-1 empirical claim about achieving an effect/replication rather than a specific causal claim about a component’s role. No explicit hedge terms appear in the sentence."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,Replication-style feature steering experiments using SAEs on Llama 3.1 (intervening by clamping/steering coefficients across layers and contexts),2,1,0,4,0,"The supporting evidence described in the paper involves direct interventions (feature steering/clamping) and observing output changes across layer choice, coefficient magnitude, and prompt context, which is a Rung 2 interventional method. However, the quoted claim itself is framed as an empirical robustness/fragility observation (“exhibits substantial fragility”, “sensitivity to …”), not a mechanistic attribution about what internal components compute/represent. It is essentially reporting that outcomes vary with experimental conditions, i.e., an observed dependence/association pattern, so claim_rung=1. (If it had said steering *causes* failures or that a specific internal mechanism is responsible, it would move toward R2/R3.)"
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,Activation logging / SAE feature attribution (observing SAE feature activations and comparing features; no explicit intervention implied in the claim),1,1,0,4,0,"The claim reports observations about activation patterns (“observe non-standard activation behavior”) and an empirical difficulty in distinguishing similar features, which are both descriptive/associational statements about what is seen in activations rather than causal/mechanistic attributions. No intervention language (ablation/patching/steering) is present in the claim itself, so the supporting method is best matched to R1 observational analysis of SAE activations and qualitative comparison of features. Minor ambiguity because the paper also studies steering elsewhere, but this specific abstract sentence is framed as observation/demonstration of difficulty, not a causal effect."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,"Replication-style evaluation of SAE feature extraction and feature steering on Llama 3.1 using open-source SAEs; primarily qualitative/empirical stress-tests of steering behavior and feature interpretability (feature steering interventions, i.e., clamping features)",2,1,0,4,0,"Method: The paper’s evidence for this abstract-level statement comes from intervening on the model via SAE feature steering/clamping and observing output changes/fragility across layers, magnitudes, and contexts—an interventional setup (R2). Claim: “often fall short of the systematic reliability required for safety-critical applications” is an evaluative performance/reliability generalization about the method’s robustness, not a mechanistic attribution about internals. It does not assert a specific causal role or a unique mechanism; it’s closer to an empirical adequacy/robustness assessment (R1). No explicit hedge words (e.g., may/suggests) appear in the quoted sentence, so hedge_flag=0."
2507.08802,2507.08802-01,any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative,abstract,3,"Theoretical proof / existence result (Theorem 1: under assumptions, any algorithm can be shown to be an input-restricted distributed abstraction of any DNN; i.e., existence of perfect alignment maps)",3,3,0,5,0,"The claim is a strong, general mechanistic/meta-method statement: unrestricted causal abstraction becomes vacuous because any NN can be mapped to any algorithm. This is supported in the paper by a formal existence proof (Theorem 1) rather than purely observational or single intervention experiments. Under the codebook, such a proof-level counterfactual/uniqueness-style argument about what mappings must exist (and the resulting vacuity) fits Rung 3 for method. The claim itself asserts a universal mapping result and a conclusion about the method’s informativeness (not merely correlation or a specific causal effect), which is also Rung 3. No explicit hedging language is present in the quoted claim."
2507.08802,2507.08802-02,it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task,abstract,3,Distributed Alignment Search (DAS) with non-linear alignment maps; evaluated via interchange intervention accuracy (IIA) on randomly initialized / task-incompetent models,2,2,0,5,0,"Method: DAS/IIA is an interventional causal-abstraction-style evaluation (interchange interventions in latent space via learned alignment map ϕ), so it is Rung 2. Claim: the statement asserts an existence/achievability result about an intervention-based alignment metric—i.e., that one can obtain perfect mapping/alignment (100% IIA) even when the base model cannot solve the task. This is a causal/interventional performance claim about the outcome of applying the method, not a mechanistic attribution of what the model ‘encodes/uses’ to solve the task, so it is Rung 2 rather than Rung 3."
2507.08802,2507.08802-03,randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task,abstract,3,"Distributed Alignment Search (DAS) / interchange interventions with learned non-linear alignment maps; evaluated by interchange-intervention accuracy (IIA) on IOI, including randomly initialized LMs",2,2,0,5,0,"Method: DAS uses explicit interventions in latent space (interchange interventions) and measures output changes via IIA, which is interventional evidence (R2). Claim: “reach 100% interchange-intervention accuracy” is a performance/effect-of-intervention statement about the intervention metric, not a mechanistic attribution (no ‘encodes/represents/uses’ language). Therefore claim_rung is R2. No hedging present."
2507.08802,2507.08802-04,causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information,abstract,3,"Theoretical proof (Theorem 1) plus supporting DAS/IIA experiments with increasingly complex (non-linear) alignment maps, including on randomly initialized models",1,3,2,4,0,"Method: The paper’s main support for this statement is (i) a formal existence/vacuity theorem about causal abstraction with unrestricted alignment maps, and (ii) empirical DAS results showing near-perfect IIA even for random/unsolved models. In the codebook’s 3-rung scheme, these are not standard observational/patching/scrubbing MI methods; the closest match is R1 (no direct causal intervention on native model components to establish a specific internal mechanism; the experiments intervene in a learned latent space via an alignment map and evaluate IIA, which functions more like an evaluation/association about the method’s behavior than a component-level causal claim). Claim: “causal abstraction is not enough… becomes vacuous without assumptions about how models encode information” is a broad mechanistic/meta-method conclusion about what causal abstraction can and cannot establish for mechanistic interpretability (a strong ‘not enough/vacuous’ claim about the method’s ability to yield genuine mechanisms), which fits R3 (mechanistic/strong responsibility-level framing rather than a narrow causal-effect statement). No explicit hedge (stated as a conclusion in the abstract)."
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,Subspace activation patching / subspace interventions (activation patching / DAS-style interventions),2,3,1,4,1,"Method: the claim is supported by subspace interventions/activation patching (and DAS-optimized subspace patches), which are explicit interventions on activations → Method Rung 2. Claim: it attributes the observed behavioral change to a specific internal causal story—""activating a dormant parallel pathway""—i.e., a mechanistic explanation of how the intervention works rather than merely stating that intervening changes outputs. This is a functional/mechanistic pathway claim (R3) rather than a pure causal-effect claim (R2). The wording is hedged with ""may be achieved"" → hedge_flag=1. Confidence is high but not maximal because the sentence is somewhat general/illustrative in the abstract, though the mechanistic language ('dormant parallel pathway') clearly pushes it to R3."
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,"Subspace activation patching / subspace interventions (activation patching on learned linear subspaces, incl. DAS-optimized subspaces) demonstrated via empirical case studies (IOI, factual recall)",2,3,1,4,1,"Method: the paper’s evidence for this abstract claim comes from interventional experiments—patching activations along learned subspaces and measuring downstream output changes, plus decompositions into nullspace/rowspace and comparisons to full-component patching—so method_rung=2. Claim: “can lead to an illusory sense of interpretability” is not merely reporting a causal effect of an intervention; it asserts a higher-level interpretability failure mode (i.e., that the intervention can produce misleading mechanistic attributions/‘interpretability illusion’), which is a mechanistic/epistemic claim about faithfulness of explanations rather than just ‘intervening changes outputs’, so claim_rung=3. Hedging: ‘can’ indicates possibility rather than universal statement, so hedge_flag=1."
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,"Mixed: (i) distilled mathematical/toy example (theoretical construction, not an empirical internal-method rung); (ii) empirical demonstrations in two real-world domains using subspace activation patching / DAS-style optimized subspace interventions and related ablations (interventional). For rung coding, the relevant empirical method is activation patching / subspace interventions (R2).",2,2,0,4,0,"The claim is that the authors 'demonstrate this phenomenon' in (a) a toy mathematical example and (b) two real-world domains. In the real-world domains, their evidence relies on interventions (subspace activation patching / DAS-found subspaces, plus decompositions/ablations showing effect dependence), which is Method R2. The claim itself is an existence/causal-effect claim that the phenomenon occurs in those settings (not a unique-mechanism or full mechanistic attribution like 'the model uses X to do Y'), so Claim R2. No explicit hedging ('demonstrate' is assertive)."
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,"Cites/relies on prior empirical work (Hase et al., 2023) comparing rank-1 fact editing vs causality-based localization; no new intervention described in the claim itself",1,1,0,4,0,"The quoted statement asserts an empirical mismatch/association (‘inconsistency’) between two measured outcomes (editing performance vs localization), without itself making a direct causal/mechanistic attribution about internals. In the abstract it is presented as a descriptive empirical observation (R1 claim). Since the claim as written does not specify an interventional method performed by this paper (it references a previous observation), the safest method rung for this claim-text alone is R1 (observational/citation-based), even though the underlying cited work uses interventions."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,Activation patching (descriptive/meta claim about the technique; no specific experiment reported),2,1,0,4,0,"The sentence is a high-level descriptive claim about the technique’s popularity and that it has interpretive subtleties; it does not assert a specific causal/mechanistic fact about model internals. Thus the claim itself is not about causal effects or mechanisms (R1). The method referenced is activation patching, which is an interventional technique (R2), even though here it’s mentioned in a meta way rather than used to support an empirical internal claim."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching (discussion/best-practices paper; claim not tied to a specific new experiment),2,1,0,4,1,"The statement is a methodological caution about sensitivity: changing patching hyperparameters may change the interpretability outcomes. It does not assert a specific causal mechanism inside the model (R3) nor a direct causal effect of an internal component on behavior (R2); it’s closer to an associational/robustness claim about variability of results across analysis settings (R1). The wording is explicitly hedged (“could”). The paper’s core technique under discussion is activation patching, an interventional method (R2), but this particular claim is not itself an internal causal claim."
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,"Activation patching (systematic empirical comparison of corruption methods, metrics, and sliding-window variants)",2,1,0,5,0,"The claim is that the authors ""systematically examine the impact"" of methodological details. This is an empirical/observational claim about their study design and comparative evaluation, not a mechanistic or causal claim about specific internals (no assertion that some component causes/implements behavior). The underlying method they use is activation patching, which is an interventional technique (R2). The claim itself is essentially descriptive: they examine/measure how results vary with methodological choices, which fits R1 (associational/comparative) rather than R2/R3."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,"Activation patching / causal tracing variants with systematic hyperparameter sweeps (corruption method, evaluation metric, sliding window patching)",2,2,0,5,1,"Method: the paper’s core evidence comes from activation patching experiments that intervene on internal activations (clean/corrupt/patched runs) while varying patching hyperparameters, which is Rung 2 (interventional). Claim: “could lead to disparate interpretability results” is a causal sensitivity claim about methodological choices affecting localization/circuit-discovery outcomes (i.e., changing X changes observed interpretability result Y), not a mechanistic attribution of what components compute. The modal “could” is an explicit hedge → hedge_flag=1."
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,Systematic activation patching across layers (causal intervention / activation substitution),2,3,1,4,0,"The paper’s stated supporting method for this abstract claim is activation patching, which is an interventional technique establishing causal effects of swapping layer activations on the sentiment output (Method Rung 2). The claim text uses functional/mechanistic language: “early layers (0-3) act as lexical sentiment detectors” and “encoding stable position specific polarity signals,” which attributes a specific computational role/representation to those layers (detectors/encoding), going beyond a pure causal-effect statement like “patching early layers changes sentiment” (R2). Under the codebook’s polysemy rules, ‘encoding’ without an explicit decodability framing defaults to a mechanistic reading (R3), and ‘act as detectors’ is a functional attribution (R3). No explicit hedge terms appear, so hedge_flag=0."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,Systematic activation patching across all 12 layers (causal intervention / activation patching),2,2,0,4,0,"The supporting method described is activation patching, which is an interventional technique establishing causal effects of substituting layer activations on the sentiment output (method rung 2). The claim says contextual phenomena are ""integrated primarily in late layers (8–11),"" which is a causal localization/mediation claim about where contextual integration happens in the network, not a full mechanistic functional attribution or uniqueness claim (so claim rung 2 rather than rung 3). No explicit hedging terms (e.g., may/suggests) appear in the quoted claim."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,Systematic activation patching across all 12 layers (causal intervention / activation patching),2,3,1,4,0,"The supporting method described in the abstract is activation patching, an interventional technique that measures output changes under activation substitution, so method_rung=2. The claim asserts a property of the model’s internal computation—“sentiment computation differs from the predicted hierarchical pattern”—which is a mechanistic/functional attribution about how the model computes sentiment (not merely that interventions change outputs), so claim_rung=3. It is stated without explicit hedging. Slight ambiguity because it could be read as a summary of falsified hypotheses (an R2-style causal conclusion), but the phrasing ‘sentiment computation differs’ leans mechanistic."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,Fine-grained Cross-modal Causal Tracing (FCCT) via activation patching/causal tracing: add Gaussian noise to image (corrupted run) and restore specific MHSA activations from clean run; quantify effect with Recovery Rate (RR).,2,3,1,4,0,"The supporting evidence is an interventional causal-tracing/activation-patching setup (clean/corrupted/patched runs) measuring output probability recovery, which is Rung 2. The claim text uses functional/mechanistic attribution—“play a critical role in aggregating cross-modal information”—which goes beyond a pure causal-effect statement (e.g., ‘patching these MHSAs increases RR’) and asserts what the component does (aggregation), matching the codebook’s R3 functional-verb/mechanistic narrative pattern. No explicit hedge terms appear in the claim as quoted."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,Fine-grained Cross-modal Causal Tracing (FCCT) via activation patching/causal tracing with Gaussian-noise corruption and restoration; quantified by Recovery Rate (RR),2,3,1,4,0,"The supporting evidence described is causal tracing/activation patching: corrupt the image with Gaussian noise, then restore (patch) specific FFN/MLP activations from a clean run and measure recovery in output probability (RR). This is an interventional method establishing causal effects under interventions → method_rung 2. The claim, however, uses mechanistic/storage language and a structured functional narrative: 'three-stage hierarchical progression' plus 'storage and transfer of visual object representations' attributes a representational mechanism to FFNs (how information is stored/transferred across layers), which goes beyond a pure causal-effect statement ('patching FFN at layer l increases RR') and reads as a mechanistic account → claim_rung 3. No explicit hedge terms (e.g., may/suggests) appear → hedge_flag 0."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,"Intermediate Representation Injection (IRI): inference-time intervention that injects/restores intermediate MHSA/MLP activations into later layers, scaled by Recovery Rates from FCCT (activation patching/causal tracing with Gaussian-noise corruption).",2,2,0,4,0,"The supporting method is an explicit intervention on internal activations (representation injection), which is Rung 2. The claim says IRI ""reinforces"" visual object information flow—this is a causal-effect framing (intervening to strengthen information flow), not a full mechanistic attribution like ""IRI is the mechanism"" or ""these components compute X"". No explicit hedge terms (may/suggest) appear."
2601.05679,2601.05679-01,many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens,abstract,3,Causal token injection (inserting top-activating tokens into non-reasoning text and measuring feature activation shift; effect sizes via Cohen’s d / significance tests),2,2,0,5,0,"The supporting evidence is an explicit intervention on the model’s inputs (token injection) and measurement of resulting activation changes, which is Rung 2 interventional. The claim asserts that injecting a few associated tokens causes many candidate features to activate (45–90%), i.e., an intervention changes an internal variable; this is a causal-effect-under-intervention statement rather than a mechanistic/functional attribution, so claim_rung is also R2. No explicit hedging terms appear in the quoted claim."
2601.05679,2601.05679-02,LLM-guided falsification produces targeted non-reasoning inputs that trigger activation,abstract,3,LLM-guided falsification / adversarial counterexample construction (generate non-reasoning inputs; then measure SAE feature activation on those inputs). This is an interventional input-level manipulation rather than a direct model-internal intervention.,2,2,0,4,0,"The method actively intervenes on the model’s inputs by constructing targeted non-reasoning counterexamples and then observing changes in feature activation; this goes beyond passive correlation/contrastive analysis, so it fits Rung 2 (interventional) under the codebook’s broad notion of intervention (though it is not activation patching/ablation). The claim asserts a causal effect of that procedure—producing inputs that *trigger activation*—which is a causal/behavioral-effect statement (R2), not a mechanistic attribution about what the feature computes (R3). No explicit hedge is present."
2601.05679,2601.05679-03,sparse decompositions can favor low-dimensional correlates that co-occur with reasoning,abstract,3,Stylized theoretical analysis of sparsity-regularized decoding (ℓ1 / Top-K) showing suppression of high-dimensional components; supported by empirical token-injection/falsification experiments but the quoted claim itself is a general mechanism claim,1,3,2,4,1,"The claim attributes a general mechanistic tendency to SAEs (“sparse decompositions can favor…”) about what the decomposition is doing/biased toward, i.e., a mechanism-level statement rather than a mere correlation (R3). The evidence described is primarily a stylized mathematical analysis plus empirical demonstrations; under the codebook’s method ladder, this is not a direct model-internal intervention establishing a specific causal effect in a particular model component, so it does not cleanly qualify as R2/R3 method evidence. Treat as observational/associational for rung purposes (R1). The wording uses the hedge ‘can’, so hedge_flag=1."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,Steering vector intervention (additive residual-stream steering at last layer) with logit-lens readout and behavioral validation via token-prefixing,2,3,1,4,0,"Method: they intervene by adding a trained steering vector to the residual stream (and evaluate output changes / performance), which is an interventional causal method (R2). Claim: “acts like a token-substitution bias concentrated on the first generated token” is a functional/mechanistic characterization of what the vector is doing (token substitution / biasing mechanism and where it acts), not merely that intervening changes outputs; this is R3 per codebook (functional attribution: ‘acts like’ + mechanistic narrative). No explicit hedge (stated as fact). Minor ambiguity because ‘acts like’ is an analogy and could be read as a descriptive summary of observed logit shifts (R2-ish), but overall it attributes a mechanism, so R3."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,Interventional analysis via within-layer steering placement/patching (Skip-Attn/Skip-Layer; Steer-Q/K/V-Proj / head patching) plus token-level probability shift analysis (logit-lens-style readout),2,3,1,4,0,"Method: The paper intervenes by injecting the trained penultimate-layer steering vector at different points in the final block (e.g., Skip-Attn, Skip-Layer, projection-specific injection) and measures downstream accuracy changes, which is causal/interventional evidence (R2). Claim: “operates through the MLP and unembedding” is a mechanistic pathway attribution (functional description of how the vector works), and “preferentially up-weighting process words” assigns a specific computational effect on token probabilities; this goes beyond a pure causal-effect statement (“intervening changes Y”) into mechanism/functional role language, so R3. No explicit hedge words present."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,Steering-vector transfer experiment: swap trained (all-layer) steering vectors from a donor model into a closely matched recipient model and measure downstream benchmark performance (Table 1 / Section 7). This is an intervention on the model’s residual stream activations at inference time.,2,2,0,4,0,"The supporting evidence is an explicit intervention (inserting donor steering vectors into another model) and measuring behavioral change, so method_rung=2. The claim “steering vectors transfer to other models” asserts a causal/functional effect under that intervention (they still produce non-trivial performance gains when moved), not a unique mechanism or counterfactual necessity, so claim_rung=2. Minor ambiguity: ‘transfer’ could be read as a stronger mechanistic claim about preserved internal directions, but the quoted claim as given is primarily about observed cross-model effectiveness."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,Steering vectors (Contrastive Activation Addition) applied as an inference-time activation intervention; evaluated via logit-difference propensity metric on held-out prompts,2,1,0,5,0,"The method involves an explicit intervention (adding λs_l to the residual stream), so method_rung=2. However, the claim itself is purely descriptive about observed outcomes—net positive average steering effect and high variance across samples—without asserting a causal/mechanistic account of internals (no 'mediates/controls/represents' language). Thus claim_rung=1 (empirical association/measurement statement about effect distribution). No explicit hedging terms are present."
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,Cosine similarity / correlation analysis between training-set activation-difference geometry and downstream steering effectiveness (observational; no intervention used to establish the predictive relationship).,1,1,0,5,0,"The claim is explicitly predictive/associational: “higher cosine similarity … predicts more effective steering.” This is a statistical relationship between a geometric measure computed from logged activations and an effectiveness metric, without asserting causal control or a mechanistic function. The supporting method is correlational analysis (R1), so both method and claim are R1."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Steering vectors (CAA) with inference-time activation addition; plus observational geometry analysis of activation differences (cosine similarity / separability) used to predict steering effectiveness,2,3,1,4,0,"The paper’s core empirical setup includes an interventional method (adding λs_l to residual stream at inference and measuring logit-difference changes), which is Rung 2. The specific claim attributes unreliability to an internal representational condition: the target behavior ‘not represented by a coherent direction’. This is mechanistic/representational language (‘represented by’) explaining when steering works, not merely reporting that cosine similarity predicts performance. That pushes the claim to Rung 3 (mechanistic narrative about representation geometry underlying reliability). No explicit hedge words (‘may/suggests’) appear in the quoted claim, so hedge_flag=0."
2301.04709,2301.04709-01,Causal abstraction provides a theoretical foundation for mechanistic interpretability,abstract,3,Theoretical/mathematical framework proposal (no empirical intervention/measurement on model internals; conceptual unification),1,3,2,5,0,"The quoted statement is a broad mechanistic-level framing claim: that causal abstraction *provides a theoretical foundation* for mechanistic interpretability. This is not an associational or narrow causal-effect claim; it attributes an overarching explanatory/structural role (R3-style ‘foundation’/mechanism-level narrative). In the abstract it is presented as a declarative contribution with no hedge. The supporting work in the paper is primarily formal/theoretical rather than empirical interventions on a trained model, so under the codebook’s empirical-method ladder it does not rise above observational evidence; thus method_rung=1. The claim therefore exceeds the empirical method rung (overclaim pattern: theory/framework claim stated as established)."
2301.04709,2301.04709-02,generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation,abstract,3,Theoretical/mathematical generalization (formal definitions + theorems extending causal abstraction from hard/soft interventions to interventionals / mechanism transformation),1,1,0,4,0,"This is a contribution claim about extending a formal theory, not an empirical claim about learned model internals. The paper supports it via proofs/definitions rather than observational or interventional experiments on a model. Under the provided codebook (which is oriented around empirical MI methods), the closest fit is to treat the support as non-empirical; I map that to method_rung=1 by default (no intervention on a trained model). The claim itself is also not a causal/mechanistic attribution about a model component; it states they generalize a theory, so claim_rung=1 (descriptive/associational-level statement rather than R2 causal effect or R3 mechanism-of-model claim). Minor ambiguity only because the word 'mechanism' is used in a causal-model-theory sense, not the MI 'model internals' sense."
2301.04709,2301.04709-03,unifying a variety of mechanistic interpretability methods in the common language of causal abstraction,abstract,3,Theoretical/conceptual unification (no empirical intervention or measurement; formal framework/language proposal),1,1,0,4,0,"The quoted statement is a high-level contribution claim about providing a common language that ‘unifies’ various methods. In this paper context it is supported by formal definitions and conceptual mappings, not by empirical experiments on model internals. Under the codebook’s empirical-method ladder, this is best treated as non-interventional/observational evidence (effectively R1) since there is no causal intervention study being reported. The claim itself is not a causal/mechanistic attribution about a specific model component; it is a descriptive/meta-level claim about organizing methods, so it fits R1 (associational/descriptive) rather than R2/R3. Minor ambiguity: one could argue it’s not an empirical claim at all, but given the forced rung choice, R1/R1 is the closest match."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,Software/library implementation claim (engineering description; no empirical internal-analysis method invoked),1,1,0,4,0,"The quoted statement is a capability claim about the pyvene library (“supports customizable interventions…”) rather than an empirical claim about model internals (no representation/causal/mechanistic attribution). It is effectively an observational/engineering assertion that the tool can target various PyTorch modules. Under the codebook’s rungs (which are framed around evidence about model internals), this best maps to R1: no causal/mechanistic claim about internals is being made, just that the library provides functionality. No hedging language is present."
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,Software/library contribution enabling intervention hooks and configurable intervention schemes (no specific empirical evaluation method directly supporting this claim in the quoted sentence),1,1,0,4,0,"The claim is a tooling/engineering capability statement: that pyvene offers a unified, extensible framework to perform interventions. It does not assert a causal/mechanistic fact about model internals; it’s essentially an observational/descriptive claim about the library’s functionality. Since no specific intervention experiment is invoked to support this particular sentence, the appropriate method rung is treated as non-interventional/associational (R1) for this claim. The claim itself is not mechanistic (no ‘encodes/represents/uses’), and it does not claim causal effects on model behavior; it just states the framework exists and is unified/extensible, so claim_rung=1."
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,"Intervention-based interpretability analyses (causal abstraction; knowledge localization via interventions such as interchange/activation patching, causal tracing, DAS)",2,1,0,4,0,"The cited analyses (causal abstraction and knowledge localization) are typically supported by interventional techniques (e.g., activation patching/interchange interventions, causal tracing, trainable interventions like DAS), which are Rung 2 methods. However, the quoted statement is not an empirical claim about a specific internal mechanism; it is a high-level, evaluative/illustrative claim about demonstrating the library’s ‘power’ via example analyses. It does not assert causal/mechanistic facts about model internals (no ‘X mediates/encodes/causes Y’), so the claim itself is best treated as a low-rung, non-mechanistic performance/illustration claim (R1)."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,Sparse Autoencoder (SAE) training / decomposition of residual stream activations into latent features (dictionary learning-style reconstruction + sparsity); no direct causal intervention implied for this specific claim,1,3,2,4,0,"The claim asserts a mechanistic property of the representation—SAEs ""decompose"" entangled hidden states into a ""disentangled feature space""—which is stronger than an observational statement like ""features are decodable"" or ""some features correlate with strategies."" In the codebook, ""disentangle""/""decompose into features"" is treated as a mechanistic/representational attribution (R3) unless clearly framed as mere decodability. The supporting method for this specific statement is SAE training/objective (reconstruction+sparsity) and feature inspection, which is observational/associational with respect to disentanglement (R1); the paper later uses steering interventions, but those support control-effectiveness claims rather than establishing that the SAE space is truly disentangled in a causal/mechanistic sense. No explicit hedge (e.g., 'tends to', 'aims to') appears in the quoted claim."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE feature attribution / feature identification via logit-lens-based recall (Stage 1) plus interventional steering evaluation/ranking on a validation set (Stage 2); overall supported by steering interventions on hidden states,2,1,0,4,0,"The claim is that the method can *identify* (select/pick out) strategy-specific SAE features from a large pool. This is primarily a feature-selection/association claim rather than a mechanistic functional attribution: it does not say the features *encode/compute/control* the strategy, only that they are identified as strategy-specific. In the paper, the identification pipeline includes an interventional evaluation step (steering with candidate features and judging strategy presence), which is R2 evidence, but the claim itself is about successful identification/selection, which fits R1 (associational/operational labeling: ‘these are the strategy-specific features we found’). No explicit hedging in the quoted claim."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,"Interventional steering via SAE feature injection into residual stream (activation-based steering vectors), with control effectiveness measured by generating steered continuations and comparing to baseline using LLM-judge evaluations (plus baseline comparisons).",2,1,0,5,0,"The supporting evidence is an intervention on the model’s hidden states (injecting a feature direction/control vector during generation), so method_rung=2. The claim itself is a comparative performance statement about an evaluation metric (“outperforms … by over 15% in control effectiveness”), not a causal/mechanistic attribution about internals; it’s essentially an empirical outcome/association claim about method effectiveness, so claim_rung=1. No explicit hedging in the quoted claim."
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,SAE-based steering intervention on residual stream activations (injecting SAE feature direction fs with coefficient α during generation) evaluated via error-correction experiments (Table 3),2,2,0,4,0,"The supporting evidence is an explicit intervention on model internals (adding a control vector/feature direction to the residual stream) and measuring downstream behavioral change (improved error-correction rates), which is Rung 2 methodology. The claim states that controlling strategies can redirect the model from erroneous to correct paths—i.e., an intervention changes outcomes—without asserting a unique underlying mechanism/circuit or counterfactual necessity, so it is a causal-effect claim (Rung 2). No explicit hedge terms (e.g., may/suggest) are present."
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,No direct empirical method in this paper supports this abstract-level statement; it is presented as a background claim citing prior empirical studies (at most observational evidence such as activation analysis/probing in the cited works),1,3,2,4,0,"The claim uses strong mechanistic/ontological language: ""represent meaningful concepts as directions"" asserts that internal representations correspond to concepts (a representational attribution), which the codebook treats as R3 absent an explicit decodability framing. In this paper’s abstract it is not tied to a specific intervention; it is a general empirical assertion attributed to prior work, so the best-supported rung from within-paper methods is effectively none/observational (R1). No explicit hedge (e.g., may/suggests) is present."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,"Theoretical/mathematical analysis (unified optimization formulation; proofs of properties like piecewise biconvexity, characterization of solution set/spurious minima)",1,1,0,4,0,"The quoted statement is a priority/novelty claim about the paper’s contribution (“first unified theoretical framework”), not an empirical claim about model internals. It does not assert a causal/mechanistic property of a trained model component; it asserts they provide a unifying formulation. Under the codebook’s empirical-rung scheme, this best fits R1 (descriptive/associational) rather than R2/R3. The supporting method is theoretical rather than observational/interventional; since the method ladder only defines empirical MI methods, the closest mapping is R1 (non-interventional). No explicit hedge (e.g., ‘we believe’, ‘may’)."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,"Theoretical analysis / proofs characterizing SDL optimization landscape (piecewise biconvexity, spurious minima) under Linear Representation Hypothesis assumptions; supported by synthetic benchmark experiments (Linear Representation Bench)",1,3,2,4,0,"The claim is an explanatory/mechanistic attribution: the authors say they provide ""theoretical explanations"" for why internal phenomena (feature absorption, dead neurons) occur. This goes beyond reporting correlations or intervention effects and asserts an underlying mechanism/understanding, which maps to claim_rung 3 (mechanistic narrative). The supporting method is primarily non-interventional theory (no direct causal intervention on a trained model’s internals in the claim itself), so method_rung is best fit as R1 under this codebook’s empirical-method ladder. The claim is stated without hedging (""we provide""), hence hedge_flag 0. Minor ambiguity because the paper also includes experiments, but the quoted claim specifically foregrounds theory as the explanatory method."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,"No direct empirical method specified for this statement in the quoted claim (background framing in abstract; at most supported by prior observational analyses / toy-model evidence cited elsewhere, e.g., Elhage et al. 2022b)",1,3,2,4,0,"The claim attributes a functional/mechanistic explanation for performance—""achieve remarkable performance through superposition""—and defines superposition as ""encoding multiple features as overlapping directions,"" which is mechanistic/representational language (R3: 'through', 'encoding'). The sentence itself does not reference an intervention; in the abstract it functions as a motivating premise rather than a result tied to a specific experiment in this paper, so the best match for method support is non-interventional/background evidence (R1). No explicit hedge terms (e.g., may/suggests) appear."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,Information-theoretic analysis applied to sparse autoencoder (SAE) activations (entropy/perplexity-based metric computed from logged activations),1,2,1,4,0,"The claim is that the authors introduce a framework that *measures* an internal property (“effective degrees of freedom”) of a neural representation. The supporting approach, as described in the abstract/paper context, computes Shannon entropy over SAE activation statistics—i.e., it relies on extracting/aggregating activations and applying a derived metric, without intervening on the model to test causal effects. That places the method at Rung 1 (observational/associational: activation logging + SAE feature attribution/statistics). The claim itself goes beyond a purely correlational statement (“correlates with…”) and asserts capability/validity of a measurement framework for an internal quantity, which is best coded as a causal/functional measurement claim (R2) rather than a mechanistic attribution (R3): it does not say the model ‘uses’ or ‘computes’ something, only that the framework measures a property. No explicit hedging language is present."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,Toy-model validation using SAE-based superposition metric; correlation analysis against ground-truth/reference superposition computed from toy model weights (observational/statistical association),1,1,0,5,0,"The claim is explicitly about a strong correlation between the proposed metric and ground truth in toy models. This is an associational/statistical performance statement (R1) rather than a causal/mechanistic attribution. The supporting method is computing correlations (e.g., r values) between measured quantities without intervening to test causal effects, so method_rung is also R1."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,SAE-based superposition metric (entropy of SAE activations) applied across PGD adversarial training conditions; observational correlation between training regime and measured effective features/robustness (no targeted internal intervention to establish causality),1,2,1,4,0,"The evidence described is measuring a derived internal metric (effective features from SAE activations) and comparing it across models trained with/without adversarial training, alongside robustness outcomes. This is not an intervention on specific internal components (e.g., patching/ablation/steering) to show causal mediation; it is primarily associational across training conditions, so method_rung=1. The claim asserts an effect of adversarial training ('can increase effective features while improving robustness') and uses this to reject a causal hypothesis ('contradicting the hypothesis that superposition causes vulnerability'), which is a causal-level conclusion about the relationship between superposition and vulnerability; thus claim_rung=2. No explicit hedge terms appear in the quoted sentence, so hedge_flag=0."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,Downstream probing evaluation of SAE features (kNN/logistic regression/XGBoost probes on SAE latents/truncated reconstructions) comparing equivariant vs regular SAEs; plus training SAEs with symmetry-aware objective,1,1,0,5,0,"The evidence directly supporting “more useful in downstream tasks” is comparative probe performance on downstream binary classification tasks using frozen SAE-derived representations. This is an observational/associational evaluation (R1): it measures that features enable better probe performance, not that they causally mediate base-model behavior. The claim itself is also R1: “more useful” is an empirical performance association claim about downstream task utility, without mechanistic/causal language (no ‘causes/enables/controls/represents’). No explicit hedge in the quoted text."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,Optimize a linear activation-space transformation matrix M by minimizing squared error between transformed activations and predicted activations (LM; report R^2 variance explained),1,1,0,5,0,"The evidence is a goodness-of-fit/variance-explained result (R^2) for a learned linear map M predicting ψ(g^p x) from ψ(x). This is observational/associational (no causal intervention on the base model’s internals; just modeling the relationship between activations under input rotations), so method_rung=1. The claim language “can explain how their activations transform” is effectively a variance-explained/linear-predictability statement about the mapping between activations across rotations, not a mechanistic attribution of a component’s function, so claim_rung=1. No explicit hedge terms are present."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,Binary probing tasks over SAE latents/truncated reconstructions using kNN/logistic regression/XGBoost; comparison of probe performance across equivariant vs regular SAEs (observational evaluation),1,1,0,5,0,"The evidence described is probing/classification performance comparisons, which are correlational/associational (R1): they show that information useful for the probe is present/accessible in the learned features, not that the features causally produce model behavior or constitute a mechanism. The claim itself is about superior probing performance (an empirical performance comparison), not about causal mediation or functional roles, so it is also R1. No explicit hedge words appear in the quoted claim."
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Activation steering via Contrastive Activation Addition (CAA): add learned steering vector λs_l to residual stream at a chosen layer during inference; evaluated by measuring changes in topic/sentiment/toxicity/readability metrics vs steering strength (interventional activation modification).,2,2,0,5,0,"The supporting method is an explicit intervention on internal activations (adding λs_l), so method_rung=2. The claim ‘steering effectively controls the targeted summary properties’ is a causal efficacy statement about the effect of intervening with steering on output properties (control/influence), not a mechanistic attribution about what a component represents/encodes, so claim_rung=2. No explicit hedging (e.g., may/suggests) appears in the quoted claim."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,"Steering vectors / Contrastive Activation Addition (CAA) with varying steering strength λ; evaluate intrinsic (e.g., perplexity, distinct-2) and extrinsic (e.g., ROUGE, BERTScore) quality metrics under intervention",2,2,0,5,0,"Method: they intervene on the model by adding λ·s_l to residual stream activations at inference time (activation steering), then measure downstream changes in quality metrics—this is interventional (R2). Claim: “high steering strengths consistently degrade both intrinsic and extrinsic text quality” is a causal effect statement about the intervention (high λ) worsening quality, not a mechanistic attribution of how the model works; thus claim_rung=2. No explicit hedging (e.g., may/suggests), and the abstract wording is assertive (“consistently degrade”)."
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,"Activation steering via Contrastive Activation Addition (CAA): add learned steering vector λs_l to residual stream during inference; plus prompt engineering; evaluated by measuring changes in topical focus/sentiment/toxicity/readability metrics under these interventions (Sections 3.2, 3.5, 4.5; Table 2; Figures 9–12).",2,2,0,4,0,"The supporting evidence is interventional: they actively intervene on the model at inference time by (i) adding a steering vector to activations and (ii) changing the prompt, then measure resulting behavioral changes. That places the method at Rung 2. The claim “combining steering and prompting yields the strongest control over text properties” is a comparative causal/effectiveness statement about interventions producing larger behavioral shifts than either alone, not a mechanistic attribution about what internal components compute/represent. So the claim is Rung 2 (causal effect of interventions). No explicit hedge (e.g., ‘may/suggests’) in the quoted claim."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,No empirical method used (conceptual/philosophical framing claim in abstract; not supported by observational or interventional analysis in this paper excerpt),1,1,0,4,0,"The quoted statement is a high-level framing claim about the usefulness of a causal-abstraction perspective (“provides a fruitful lens”), not an empirical claim about specific model internals (no component/feature/head, no measured association, no intervention effect). Since there is no described empirical method directly supporting it, the appropriate method rung defaults to R1 (non-interventional/none). The claim itself is not mechanistic/functional attribution (R3) nor a causal effect claim (R2); it is an evaluative/associational-style assertion about a perspective being useful, best mapped to R1. No explicit hedge words (may/suggests) appear."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,Theoretical/philosophical argument proposing a framework (no empirical method on model internals),1,1,0,4,0,"The quoted sentence is a contribution/positioning claim about what the paper provides (“we offer an account…”), not an empirical claim about specific model internals. No observational or interventional analysis is performed to support a factual/causal/mechanistic claim about a model component; it’s a conceptual framework grounded in causal abstraction. Under the codebook’s empirical-claim rungs, this best fits R1 (non-causal, non-mechanistic empirical content). Method rung is set to R1 by default since there is no intervention/patching/ablation etc.; it is not an empirical internal-analysis method at all (but the schema forces a rung, and R1 is the closest)."
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,Activation patching / causal tracing-style residual stream patching between source and target translation prompts (interventional),2,3,1,4,0,"The supporting evidence described in the abstract and main text is an intervention: extracting residual-stream latents from a source prompt and inserting them into a target prompt at specific layers, then measuring changes in next-token probabilities. This is Rung 2 (interventional causal effect under patching). The claim, however, uses mechanistic/representational language: 'the output language is encoded in the latent' and asserts a specific internal organization ('earlier layer than the concept'), which goes beyond a pure causal-effect statement (e.g., 'patching at earlier layers affects language more than concept'). Under the codebook’s polysemy rule for 'encoded', absent an explicit decodability framing, it defaults to a mechanistic reading (R3). No explicit hedge terms appear in the quoted claim."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,Activation patching (copying residual stream latents from a source prompt into a target prompt and measuring changes in next-token probabilities / translation output),2,2,0,4,0,"The supporting method is activation patching, an explicit intervention on internal activations, so method_rung=2. The claim asserts that intervening via patching can selectively change the translated concept while holding language fixed (and conversely change language while holding concept fixed). This is a causal manipulability/sufficiency claim about the effect of an intervention (“through activation patching alone”), not a unique-mechanism or counterfactual-necessity claim, so claim_rung=2. No explicit hedging terms are present."
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,Activation patching / mean activation patching intervention (copying residual-stream latents from source prompts into target prompt; comparing translation performance),2,2,0,5,0,"The supporting evidence is an explicit intervention: they patch (insert) the mean latent representation of a concept across languages into a target forward pass and measure the effect on translation performance, which is a Rung 2 interventional method. The claim itself is a causal effect statement about the intervention outcome (“patching with the mean representation … improves translation”), not a mechanistic attribution about what the model ‘represents/encodes’ or ‘uses’ in general. Therefore the claim is Rung 2 (causal effect under intervention), with no explicit hedging."
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,Activation patching (extract/insert residual stream latents between source and target prompts; including mean-latent patching across languages),2,3,1,4,0,"The supporting evidence described in the abstract and body is activation patching/latent insertion and measuring output changes (an intervention), which is Method Rung 2. The claim asserts the existence of ‘language-agnostic concept representations’ inside the model—i.e., a representational/mechanistic attribution about what internal states are (representations), not merely that some intervention changes behavior. Under the codebook, ‘representations’/‘encoded’ language typically defaults to a mechanistic reading (R3) unless clearly framed as decodability; here it is framed as existence of internal language-agnostic concept representations. The phrase ‘provide evidence for’ is not an explicit hedge like ‘may/suggests/potentially’, so hedge_flag=0."
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,Activation patching (resample ablation / causal mediation analysis) on MLP layers and identity-token-position patching; supported by attention-pattern inspection,2,3,1,4,0,"The primary evidence described in the paper for early-layer roles comes from activation patching interventions (R2). However, the claim attributes a functional role to early MLP layers—""process its semantic content""—which is a mechanistic/functional interpretation rather than a direct statement of an intervention effect (e.g., ""patching early MLP layers changes output""). Under the codebook, functional attributions like ""processes semantic content"" are R3. The claim is stated without hedging in the abstract."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,Activation patching (de-noising / resample ablation) on MLP and MHA layers; plus some attention-pattern analysis as supporting observational evidence,2,3,1,4,0,"The primary supporting method described for the paper’s internal-component conclusions is activation patching, an interventional technique (R2) that tests causal effects of swapping activations between clean/corrupt runs. The claim text, however, makes a mechanistic/functional attribution: early MLP layers ‘transform persona tokens into richer representations’ and these representations are ‘used by’ middle MHA layers. This is not merely ‘patching X changes Y’ (R2), but a narrative about what computations the layers perform and how information flows (transform → representation → used by), which fits R3 mechanistic language per the codebook. The abstract phrasing is unhedged (no “may/suggests”), so hedge_flag=0."
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,Activation patching to select important heads (interventional) plus value-weighted attention pattern analysis/visualization to assess what they attend to (observational),2,1,0,4,0,"The claim is that certain heads 'disproportionately attend to' particular identity tokens. This is an associational statement about attention allocation (attention pattern analysis), not a causal/mechanistic attribution (no 'controls/causes/implements'). Although the paper uses activation patching to identify which heads have large effects, the specific 'attend to racial and color-based identities' part is supported by attention visualization/value-weighted attention comparisons (R1). With multiple methods present, we take the highest rung directly supporting the claim: the attention-pattern evidence is R1, but head selection is via patching (R2); overall method_used is mixed, and method_rung is set to 2 because the identification of 'specific heads' is grounded in an intervention-based importance screen. No explicit hedging ('may/suggest') appears."
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,"Causal Layer Attribution via Activation Patching (CLAP) / activation patching of the first feedforward (MLP) layer, measuring logit-difference recovery",2,3,1,4,0,"The supporting method is activation patching (an intervention replacing corrupted activations with clean ones and measuring recovery), which is Method Rung 2. The claim contains a causal-result part (“patching ... recovered 56% of correct preference”), which would be Claim Rung 2, but it then makes a mechanistic/general representational conclusion: “demonstrating that associative knowledge is distributed.” ‘Knowledge is distributed’ is a representation-structure claim about where/how the model stores/uses information (mechanistic attribution beyond the specific intervention effect), so Claim Rung 3. No explicit hedge words are present."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation patching / causal layer attribution via activation patching (CLAP) on the final output layer (patch corrupted activations/weights with clean ones and measure recovery),2,3,1,4,0,"The supporting method is an explicit intervention (activation/weight patching) measuring behavioral/logit-difference recovery, which is Method Rung 2. The claim goes beyond a causal effect (‘patching restores accuracy’) to a mechanistic localization statement (‘definitional knowledge is localised’), which attributes where knowledge is stored/represented in the network—Claim Rung 3 per the codebook’s storage/localization language. No explicit hedge terms (e.g., may/suggests) appear."
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Causal Layer Attribution via Activation Patching (CLAP) / activation patching across layers measuring logit-difference recovery,2,3,1,4,0,"Method: CLAP is an interventional activation patching procedure (replace corrupted activations/weights with clean ones and measure recovery), which is Rung 2. Claim: “factual knowledge is more localized” and “associative knowledge depends on distributed representations” is a mechanistic/representational attribution about how knowledge is organized/stored in the network (localization vs distributed representation), not merely that patching certain layers changes outputs. This goes beyond a causal effect statement (R2) into a representation/organization claim (R3). No explicit hedge words appear in the quoted claim, so hedge_flag=0."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,"Universal Sparse Autoencoder training on activations with reconstruction loss (dictionary learning/SAE feature extraction); qualitative visualization of learned concepts (heatmaps, activation maximization)",1,3,2,4,0,"Method: Training an SAE/USAE to reconstruct activations and then visualizing/inspecting learned features is primarily observational about internals (no causal intervention on model behavior), so R1. Claim: says the model learns a 'universal concept space' that can both 'reconstruct' and 'interpret' internal activations. 'Interpret' and 'concept space' are mechanistic/semantic attribution language (implying the latents correspond to human-meaningful concepts), which is a rung-3 style claim rather than a purely correlational/reconstruction-performance statement. No explicit hedge."
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,"Universal Sparse Autoencoder (USAE) training via joint reconstruction of multiple models’ activations (TopK SAE / dictionary learning on activations; evaluated with cross-model reconstruction R², firing entropy/co-firing, and qualitative visualizations)",1,3,2,4,0,"The direct evidence for the statement is the USAE/SAE dictionary-learning setup plus reconstruction/visualization metrics. This is not an intervention on the base models’ computations (no ablation/patching/steering), so it is observational/associational (R1). The claim uses mechanistic/ontological language—“captures common factors of variation—concepts—across different tasks, architectures, and datasets”—which asserts that the learned dictionary corresponds to underlying shared concepts/factors, i.e., a representational/mechanistic attribution rather than a purely decodability/reconstruction claim. No explicit hedge is present."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,"Universal Sparse Autoencoder training on activations + qualitative feature visualization/heatmaps + reconstruction R² + universality metrics (firing entropy, co-firing) + energy-based importance (all observational; no causal interventions on model behavior)",1,3,2,4,0,"The evidence described for this abstract claim is based on training/inspecting a sparse dictionary over activations and reporting reconstruction (R²), co-firing/entropy statistics, and qualitative visualizations—these are associational/observational analyses of internal activations (R1), not interventions like ablation/patching/steering on downstream behavior. The claim uses mechanistic/ontological language: “discover … universal concepts” and “semantically coherent” treats learned latents as real concepts shared across models (representation/feature attribution), which per codebook defaults to R3 rather than a purely decodability-style R1 statement. “Important” is supported by an energy-to-reconstruction metric, still observational. No explicit hedge terms (may/suggest) appear."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,Sparse Autoencoder (SAE) feature attribution / interpretation (dictionary learning; explaining SAE features without direct causal intervention),1,2,1,4,0,"The claim is in the abstract and asserts that SAEs *clarify model behavior* by explaining *single-meaning atomic features*. The supporting method described is primarily SAE training + feature explanation/attribution (observing which learned features activate and generating natural-language explanations), which is observational/associational (R1) because it does not intervene on the base model to show causal effects on behavior. The claim goes beyond mere decodability/association: “clarify model behavior” implies that interpreting these features provides explanatory leverage about behavior (an effect on understanding/behavioral explanation), which is stronger than R1 and fits best as a causal/utility claim (R2) rather than a full mechanistic ‘the model computes X’ claim (R3). No explicit hedge terms are present."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,"Sparse Autoencoder (SAE) training across configurations + concept-specific interpretability evaluation metrics (L0,t, ICDF) based on activation frequency differences on Concept Contrastive Query Pairs; plus downstream explanation/simulation used to build neuron database (primarily observational attribution/metrics rather than causal intervention).",1,2,1,4,0,"The claim says Safe-SAIL ""systematically identifies"" the SAE with the ""best"" concept-specific interpretability—an effectiveness/selection claim about the framework producing an outcome (choosing the best SAE), not merely reporting correlations. However, the evidence described for SAE selection is based on observational evaluation metrics (activation frequency distributions, L0,t/ICDF trends, neuron counts/explanation granularity) without intervening on model components to show causal effects on behavior; thus method_rung=1. The claim is stronger than a pure associational statement (it asserts the framework can successfully pick the best SAE), but it does not explicitly assert a unique mechanism of how the model works, so claim_rung=2 rather than 3."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,Sparse Autoencoders (SAE) feature extraction / attribution (training SAEs and interpreting which features activate on safety-related data); no direct causal intervention implied in the quoted claim,1,2,1,3,0,"The claim says the extracted features ""effectively capture high-risk behaviors,"" which goes beyond a purely associational/decodability statement (R1) and asserts effectiveness at capturing the target behaviors (an outcome/utility claim). It does not explicitly attribute a unique mechanism (no ""encodes/represents/underlies""), so it’s not R3. The methods described for extraction are primarily SAE-based feature discovery and explanation (observational), which are R1 unless paired with interventions; the claim itself does not mention ablation/patching/steering. Some ambiguity remains because ""capture"" could be read as merely descriptive (R1), but in abstract-level framing it typically implies functional adequacy, hence R2."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,Attention analysis (observational) + causal mediation via activation patching / zero ablation / attention knockout (CountScope-style causal probing),2,3,1,4,0,"The paper supports its mechanistic story using interventional techniques (activation patching, zero ablation, attention knockout; plus CountScope described as a causal probing/patching method), which are Rung 2. The claim text itself makes a mechanistic functional attribution: that the model ‘computes and stores’ latent counts in specific token representations (final item representations). ‘Computed and stored’ is storage/mechanism language, not merely decodability or causal mediation, so it is a Rung 3 claim under the codebook (similar to ROME-style ‘stores’). No explicit hedge is present. Slight ambiguity because some evidence is framed as decoding/probing, but the claim goes beyond ‘encoded/decodable’ to ‘computed and stored,’ hence R3."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,Attention analysis (attention visualization/weight inspection) plus causal mediation via interventions (activation patching; attention knockout/ablation) used to support the transfer pathway claim,2,3,1,4,0,"The claim attributes a specific internal function to components: that counts are *transferred* to intermediate steps *via dedicated attention heads*. This is a mechanistic/functional attribution (heads do the transfer), which the codebook treats as R3 (similar to 'head performs X' / 'uses X to do Y'), and it also implies specialization ('dedicated'). The paper does include interventional evidence (attention knockout identifying influential heads; activation patching/cross-context patching affecting outputs), which is R2 methodologically, but these interventions establish causal influence/mediation rather than uniquely establishing the full mechanistic story or that the heads are truly 'dedicated' in a strong sense. No explicit hedge language appears in the quoted claim."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,Behavioral experiments / evaluation of counting accuracy under structured input + intermediate reasoning steps (test-time prompting; no internal intervention required for this claim),1,2,1,4,0,"The quoted claim is an outcome claim about the strategy’s effect on model performance: it says the strategy ""enables"" LLMs to ""surpass architectural limitations"" and ""achieve high accuracy"" on large-scale counting. The direct evidence for this is the reported accuracy/MAE improvements in the behavioral results tables/figures, which are observational comparisons across prompting conditions (no activation/weight intervention is needed to support the performance claim). Thus the supporting method is R1. The language ""enables"" is causal/efficacy framing (R2) rather than merely correlational (R1), but it does not attribute a specific internal mechanism (so not R3)."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,Conceptual/theoretical analogy (no empirical intervention or observational measurement; framing claim),1,1,0,4,0,"The quoted statement is a high-level framing/interpretive claim about how to conceptualize neuron identification (as an inverse of supervised learning). It does not assert a specific internal mechanism of a model component, nor a causal effect from an intervention; it’s essentially a definitional/analogical relationship used to motivate theory. Since no interventional method is used to support it (and it’s not an empirical correlational result either), the closest fit in the codebook is to treat the supporting ‘method’ as non-interventional/theoretical (mapped to R1 by default) and the claim itself as non-causal, non-mechanistic (R1). No explicit hedge words like “may/suggests” appear."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,Statistical learning theory / theoretical generalization-bound derivation for similarity metrics (no model intervention; analysis of empirical similarity estimators on i.i.d. probing datasets),1,3,2,4,0,"Method: The paper’s support for this statement is a theoretical/observational analysis of estimation/generalization error for similarity metrics under i.i.d. sampling and finite concept sets; it does not intervene on model internals to establish causal effects, so it fits Rung 1 (associational/observational evidence about measurement reliability). Claim: “guarantee faithfulness” is a strong mechanistic/trust claim that the identified concept “faithfully represents the neuron’s underlying function,” i.e., that the explanation is truly correct/faithful, which goes beyond correlational decodability and reads as a high-level guarantee about the explanation’s validity; per codebook this is best treated as Rung 3 (mechanistic/‘can be trusted’ guarantee), not merely R1/R2. No explicit hedge (e.g., ‘may’, ‘suggests’) is present."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,Bootstrap ensemble procedure (resampling probing dataset; repeated neuron identification runs; aggregation into prediction set with coverage guarantee),1,2,1,4,0,"Method: The described procedure is a statistical resampling/aggregation over probing datasets and does not intervene on model internals (no ablation/patching/steering), so it is observational/associational (R1). Claim: It asserts the procedure ""quantifies stability"" and provides ""guaranteed coverage probability""—a causal/mechanistic claim about internal computation is not made, but it does claim a guarantee about the output of the procedure (a stronger-than-association property), best fit as R2 rather than R1. Not R3 because it does not attribute a specific internal mechanism/function to neurons. No explicit hedging."
