{
  "timestamp": "2026-02-19T09:07:07.772846+00:00",
  "paper_id": "2410.08417",
  "claim_id": "2410.08417-04",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2410.08417\nTitle: Bilinear MLPs enable weight-based mechanistic interpretability\nFull text:\n                      Published as a conference paper at ICLR 2025\n\n\n           BILINEAR MLPS ENABLE WEIGHT-BASED\n          MECHANISTIC INTERPRETABILITY\n\n\n                             Michael T. Pearce∗          Thomas Dooms∗             Alice Rigg\n                                   Independent               University of Antwerp          Independent\n                             pearcemt@              thomas.dooms@         rigg.alice0@\n                        alumni.stanford.edu         uantwerpen.be          gmail.com\n\n\n                                         Jose Oramas                        Lee Sharkey\n                                  University of Antwerp, sqIRL/IDLab                Apollo Research\n2025                      jose.oramas@uantwerpen.be          lee@apolloresearch.ai\nJun                                   ABSTRACT\n                  A mechanistic understanding of how MLPs do computation in deep neural net-\n                           works remains elusive.  Current interpretability work can extract features from25\n                             hidden activations over an input dataset but generally cannot explain how MLP\n                              weights construct features.  One challenge is that element-wise nonlinearities\n                                introduce higher-order interactions and make it difficult to trace computations\n                              through the MLP layer.  In this paper, we analyze bilinear MLPs, a type of\n                            Gated Linear Unit (GLU) without any element-wise nonlinearity that neverthe-\n                                    less achieves competitive performance. Bilinear MLPs can be fully expressed in[cs.LG]                        terms of linear operations using a third-order tensor, allowing flexible analysis of\n                                 the weights. Analyzing the spectra of bilinear MLP weights using eigendecom-\n                                 position reveals interpretable low-rank structure across toy tasks, image classifi-\n                                   cation, and language modeling. We use this understanding to craft adversarial\n                              examples, uncover overfitting, and identify small language model circuits directly\n                           from the weights alone. Our results demonstrate that bilinear layers serve as an\n                                  interpretable drop-in replacement for current activation functions and that weight-\n                             based interpretability is viable for understanding deep-learning models.\n\n\n                1  INTRODUCTION\n\n                        Multi-layer perceptrons (MLPs) are an important component of many deep learning models, in-\n                       cluding transformers (Vaswani et al., 2017). Unfortunately, element-wise nonlinearities obscure the\n                         relationship between weights, inputs, and outputs, making it difficult to trace a neural network’s\n                      decision-making process. Consequently, MLPs have previously been treated as undecomposablearXiv:2410.08417v2\n                    components in interpretability research (Elhage et al., 2021).\n\n                    While early mechanistic interpretability literature explored neural network weights (Olah et al.,\n                     2017; 2020; Voss et al., 2021; Elhage et al., 2021), activation-based approaches dominate contem-\n                      porary research (Petsiuk et al., 2018; Ribeiro et al., 2016; Simonyan et al., 2014; Montavon et al.,\n                       2018). In particular, most recent studies on transformers use sparse dictionary learning (SDL) to\n                   decompose latent representations into an overcomplete basis of seemingly interpretable atoms (Cun-\n                   ningham et al., 2024; Bricken et al., 2023b; Marks et al., 2024; Dunefsky et al., 2024). However,\n                   SDL-based approaches only describe which features are present, not how they are formed or what\n                           their downstream effect is. Previous work has approximated interactions between latent dictionaries\n                      using linear and gradient-based attribution (Marks et al., 2024; Ge et al., 2024), but these approaches\n                         offer weak guarantees of generalization. To ensure faithfulness, ideally, we would be able to capture\n                       nonlinear feature interactions in circuits that are grounded in the model weights.\n\n                           *Equal contribution\n                        Code at: https://github.com/tdooms/bilinear-decomposition\n\n\n                                                           1\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nOne path to better circuit discovery is to use more inherently interpretable architectures. Previous\nwork constrains the model into using human-understandable components or concepts (Koh et al.,\n2020; Chen et al., 2019), but this typically requires labeled training data for the predefined con-\ncepts and involves a trade-off in accuracy compared to learning the best concepts for performance\n(Henighan et al., 2023). Ideally, we could more easily extract and interpret the concepts that mod-\nels naturally learn rather than force the model to use particular concepts. To this end, Sharkey\n(2023) suggested that bilinear layers Lin et al. (2015); Li et al. (2017); Chrysos et al. (2021) of\nthe form g(x) = (W x) ⊙(V x) are intrinsically interpretable because their computations can be\nexpressed in terms of linear operations with a third order tensor. This enables the use of tensor\nor matrix decompositions to directly understand the weights. Moreover, bilinear layers outperform\nReLU-based transformers in language modeling (Shazeer, 2020) and have performance only slightly\nbelow SwiGLU, which is prevalent in competitive models today (Touvron et al., 2023).\n\nTensor decompositions have long been studied in machine learning (Cichocki et al., 2015; Panagakis\net al., 2021; Sidiropoulos et al., 2017) where most applications are based on an input dataset. Using\ndecompositions to extract features directly from the weights of tensor-based models is less explored.\nHere, we show that bilinear MLPs can be decomposed into functionally relevant, interpretable com-\nponents by directly decomposing the weights, without using inputs. These decompositions reveal\na low-rank structure in bilinear MLPs trained across various tasks. In summary, this paper demon-\nstrates that bilinear MLPs are an interpretable drop-in replacement for ordinary MLPs in a wide\nrange of settings. Our contributions are as follows:\n\n      1. In section 3, we introduce several methods to analyze bilinear MLPs. One method decom-\n        poses the weights into a set of eigenvectors that explain the outputs along a given set of\n         directions in a way that is fully equivalent to the layer’s original computations.\n\n      2. In section 4, we showcase the eigenvector decomposition across multiple image classifica-\n         tion tasks, revealing an interpretable low-rank structure. Smaller eigenvalue terms can be\n         truncated while preserving performance. Using the eigenvectors, we see how regularization\n        reduces signs of overfitting in the extracted features and construct adversarial examples.\n\n      3. Finally, in section 5, we analyze how bilinear MLPs compute output features from input\n         features, both derived from sparse dictionary learning (SDL). We highlight a small circuit\n          that flips the sentiment of the next token if the current token is a negation (“not”). We\n         also find that many output features are well-correlated with low-rank approximations. This\n         gives evidence that weight-based interpretability can be viable in large language models.\n\n\n2  BACKGROUND\n\nThroughout, we use conventional notation as in Goodfellow et al. (2016). Scalars are denoted by s,\nvectors by v, matrices by M, and third-order tensors by T. The entry in row i and column j of a\nmatrix M is a scalar and therefore denoted as mij. We denote taking row i or column j of a matrix\nby mi: and m:j respectively. We use ⊙to denote an element-wise product and ·axis to denote a\nproduct of tensors along the specified axis.\n\nDefining bilinear MLPs. Modern Transformers (Touvron et al., 2023) feature Gated Linear Units\n(GLUs), which offer a performance gain over standard MLPs for the same number of parameters\n(Shazeer, 2020; Dauphin et al., 2017). GLU activations consist of the component-wise product\nof two linear up-projections of size (dhidden, dinput), W and V , one of which is passed through a\nnonlinear activation function σ(Equation 1). The hidden activations g(x) then pass through a down-\nprojection P of size (doutput, dhidden). We omit biases for brevity.\n\n\n                                g(x) = (W x) ⊙σ(V x)\n                                                                                                   (1)\n                        GLU(x) = P (g(x))\n\nA bilinear layer is a GLU variant that omits the nonlinear activation function σ. Bilinear layers beat\nordinary ReLU MLPs and perform almost as well as SwiGLU on language modeling tasks (Shazeer,\n2020). We corroborate these findings in Appendix I, and show that bilinear layers achieve equal loss\nwhen keeping training time constant and marginally worse loss when keeping data constant.\n\n\n                                       2\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nInteraction matrices and the bilinear tensor. A bilinear MLP parameterizes the pairwise interac-\ntions between inputs. One way to see this is by looking at how a single output g(x)a is computed.\n\n                               g(x) = (W x) ⊙(V x)\n                               g(x)a = (wTa:x) (vTa:x)\n                       = xT (wa:vTa:)x\nWe call the (dinput, dinput) matrix wa:vTa: = Ba:: an interaction matrix since it defines how each pair\nof inputs interact for a given output dimension a.\n\nThe collection of interaction matrices across the output axis can be organized into the third-order\nbilinear tensor, B, with elements baij = waivaj, illustrated in Figure 1A. The bilinear tensor allows\nus to easily find the interaction matrix for a specific output direction u of interest by taking a product\nalong the output axis, u  ·out B, equal to a weighted sum over the neuron-basis interaction matrices,\nPa uawa:vTa:. As written, B has size (dhidden, dinput, dinput) but we will typically multiply the down-\nprojection P into B resulting in a (doutput, dinput, dinput) size tensor.\n\nSimplifications due to symmetry. Because an interaction matrix is always evaluated with two\ncopies of the input x, it contains redundant information that does not contribute to the activation.\nAny square matrix can be expressed uniquely as the sum of a symmetric and anti-symmetric matrix.\n\n                             1              1\n                                                                                            a:: )                          Ba:: = 2(Ba:: + BT                                                                  a:: ) + 2(Ba:: −BT\n                                   |  Bsym{za::   }      |  Banti{za::   }\n\nHowever, evaluating an anti-symmetric matrix A with identical inputs yields 0 and can be omitted:\n\n                    xT Ax = xT (−AT )x = −(xT Ax)T = 0.\nTherefore, only the symmetric part Bsyma::   contributes. From here on, we drop the ·sym superscript\nand assume the symmetric form for any interaction matrix or bilinear tensor (baij = 2(waivaj1     +\nwajvai)). Symmetric matrices have simpler eigendecompositions since the eigenvalues are all real-\nvalued, and the eigenvectors are orthogonal by the spectral theorem.\n\nIncorporating biases. If a bilinear layer has biases, we can augment the weight matrices to adapt\nour approach. Given activations of the form g(x) = (W x+b1)⊙(V x+b2), define W ′ = [W ; b2],\nV ′ = [V ; b2], and x′ = [x, 1]. Then, g(x) = (W ′x′) ⊙(V ′x′) in a bilinear layer with biases.\nIn subsection 4.3, we study a toy classification task using a model trained with biases, illustrating\nhow biases can be interpreted using the same framework. For the rest of our experiments, we used\nmodels without biases for simplicity, as it did not harm performance. See Appendix L for details.\n\n3  ANALYSIS METHODS\n\nSince bilinear MLPs can be expressed in terms of a third-order tensor, B, they can be flexibly\nanalyzed using techniques from linear algebra, such as decompositions and transformations. The\nchoice of analysis approach depends on what additional information, in terms of previously obtained\ninput or output features, is provided.\n\n3.1  INPUT / OUTPUT FEATURES →DIRECT INTERACTIONS\n\nIf we have already obtained meaningful sets of features for the bilinear MLP’s inputs and outputs,\nfor example from a set of latent feature dictionaries F in and F out, then we can directly study the in-\nteractions between these features and understand how the output features are constructed from input\nones. We can transform the bilinear tensor into the feature basis via ˜babc = Pijk faiout bijk fbjin fck.in\nFor a given set of sparse input and output activations, only a small subset of the interactions (with\na, b, c all active) will contribute, and the statistics of these active interactions can be studied.\n\nFor dictionaries obtained from sparse autoencoders (SAEs) we can instead use the output SAE’s\nencoder directions in the transformation: ˜babc = Pijk eaiout bijk fbjin fck.in Then the output activations\nare zoutc = ReLU(Pab ˜babczina zinb ) in terms of the input directions zin.  In section 5, we use this\napproach to identify the top relevant interactions for features in a language model.\n\n\n                                       3\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nA)    Bilinear layer               Bilinear tensor       B)        Multiply by output       Interaction Matrix   Eigendecomposition\n                                                                            vector\n\n\n\n\n                                                                                                  in\n                           out                           out\n                                                                                          in\n\n\n\n\nFigure 1: A) Two ways to represent a bilinear layer, via an elementwise product or the bilinear\ntensor. B) Diagram of the eigendecomposition technique.  Multiplying the bilinear tensor by a\ndesired output direction u produces an interaction matrix Q that can be decomposed into a set of\neigenvectors v and associated eigenvalues λi.\n\n\n3.2  OUTPUT FEATURES →EIGENDECOMPOSITION\n\nGiven a set of meaningful features for the MLP outputs, we can identify the most important input\ndirections that determine the output feature activations. The output features could come from a\ndictionary, from the unembedding (shown in section 4), or from the decompilation of later layers.\n\nThe interaction matrix, Q = u ·out B for a given output feature u can be decomposed into a set\nof eigenvectors (Figure 1). Since Q can be considered symmetric without loss of generality (see\nsection 2), the spectral theorem gives\n\n                                              d\n                   Q = X λi vivTi                                        (2)\n                                                                    i\nwith a set of d (the rank of W , V ) orthonormal eigenvectors vi and real-valued eigenvalues λi. In\nthe eigenvector basis, the output in the u-direction is\n\n                                              d\n                          xT Qx = X λi (vTi x)2                                   (3)\n                                                                     i                                                                           activation|   {z for vi}\nwhere each term can be considered the activation for the eigenvector vi of size (dinput). That is, the\nbilinear layer’s outputs are quadratic in the eigenvector basis.\n\nThe eigenvector basis makes it easy to identify any low-rank structure relevant to u. The top eigen-\nvectors by eigenvalue magnitude give the best low-rank approximation to the interaction matrix Q\nfor a given rank. And since the eigenvectors diagonalize Q, there are no cross-interactions between\neigenvectors that would complicate the interpretation of their contributions to u.\n\n3.3  NO FEATURES →HIGHER-ORDER SVD\n\nIf we have no prior features available, it is still possible to determine the most important input and\noutput directions of B through a higher-order singular value decomposition (HOSVD). The simplest\napproach that takes advantage of the symmetry in B is to reshape the tensor by flattening the two\ninput dimensions to produce a (doutput, d2input) shaped matrix and then do a standard singular value\ndecomposition (SVD). Schematically, this gives\n                                    Bout,in×in = X σi u(i)out ⊗q(i)in×in\n                                                                   i\nwhere q can still be treated as an interaction matrix and further decomposed into eigenvectors as\ndescribed above. We demonstrate this approach for an MNIST model in Appendix D.\n\n4  IMAGE CLASSIFICATION: INTERPRETING VISUAL FEATURES\n\nWe consider models trained on the MNIST dataset of handwritten digits and the Fashion-MNIST\ndataset of clothing images. This is a semi-controlled environment that allows us to evaluate the\n\n\n                                       4\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nA)    Eigenvector = vi        Input = x    Activation = λi(vi Tx)2   B)          1                  2                  3                  4                  5\n\n\n                                      Low\n\n\n                                              High\n\n                                                                                               trouser                pullover                dress                  coat                 sandal\n\n                                              High\n              Positive\n                                      Low\n            Negative\n\n\nFigure 2: A) Eigenvector activations are quadratic in the input and have a large magnitude if an\ninput aligns with the positive (blue) regions or the negative (red) regions, but not both. B) Top\neigenvectors for single-layer MNIST and Fashion-MNIST models, revealing the most significant\npatterns learned for each class. In MNIST, eigenvectors represent components of the target class,\nwhile Fashion-MNIST eigenvectors function as localized edge detectors. Best viewed in color.\n\n\ninterpretability of eigenvectors computed using the methods in subsection 3.2. This section analyses\na shallow feedforward network (FFN) consisting of an embedding projection, a bilinear layer, and a\nclassification head; see Appendix G for details.\n\nFirst, we qualitatively survey the eigenvectors and highlight the importance of regularization in\nfeature quality. Second, we consider the consistency of eigenvectors across training runs and sizes.\nThird, we turn toward an algorithmic task on MNIST, where we compare the ground truth with\nthe extracted eigenvectors.  Lastly, we use these eigenvectors to construct adversarial examples,\ndemonstrating their causal importance.\n\n\n4.1  QUALITATIVE ASSESSMENT: TOP EIGENVECTORS APPEAR INTERPRETABLE\n\nThe eigenvectors are derived using the unembedding directions for the digits as the output directions\nu to obtain interaction matrices Q = u·out B that are then decomposed following subsection 3.2. So\neach unembedding direction (digit) has a corresponding set of eigenvectors, although we may refer\nto the full collection as the eigenvectors of the layer or model.\n\nWe can visualize them by projecting them into the input space using the embedding weights. Be-\ncause the activation of an eigenvector v with eigenvalue λi is quadratic in the input, λ(vT x)2, the\nsign of the eigenvector v is arbitrary. The quadratic leads to XOR-like behavior where high overlap\nwith an eigenvector’s positive regions (blue) or the negative regions (red)—but not both—leads to\nlarge activation magnitude, while the overall sign is determined by the eigenvalue (Figure 2A).\n\nFor MNIST, the top positive eigenvector for each output class emphasizes a curve segment specific\nto its digit or otherwise resembles a prototypical class image (Figure 2B). Top eigenvectors for\nFMNIST function as localized edge detectors, focusing on important edges for each clothing article,\nsuch as the leg gap for trousers. The localized edge detection relies on the XOR-like behavior of the\neigenvector’s quadratic activation.\n\n\n\n\n\n                                     0.14\n\n                                     0.11\n                                     0.10\n\n\n\n                                     0.00              20\n\n                                     0.00\n\n\n\n                                     -0.10\n                                     -0.13\n                                     -0.16\n                                     -0.18\n                                                  20\n\nFigure 3: The top four positive (top) and negative (bottom) eigenvectors for the digit 5, ordered\nfrom left to right by importance. Their eigenvalues are highlighted on the left. Only 20 positive and\n20 negative eigenvalues (out of 512) are shown on the left images. Eigenvectors tend to represent\nsemantically and spatially coherent structures.\n\n\n                                       5\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                97.6%                    98.1%                    98.1%                    97.8%                    97.2%\n\n\n\n\n\n                                                             Noise\n        norm=0                                                                                       norm=1\n\nFigure 4: Top eigenvector for models trained with varying Gaussian input noise. For reference, the\nnorm of an average digit is about 0.3; adding noise with a norm of 1 results in a heavily distorted\nbut discernible digit. Finally, the test accuracy for each model is shown at the top.\n\n\n\nOnly a small fraction of eigenvalues have non-negligible magnitude (Figure 3). Different top eigen-\nvectors capture semantically different aspects of the class. For example, in the spectrum for digit\n5, the first two positive eigenvectors detect the 5’s horizontal top stroke but at different positions,\nsimilar to Gabor filters. The next two positive eigenvectors detect the bottom segment. The negative\neigenvectors are somewhat less intuitive but generally correspond to features that indicate the digit\nis not a five, such as an upward curve in the top right quadrant instead of a horizontal stroke. In\nAppendix C, we study this technique towards explaining an input prediction. Details of the training\nsetup are outlined in Appendix G while similar plots for other digits can be found in Appendix A.\n\nBecause we can extract features directly from model weights, we can identify overfitting in image\nmodels by visualizing the top eigenvectors and searching for spatial artifacts.  For instance, the\neigenvectors of unregularized models focus on certain outlying pixels (Figure 4). We found adding\ndense Gaussian noise to the inputs (Bricken et al., 2023a) to be an effective model regularizer,\nproducing bilinear layers with more intuitively interpretable features. Increasing the scale of the\nadded noise results produces more digit-like eigenvectors and results in a lower-rank eigenvalue\nspectrum (Appendix E). These results indicate that our technique can qualitatively help uncover\noverfitting or other unwanted behavior in models. Furthermore, it can be used to evaluate the effect\nof certain regularizers and augmentation techniques, as explored in Appendix B.\n\n\n4.2  QUANTITATIVE ASSESSMENT: EIGENVECTORS LEARN CONSISTENT PATTERNS\n\nOne important question in machine learning is whether models learn the same structure across train-\ning runs (Li et al., 2016) and across model sizes (Frankle & Carbin, 2019). In this section, we study\nboth and find that eigenvectors are similar across runs and behave similarly across model sizes.\nFurthermore, we characterize the impact of eigenvector truncation on classification accuracy.\n\nBoth the ordering and contents of top eigenvectors are very consistent across runs. The cosine\nsimilarities of the top eigenvector are between 0.8 and 0.9 depending on size (Figure 5). Generally,\n\n\n   A)                                       B)\n                                 Similarity Across Eigenvectors                                               Truncation Across Sizes\n\n\n                      1                                                    Model Size            100%                                                    Model Size\n                        0.9                                                              30                    50%                                                              30\n                                                                                     50                                                                                   50\n                        0.8                                                              100                          error    20%                                                              100\n                                                                                     300                                                                                  300                                                                         similarity   0.7                                                              500              10%                                                              500\n                                                                                     1000                                                                                 1000\n                        0.6                                                                                  5%                                            Cosine                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Classification\n                        0.5                                                                                  2%\n\n                        0.4                                                             1%\n                       0             5             10            15                                        0        5        10       15       20       25       30\n\n                                            Eigenvector rank                                                                  Eigenvector rank (per digit)\n\n\nFigure 5: A) The similarity between ordered eigenvectors of the same model size averaged over all\ndigits. This shows that equally sized models learn similar features. B) Resulting accuracy after only\nretaining the n most important eigenvalues (per digit). Both plots are averaged over 5 runs with the\n90% confidence interval shown.\n\n\n                                       6\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nincreasing model sizes results in more similar top eigenvectors. Further, truncating all but the top\nfew eigenvectors across model sizes yields very similar classification accuracy. This implies that,\nbeyond being consistently similar, these eigenvectors have a comparable impact on classification. In\nAppendix F, we further study the similarity of eigenvectors between sizes and show that retaining\nonly a handful of eigenvectors results in minimal accuracy drops (0.01%).\n\n\n4.3  COMPARING WITH GROUND TRUTH: EIGENVECTORS FIND COMPUTATION\n\nTo perform a ground-truth assessment of eigenvectors, we consider a task from a mechanistic in-\nterpretability challenge, where the goal was to determine the labeling function (training objective)\nfrom a model Casper (2023). Specifically, the challenge required reverse-engineering a binary im-\nage classifier trained on MNIST, where the label is based on the similarity to a specific target image.\nThe model predicted ‘True’ if the input has high cosine similarity to this target or high similarity to\nthe complement (one minus the grayscale) of that target and ‘False’ otherwise. This target is chosen\nas an instance of a ‘1’.\n\nPrevious work (Stefan Heimersheim, 2023) reverse-engineered this through a combination of meth-\nods, all requiring careful consideration and consisting of non-trivial insights. Furthermore, the meth-\nods required knowledge of the original dataset and a hint of what to look for. While our method does\nnot work on the original architecture, we show that we do not require such knowledge and can extract\nthe original algorithm from the weights alone.\n\nWe perform our decomposition on the output difference (True −False) since this is the only mean-\ningful direction before the softmax. This consistently reveals one high positive eigenvalue; the rest\nare (close to) zero (Figure 6). The most positive eigenvector is sufficient for completing the task;\nit computes the exact similarity we want.  If the input is close to the target, the blue region will\nmatch; if it is close to the complement, the red will match; if both are active simultaneously, they\nwill somewhat cancel out. The remaining two eigenvectors are separated as they seem to overfit the\ndata slightly; the negative eigenvector seems to penalize diagonal structures.\n\nContrary to other models, this task greatly benefited from including biases. This arises from the\nfact that the model must not only compute similarity but also make its binary decision based on a\nlearned threshold.  If no bias is provided, the model attempts to find quadratic invariances in the\ndata, which don’t generalize well, especially given the important but sensitive role of this threshold\nin classification.  Here, the bias (shown in the bottom corner of Figure 6) represents a negative\ncontribution. The role of biases in bilinear layers is further discussed in Appendix L.\n\n\n\n                                 0.19\n\n\n\n\n\n                                 0.02\n                                 0.00\n                                                10                                                          Target\n\n                                 0.00\n\n\n\n\n                                 -0.02\n\n                                 -0.04\n                                                10                                                        Bias\n\nFigure 6: Eigenvalues and eigenvectors of a model trained to classify based on similarity to a target.\nThe most important eigenvector (top-left) is a generalizing solution; the other features sharpen the\ndecision boundary based on the training dataset. The latter features disappear with increased regu-\nlarization. On the right, the target digit is shown along with the learned bias from the model.\n\n\n\n4.4  ADVERSARIAL MASKS: GENERAL ATTACKS FROM WEIGHTS\n\nTo demonstrate the utility of weight-based decomposition, we construct adversarial masks for the\nMNIST model without training or any forward passes. These masks are added to the input, leading\nto misclassification as the adversarial digit. The effect is similar to steering, but the intervention is\nat the input instead of the model internals.\n\n\n                                       7\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n      A)\n\n\n\n\n\n                                   Adversarial      Misclassified     Random\n                Eigenvector\n                             Mask         Example        Mask\n       B)\n\n\n\n\n\nFigure 7: Examples of an adversarial mask constructed from the given eigenvector along for models\ntrained A) with Gaussian noise regularization (std 0.15) and B) without regularization. The average\naccuracy and the rate of misclassification as the adversarial digit show stronger effects for adversarial\nmasks than random baselines. In B), the mask is only applied to the outer edge of pixels that are\nactive on less than 1% of samples.\n\n\nWe construct the adversarial masks from the eigenvectors for specific digits. One complication is\nthat the eigenvectors can have nontrivial cosine similarity with each other, so an input along a single\neigenvector direction could potentially activate multiple eigenvectors across different digits. To help\navoid this, we construct the mask mi for a given eigenvector vi: as the corresponding row of the\npseudoinverse (V +)i: for a set of eigenvectors V (specifically the top 10 positive). In an analogy to\nkey-value pairs, the pseudoinverses effectively act like keys that activate with more specificity than\nthe eigenvectors themselves, since vj: · (V +)i: = δij.\n\nWe construct an adversarial mask from an eigenvector for the digit 3 (Figure 7A). Even though the\noriginal eigenvector resembles the digit, the pseudoinverse-based mask does not (see Appendix M\nfor more examples). The accuracy, averaged over masks from the top three eigenvectors, drops\nsignificantly more than the baseline of randomly permuting the mask despite regularizing the model\nduring training using dense Gaussian noise with a standard deviation of 0.15. The corresponding\nrise in misclassification indicates effective steering towards the adversarial digit.\n\nIlyas et al. (2019) observe that adversarial examples can arise from predictive but non-robust features\nof the data, perhaps explaining why they often transfer to other models. Our construction can be\nseen as a toy realization of this phenomenon because the masks correspond to directions that are\npredictive of robust features but are not robust. We construct masks that only exploit the patterns\nof over-fitting found on the outer edge of the image for a model trained without regularization\n(Figure 7B). Since we can find this over-fitting pattern from the eigenvectors, in a general way, we\ncan construct the mask by hand instead of optimizing it.\n\n\n5  LANGUAGE: FINDING INTERACTIONS BETWEEN SAE FEATURES\n\nEach output of a bilinear layer is described by weighted pairwise interactions between their input\nfeatures. Previous sections show that this can be successfully leveraged to trace between a bilinear\nlayer’s inputs and outputs. Here, we turn towards tracing between latent feature dictionaries ob-\ntained by training sparse autoencoders (SAEs) on the MLP inputs or outputs for a 6-layer bilinear\ntransformer trained on TinyStories (Eldan & Li, 2023) (see training details in Appendix G).\n\n\n5.1  SENTIMENT NEGATION CIRCUIT\n\nWe focus on using the eigendecomposition to identify low-rank, single-layer circuits in a bilinear\ntransformer. We cherry-pick and discuss one such circuit that takes input sentiment features and\nsemantically negates them. Unlike previous work on sparse feature circuits (Marks et al., 2024) that\nrelies on gradient-based linear approximations, we identify nonlinear interactions grounded in the\nlayer’s weights that contribute to the circuit’s computation.\n\n\n                                       8\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n A)                                B)      not-bad                  0.6             C)\n                                                                             feature        -\n\n                                                                                                                 0.4\n\n\n\n                                                                                                                 0.2                                                                                                                                                                                                                                                                                                          eigenvector   +                     +\n                                                                                                                                                                                                                        negative    -0.6     -0.4     -0.2        -0.2   0.2     0.4      0.6\n                                                                                 Top\n                                                                                                                         -0.4\n                                                                -    -0.6            not-goodfeature\n\n                                                         Top positive eigenvector\n\nFigure 8: The sentiment negation circuit that computes the not-good and not-bad output features.\nA) The interaction submatrix containing the top 15 interactions. B) The projection of top interacting\nfeatures onto the top eigenvectors using cosine similarity. The symbols for different clusters match\nthe labels in A. Clusters coincide with the projection of meaningful directions such as the difference\nin “bad” vs “good” token unembeddings and the MLP’s input activations for the input “[BOS] not”.\nC) The not-good feature activation compared to its approximation by the top two eigenvectors.\n\n\n\nThe sentiment negation circuit computes the activation of two opposing output features in layer 4\n(index 1882 and 1179) that form a fully linear subspace. The cosine similarity of their decoder\nvectors is -0.975. Based on their top activations, the output features activate on negation tokens\n(“not”, “never”, “wasn’t”) and boosts either positive sentiment tokens (“good”, “safe”, “nice”) or\nnegative sentiment tokens (“bad”, “hurt”, “sad”), so we denote the two features as the not-good and\nthe not-bad features respectively. See Appendix O for the top activations of all features mentioned.\n\nFocusing on the not-good feature, the top interactions for computing its activations resemble an\nAND-gate (Figure 8A). Input features that boost negative sentiment tokens (blue squares) have\nstrong positive interactions with negation token features (green triangles), but both have negligi-\nble self-interactions. So, both types of input features are needed to activate the not-good feature\nand flip the boost from negative to positive sentiment. The one positive sentiment feature (orange\ndownward triangle) interacts with the opposite sign. The interactions shown are significantly larger\nthan the typical cross-interactions with a standard deviation of 0.004 (Figure 27.\n\nThe eigenvalue spectrum has one large positive (0.62) and one large negative value (-0.66) as outliers\n(Figure 27). We can see the underlying geometry of the circuit computation by projecting the input\nfeatures onto these eigenvectors (Figure 8). By itself, a positive sentiment feature (blue squares)\nwould equally activate both eigenvectors and cancel out, but if a negation feature is also present,\nthe positive eigenvector is strongly activated. The activation based on only these two eigenvectors,\nfollowing Equation 3, has a good correlation (0.66) with the activation of the not-good feature,\nparticularly at large activation values (0.76), conditioned on the not-good feature being active.\n\n\n5.2  LOW-RANK APPROXIMATIONS OF OUTPUT FEATURE ACTIVATIONS\n\nThe top eigenvectors can be used to approximate the activations of the SAE output features using a\ntruncated form of Equation 3. To focus on the more meaningful tail of large activations, we compute\nthe approximation’s correlation conditioned on the output feature being active. The correlations\nof inactive features are generally lower because they are dominated by ‘noise’. We evaluate this\non three bilinear transformers at approximately 2/3 depth: a 6-layer TinyStories (‘ts-tiny’) and two\nFineWeb models with 12 and 16 layers (‘fw-small’ and ‘fw-medium’).\n\nWe find that features are surprisingly low-rank, with the average correlation starting around 0.65 for\napproximations by a single eigenvector and rising steadily with additional eigenvectors (Figure 9A).\nMost features have a high correlation (> 0.75) even when approximated by just two eigenvectors\n(Figure 9B). Scatter plots for a random sample of features show that the low-rank approximation\noften captures the tail dependence well (Figure 9C). Interestingly, we find the approximation to\ndrastically improve with longer SAE training times while other metrics change only slightly. This\nindicates a ‘hidden’ transition near convergence and is further discussed in Appendix H. Overall,\nthese results suggest that the interactions that produce large output activations are low-rank, making\ntheir interpretability potentially easier.\n\n\n                                       9\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nA)        1   Feature activation approximation      B)               Approx. by top 2 eigenvectors        C)             0.78         0.82         0.89\n                    fw-medium\n            0.95       fw-smallts-tiny                                                      300\n\n              0.9                                                                250\n                                                                                                                                           0.96         0.89         0.72\n            0.85\n              0.8                                                                                                                                         Count 150                                     Correlation 0.75                                                                200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Approximation\n                                                                                                                                           0.96         0.46         0.85\n              0.7                                                                100\n\n\n            0.65                                                                 50\n\n\n              0.6\n              0               20              40              60                 0         −0.25       0        0.25        0.5       0.75       1\n                               Top eigenvectors                                                        Correlation (active only)                                                     Activation\n\nFigure 9: Activation correlations with low-rank approximations for differently-sized transformers.\nA) Average correlation over output features computed over every input where the feature is active.\nB) The distribution of active-only correlations for approximations using the top two eigenvectors.\nC) Scatter plots for a random set of nine output features on ‘fw-medium’. Approximations use the\ntop two eigenvectors. Low correlation scores generally only occur on low-activation features.\n\n\n6  DISCUSSION\n\nSummary. This paper introduces a novel approach to weight-based interpretability that leverages\nthe close-to-linear structure of bilinear layers. A key result is that we can identify the most im-\nportant input directions that explain the layer’s output along a given direction using an eigenvector\ndecomposition. The top eigenvectors are often interpretable, for example for MNIST they function\nas edge-detectors for strokes specific to each digit. The lack of element-wise nonlinearity in bilinear\nMLPs allows us to transform their weights into interaction matrices that connect input to output\nfeatures and then extract the low-rank structure. In language models, we find that many SAE output\nfeatures are well-approximated by low-rank interaction matrices, particularly at large activations.\nWe highlighted one example of an extracted low-rank circuit that flips the sentiment of the next to-\nken if the current token is a negation (“not”). The behavior of this circuit can be easily understood in\nterms of the top eigenvectors, whereas finding a similar circuit in conventional MLPs would be more\ndifficult. Overall, our results demonstrate that bilinear MLPs offer intrinsic interpretability that can\naid in feature and circuit extraction.\n\nImplications. The main implication of our work is that weight-based interpretability is viable, even\nfor large language models. Bilinear MLPs can replace conventional MLPs in transformers with min-\nimal cost while offering intrinsic interpretability due to their lack of element-wise nonlinearities and\nclose-to-linear structure. Current circuit analysis techniques rely on gradient-based approximations\n(Syed et al., 2023; Marks et al., 2024) or use transcoders (Dunefsky et al., 2024) to approximate\nMLPs. Both approaches depend on an input dataset, potentially leading to poor performance out-\nof-distribution, and they may not fully capture the nonlinear computations in MLPs. In contrast,\nbilinear MLPs can be transformed into explicit feature interaction matrices and decomposed in a\nway fully equivalent to the original computations. Extracting interactions more directly from the\nweights should lead to better, more robust circuits. Weight-based interpretability may also offer\nbetter safety guarantees since we could plausibly prove bounds on a layer’s outputs by quantifying\nthe residual weights not captured in a circuit’s interactions.\n\nLimitations. Application of our methods typically relies on having a set of meaningful output direc-\ntions available. In shallow models, the unembedding directions can be used, but in deeper models,\nwe rely on features derived from sparse autoencoders that are dependent on an input dataset. An-\nother limitation is that, although the eigenvalue spectra are often low-rank and the top eigenvectors\nappear interpretable, there are no guarantees the eigenvectors will be monosemantic. We expect that\nfor high-rank spectra, the orthogonality between eigenvectors may limit their interpretability. Ap-\nplying sparse dictionary learning approaches to decompose the bilinear tensor may be a promising\nway to relax the orthogonality constraint and find interpretable features from model weights.\n\n\nACKNOWLEDGEMENTS\n\nWe are grateful to Narmeen Oozeer, Nora Belrose, Philippe Chlenski, and Kola Ayonrinde for help-\nful feedback on the draft. We are grateful to the AI Safety Camp program where this work first\n\n\n                                       10\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nstarted and to the ML Alignment & Theory Scholars (MATS) program that supported Michael and\nAlice while working on this project. We thank CoreWeave for providing compute for the finetuning\nexperiments. This research received funding from the Flemish Government under the ”Onderzoek-\nsprogramma Artifici¨ele Intelligentie (AI) Vlaanderen” programme.\n\nCONTRIBUTIONS\n\nMichael performed the bulk of the work on the MNIST analysis and provided valuable insights\nacross all presented topics. Thomas worked on the Language Models section and was responsible\nfor code infrastructure. The paper was written in tandem, each focusing on their respective section.\n\nETHICS STATEMENT\n\nThis paper proposes no advancements to the state-of-the-art in model capabilities. Rather, it provides\nnew methods to analyze the internals of models to increase our understanding. The only misuse the\nauthors envision is using this technique to leak details about the dataset that the model has learned\nmore efficiently. However, this can be avoided by using this technique during safety evaluation.\n\nREPRODUCIBILITY STATEMENT\n\nWe aspired to make this work as reproducible as possible. First, Appendix G (among others) aims\nto provide detailed and sufficient descriptions to independently recreate our training setups. Second,\nour code (currently public but not referenced for anonymity) contains separate files that can be\nused to generate the figures in this paper independently. We used seeds across training runs so that\nrecreated figures would be equivalent. Third, all models that are compute-intensive to train, such as\nthe SAEs and the LMs, will be shared publicly. Lastly, we will publish an interactive demo, which\nwill allow independent analysis of the figures in Appendix A, Appendix B, and Appendix O in a\nway this document cannot.\n\nREFERENCES\n\nTrenton Bricken, Rylan Schaeffer, Bruno Olshausen, and Gabriel Kreiman. Emergence of sparse\n  representations from noise. In International Conference on Machine Learning, pp. 3148–3191.\n  PMLR, 2023a.\n\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly,\n  Nicholas L. Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\n  Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen,\n  Brayden McLean, Josiah E. Burke, Tristan Hume, Shan Carter, Tom Henighan, and Chris Olah.\n  Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. Trans-\n  former Circuits Thread, October 2023b. URL https://transformer-circuits.pub/\n  2023/monosemantic-features/index.html.\n\nStephen  Casper.     Eis  vii:  A  challenge  for  mechanists,  2023.   URL https://\n  www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/KSHqLzQscwJnv44T8. AI\n  Alignment Forum, Part 7 of the Engineer’s Interpretability Sequence, posted on February 18,\n  2023.\n\nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin.\n  This looks like that: Deep learning for interpretable image recognition, 2019. URL https:\n  //arxiv.org/abs/1806.10574.\n\nGrigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Panagakis,\n  and Stefanos Zafeiriou. Deep polynomial neural networks. IEEE transactions on pattern analysis\n  and machine intelligence, 44(8):4021–4034, 2021.\n\nAndrzej Cichocki, Danilo Mandic, Lieven De Lathauwer, Guoxu Zhou, Qibin Zhao, Cesar Caiafa,\n  and HUY ANH PHAN. Tensor decompositions for signal processing applications: From two-\n  way to multiway component analysis. IEEE Signal Processing Magazine, 32(2):145–163, March\n\n\n                                       11\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n  2015. ISSN 1053-5888. doi: 10.1109/msp.2013.2297439. URL http://dx.doi.org/10.\n  1109/MSP.2013.2297439.\n\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey.   Sparse\n  Autoencoders Find Highly  Interpretable Features  in Language Models.   ICLR, January\n  2024.  doi: 10.48550/arXiv.2309.08600. URL http://arxiv.org/abs/2309.08600.\n  arXiv:2309.08600 [cs].\n\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\n  convolutional networks, 2017.\n\nJacob  Dunefsky,   Philippe  Chlenski,  and  Neel  Nanda.      Transcoders  enable   fine-\n  grained   interpretable   circuit   analysis   for   language   models,   2024.     URL\n  https://www.alignmentforum.org/posts/YmkjnWtZGLbHRbzrP/\n  transcoders-enable-fine-grained-interpretable-circuit.\n\nRonen Eldan and Yuanzhi Li.  Tinystories: How small can language models be and still speak\n  coherent english?, 2023.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n  Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n  Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n  Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n  Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n  2021. https://transformer-circuits.pub/2021/framework/index.html.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n  networks, 2019. URL https://arxiv.org/abs/1803.03635.\n\nLeo Gao, Tom Dupr´e la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever,\n  Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders, 2024. URL https:\n  //arxiv.org/abs/2406.04093.\n\nXuyang Ge, Fukang Zhu, Wentao Shu, Junxuan Wang, Zhengfu He, and Xipeng Qiu.  Auto-\n  matically identifying local and global circuits with linear computation graphs.  arXiv preprint\n  arXiv:2405.13868, 2024.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\n  MIT Press, 2016.\n\nTom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort,\n  Nicholas Schiefer, and Christopher Olah.  Superposition, memorization, and double descent.\n  Transformer Circuits Thread, 2023.  URL https://transformer-circuits.pub/\n  2023/toy-double-descent/index.html.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\n  Madry. Adversarial examples are not bugs, they are features. Advances in neural information\n  processing systems, 32, 2019.\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and\n  Percy Liang. Concept bottleneck models, 2020. URL https://arxiv.org/abs/2007.\n  04612.\n\nYanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou.  Factorized bilinear models for image\n  recognition. In Proceedings of the IEEE international conference on computer vision, pp. 2079–\n  2087, 2017.\n\nYixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do\n   different neural networks learn the same representations?, 2016. URL https://arxiv.org/\n  abs/1511.07543.\n\nTsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.  Bilinear cnn models for fine-grained\n   visual recognition. In Proceedings of the IEEE international conference on computer vision, pp.\n  1449–1457, 2015.\n\n\n                                       12\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nSamuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\n  Sparse feature circuits: Discovering and editing interpretable causal graphs in language models.\n  arXiv preprint arXiv:2403.19647, 2024.\n\nGr´egoire Montavon, Wojciech Samek, and Klaus-Robert M¨uller. Methods for interpreting and un-\n  derstanding deep neural networks.  Digital Signal Processing, 73:1–15, February 2018. ISSN\n  1051-2004.  doi:  10.1016/j.dsp.2017.10.011. URL http://dx.doi.org/10.1016/j.\n  dsp.2017.10.011.\n\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:\n  10.23915/distill.00007. https://distill.pub/2017/feature-visualization.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n  Zoom In: An Introduction to Circuits. Distill, March 2020. URL https://distill.pub/\n  2020/circuits/zoom-in.\n\nYannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield, Mihalis A. Nicolaou, Anima\n  Anandkumar, and Stefanos Zafeiriou.  Tensor methods in computer vision and deep learning.\n  Proceedings of the IEEE, 109(5):863–890, May 2021. ISSN 1558-2256.  doi: 10.1109/jproc.\n  2021.3074329. URL http://dx.doi.org/10.1109/JPROC.2021.3074329.\n\nGuilherme Penedo, Hynek Kydl´ıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin\n   Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the\n   finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557.\n\nVitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of\n  black-box models, 2018. URL https://arxiv.org/abs/1806.07421.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Explaining\n  the predictions of any classifier, 2016. URL https://arxiv.org/abs/1602.04938.\n\nLee Sharkey. A technical note on bilinear layers for interpretability. 2023.\n\nNoam Shazeer. Glu variants improve transformer, 2020.\n\nNicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E. Papalexakis,\n  and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE\n  Transactions on Signal Processing, 65(13):3551–3582, 2017. doi: 10.1109/TSP.2017.2690524.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\n  Visualising image classification models and saliency maps, 2014. URL https://arxiv.\n  org/abs/1312.6034.\n\nMarius Hobbhahn  Stefan Heimersheim.    solving-the-mechanistic-interpretability-challenges,\n  2023.  URL https://www.alignmentforum.org/posts/sTe78dNJDGywu9Dz6/\n  solving-the-mechanistic-interpretability-challenges-eis-vii.   Ac-\n  cessed: 2024-09-02.\n\nAaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit\n  discovery, 2023. URL https://arxiv.org/abs/2310.10348.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\n  lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\n   Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\n  Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\n  Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\n  Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\n  Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\n  Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\n  Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\n  Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\n  Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\n  Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n  2023.\n\n\n                                       13\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n  Łukasz Kaiser, and Illia Polosukhin.  Attention is All you Need.  NeurIPS, 30, 2017. URL\n  https://arxiv.org/abs/1706.03762.\n\nChelsea Voss, Nick Cammarata, Gabriel Goh, Michael Petrov, Ludwig Schubert, Ben Egan,\n  Swee Kiat Lim, and Chris Olah. Visualizing weights. Distill, 2021. doi: 10.23915/distill.00024.\n  007. https://distill.pub/2020/circuits/visualizing-weights.\n\n\n\n\n\n                                       14\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nA  EIGENSPECTRA: SHOWING EIGENVECTORS ACROSS DIGITS\n\nThe following are plots showing multiple positive and negative eigenvectors for certain digits. Posi-\ntive features either tend to look for specific patterns in the target class (first eigenvector of 2, match-\ning the bottom part) or tend to match an archetypal pattern (second eigenvector of 6, matching the\nwhole digit). Negative eigenvectors tend to look for a specific part that would change the class of\nthe digit. For instance, if the pattern highlighted by the first negative eigenvector of 4 were on, it\nwould most likely be a 9.\n\n\n       0.14\n\n       0.11\n       0.09\n\n\n\n\n       0.00\n                       20\n\n       0.00\n\n\n\n\n      -0.10\n\n      -0.13\n      -0.15\n                       20\n\n                              Figure 10: eigenvectors for digit 2.\n\n\n\n       0.15\n\n       0.12\n\n\n\n\n\n       0.00\n                       20\n\n       0.00\n\n\n\n      -0.12\n      -0.14\n\n      -0.22\n                       20\n\n                              Figure 11: eigenvectors for digit 4.\n\n\n\n       0.15\n       0.13\n       0.10\n\n\n\n\n       0.00\n                       20\n\n       0.00\n\n\n\n\n      -0.12\n      -0.14\n      -0.17\n                       20\n\n                              Figure 12: eigenvectors for digit 6.\n\n\n\n                                       15\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nB  REGULARIZATION & AUGMENTATION: ABLATIONS & OBSERVATIONS\n\nFollowing the observation that regularization improves feature interpretability for image classifiers,\nwe study several popular regularization and augmentation techniques.  In summary, input noise\nsparsifies the features, while geometric transformations blur the features. Some popular techniques,\nsuch as dropout, have little impact on features.\n\n\nB.1  REGULARIZATION\n\nInput noise has the largest impact on features from any of the explored techniques. Specifically,\nwe found dense Gaussian noise (already depicted in Figure 4) to provide the best trade-off between\nfeature interpretability and accuracy. We also considered sparse salt-and-pepper noise (blacking or\nwhiting out pixels), which resulted in both lower accuracy and interpretability. Lastly, we explored\nPerlin noise, which is spatially correlated and produces smooth patches of perturbation. However,\nthis performed worst of all, not fixing the overfitting.\n\nModel noise adds random Gaussian noise to the activations. This had no measurable impact on any\nof our experiments. However, this may simply be because our models are quite shallow.\n\nWeight decay generally acts as a sparsifier for eigenvalues but does not significantly impact the\neigenvectors. This is extremely useful as it can zero out the long tail of unimportant eigenvalues,\nstrongly reducing the labor required to analyze a model fully (more details in Appendix E).\n\nDropout did not seem to impact our models. Overfitting was still an issue, even for very high values\n(> 0.5). We suspect this may change in larger or capacity-constrained models.\n\n\nB.2  AUGMENTATION\n\nTranslation stretches features in all directions, making them smoother. The maximal shift (right)\nis about 7 pixels in each direction, which is generally the maximal amount without losing impor-\ntant information. Interestingly, translation does not avoid overfitting but rather results in smoother\noverfitting patches. High translation results in split features, detecting the same pattern in different\nlocations (Figure 13). This also results in a higher rank.\n\nRotation affects the features in the expected manner. Since rotating the digit zero does not signifi-\ncantly impact features, we consider the digit 5, which has a mix of rotation invariance and variance.\nAgain, it does not stop the model from learning overfitting patches near the edges without noise.\nThese features become broader with increased rotation.\n\nBlur does not significantly affect features beyond making them somewhat smoother. Again, it still\noverfits certain edge pixels in a blurred manner without noise.\n\n\n                    0.14\n                    0.12\n                    0.09\n\n\n\n\n                    0.00\n                                   20\n\n                    0.00\n\n\n\n\n                    -0.09\n                    -0.11\n                    -0.13\n                                   20\n\nFigure 13: Important eigenvectors for a model trained with high translation regularization (7 pixels\non either side). Similar patterns manifest as multiple eigenvectors at different locations.\n\nAll these augmentations are shown separately in Figure 14. Combining augmentations has the ex-\npected effect. For instance, blurring and rotation augmentation yield smooth and curvy features.\n\n\n                                       16\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                                  97.6%             98.0%             97.6%             97.3%             96.8%\n\n                                                  norm\n                                      0.4\n\n\n\n\n\n                                  98.0%             98.1%             97.8%             97.5%             97.0%\n\n                                                          Noise\n\n\n\n\n\n                                  98.1%             98.2%             97.7%             97.3%             96.7%\n\n\n                                                  norm\n\n             0\n\n                                                                          Translation\n                               0 pixels                                                                    7 pixels\n\n\n                                  97.6%             97.9%             98.0%             97.9%             97.5%\n\n                                                  norm\n                                      0.4\n\n\n\n\n\n                                  98.0%             98.2%             98.1%             97.9%             97.5%\n\n                                                          Noise\n\n\n\n\n\n                                  98.1%             98.0%             98.0%             97.8%             97.4%\n\n\n                                                  norm\n\n             0\n\n                                                                        Rotation\n                               0 degrees                                                                40 degrees\n\n\n                                  97.6%             97.6%             97.6%             97.4%             97.0%\n\n                                                  norm\n                                      0.4\n\n\n\n\n\n                                  98.1%             98.1%             98.0%             97.8%             97.6%\n\n                                                          Noise\n\n\n\n\n\n                                  98.1%             98.1%             98.1%             98.0%             97.8%\n\n\n                                                  norm\n\n             0\n\n                                                                            Blur\n                               0 sigma                                                                    1 sigma\n\n\n          Figure 14: Important eigenvectors for models with different hyperparameters.\n\n\n\n\n\n                                       17\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nC  EXPLAINABILITY: A SMALL CASE STUDY WITH EIGENVECTORS\n\n\nWhile this paper focuses on bilinear layers for interpretability, the proposed techniques can also be\nused for post-hoc explainability to understand what has gone wrong. Our explanations are generally\nnot as human-friendly as other methods but are fully grounded in the model’s weights. This section\nexplores explaining two test-set examples, one correctly classified and one incorrectly.\n\nThe figures are divided into three parts. The left line plots indicate the sorted eigenvector activation\nstrengths for the digits with the highest logits. The middle parts visualize the top positive and\nnegative eigenvectors for each digit. The right displays the input under study and the related logits.\n\nThe first example, a somewhat badly drawn five, results in about equal positive activations for the\noutput classes 5, 6, and 8 (which all somewhat match this digit). The negative eigenvectors are most\nimportant in this classification, where class 5 is by far the least suppressed. This is an interesting\nexample of the model correctly classifying through suppression.\n\nThe second example, a seven-y looking two, is actually classified as a 7. From looking at the top\neigenvectors of the digit 2 (shown in Figure 10), we see that the more horizontal top stroke and\nmore vertical slanted stroke activates the top eigenvector for the digit 7 more strongly than the 2-\neigenvectors that look for more curved and slanted strokes. The negative eigenvectors are not very\nimportant in this incorrect classification.\n\n\n\n                                    5                  6                  8                     input\n       3.88\n       3.20\n\n\n\n\n\n       0.00\n                         10\n                                    5                  6                  8                        logits\n       0.00                                                                                                       5\n\n       -0.72                                                                                              6\n                                                                                                            8\n\n\n       -2.09\n       -2.51\n                         10\n\nFigure 15: Study of a correctly classified 5. The output is strongly influenced by negative eigenvec-\ntors, resulting in strong suppression for the other digits.\n\n\n\n\n\n                                    2                  7                  8                     input\n       4.67\n\n\n\n\n       1.81\n       1.25\n\n       0.00\n                         10\n                                    2                  7                  8                        logits\n       0.00\n                                                                                                          7\n                                                                                                 2\n                                                                                                            8\n       -0.43\n       -0.51\n\n\n       -0.87\n                         10\n\nFigure 16: Study of a misclassified 2. The model mostly classifies twos based on the bottom line\nand top curve, which are both only partially present.\n\n\n\n\n                                       18\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nD  HOSVD: FINDING THE MOST IMPORTANT SHARED FEATURES\n\nIn the case that no output features are available or we wish to find the dominant output directions,\nwe can use HOSVD on the B tensor (described in subsection 3.3).  Intuitively, this reveals the\nmost important shared features. We demonstrate this approach on the same MNIST model used in\nsection 4.\n\nInstead of contributing to a single output dimension, each interaction matrix can contribute to an\narbitrary direction, shown at the bottom right (”contributions”). Further, the importance of the con-\ntributions is determined by their singular value, which is shown at the top right. The remainder of\nthe visualization shows the top eigenvalues and the corresponding eigenvectors.\n\nThe most important output direction separates digits with a prominent vertical line (1, 4, and 7)\nfrom digits with a prominent horizontal line (5 specifically). Similarly, the second most important\ndirection splits horizontal from vertical lines but is more localized to the top half. Specifically, it\nsplits by the orientation of the top stroke (whether it starts/ends left or right).\n\n\n        + eigenvalues                    + eigenvectors                                   singular value\n       0.34                                                                                          0.63\n\n\n\n\n\n         0                                                                                   0\n                         20\n               - eigenvalues                                     - eigenvectors                                   contributions\n         0\n                                                                                                1    4\n                                                                                                  2       7\n\n\n                                                                                               0                                                                                                   3    6  8 9\n                                                                                                       5\n       -0.35\n                         20\n\nFigure 17: The most important output direction of an MNIST model roughly splits digits by its\nhorizontality or verticality.\n\n\n\n        + eigenvalues                    + eigenvectors                                   singular value\n       0.39\n                                                                                                      0.56\n\n\n\n\n\n         0                                                                                   0\n                         20\n               - eigenvalues                                     - eigenvectors                                   contributions\n         0\n\n                                                                                                  2 3     7  9\n\n                                                                                               0 1     5    8\n                                                                                                     4\n\n       -0.33                                                                                              6\n                         20\n\nFigure 18: The second most important output direction of an MNIST model splits digits according\nto the orientation of its top stroke.\n\nWe observe that the output directions uncovered through HOSVD somewhat correspond to mean-\ningful concepts, albeit sometimes dominated by a specific digit (such as 5 and 6). Less significant\ndirections often highlight specific portions of digits that seem meaningful but are more challenging\nto describe. In summary, while in the case of MNIST, the results are not particularly more inter-\npretable than decomposing according to digits, we believe this technique increases in utility (but\nalso computational cost) as the number of output classes increases.\n\n\n                                       19\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nE  SPARSITY: WEIGHT DECAY VERSUS INPUT NOISE\n\nThroughout, we make the claims that input noise helps create cleaner eigenvectors and that weight\ndecay results in lower rank; this appendix aims to quantify these claims. To quantify near-sparsity,\nwe use (L1/L2)2, which can be seen as a continuous version of the L0 norm, accounting for near-\nzero values. We analyze both the eigenvalues, indicating the effective rank, and the top eigenvectors,\nindicating the effective pixel count.\n\nAs visually observed in Figure 4, this analysis (left of Figure 19) shows that input noise plays a large\nrole in determining the eigenvector sparsity; weight decay does not. On the other hand, input noise\nincreases the number of important eigenvectors while weight decay decreases it. Intuitively, input\nnoise results in specialized but more eigenvectors, while weight decay lowers the rank.\n\n\n\n                    Eigenvector Sparsity                                                    Eigenvalue Sparsity\n\n\n                                                                                                                                                                180\n    1.0                                                                                                1.0\n\n                                                                           400                                                                                  160\n\n    0.8                                                                     380             0.8                                                                     140\n\n\n                                                                                                                                                                120\n                                                                           360\n Decay 0.6                                                                                                                                                                                  Decay 0.6                                                                     100\n\n\n                                                                                                                                                                80 Weight 0.4                                                                     340                    Weight 0.4\n                                                                           320\n                                                                                                                                                                60\n\n    0.2                                                                     300             0.2                                                                     40\n\n\n                                                                           280                                                                                  20\n    0.0                                                                                                0.0\n\n                                                                                                                                                                0\n           0.0        0.2        0.4        0.6        0.8        1.0                                          0.0        0.2        0.4        0.6        0.8        1.0\n                             Input Noise                                                                           Input Noise\n\nFigure 19: Measuring the approximate L0 norm of eigenvalues (left) and the top 5 eigenvectors\n(right) with varying Gaussian input noise and weight decay.\n\n\nF  TRUNCATION & SIMILARITY: A COMPARISON ACROSS SIZES\n\nThis appendix contains a brief extension of the results presented in Figure 5.\n\n\n\n\n                     Truncation Across Sizes                                             Similarity Across Eigenvectors                                                                                       1\n                                                                                                                                                                                         30\n\n    100%                                                    Model Size           1                                                    Model Size        50                                                              0.8\n                                                                                                                                                   30                                                                       30                                                                                                 0.8                                                                                                                                                   50                                                                       50\n                                                                                                                                                                                        100                                                              0.6                                                                                                                                                   100                                                                       100\n                                                                                                 0.6                                                                                                                                                   300Drop   10%                                                                       300\n                                                                                                                                                   500                                                                                                                                                                                                                                                                                                                                                                similarity                                                                       500\n                                                                                                                                                   1000        300                                                              0.4                                                                       1000                                                                                                 0.4Accuracy   1%                                                                                                                                                                                           Cosine\n                                                                                                 0.2                                                                                    500                                                              0.2\n\n     0.1%0        5        10       15       20       25       30                     00             5             10            15                               1000\n                        Eigenvector rank (per digit)                                                          Eigenvector rank                                                 30      50     100     300     500    1000        0\n\nFigure 20: A) The accuracy drop when truncating the model to a limited number of eigenvectors.\nThe first few eigenvectors result in a similar drop across sizes. Narrow models tend to naturally\nremain more low-rank. In general, few eigenvectors are necessary to recover the full accuracy. B)\nInter-model size similarity of eigenvectors (using 300 as a comparison point). The top features for\nsimilarly-sized models are mostly similar. C) A similarity comparison between all model sizes for\nthe top eigenvector.\n\n\n\n\n\n                                       20\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nG  EXPERIMENTAL SETUPS: A DETAILED DESCRIPTION\n\nThis section contains details about our architectures used and hyperparameters to help reproduce\nresults. More information can be found in our code [currently not referenced for anonymity].\n\n\nG.1  IMAGE CLASSIFICATION SETUP\n\nThe image classification models (section 4) in this paper consist of three parts: an embedding, the\nbilinear layer, and the head/unembedding, as shown in Figure 21. The training hyperparameters\nare found in Table 1. However, since the model is small, these parameters (except input noise;\nAppendix B) do not affect results much.\n\n\n\n\n                                         W\n\n\n                                                    784               512                          512               10\n                                       Input        Embed                             x       Head          Output\n\n\n                                                        V\n\n\n                       Figure 21: The architecture of the MNIST model.\n\n\n                     MNIST Training Parameters\n\n                           input noise norm                0.5\n                          weight decay                    1.0\n                           learning rate                0.001\n                         batch size                  2048\n                           optimizer           AdamW\n                           schedule           cosine annealing\n                         epochs                    20-100\n\n        Table 1: Training setup for the MNIST models, unless otherwise stated in the text.\n\n\nG.2  LANGUAGE MODEL SETUP\n\nThe language model used in section 5 is a 6-layer modern transformer model (Touvron et al., 2023)\nwhere the SwiGLU is replaced with a bilinear MLP (Figure 22). The model has about 33 million\nparameters. The training setup is detailed in Table 3. As the training dataset, we use a simplified\nand cleaned version of TinyStories (Eldan & Li, 2023) that remedies the following issues.\n\n       Contamination: About 20% of stories are exact duplicates (in both train and test).\n\n        Corruption: Some stories contain strange symbol sequences akin to data corruption.\n\nFurthermore, we use a custom interpretability-first BPE tokenizer. The tokenizer is lower-case only,\nsplits on whitespace, and has a vocabulary of only 4096 tokens.\n\n\n\n\n                                                                                                  2048\n                                           W\n                                               512                      512                                     512\n                                 Norm    attn       Norm             x    P\n\n                                                                   V\n                                                                                                  2048\n\n                                                             +                                  +                                                                                                Embed                                                                                                                                                                                                                                                                                                                                                                                                                                   Unembed\n\n\nFigure 22: Simplified depiction of a bilinear transformer model. A model dimension of 512 is used,\nand an expansion factor of 4 (resulting in 2048 hidden dimensions in the MLP).\n\n\n                                       21\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n                            TinyStories Training Parameters\n\n                           weight decay                 0.1\n                           batch size                 512\n                             context length             256\n                             learning rate              0.001\n                            optimizer          AdamW\n                            schedule               linear decay\n                          epochs                     5\n                            tokens            ± 2B\n                                 initialisation                gpt2\n\n      Table 2: Tinystories training setup. Omitted parameters are the HuggingFace defaults.\n\n\n\nThe models used in the experiments shown in Figure 9 are trained of the FineWeb dataset (Penedo\net al., 2024). These follow the architecture of GPT2-small (12 layers) and GPT2-medium (16 layers)\nbut have bilinear MLPs.  Their parameter count is 162M and 335M, respectively. Both use the\nMixtral tokenizer.\n\n\n                        FineWeb Training Parameters\n\n                            weight decay               0.1\n                            batch size               512\n                              context length           512\n                              learning rate             6e-4\n                             optimizer        AdamW\n                             schedule            linear decay\n                             tokens          ± 32B\n                                   initialisation             gpt2\n\n       Table 3: FineWeb training setup. Omitted parameters are the HuggingFace defaults.\n\n\n\nG.3  SPARSE AUTOENCODER SETUP\n\nAll discussed SAEs use a TopK activation function, as described in Gao et al. (2024). We found\nk = 30 to strike a good balance between sparseness and reconstruction loss. section 5 studies quite\nnarrow dictionaries (4x expansion) for simplicity. The exact hyperparameters are shown in Table 4,\nand the attained loss added (Equation 4) across layers is shown in Figure 23.\n\n\n                                          Lpatch −Lclean\n                               Ladded =                                                    (4)\n                                                Lclean\n\n\n\n\n\n                                                               0.3                                                               resid_mid\n\n                                                           0.25\n                                                                                                            Added  0.150.2\n                                                                                       Loss    0.1                                                           mlp_out\n\n                                                           0.05\n\n                                                       0\n                                                            1          2          3          4          5          6\n\n                                                                                        Layer\n\n\n            Figure 23: Loss added for the mlp out and resid mid SAEs across layers.\n\n\n                                       22\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n                      SAE Training Parameters\n\n                         expansion                     4x\n                      k                             30\n                         batch size                   4096\n                           learning rate                   1e-4\n                          optimizer            AdamW\n                          schedule             cosine annealing\n                          tokens            ± 150M\n                           buffer size           ± 2M\n                         normalize decoder             True\n                              tied encoder init               True\n                         encoder bias                    False\n\n                           Table 4: SAE training hyperparameters.\n\n\nH  CORRELATION: ANALYZING THE IMPACT OF TRAINING TIME\n\nFigure 9 shows how a few eigenvectors capture the essence of SAE features. This section discusses\nthe impact of SAE quality, measured through training steps, on the resulting correlation. In short,\nwe find features of SAEs that are trained longer are better approximated with few eigenvectors.\n\nWe train 5 SAEs with an expansion factor of 16 on the output of the MLP at layer 12 of the ‘fw-\nmedium’ model. Each is trained twice as long as the last. The feature approximation correlations\nare computed over 100K activations; features with less than 10 activations (of which there are less\nthan 1000) are considered dead and not shown. The reconstruction error and loss recovered between\nSAEs differ only by 10% while the correlation mean changes drastically (Table 5). The correlation\ndistribution is strongly bimodal for the ‘under-trained’ SAEs (shown in Figure 24 with darker col-\nors). Given more training time, this distribution shifts towards higher correlations. The activation\nfrequencies of features are mostly uncorrelated with their approximations.\n\n                Training steps (relative)    1     2     4     8    16\n               Normalized MSE         0.17   0.16   0.16   0.15   0.15\n               Loss recovered           0.60   0.61   0.65   0.65   0.66\n                Correlation mean         0.17   0.28   0.42   0.52   0.59\n\nTable 5: The SAE metrics along with the mean of the correlations shown in Figure 24. The correla-\ntion improves strongly with longer training, while other metrics only change marginally.\n\n\n\n\n                             800\n\n\n                             700\n\n\n                             600\n\n\n                             500\n                                                           Count 400\n\n\n                             300\n\n\n                             200\n\n\n                             100\n\n\n                               0\n                                    −0.5       −0.25        0          0.25         0.5         0.75         1\n                                                              Correlation (active only)\n\nFigure 24: Feature approximation correlations using two eigenvectors across SAEs with different\ntraining times. Darker is shorter, and bright is longer. This shows a clear bimodal distribution for\n‘under-trained’ SAEs, which vanishes upon longer training, indicating some form of convergence.\n\n\n\n                                       23\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nI  BILINEAR TRANSFORMERS: A LOSS COMPARISON\n\nWhile experiments on large models show bilinear layers to only marginally lag behind SwiGLU\n(Shazeer, 2020), this section quantifies this accuracy trade-off through compute efficiency. We per-\nformed our experiments using a 6-layer transformer model trained on TinyStories. For these exper-\niments, we use d model = 512 and d hidden = 2048, resulting in roughly 30 million parameters.\nHowever, we have found these results to hold across all sizes we tried.\n\n                                        Bilinear  ReGLU  SwiGLU\n                     constant epochs    1.337     1.332      1.321\n                      constant time     1.337     1.337      1.336\n\nTable 6: The loss of language models with varying MLP activation functions. Bilinear layers are\n6% less data efficient but equally compute efficient.\n\n\nConsidering the data efficiency (constant epochs), both SwiGLU and ReGLU marginally beat the\nbilinear variant. Concretely, SwiGLU attains the same final loss of the bilinear variant in 6% less\nepochs. On the other hand, when considering compute efficiency (constant time), we see that these\ndifferences vanish 1. Consequently, if data is abundant, there is little disadvantage to using bilinear\nlayers over other variants.\n\n\nJ  FINETUNING: YOUR TRANSFORMER IS SECRETLY BILINEAR\n\nMany state-of-the-art open-source models use a gated MLP called SwiGLU (Touvron et al., 2023).\nThis uses the following activation function Swishβ(x) = x · sigmoid(βx). We can vary the β\nparameter to represent common activation functions. If β = 1, that corresponds to SiLU activation,\nused by many current state-of-the-art models. β = 1.7 approximates a GELU and β = 0 is simply\nlinear, corresponding to our setup. Consequently, we can fine-tune away the gate by interpolating β\nfrom its original value to zero. This gradually converts an ordinary MLP into its bilinear variant.\n\nTo demonstrate how this approach performs, we fine-tuned TinyLlama-1.1B, a 1.1 billion-parameter\ntransformer model pretrained on 3 trillion tokens of data, using a single A40 GPU. For simplicity,\nwe trained on a slice of FineWeb data. Due to computing constraints, we only tried a single schedule\nthat linearly interpolates towards β = 0 during the first 30% (120M tokens) and then fine-tunes for\nthe remaining 70% (280M tokens). We compare this to a baseline that does not vary β during fine-\ntuning, corresponding to continued training. We use this baseline to compensate for the difference\nin the pretraining distribution of TinyLlama (consisting of a mixture of RedPajama and StarCoder\ndata). This shows that this fine-tuning process increases the loss by about (0.05) but seems to benefit\nfrom continued training (Figure 25). We plan to extend this result with a more thorough search\nfor an improved schedule, which will probably result in a lower final loss. We also expect longer\ntraining runs to close to gap even further.\n\n\n\n                                                                                                         finetune       baseline\n                                                       2.35\n\n\n\n\n\n                                                          2.3\n\n\n                                                                                  Loss 2.25\n                                                                                                                                  2.217\n\n\n\n                                                          2.2\n\n\n                                                                                                                                  2.168\n\n                                                       2.150              20k             40k             60k             80k            100k\n                                                                                     Step\n\nFigure 25: Comparison of fine-tuning versus a baseline over the course of 400M tokens. The loss\ndifference is noticeable but decreases quickly with continued training.\n\n\n   1An improvement in implementation efficiency, such as fusing kernels, may change this fact.\n\n\n                                       24\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nK  TENSOR DECOMPOSITIONS: EXTRACTING SHARED FEATURES\n\nGiven a complete or over-complete set of m u-vectors, we can re-express B in terms of the eigen-\nvectors, which amounts to a change of basis. To avoid multiple contributions from similar u-vectors,\nwe have to use the pseudo-inverse, which generalizes the inverse for non-square matrices. Taking\nthe u-vectors as the columns of U, the pseudo-inverse U + satisfies UU + = I, as long as U is full\nrank (equal to d). Then\n\n                    m\n                B = X u+:k ⊗Qk                                                  (5)\n                                 k\n\n                    m   d\n                = X X λ{k,i} u+:k ⊗v{k,i} ⊗v{k,i}                           (6)\n                                 k    i\nwhere u+:k are the rows of U + and Qk = Pi λ{k,i}v{k,i} ⊗v{k,i} is the eigendecomposition of\nthe interaction matrix corresponding for uk :. We can then recover the interaction matrices from\nQk = uk: ·out B using the fact that uk: · u+:k′ = δkk′ (Kronecker delta). Note that the eigenvectors\nwithin a single output direction k are orthogonal but will overlap when comparing across different\noutput directions.\n\nL  BILINEAR LAYERS: A PRACTICAL GUIDE\n\nBilinear layers are inherently quadratic; they can only model the importance of pairs of features,\nnot single features. Interestingly, we haven’t found this to be an issue in real-world tasks. However,\nlinear structure is important for some toy tasks and, therefore, merits some reflection.  Without\nmodification, bilinear layers will model this linear relation as a quadratic function. To resolve this,\nwe can add biases to the layer as follows: BL(x) = (W x + b) ⊙(V x + c). In contrast to ordinary\nlayers, where the bias acts as a constant value, here it acts as both a linear and a constant value. This\nbecomes apparent when expanded:\n\n\n                   BL(x) = W x ⊙V x + (cW x + bV x) + cb\n\nWe disambiguate by calling the terms ‘interaction’, ‘linear’, and ‘constant’. Theoretically, this is\nvery expressive; all binary gates and most mathematical operations can be approximated quite well\nwith it. In practice, the training process often fails to leverage this flexibility and degenerates to\nusing quadratic invariances instead of learned constants.\n\nM  ADVERSARIAL MASKS: ADDITIONAL FIGURE\n\n\n A)                         B)                         C)\n\n\n\n\n\nFigure 26: More examples of adversarial masks constructed from specific eigenvectors for models\nwith A) no regularization, B) Gaussian noise regularization with std 0.15, and C) Gaussian noise\nregularization with std 0.3.\n\n\nN  LANGUAGE: FURTHER DETAILS FOR FEATURE CIRCUITS\n\n\n\n\n                                       25\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n    A)                                   B)\n\n\n\n\n\nFigure 27: A) Histogram of self-interactions and cross-interactions. B) The eigenvalue spectrum\nfor the sentiment negation feature discussed in section 5. The dashed red line gives spectrum for a\nrandom symmetric matrix with Gaussian distributed entries with the same standard deviation.\n\n\nO  INPUT FEATURES OF THE NEGATION FEATURE\n\n\n  Output Feature (1882): not good\n\n     lily wanted to climb the ladder , but  her mommy  told  her to  stay away from   it  because it was  not   safe     .  lily listened to her\n\n\n      .  they   looked  everywhere   .   the  train was  lost     .   the  train stayed in the hole , and it was  not   happy     .   the  end   .  [EOS]\n\n\n     the man was  angry   . he  grabbed   sue   , who  screamed  and  cried   . sue was very  troubled   . she  never   got   to   watch   the   play   and\n\n\n      ,  and  lily still couldn  ’    t  find her beloved  jacket     .   she  felt so  sad   that  she  stopped   playing   with   her   friends  and  stopped\n\n\n    big dog     ,   and max said , ” i  didn    ’    t  like  the   big  dog     .  he wasn    ’    t  honest  like  me   . ” lily\n\n\n     sick   . they  called  the doctor , who said he was very i   ll   .  sadly   ,  the  small boy  never  recove  red  and he passed\n\n  Output Feature (1179): not bad\n\n  me to the park , car ! ”  the  car  didn    ’    t   resp ond , but  lily  didn  ’    t  mind   . she was just happy\n\n\n   was a  bee  ! susie was afraid at first   ,  but mommy   explained  that  the   bee was friendly  and   would   not   hurt   her     .  susie was curious\n\n\n   was proud of himself . he was  scared   to  collect the lemons , but he didn  ’    t   give  up   . he  collected    all\n\n\n   became friends . they went on many adventures together . the bear was  still  clumsy   ,  but he  didn    ’    t  mind   . he was happy to\n\n\n   made many new friends and had  lots  of  fun   .  the  little frog was always very busy , but he  never  forgot  the  cow  ’ s  advice\n\n\n     ” tom says . he gives him a hug . the snowman is cold  and  wet , but tom  does   not  mind   . he  likes  the snowman\n\n           Table 7: Output SAE features in the negation feature discussed in section 5.\n\n\n\n\n\n                                       26\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n Input Feature (326): crashing and breaking\n\n  but   ,   as   the   lady  was  carrying the birdie across the  road     ,   a   car  didn ’   t   see   them   and    hit  them   both     .   the   birdie\n\n\n  takes an egg and taps it on the edge of the bowl   .   but   he   tap   s   too   hard   and   the   egg   breaks   in   his   hand\n\n\n   and   faster     .   he   feels   dizzy   from   the   speed     .   he   does   not   see   the   rock   in   his  way     .   he    hit   s   the   rock   and   his\n\n Input Feature (1376): dangerous actions\n\n  ” no , sweetie   ,   you   can    ’    t   touch   the   fish     .   they   are   not   for   touch   ing     .   they   might   bite  you   or   get   scared\n\n\n  with them ! ” tom was worried . he said ” lila   ,  we   should  not touch  the   balloons     .   they   are   not   our   s     .  maybe\n\n\n  was worried and said , ” no , sam   .  you   must n  ’   t go   in    !    it   would   be   too   dangerous     .  ” but sam\n\n Input Feature (1636): nervous / worried\n\n  big adventure . he decided to take a trip to the tip . joe was so excited ,  but   he  was   a    little   worried   too     .   he\n\n\n  two people doing yoga . joe was fascinated and he clapped with excitement .  but   the   people   were n  ’    t   happy     .   one\n\n\n   forest and soon he spotted a lovely tree with lots of delicious fruit . he was excited ,  but   he  was   also   a    bit   nervous     .   he\n\n Input Feature (123): negative attribute\n\n  [BOS] once upon a time there was a poor rug   .    it  was   old   and   fa   de   d     ,   but    it\n\n\n  [BOS] once upon a time , there was a beach . the beach was very  filthy     .    it   had   a    lot   of   trash  on    it\n\n\n   to say hello . she ran to the door and knocked on it , but it was  filthy   and   covered   in    dirt     .  the new kid opened\n\n       Table 8: SAE features that contribute to the negation feature discussed in section 5.\n\n\n\n\n\n                                       27\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n Input Feature (990): a bad turn\n\n      .  she went up to him and asked him why  he  was   crying   . the little  boy   said     ,   ”    i   lost  my    li   p   bal  m\n\n\n  a little boy  crying     .  she went to him and asked why  he  was   sad   . the little  boy   said   he   lost   his   toy   and  didn  ’\n\n\n   h     .  spot asked ,  ” why are you  sad , little  bird   ?  ” the  bird   replied     ,   ”    i   lost  my   favorite   toy   in   the\n\n Input Feature (1929): inability to do something / failure\n\n     ’   t listen to him   .  max   tried   again   and   again     ,   but   the   water   wouldn    ’   t answer   .   he   said   please   and   asked   nicely     ,\n\n\n  how   .   he   tried  moving    it     ,  pushing   it     ,   pull   ing    it     .   nothing   seemed   to  work     .   he  was   getting   frustrated     .   he   asked   his\n\n\n  couldn ’ t reach   it   on  the counter   .   she   tried   and   tried     ,   but    it  was   too   high     .    lily    ’   s  mom  came   into   the\n\n Input Feature (491): body parts\n\n  a bucket  and   a   shovel  and start to fill the bucket with soil   .  they press  the  soil hard  with   their   hands     .  they turn the bucket\n\n\n  day , he hopped up to a magazine that was lying on the ground . he poked it with  his   furry   foot  and it felt hard\n\n\n  spider on the ceiling . she wanted to catch it , so she reached up  to  pinch it  with   her   fingers   . but the spider was\n\n Input Feature (947): bad ending (seriously)\n\n    the   doctor     ,  who   said   he  was   very    i    ll     .   sadly     ,   the   small   boy   never   rec ove  red   and   he   passed  away     .  [EOS]  [EOS]\n\n\n     ’    t  make    it   out   of   the   water     .   her   family  was   very   sad   and   missed   her   very  much     .   they   remembered  how   jolly   and   happy   she\n\n\n   was   very   sad     .   his   friends   could   not   help  him     .   the   hippo   stayed   stuck   with   the   disgusting   wrap   forever     .  [EOS]\n\n Input Feature (882): being positive\n\n    , there was a musician who wandered through the countryside . he was quite weak ,  but   he  was   determined   to  make    it  to the\n\n\n   finger ! she yelped and quickly pulled away . the little girl was a bit scared  but   she  was   also   brave     .   she   did   not\n\n\n  was tall and had long , bony fingers . the little boy was a bit scared ,  but   he  was   also   very   curious     .   he   went\n\n Input Feature (240): negation of attribute\n\n  ran to it and asked the others what it was . he was so excited to know that it wasn   t   just   a  strange big box it\n\n\n  explore a big tunnel and see what he might find inside . joe  s mum told him that it wasn   t   a  safe tunnel and he should\n\n\n  go swimming in the ocean . he asked his mom to take him . his mom explained that it was  not  safe to swim alone and she\n\n Input Feature (766): inability to perform physical actions\n\n  but one day , he got sick and started to suffer .  his  body  hurt  and he couldn ’   t   play   with  his friends  anymore   .\n\n\n  [BOS] one day , a pretty bird  hurt    its   wing   . it  could   not    fly   . a kind girl named\n\n\n   to play with him too . one day , the puppy fell down and  hurt   his  leg . he could  not   play  and run like before .\n\n Input Feature (1604): avoiding bad things\n\n    it in a more careful way . so he started playing more carefully , making sure that he wouldn ’   t   get  too dirty . he kept\n\n\n   shelf . she smiled and said yes . jimmy was careful to pull it off the shelf so it would  not  break . he proudly showed it\n\n\n  jane noticed the ice was very icy . mary suggested they wear gloves , so that the cold would  not  hurt their hands . jane agreed\n\n Input Feature (1395): positive ending\n\n    . the old man watched the pony with a smile on his face . the pony was happy and no   longer   scared   . she had found a\n\n\n  and jane seeing a deer in the woods and choosing to  stay  and appreciate it ,  instead   of   trying   to   catch    it   . [EOS]\n\n\n    . sam was happy , and so was the ball . now   , sam and the ball could play together  without  mud   . they rolled and bounced\n\n  Table 9: SAE features that contribute to the negation feature discussed in section 5 (continued).\n\n\n\n\n\n                                       28\n\n## Claim to Classify\n\"A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions\"\nLocation in paper: body\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Weight-based analysis of bilinear MLP weights via interaction matrices + eigendecomposition, using SAE-derived input/output features; supported by observational statistics (top interactions, cosine-similarity projections, activation correlation with low-rank approximation). No direct causal intervention/ablation/patching reported for this specific circuit claim.",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The evidence described for the ‘sentiment negation circuit’ is based on analyzing weights (interaction submatrix), projecting SAE features onto eigenvectors, and reporting correlations between a truncated eigenvector approximation and feature activations—these are observational/associational analyses without interventions on the model’s internals, so method_rung=1. The claim text uses mechanistic/functional language (‘circuit’, ‘computes’, ‘via AND-gate-like interactions’) attributing a specific computation to an internal structure, which is a rung-3 mechanistic claim rather than a mere correlation/decodability statement. The sentence is not explicitly hedged (no “may/suggests”), so hedge_flag=0."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 26198,
      "completion_tokens": 243,
      "total_tokens": 26441
    },
    "finish_reason": "stop"
  }
}