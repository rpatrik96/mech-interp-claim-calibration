{
  "timestamp": "2026-02-19T09:02:58.784074+00:00",
  "paper_id": "2402.17700",
  "claim_id": "2402.17700-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2402.17700\nTitle: RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\nFull text:\n                 RAVEL: Evaluating Interpretability Methods on\n                      Disentangling Language Model Representations\n\n                    Jing Huang            Zhengxuan Wu           Christopher Potts\n                  Stanford University           Stanford University           Stanford University\n               hij@stanford.edu      wuzhengx@stanford.edu    cgpotts@stanford.edu\n\n                     Mor Geva                          Atticus Geiger\n                            Tel Aviv University                     Pr(Ai)2R Group\n                    morgeva@tauex.tau.ac.il             atticusg@gmail.com\n\n                          Abstract                                             Inputs        Task: Localize the    Expected\n                                                                                                                 Continent Attribute    Outputs\n\n                                                                                                         is in                                                                                                               the                   Individual neurons participate in the represen-             Cause Paris                                                                                                                                           Asia                                                                                                    continent                                                                                                              of\n                     tation of multiple high-level concepts. To what                           ❄LM\n                   extent can different interpretability methods                Isolate Paris is in the                        France                                                                                                     country of\n                   successfully disentangle these roles? To help                                                                        Continent Subspace\n                                                                                                                                                                                                Tokyo2024            address this question, we introduce RAVEL              Interchange Intervention                                Paris\n                                                                                              with Bijective Featurizer F\n                 (Resolving Attribute–Value Entanglements in                                                                          Other Attributes\n               Language Models), a dataset that enables\n                                                                                                     Tokyo is a         ❄LMAug             tightly controlled, quantitative comparisons                     large city.\n                between a variety of existing interpretabil-\n                      ity methods. We use the resulting concep-26\n                    tual framework to define the new method        Figure 1: An overview of the RAVEL benchmark, which\n                  of Multi-task Distributed Alignment Search        evaluates how well an interpretability method can find\n              (MDAS), which allows us to find distributed        features that isolate the causal effect of individual at-\n                   representations satisfying multiple causal cri-         tributes of an entity.\n                      teria. With Llama2-7B as the target language[cs.CL]           model, MDAS achieves state-of-the-art results        To facilitate these method comparisons, we in-\n               on RAVEL, demonstrating the importance of       troduce a diagnostic benchmark, RAVEL (Resolv-\n                 going beyond neuron-level analyses to identify                                                              ing Attribute–Value Entanglements in Language\n                   features distributed across activations. We re-\n                                                           Models). RAVEL evaluates interpretability meth-\n                    lease our benchmark at https://github.com/\n                                                          ods on their ability to localize and disentangle the                explanare/ravel.\n                                                                        attributes of different types of entities encoded as\n          1  Introduction                                     text inputs to language models (LMs). For exam-\n                                                                        ple, the entity type “city” has instances such as       A central goal of interpretability is to localize an\n                                                                 “Paris” or “Tokyo”, which each have attributes for              abstract concept to a component of a deep learning\n                                                                 “continent”, namely “Europe” and “Asia”. An in-           model that is used during inference. However, this\n                                                                          terpretability method must localize this attribute to                is not as simple as identifying a neuron for each\n                                                             a group of neurons N, learn a featurizer F (e.g., a             concept, because neurons are polysemantic – they\n                                                                       rotation matrix or sparse autoencoder), and identify              represent multiple high-level concepts (Smolensky,arXiv:2402.17700v2                                                       a feature F (e.g., a linear subspace of the residual            1988; Rumelhart et al., 1986; McClelland et al.,\n                                                            stream in a Transformer) for the attribute. RAVEL            1986; Olah et al., 2020; Cammarata et al., 2020;\n                                                                  contains five types of entities (cities, people names,            Bolukbasi et al., 2021; Gurnee et al., 2023).\n                                                                     verbs, physical objects, and occupations), each with               Several recent interpretability works (Bricken\n                                                                         at least 500 instances, at least 4 attributes, and at                et al., 2023; Cunningham et al., 2024; Geiger et al.,\n                                                                          least 50 prompt templates per entity type.            2023b; Wu et al., 2023) tackle this problem us-\n             ing a featurizer that disentangles the activations of     The metric we use to assess interpretability meth-\n            polysemantic neurons by mapping to a space of   ods is based on interchange interventions (also\n           monosemantic features that each represent a dis-   known as activation patching). This operation has\n               tinct concept.  Intuitively, these methods should   emerged as a workhorse in interpretability, with\n            have a significant advantage over approaches that   a wide swath of research applying the technique\n              identify concepts with sets of neurons. However,    to test if a high-level concept is stored in a model\n             these methods have not been benchmarked.          representation and used during inference (Geiger\n\n\n                                                    1\n\net al., 2020; Vig et al., 2020; Geiger et al., 2021;         Entity         Attributes            # Entities    # Prompt\n                                                                 Type                                           Templates\nLi et al., 2021; Finlayson et al., 2021; Meng et al.,\n                                                                           City          Country, Language,        3552         150\n2022; Chan et al., 2022; Geva et al., 2023; Wang                        Latitude, Longitude,\net al., 2023; Hanna et al., 2023; Conmy et al., 2023;                    Timezone, Continent\nGoldowsky-Dill et al., 2023; Hase et al., 2023;       Nobel       Award Year, Birth Year,      928         100\n                                                                          Laureate     Country of Birth, Field,\nTodd et al., 2024; Feng and Steinhardt, 2024; Cun-                   Gender\nningham et al., 2024; Huang et al., 2023; Tigges        Verb          Definition, Past Tense,       986          60\net al., 2023; Lieberum et al., 2023; Davies et al.,                      Pronunciation, Singular\n2023; Hendel et al., 2023; Ghandeharioun et al.,        PhysicalObject        BiologicalColor, Size,Category,Texture         563          60\n2024).                                                               Occupation   Duty, Gender Bias,          799          50\n   Specifically, we use the LM to process a prompt                        Industry, Work Location\nlike “Paris is in the continent of” and then\n                                                        Table 1: Types of entities and attributes in RAVEL.\nintervene on the neurons N to fix the feature F to\nbe the value it would have if the LM were given a    2. Causal: Interpretability methods should ana-\nprompt like “Tokyo is a large city.” If this       lyze the causal effects of model components on\nleads the LM to output “Asia” instead of “Europe”,     model input–output behaviors.\nthen we have evidence that the feature F encodes                                                          3. Generalizable: The causal effects of the iden-\nthe attribute “continent”. Then, we perform the                                                                  tified components should generalize to similar\nsame intervention when the LM processes a prompt                                                        inputs that the underlying model makes correct\nlike “People in Paris speak”. If the LM outputs                                                         predictions for.\n“French” rather than “Japanese’, then we have\n                                                          4. Isolating individual concepts: Interpretability\nevidence that the feature F has disentangled the\n                                                methods should isolate causal effects of individ-\nattributes “continent” and “language”.\n                                                        ual concepts involved in model behaviors.\n  A variety of existing interpretability methods are\neasily cast in the terms needed for RAVEL evalu-   The goal of RAVEL is to assess the ability of meth-\nations, including supervised probes (Peters et al.,   ods to isolate individual explanatory factors in\n2018; Hupkes et al., 2018; Tenney et al., 2019;   model representations (desideratum 4), and do so\nClark et al., 2019), Principal Component Analysis    in a way that is faithful to how the target models\n(Tigges et al., 2023; Marks and Tegmark, 2023),   work (desideratum 1). The dataset train/test struc-\nDifferential Binary Masking (DBM: Cao et al.    ture seeks to ensure that methods are evaluated by\n2020; Csordás et al. 2021; Cao et al. 2022; Davies   how well their explanations generalize to new cases\net al. 2023), sparse autoencoders (Bricken et al.,   (desideratum 3), and RAVEL is designed to support\n2023; Cunningham et al., 2024), and Distributed    intervention-based metrics that assess the extent\nAlignment Search (DAS: Geiger et al. 2023b; Wu    to which methods have found representations that\net al. 2023). Our apples-to-apples comparisons re-   causally affect the model behavior (desideratum 2).\nveal conceptual similarities between the methods.     RAVEL  is carefully curated as a diagnostic\n   In addition, we propose multi-task training ob-   dataset for the attribute disentanglement problem.\njectives for DBM and DAS. These objectives allow   RAVEL has five types of entity, where every in-\nus to find representations satisfying multiple causal    stance has every attribute associated with its type.\ncriteria, and we show that Multi-task DAS is the    Table 1 provides an overview of RAVEL’s structure.\nmost effective of all the methods we evaluate at\n                                        The Attribute Disentanglement Task  We beginidentifying disentangled features. This contributes\nto the growing body of evidence that interpretabil-   with a set of entities E = {E1, . . . , En}, each with\nity methods need to identify features that are dis-    attributes A = {A1, . . . , Ak}, where the correct\ntributed across neurons.                             value of A for E is given by AE. Our interpretabil-\n                                                             ity task asks whether we can find a feature F that\n2  The RAVEL Dataset                       encodes the attribute A separately from the other\n                                                            attributes A \\ {A}. For Transformer-based models\nThe design of RAVEL is motivated by four high-                                               (Vaswani et al., 2017), a feature might be a dimen-\nlevel desiderata for interpretability methods:                                                   sion in a hidden representation of an MLP or a\n1. Faithful: Interpretability methods should accu-    linear subspace of the residual stream.\n    rately represent the model to be explained.       We do not know a priori the degree to which it is\n\n\n                                         2\n\npossible to disentangle a model’s representations.   across unseen entities and contexts. Each setting\nHowever, our benchmark evaluates interpretability    has a predefined train/dev/test structure. In Entity,\nmethods according to the desiderata given above    for each entity type, we randomly split the entities\nand so methods will need to be faithful to the    into 50%/25%/25% for train/dev/test, but use the\nmodel’s underlying structure to succeed. In other   same set of prompt templates across the three splits.\nwords, assuming methods are faithful, we can favor    In Context, for each attribute, we randomly split\nmethods that achieve more disentanglement.         the prompt templates into 50%/25%/25%, but use\n                                                     the same set of entities across the three splits.\n2.1  Data Generation\n                                                     Filtering for a Specific Model  When evaluatingSelecting Entity Types and Attributes  We first\n                                                         interpretability methods that analyze a model M,identify entity types from existing datasets that\n                                    we generally focus on a subset of the instancespotentially have thousands of instances (see Ap-\n                                           where M correctly predicts the values of the at-pendix A.1), such as cities or famous people. More-\n                                                         tributes (see Appendix A.2).  This allows us toover, each entity type has multiple attributes with\n                                                 focus on understanding why models succeed, anddifferent degrees and types of associations. For\n                                                                             it means that we don’t have to worry about howexample, for attributes related to city, “country” en-\n                                            methods might have different biases for incorrecttails “continent”, but not the reverse; “country” is\n                                                       predictions.predictable from “timezone” but non-entailed; and\n“latitude” and “longitude” are the least correlated\n                                                    2.2  Interpretability Evaluation\ncompared with the previous two pairs, but have\nidentical output spaces. These entity types together   Interchange Interventions A central goal of\ncover a diverse set of attributes such that predicting   RAVEL is to assess methods by the extent to which\nthe value of the attribute uses factual, linguistic, or    they provide causal explanations of model behav-\ncommonsense knowledge.                             iors (desideratum 2). To build such analyses, we\n                                             need to put models into counterfactual states that\nConstructing Prompts We consider two types                                                 allow us to isolate the causal effects of interest.\nof prompts: attribute prompts and entity prompts.\n                                            The fundamental operation for achieving this is\nAttribute prompts PAE  contain mentions of E                                                    the intervention (Spirtes et al., 2000; Pearl, 2001,\nand instruct the model to output the attribute\n                                                 2009): we change the value of a model-internal\nvalue AE.   For example, E = Paris  is an                                                          state and study the effects this has on the model’s\ninstance of the type “city”, which has an  at-\n                                                    input–output behavior. In more detail: let M(x) be\ntribute A = Continent that can be queried with\n                                                      the entire state of the model when M receives input\nprompts “Paris  is  in  the  continent  of”.\n                                                     x, i.e., the set of all input, hidden, and output rep-\nPrompts can also be JSON-format, e.g., “{\"city\":\n                                                       resentations created during inference. Let MN←n\n\"Paris\", \"continent\":\"”, which reflects how\n                                               be the model where neurons N are intervened upon\nentity–attribute association might be encoded in\n                                             and fixed to take on the value n ∈Values(N).\ntraining data. For each format, we do zero- and few-\n                                                   Geiger et al. (2023b) generalize this operation to\nshot prompting. In addition to attribute prompts,\n                                                    intervene upon features that are distributed across\nentity prompts WE contain mentions of the E, but\n                                                 neurons using a bijective featurizer F. Let MF←f\ndoes not query any A ∈A. For example, “Tokyo\n                                               be the model where neurons N are projected into a\nis a large city”. We sample entity prompts\n                                                        feature space using F, the feature F is fixed to take\nfrom the Wikipedia corpus.1\n                                            on value f, and then the features are projected back\n  For a set of entities E and a set of attributes to                                                       into the space of neural activations using F−1. If\ndisentangle A, the full set of prompts is\n                                      we let τ(M(x)) be the token that a model predicts\n                                                       for a given prompt x ∈D, then comparisons be-  D = {x : x ∈PAE ∪WE, E ∈E, A ∈A}\n                                             tween τ(M(x)) and τ(MF←f(x)) yield insights\nGenerating Splits  RAVEL offers two settings,    into the causal role that F plays in model behavior.\nEntity and Context, to evaluate the generalizabil-     However, most conceivable interventions fix\nity (desideratum 3) of an interpretability method   model representations to be values that are never re-\n                                                       alized by any input. To characterize the high-level\n   1We  use  the  20220301.en  version  pre-processed\n                                                   conceptual role of a model representation, we needby HuggingFace at https://huggingface.co/datasets/\nwikipedia                                         a data-driven intervention that sets a representation\n\n\n                                         3\n\nto values it could actually take on. This is achieved     To balance these two objectives, we define the\nby the interchange intervention, which fixes a fea-   Disentangle score as a weighted average between\nture F to the value it would take if a different input   Cause and Iso.\nx′ were provided:\n                        def                            Disentangle(A, F, M, D) =\n  II(M, F, x, x′) =\n                                              1                                                  h                                   i                                                 Cause(A, F, M, D) + Iso(A, F, M, D)\n            τ  MF←GetFeature(M(x′),F)(x)    (1)     2\nwhere GetFeature(M(x′), F) is the value of F in     The score on RAVEL for an entity type is its\nM(x′). Interchange interventions represent a very   average Disentangle score over all attributes.\ngeneral technique for identifying abstract causal                                                       In practice, two attributes might not be fully dis-\nprocesses that occur in complex black-box systems    entanglable in the model M so there is no guar-\n(Beckers and Halpern, 2019; Beckers et al., 2020;                                                  antee that it is possible to find a feature F that\nGeiger et al., 2023a).                                                   achieves Cause = 1 and Iso = 1 at the same time.\n                                            However, evidence that two attributes might notEvaluation Data  For evaluation, each interven-\ntion example consists of a tuple: an input x ∈PAE,   be separable is an insight into how knowledge is\nan input x′ ∈PA′E′ ∪WE′, a target attribute A∗,    structured in the model.\nand an intervention label y.  If A∗= A, then\n                                       3  Interpretability Methodsy  is AE′ and otherwise y  is AE.  For exam-\nple, if the set of “city” entities to evaluate on is                                  We use RAVEL to evaluate a variety of interpretabil-\n{\"Paris\", \"Tokyo\"} and the goal is to disentan-                                                               ity methods on their ability to disentangle attributes\ngle the “country” attribute from the “continent” at-                                                while generalizing to novel templates and enti-\ntribute, then the set of test examples becomes the                                                                 ties.  Each method uses data from the training\none shown in Figure 1.                                                             split to find a set of neurons N, learn a featur-\nMetrics   If M achieves behavioral success on    izer F, and find a feature FA that captures an\na dataset, we can use that dataset to evaluate an    attribute A ∈A independent from the other at-\ninterpretability method on its ability to identify a    tributes.  In Section 4, we describe the baseline\ncollection of neurons N, a featurizer F for those   procedure we use for considering different sets of\nneurons, and a feature F that represents an attribute    neurons. In this section, we define methods for\nA separately from all others attributes A \\ {A}.      learning a featurizer and identifying a feature given\n   If F encodes A, then interventions on F should   a set of neurons. For each method, the core inter-\nchange the value of A. When M is given a prompt    vention for A is given by II(M, FA, x, x′) where\nx ∈PAE, we can intervene on F to set the value to   FA is defined by the method. In this section, we\nwhat it would be if a second prompt x′ ∈PA′E′ ∪    use GetVals(M(x), N) to mean the activations of\nWE′ were provided. The token predicted by M   neurons N when M processes input x.\nshould change from AE to AE′:\n                                                    3.1 PCA\n                              def\n  Cause(A, F, M, D) =\n                                                     Principal Component Analysis (PCA) is a dimen-\n            ED II(M, F, x, x′) = AE′        sionality reduction method that minimizes informa-\nIf F isolates A, then interventions on F should not    tion loss. In particular, given a set of real valued\ncause the values of other attributes A∗∈A \\ {A}    vectors V ⊂Rn, |V| > n, the principal compo-\nto change. When M is given a prompt x∗∈PA∗E  ,   nents are n orthogonal unit vectors p1, . . . , pn that\nwe can again intervene on F to set the value to what   form an n × n matrix:\nit would be if a second prompt x′ ∈PA′E′ ∪WE′\n                                              PCA(V) = p1   . . .  pnwere provided. The token predicted by M should\nremain A∗E:\n                                                For our purposes, the orthogonal matrix formed by\n                           def\n  Iso(A, F, M, D)           =                               the principal components serves as a featurizer that\n    1                                    maps neurons N into a more interpretable space\n    X ED II(M, F, x∗, x′) = A∗E      (Chormai et al., 2022; Marks and Tegmark, 2023;|A \\ {A}|\n         A∗∈A\\{A}                               Tigges et al., 2023). Given an attribute A, a training\n\n\n                                         4\n\ndataset D from RAVEL for a particular entity type,   3.3  Relaxed Linear Adversarial Probe\na model M, and a set of neurons N, we define\n                                                Supervised probes are a popular interpretability\n  FA(n) =                                        technique for analyzing how neural activations cor-\n                                                           relate with high-level concepts (Peters et al., 2018;     nT PCA({GetVals(M(x), N) : x ∈D})\n                                          Hupkes et al., 2018; Tenney et al., 2019; Clark\n  PCA is an unsupervised method, so there is no    et al., 2019). When probes are arbitrarily powerful,\neasy way to tell what information is encoded in    this method is equivalent to measuring the mutual\neach principal component. To solve this issue, for    information between the neurons and the concept\neach attribute A ∈A we train a linear classifier   (Pimentel et al., 2020; Hewitt et al., 2021). How-\nwith L1 regularization to predict the value of A    ever, probes are typically simple linear models in\nfrom the featurized neural representations. Then,   order to capture how easily the information about\nwe define the feature FA to be the set of dimensions   a concept can be extracted. Probes have also been\nassigned a weight by the classifier that is greater   used to great effect on the task of concept erasure\nthan a hyperparameter ϵ.                           (Ravfogel et al., 2020; Elazar et al., 2021; Ravfogel\n                                                            et al., 2022).\n3.2  Sparse Autoencoder\n                                                  Following the method of Ravfogel et al. (2022),\nA recent approach to featurization is to train an                                     we train a relaxed linear adversarial probe (RLAP)\nautoencoder to project neural activations into a                                                         to learn a linear subspace parameterized by a set of\nhigher dimensional sparse feature space and then                                         k orthonormal vectors W ∈Rk×n that captures an\nreconstruct the neural activations from the features                                                           attribute A, using the following loss objective:\n(Bricken et al., 2023; Cunningham et al., 2024).\nWe train a sparse autoencoder on the loss\n                                               min maxX CE θT f, AEx\nX ||GetVals(M(x), N)− W2f +b2  ||2+||f||1                   θ  W x∈D\nx∈D\n f = ReLU(W1(GetVals(M(x), N) −b2) + b1)           f = (I −W T W) GetVals(M(x), N)\nwith W1 ∈Rn×m, W2 ∈Rm×n, b1 ∈Rm, and\nb2 ∈Rn. To construct a training datset, we sam-   where f is the representation of the entity with\nple 100k sentences from the Wikipedia corpus for    the attribute information erased, and θ is a linear\neach entity type, each containing a mention of an    classifier that tries to predict the attribute value\nentity in the training set. We extract the 4096-  AEx from the erased entity representation.\ndimension hidden states of Llama2-7B at the target    We define the F using the set of k orthonormal\nintervention site as the input for training a sparse    vectors that span the row space of W and the set of\nautoencoder with 16384 features.              n −k orthonormal vectors that span the null space:\n  We use the autoencoder to define a featurizer\n                                         FA(n) = n r1   . . .  rk  uk+1   . . .  un     FA(n) = ReLU(W1(n −b2) + b1)\nand an inverse F−1A (n) = W2n + b2.                                           Our feature FA is the first k dimensions of the\n  An important caveat to this method is that the\n                                                      feature space, i.e. the row space of W. Intuitively,\nfeaturizer is only truly invertible if the autoencoder\n                                                    since the linear probe was trained to extract the\nhas a reconstruction loss of 0. The larger the loss\n                                                          attribute A, the rowspace is the linear subspace of\nis, the more unfaithful this interpretability method\n                                                    neural activations that the probe is “looking at” to\nis to the model being analyzed. All other methods\n                                        make predictions.\nconsidered use an orthogonal matrix, which is truly\ninvertible up to floating point precision.\n                                                    3.4  Differential Binary Masking\n  Similar to PCA, sparse autoencoders are an un-\nsupervised method that does not produce features    Differential Binary Masking (DBM) learns a binary\nwith obvious meanings. Again, to solve this issue,   mask to select a set of neurons that causally rep-\nfor each attribute A ∈A we train a linear classifier    resents a concept (Cao et al., 2020; Csordás et al.,\nwith L1 regularization and define the feature FA to   2021; Cao et al., 2022; Davies et al., 2023). The\nbe the set of dimensions assigned a weight that is    loss objective used to train the mask is a combina-\ngreater than a hyperparameter ϵ.                      tion of matching the counterfactual behavior and\n\n\n                                         5\n\n    Method      Supervision    Entity   Context\n\n     Full Rep.      None         40.5      39.5\n   PCA         None         39.5      39.1\n   SAE         None         48.6      46.8\n   RLAP         Attribute       48.8      50.9\n   DBM       Counterfactual    52.2      49.8\n   DAS       Counterfactual    56.5      57.3\n   MDBM    Counterfactual    53.7      53.9\n   MDAS     Counterfactual    60.1      65.6\n\nTable 2: The disentanglement score on RAVEL for each\ninterpretability method. Numbers are represented in %.\n\nforcing the mask to be sparse with coefficient λ:\n\n  LCause = CE(τ(MN←n(x)), AE′) + λ||m||1\n                                                      Figure 2: Cause and Iso scores for each method when\n  n = 1 −σ(m/T) ◦GetVals(M(x), N)       using different feature sizes, shown as the ratio (%)\n       + σ(m/T) ◦GetVals(M(x′), N)      between the dimension of FA and the dimension of the\n                                                      output space of F. Each method has three data points\nwhere the intervention is determined by inputs x, x′     that vary from using very few (≈1%) to half (≈50%) of\nand learnable parameter m ∈Rn, where ◦is ele-    the dimensions. Increasing feature dimensions generally\nment wise multiplication and T ∈R is a tempera-    leads to higher Cause score, but lower Iso score. Figure\n                                                           best viewed in color.\nture annealed throughout training.\n  The feature space is the original space of neural   without considering the impact on the Iso score.\nactivations, i.e., featurizer FA(n) = n. The feature  We introduce the Iso aspect into the training objec-\nFA is the set of dimensions i where 1−σ(mi/T) <    tive through multitask learning. For each attribute\nϵ for a (small) hyperparameter ϵ.           A∗∈A \\ {A}, we define the Iso objective as\n3.5  Distributed Alignment Search               LIso(A, FA, M) = CE(II(M(x), FA, x′), A∗E)\n\nDistributed Alignment Search (DAS) (Geiger et al.,  We minimize a linear combination of losses from\n2023b) learns a linear subspace of a model repre-   each task:\nsentation with a training objective defined using\ninterchange interventions. In the original work, the      LDisentangle(A, FA, M) =\nlinear subspace learned by DAS is parameterized                              LIso(A∗, FA, M)\n                                                 LCause(A, FA, M)+ Xas an n × n orthogonal matrix Q = [u1 . . . un],                                                                  |A \\ {A}|\nwhich rotates the representation into a new coor-                    A∗∈A\\{A}\ndinate system, i.e., FA(n) = Q⊤n. The set of\nfeature FA is the first k dimensions of the rotated   4  Experiments\nsubspace, where k is a hyperparameter. The matrix\n                                 We evaluate the methods described in Section 3 on\nQ is learned by minimizing the following loss:\n                                     RAVEL with Llama2-7B (Touvron et al., 2023), a\n                                                   32-layer decoder-only Transformer model, as theLCause(A, FA, M) = CE(II(M, FA, x, x′), AE′)\n                                                          target LM. Implementation details of each method\nComputing Q is expensive, as it requires comput-   are provided in Appendix B.\ning n orthogonal vectors. To avoid instantiating\n                                                    4.1  Setupthe full rotation matrix, we use an alternative pa-\nrameterization where we only learn the k ≪n  We consider the residual stream representations\northogonal vectors that form the feature FA (see    at the last token of the entity as our potential in-\nAppendix B.4).                                     tervention sites. For autoregressive LMs, the last\n                                                token of the entity tE (e.g., the token “is” in the\n3.6  Multi-task DBM and DAS                                                 case of “Paris”) likely aggregates information of\nTo address the disentanglement problem, we pro-   the entity (Meng et al., 2022; Geva et al., 2023).\npose a multitask extension of DBM (MDBM) and   For Transformer-based LMs like Llama2, an ac-\nDAS (MDAS). The original training objective of    tivation vector NLt  in the residual stream is cre-\nDBM and DAS only optimizes for the Cause score,   ated for each token t at each Transformer layer\n\n\n                                         6\n\n(a) Cause score for all attributes when intervening on the attribute features iden-  (b) The Cause, Iso, and Disentangle score\ntified by DAS (left) and MDAS (right). A Cause score of 0.62 for column  on the Entity split for the “country” feature\nContinent, row Timezone (bottom left corner), means that, when intervening on  found by MDAS. The attributes of cities be-\nthe Continent feature, the same subspace changes Timezone 62% of the time.    come more disentangled across layers.\n\n                             Figure 3: Additional results for the MDAS method.\n\n\nL. As the contributions of the MLP and attention    levels of disentanglement. Figure 3b shows how\nheads must pass through the residual stream, it    the Cause, Iso, and Disentangle scores change\nserves as a bottleneck. Therefore, we will limit our    for the “country” attribute across model layers.\nmethods to examining the set of representations\nN = {NLtE : L ∈{1, . . . , 32}}.                Methods  with  counterfactual  supervision\n                                               achieve strong  results while methods with  This simplification is only to establish baseline\n                                             unsupervised featurizers struggle. MDAS isresults on the RAVEL benchmark. We expect the\n                                                    the state-of-the-art method on RAVEL, being ablebest methods will consider other token representa-\n                                                      to achieve high Disentangle scores while onlytions, such as the remainder of the token sequence\n                                                    intervening on a feature FA with a dimensionalitythat realizes the entity.\n                                                         that is 4% of |N| where N are the neurons the\n4.2  Results                                        feature is distributed across (Figure 2). DBM,\n                               MDBM, and DAS, the other methods that are\nWe evaluate each method on every representation                                                     trained with interventions using counterfactual\nNLtE and report the highest disentanglement score    labels as supervision, achieve the next best perfor-\non test splits in Table 2. We additionally include a                                             mance. PCA and Sparse Autoencoder achieve the\nbaseline that simply replaces the full representation                                                   lowest Disentangle scores, which aligns with the\nNLtE regardless of what attribute is being target-    prior finding that disentangled representations are\nted (see Full Rep. in Table 2). A breakdown of                                                             difficult to learn without supervision (Locatello\nthe results with per-attribute Cause and Iso is in                                                           et al., 2018).  Unsurprisingly, more supervision\nAppendix C.                                                          results in higher performance.\n  In Figure 2, we show for each method, how the\nIso and Cause scores vary as we change the dimen-   Multi-task supervision is better at isolating at-\nsionality of FA, the feature targeted for interven-   tributes.  Adding multitask objectives to DBM\ntion. For RLAP, DAS, and MDAS, the dimension-   and DAS increases the overall disentanglement\nality of FA is a hyperparameter we vary directly.   score by 1.5%/4.1% and 3.6%/8.3% on  the\nFor other methods, we vary the coefficient of L1   Entity/Context split respectively. To further il-\npenalty to vary the size of FA. Details are given in    lustrate the differences, we compare DAS with\nAppendix B.                       MDAS in Figure 3a. On the left, attributes such\n  In Figure 3, we focus on using MDAS, the best    as “continent” and “timezone” are naturally entan-\nperforming method, to understand how attributes    gled with all other attributes; intervening on the\nare disentangled in Llama2-7B. Figure 3a shows    feature learned by DAS for any city attribute will\ntwo heat maps summarizing the performance of    also change these two attributes. In contrast, in\nDAS and MDAS on the entity type “city”. These    Figure 3a right, MDAS is far more successful at\nheat maps also show how attributes have different    disentangling these attributes, having small Cause\n\n\n                                         7\n\nscores in all off-diagonal entries.                    concepts in a overlapping sets of neurons (Smolen-\n                                                    sky, 1988; Olah et al., 2020; Elhage et al., 2022).\nSome groups of attributes are more difficult\n                                                 Various methods have been proposed to find com-\nto disentangle than others.  As show in Fig-\n                                                ponents that capture a concept, such as finding a\nure 3a, the attribute pairs “country–language” and\n                                                        linear subspace that modifies a concept (Ravfo-\n“latitude–longitude” are difficult to disentangle.\n                                                    gel et al., 2020, 2022; Belrose et al., 2023; Cao\nWhen we train DAS to find a feature for either\n                                                           et al., 2020; Geiger et al., 2023b) and generating\nof these attributes (Figure 3a left), the same feature\n                                                a sparse feature space where each direction cap-\nalso has causal effects on the other attribute. Even\n                                                       tures a word sense or is more interpretable (Arora\nwith the additional supervision (Figure 3a right),\n                                                             et al., 2018; Bricken et al., 2023; Cunningham et al.,\nMDAS cannot isolate these attributes. Changing\n                                               2024; Tamkin et al., 2023). However, these meth-\none of these entangled attributes has seemingly un-\n                                               ods have not been evaluated against each other on\navoidable ripple effects (Cohen et al., 2024) that\n                                                          their ability to isolate concepts. Isolating an indi-\nchange the other.  In contrast, the attribute pair\n                                                    vidual concept is also related to the goal of “disen-\n“language–continent” can be disentangled. More-\n                                                 tanglement” in representation learning (Schölkopf\nover, the pairs that are difficult to disentangle are\n                                                           et al., 2021), where each direction captures a sin-\nconsistent across all five supervised methods in\n                                                    gle generative factor. In this work, we focus on\nour experiment, despite these methods using dif-\n                                                         isolating the causal effect of a representation.\nferent training objectives. We include additional\nvisualizations in Appendix C.2.                 Knowledge Representation  in LMs  Under-\n                                                  standing knowledge representation in LMs startsAttributes are gradually disentangled across lay-\n                                                  with probing structured linguistic knowledge (Con-\ners.  The representations of different attributes\n                                             neau et al., 2018; Tenney et al., 2019; Manning\ngradually disentangle as we move towards later lay-\n                                                             et al., 2020). Recent work expands to factual knowl-\ners, as shown in Figure 3b. Early layer features\n                                               edge stored in Transformer MLP layers (Geva et al.,\nidentified by MDAS fail to generalize to unseen\n                                                2021; Dai et al., 2022; Meng et al., 2022), associa-entities, hence low Cause score. While MDAS is\n                                                         tions represented in linear structures (Merullo et al.,able to identify a feature with relatively high Cause\n                                                 2023; Hernandez et al., 2024; Park et al., 2023), andstarting at layer 8, the Iso increases from 0.5 to 0.8\n                                                  deeper study of the semantic enrichment of subject\nfrom layer 8 to layer 16. It is not until layer 16 that\n                                                     representation (Geva et al., 2023). These findingsthe highest Disentangle score is achieved.\n                                                  suggest LMs store knowledge modularly, motivat-\n5  Related Work                               ing the disentanglement objective in our work.\n\nIntervention-based Interpretability Methods   Benchmarking Interpretability Methods   Test-\nIntervention-based techniques, branching off from    ing the faithfulness of interpretability method relies\ninterchange intervention (Vig et al., 2020; Geiger   on counterfactuals. Existing counterfactual bench-\net al., 2020) or activation patching (Meng et al.,   marks use behavioral testing (Atanasova et al.,\n2022), have shown promising results in uncovering   2023; Schwettmann et al., 2023; Mills et al., 2023),\ncausal mechanisms of LMs. They play important    interventions (Abraham et al., 2022), or a combi-\nroles in recent interpretability research of LMs such    nation of both (Huang et al., 2023). Recent model\nas causal abstraction (Geiger et al., 2021, 2023b),    editing benchmarks (Meng et al., 2022; Zhong\ncausal tracing to locate factual knowledge (Meng    et al., 2023; Cohen et al., 2024) also provide coun-\net al., 2022; Geva et al., 2023), path patching or    terfactuals that have potential for evaluating in-\ncausal scrubbing to find causal circuits (Chan et al.,    terpretability methods. MQUAKE (Zhong et al.,\n2022; Conmy et al., 2023; Goldowsky-Dill et al.,   2023) and RIPPLEEDITS (Cohen et al., 2024), in\n2023), and Distributed Alignment Search (Geiger    particular, consider entailment relationships of at-\net al., 2023b). Previous works suggest that activa-    tributes, while we focus on disentanglement.\ntion interventions that result in systematic counter-\nfactual behaviors provide clear causal insights into   6  Conclusion\nmodel components.\n                                We present RAVEL a benchmark for evaluating\nIsolating Individual Concepts  LMs learn highly    the ability of interpretability methods to localize\ndistributed representations that encode multiple   and disentangle entity attributes in LMs in a causal,\n\n\n                                         8\n\ngeneralizable manner. We show how RAVEL can   used for manipulating models in undesirable ap-\nbe used to evaluate five different families of in-    plications such as triggering toxic outputs. These\nterpretability methods that are commonly used in    interpretability methods should be studied and used\nthe community. We benchmark several strong in-    in a responsible manner.\nterpretability methods on RAVEL with Llama2-7B\n                                      Acknowledgementsmodel as baselines, and we introduce a multi-task\nobjective that improves the performance of Dif-                                                This research is supported in part by grants from\nferential Binary Masking (DBM) and Distributed                                       Open Philanthropy and the Stanford Institute for\nAlignment Search (DAS). Multi-task DAS achieves                                            Human-Centered Artificial Intelligence (HAI).\nthe best results in our experiments. Results on our\nattribute disentanglement task also offer insights\ninto the different levels of entanglement between   References\nattributes and the emergence of disentangled repre-                                                      Eldar David Abraham, Karel D’Oosterlinck, Amir\nsentations across layers in the Llama2-7B model.       Feder, Yair Gat, Atticus Geiger, Christopher Potts,\n  The community has seen an outpouring of in-     Roi Reichart, and Zhengxuan Wu. 2022. CEBaB:\n                                                         Estimating the causal effects of real-world conceptsnovative new interpretability methods. However,\n                                                 on NLP model behavior. In Advances in Neural In-\nthese methods have not been systematically eval-                                                        formation Processing Systems (NeurIPS).\nuated for whether they are faithful, generalizable,\ncausally effective, and able to isolate individual    Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\n                                                    and Andrej Risteski. 2018. Linear algebraic structureconcepts. We release RAVEL2 to the community\n                                                          of word senses, with applications to polysemy. In\nand hope it will help drive the assessment and de-      Transactions of the Association of Computational\nvelopment of interpretability methods that satisfy       Linguistics (TACL).\nthese criteria.\n                                                Pepa Atanasova, Oana-Maria Camburu, Christina Li-\n                                                 oma, Thomas Lukasiewicz, Jakob Grue Simonsen,\nLimitations                                       and Isabelle Augenstein. 2023. Faithfulness tests for\n                                                             natural language explanations.  In Association for\nOur attribute disentanglement results in Section 4      Computational Linguistics (ACL).\nare based on the Llama2-7B model. While Llama2-\n                                                  Sander Beckers, Frederick Eberhardt, and Joseph Y.\n7B uses the widely adopted decoder-only Trans-\n                                                        Halpern. 2020.  Approximate causal abstractions.\nformer architecture, different model architectures       In Uncertainty in Artificial Intelligence Conference\nor training paradigms could produce LMs that fa-      (UAI).\nvor different interpretability methods. Hence, when\n                                                    Sander Beckers and Joseph Y. Halpern. 2019. Abstract-\ndeciding which interpretability method is the best                                                          ing causal models. In Conference on Artificial Intel-\nto apply to a new model, we encourage people to       ligence (AAAI).\ninstantiate RAVEL on the new model.\n                                              Nora Belrose, David Schneider-Joseph, Shauli Ravfo-\n  When choosing intervention sites, we limit our                                                                  gel, Ryan Cotterell, Edward Raff, and Stella Bider-\nsearch to the residual stream above the last entity     man. 2023. LEACE: Perfect linear concept erasure\ntoken. However, representations of attributes can       in closed form. In Advances in Neural Information\nbe distributed across multiple tokens or layers. We      Processing Systems (NeurIPS).\nencourage future work to explore different inter-                                                    Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Co-\nvention sites when using this benchmark.               enen, Emily Reif, Fernanda B. Viégas, and Martin\n                                                       Wattenberg. 2021. An interpretability illusion for\nEthics Statement                             BERT. In arXiv preprint arXiv:2104.07143.\n\n                                                    Trenton Bricken, Adly Templeton, Joshua Batson,\nIn this paper, we present an interpretability bench-                                                       Brian Chen, Adam Jermyn, Tom Conerly, Nick\nmark that aims to assess the faithfulness, general-      Turner, Cem Anil, Carson Denison, Amanda Askell,\nizability, causal effects, and the ability to isolate      Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\nindividual concepts in language models. While an       Schiefer, Tim Maxwell,  Nicholas Joseph, Zac\n                                                           Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Bray-\ninterpretability method that satisfies these criteria\n                                                  den McLean, Josiah E Burke, Tristan Hume, Shan\ncould be useful for assessing model bias or steering       Carter, Tom Henighan, and Christopher Olah. 2023.\nmodel behaviors, the same method might also be      Towards monosemanticity: Decomposing language\n                                                    models with dictionary learning. In Transformer Cir-\n   2https://github.com/explanare/ravel                     cuits Thread.\n\n\n                                         9\n\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah,   Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\n  Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben      Chang, and Furu Wei. 2022. Knowledge neurons in\n  Egan, and Swee Kiat Lim. 2020. Thread: Circuits.       pretrained transformers. In Association for Computa-\n   In Distill.                                                   tional Linguistics (ACL).\n\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz,   Xander  Davies,  Max  Nadeau,  Nikhil  Prakash,\n  and Ivan Titov. 2020. How do decisions emerge     Tamar Rott Shaham, and David Bau. 2023. Discov-\n   across layers in neural models? Interpretation with       ering variable binding circuitry with desiderata. In\n   differentiable masking.  In Empirical Methods in      arXiv preprint arXiv:2307.03637.\n  Natural Language Processing (EMNLP).\n                                                   Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nNicola De Cao, Leon Schmid, Dieuwke Hupkes, and      Goldberg. 2021. Amnesic Probing: Behavioral Ex-\n  Ivan Titov. 2022. Sparse interventions in language       planation with Amnesic Counterfactuals. In Transac-\n  models with differentiable masking. In Proceedings       tions of the Association of Computational Linguistics\n   of the Fifth BlackboxNLP Workshop on Analyzing      (TACL).\n  and Interpreting Neural Networks for NLP.\n                                                 Nelson Elhage,  Tristan Hume,  Catherine Olsson,\nLawrence Chan,  Adrià  Garriga-Alonso,  Nicholas                                                       Nicholas Schiefer, Tom Henighan, Shauna Kravec,\n  Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishin-                                                Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\n   skaya, Ansh Radhakrishnan, Buck Shlegeris, and                                                       Carol Chen, Roger Grosse, Sam McCandlish, Jared\n  Nate Thomas. 2022. Causal scrubbing: a method                                                      Kaplan, Dario Amodei, Martin Wattenberg, and\n   for rigorously testing interpretability hypotheses. In                                                        Christopher Olah. 2022.  Toy models of superpo-\n  Alignment Forum Blog post.                                                                      sition. In arXiv preprint arXiv:2209.10652.\n\nPattarawat Chormai,  Jan Herrmann, Klaus-Robert                                                            Jiahai Feng and Jacob Steinhardt. 2024. How do lan-\n   Müller, and Grégoire Montavon. 2022.  Disentan-                                                   guage models bind entities in context?   In Inter-\n  gled explanations of neural network predictions                                                          national Conference on Learning Representations\n  by finding relevant subspaces.  In arXiv preprint                                                        (ICLR).\n  arXiv:2212.14855.\n\n                                              Matthew  Finlayson,  Aaron  Mueller,   Sebastian\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\n                                                 Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan\n   Christopher D. Manning. 2019. What does BERT\n                                                         Belinkov. 2021.   Causal  analysis of  syntactic\n  look at? An analysis of BERT’s attention. In Pro-\n                                                    agreement mechanisms in neural language models.\n  ceedings of the 2019 ACL Workshop BlackboxNLP:\n                                                          In Association for Computational Linguistics and\n  Analyzing and Interpreting Neural Networks for NLP.\n                                                              International Joint Conference on Natural Language\n                                                        Processing (ACL-IJCNLP).Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,\n  and Mor Geva. 2024. Evaluating the ripple effects of\n                                                        Atticus Geiger, Hanson Lu, Thomas F Icard, and  knowledge editing in language models. In Transac-\n                                                         Christopher Potts. 2021. Causal abstractions of neu-   tions of the Association of Computational Linguistics\n                                                                     ral networks.  In Advances in Neural Information  (TACL).\n                                                        Processing Systems (NeurIPS).\nArthur Conmy, Augustine N. Mavor-Parker, Aengus\n                                                        Atticus Geiger, Christopher Potts, and Thomas Icard.  Lynch, Stefan Heimersheim, and Adrià Garriga-\n                                                        2023a. Causal abstraction for faithful model interpre-  Alonso. 2023.  Towards automated circuit discov-\n                                                                    tation. Ms., Stanford University.   ery for mechanistic interpretability. In Advances in\n  Neural Information Processing Systems (NeurIPS).\n                                                          Atticus Geiger, Kyle Richardson, and Chris Potts. 2020.\nAlexis Conneau, German Kruszewski, Guillaume Lam-      Neural natural language inference models partially\n   ple, Loïc Barrault, and Marco Baroni. 2018. What     embed theories of lexical entailment and negation. In\n  you can cram into a single $&!#* vector: Probing      Proceedings of the 2020 EMNLP Workshop Black-\n  sentence embeddings for linguistic properties.  In     boxNLP: Analyzing and Interpreting Neural Net-\n  Association for Computational Linguistics (ACL).        works for NLP.\n\nRóbert Csordás, Sjoerd van Steenkiste, and Jürgen    Atticus Geiger, Zhengxuan Wu, Christopher Potts,\n  Schmidhuber. 2021. Are neural nets modular? In-     Thomas Icard, and Noah D. Goodman. 2023b. Find-\n   specting functional modularity through differentiable       ing alignments between interpretable causal variables\n  weight masks. In International Conference on Learn-     and distributed neural representations.  In Causal\n   ing Representations (ICLR).                           Learning and Reasoning (CLeaR).\n\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert   Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir\n  Huben, and Lee Sharkey. 2024. Sparse autoencoders      Globerson. 2023.  Dissecting recall of factual as-\n   find highly interpretable features in language models.      sociations in auto-regressive language models.  In\n   In International Conference on Learning Representa-      Empirical Methods in Natural Language Processing\n   tions (ICLR).                                   (EMNLP).\n\n\n                                         10\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer    Belinda Z. Li, Maxwell I. Nye, and Jacob Andreas.\n   Levy. 2021. Transformer feed-forward layers are key-      2021. Implicit representations of meaning in neural\n  value memories. In Empirical Methods in Natural      language models.  In Proceedings of the 59th An-\n  Language Processing (EMNLP).                       nual Meeting of the Association for Computational\n                                                             Linguistics and the 11th International Joint Confer-\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce,      ence on Natural Language Processing, ACL/IJCNLP\n  Lucas Dixon, and Mor Geva. 2024.  Patchscopes:      2021, (Volume 1: Long Papers), Virtual Event, Au-\n A unifying framework for inspecting hidden repre-      gust 1-6, 2021, pages 1813–1827. Association for\n   sentations of language models.  In arXiv preprint      Computational Linguistics.\n  arXiv:2401.06102.\n                                      Tom Lieberum, Matthew Rahtz, János Kramár, Neel\n                                                   Nanda, Geoffrey Irving, Rohin Shah, and VladimirNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato,\n  and Aryaman Arora. 2023.   Localizing model      Mikulik. 2023. Does circuit analysis interpretability\n  behavior with path patching.   In arXiv preprint       scale? Evidence from multiple choice capabilities in\n                                                                 chinchilla. In arXiv preprint arXiv:2307.09458.  arXiv:2304.05969.\n\n                                                   Francesco Locatello, Stefan Bauer, Mario Lucic, Syl-\nWes Gurnee, Neel Nanda, Matthew Pauly, Katherine                                                           vain Gelly, Bernhard Schölkopf, and Olivier Bachem.\n  Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.                                                      2018. Challenging common assumptions in the un-\n  2023. Finding neurons in a haystack: Case studies                                                         supervised learning of disentangled representations.\n  with sparse probing.  In Transactions on Machine                                               CoRR, abs/1811.12359.\n  Learning Research (TMLR).\n                                                      Christopher D. Manning, Kevin Clark, John Hewitt,\nMichael Hanna, Ollie Liu, and Alexandre Variengien.      Urvashi Khandelwal, and Omer Levy. 2020. Emer-\n  2023. How does GPT-2 compute greater-than?: In-      gent linguistic structure in artificial neural networks\n   terpreting mathematical abilities in a pre-trained lan-       trained by self-supervision. In Proceedings of the\n  guage model.  In Advances in Neural Information      National Academy of Sciences (PNAS).\n  Processing Systems (NeurIPS).\n                                               Samuel Marks and Max Tegmark. 2023. The geometry\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-      of truth: Emergent linear structure in large language\n   deharioun. 2023. Does localization inform editing?      model representations of true/false datasets. In arXiv\n   surprising differences in causality-based localization       preprint arXiv:2310.06824.\n   vs. knowledge editing in language models. In Ad-\n                                                                         J. L. McClelland, D. E. Rumelhart, and PDP Research  vances in Neural Information Processing Systems\n                                                    Group, editors. 1986. Parallel Distributed Process-  (NeurIPS).\n                                                                   ing. Volume 2: Psychological and Biological Models.\n                                        MIT Press, Cambridge, MA.Roee Hendel, Mor Geva, and Amir Globerson. 2023.\n   In-context learning creates task vectors. In Empirical                                                 Kevin Meng, David Bau, Alex Andonian, and Yonatan\n  Methods in Natural Language Processing (EMNLP).                                                          Belinkov. 2022. Locating and editing factual associ-\n                                                             ations in GPT. In Advances in Neural Information\nEvan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin                                                        Processing Systems (NeurIPS).\n  Meng, Martin Wattenberg, Jacob Andreas, Yonatan\n   Belinkov, and David Bau. 2024. Linearity of relation    Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.\n  decoding in transformer language models. In Inter-      2023. A mechanism for solving relational tasks\n   national Conference on Learning Representations       in transformer language models. In arXiv preprint\n  (ICLR).                                              arXiv:2305.16130.\n\nJohn Hewitt, Kawin Ethayarajh, Percy Liang, and   Edmund Mills, Shiye Su, Stuart Russell, and Scott Em-\n   Christopher Manning. 2021. Conditional probing:      mons. 2023. Almanacs: A simulatability benchmark\n  measuring usable information beyond a baseline. In       for language model explainability. In arXiv preprint\n  Empirical Methods in Natural Language Processing      arXiv:2312.12747.\n  (EMNLP).\n                                                        Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel\n                                                  Goh, Michael Petrov, and Shan Carter. 2020. Zoom\nJing Huang,  Atticus Geiger, Karel D’Oosterlinck,\n                                                                      in: An introduction to circuits. In Distill.\n  Zhengxuan Wu, and Christopher Potts. 2023. Rig-\n   orously assessing natural language explanations of   Kiho Park, Yo Joong Choe, and Victor Veitch. 2023.\n   neurons.  In Proceedings of the 6th BlackboxNLP     The linear representation hypothesis and the geom-\n  Workshop: Analyzing and Interpreting Neural Net-                                                              etry of large language models.  In arXiv preprint\n  works for NLP.                                       arXiv:2311.03658.\n\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.   Judea Pearl. 2001. Direct and indirect effects. In Pro-\n  2018. Visualisation and “diagnostic classifiers” re-      ceedings of the Seventeenth Conference on Uncer-\n   veal how recurrent and recursive neural networks       tainty in Artificial Intelligence, UAI’01, pages 411–\n   process hierarchical structure. In Journal of Artificial      420, San Francisco, CA, USA. Morgan Kaufmann\n   Intelligence Research (JAIR).                             Publishers Inc.\n\n\n                                         11\n\nJudea Pearl. 2009. Causality. Cambridge University    Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron\n   Press.                                                 Mueller, Byron C. Wallace, and David Bau. 2024.\n                                                       Function vectors in large language models.  In In-\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,       ternational Conference on Learning Representations\n  and Wen-tau Yih. 2018. Dissecting contextual word      (ICLR).\n  embeddings: Architecture and representation.  In\n  Empirical Methods in Natural Language Processing   Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\n  (EMNLP).                                                     bert, Amjad Almahairi, Yasmine Babaei, Nikolay\n                                                         Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,      Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton\n  Ran Zmigrod, Adina Williams, and Ryan Cotterell.       Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,\n  2020. Information-theoretic probing for linguistic      Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\n   structure. In Association for Computational Linguis-      Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-\n   tics (ACL).                                          thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\n                                                              Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael       Isabel Kloumann, Artem Korenev, Punit Singh Koura,\n  Twiton, and Yoav Goldberg. 2020. Null it out: Guard-     Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\n   ing protected attributes by iterative nullspace projec-      ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\n   tion. In Association for Computational Linguistics        tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\n  (ACL).                                               bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\n                                                                    stein, Rashi Rungta, Kalyan Saladi, Alan Schel-\nShauli Ravfogel, Michael Twiton, Yoav Goldberg, and       ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\n  Ryan D Cotterell. 2022. Linear adversarial concept      ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\n   erasure.  In International Conference on Machine       Taylor, Adina Williams, Jian Xiang Kuan, Puxin\n  Learning (ICML).                                 Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-\n                                                          gela Fan, Melanie Kambadur, Sharan Narang, Au-\nD. E. Rumelhart, J. L. McClelland, and PDP Research       relien Rodriguez, Robert Stojnic, Sergey Edunov,\n  Group, editors. 1986. Parallel Distributed Process-     and Thomas Scialom. 2023. Llama 2: Open founda-\n   ing. Volume 1: Foundations. MIT Press, Cambridge,       tion and fine-tuned chat models. In arXiv preprint\n  MA.                                                 arXiv:2307.09288.\n\nBernhard Schölkopf, Francesco Locatello, Stefan Bauer,   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n  Nan Rosemary Ke, Nal Kalchbrenner, Anirudh       Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\n  Goyal, and Yoshua Bengio. 2021. Toward causal rep-      Kaiser, and Illia Polosukhin. 2017. Attention is all\n   resentation learning. Proc. IEEE, 109(5):612–634.      you need. In Advances in Neural Information Pro-\n                                                           cessing Systems (NeurIPS).\nSarah Schwettmann, Tamar Rott Shaham, Joanna\n  Materzynska, Neil Chowdhury, Shuang Li, Jacob    Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\n  Andreas, David Bau, and Antonio Torralba. 2023.      Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart\n   Find: A function description benchmark for evaluat-       Shieber. 2020. Investigating gender bias in language\n   ing interpretability methods. In Advances in Neural      models using causal mediation analysis. In Advances\n  Information Processing Systems (NeurIPS) Track on       in Neural Information Processing Systems (NeurIPS).\n  Datasets and Benchmarks.\n                                                  Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,\nPaul Smolensky. 1988. On the proper treatment of con-     Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-\n   nectionism. Behavioral and Brain Sciences, 11(1):1–       pretability in the wild: a circuit for indirect object\n   23.                                                           identification in GPT-2 small. In International Con-\n                                                            ference on Learning Representations (ICLR).\nPeter Spirtes, Clark Glymour, and Richard Scheines.\n  2000.  Causation, Prediction, and Search. MIT   Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christo-\n   Press.                                               pher Potts, and Noah D. Goodman. 2023.  Inter-\n                                                                 pretability at scale: Identifying causal mechanisms\nAlex Tamkin, Mohammad Taufeeque, and Noah D.       in alpaca. In Advances in Neural Information Pro-\n  Goodman. 2023. Codebook features: Sparse and       cessing Systems (NeurIPS).\n   discrete interpretability for neural networks. In arXiv\n                                               Zexuan Zhong, Zhengxuan Wu, Christopher Manning,   preprint arXiv:2310.17230.\n                                                           Christopher Potts, and Danqi Chen. 2023. MQuAKE:\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.      Assessing knowledge editing in language models via\n  BERT rediscovers the classical NLP pipeline.  In      multi-hop questions. In Empirical Methods in Natu-\n  Association for Computational Linguistics (ACL).          ral Language Processing (EMNLP).\n\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger,\n  and Neel Nanda. 2023.  Linear representations of\n   sentiment in large language models. In arXiv preprint\n  arXiv:2310.15154.\n\n\n                                         12\n\nA  Dataset Details\n\n\n  Attributes  |AE|  Sample Values           Sample Prompts\n\n  City\n                                        city to country: Toronto is in Canada. {E} is in,\n                  United  States,   China,\n  Country   158                          [{\"city\": \"Paris\", \"country\": \"France\"}, {\"city\": \"{E}\",\n                    Russia, Brazil, Australia\n                                            \"country\": \"\n                    Asia,  Europe,   Africa,\n                                        {E} is a city in the continent of,\n  Continent  6     North  America,  South                                        [{\"city\": \"{E}\", \"continent\": \"\n                 America\n                                        [{\"city\": \"Rio de Janeiro\", \"lat\": \"23\"},\n  Latitude   122   41, 37, 47, 36, 35\n                                            {\"city\": \"{E}\", \"lat\": \", [{\"city\": \"{E}\", \"lat\": \"\n                                        [{\"city\": \"Rome\", \"long\": \"12.5\"}, {\"city\": \"{E}\", \"long\": \",\n  Longitude  317   30, 9, 10, 33, 11                                         \"long\": \"122.4\"}, {\"city\": \"{E}\", \"long\": \"\n                                        [{\"city\": \"Beijing\", \"lang\": \"Chinese\"},\n                    English,  Spanish,  Chi-\n Language  159                              {\"city\": \"{E}\", \"lang\": \",\n                     nese, Russian, Portuguese\n                                        [{\"city\": \"{E}\", \"official language\": \"\n                  America/Chicago,\n                                        Time zone in Los Angeles is America/Santiago;\n                   Asia/Shanghai,\n                                            Time zone in {E} is,\n Timezone  267   Asia/Kolkata,                                        [{\"city\": \"New Delhi\", \"timezone\": \"UTC+5:30\"},\n                 Europe/Moscow, Amer-\n                                            {\"city\": \"{E}\", \"timezone\": \"UTC\n                   ica/Sao_Paulo\n\n Nobel Laureate\n                                        Jules A. Hoffmann won the Nobel Prize in Medicine.\n                   Medicine, Physics, Chem-\n  Field      7                                {E} won the Nobel Prize in,\n                          istry, Literature, Peace\n                                        name: {E}, award: Nobel Prize in\n                  2001, 2019, 2009, 2011,  \"name\": {E}, \"award\": \"Nobel Prize\", \"year\": \",\n Award    118\n                 2000                   laureate: Frances H. Arnold, year: 2018, laureate: {E}, year:\n Year\n                  1918, 1940, 1943, 1911,  Alan Heeger was born in 1936. {E} was born in,\n  Birth      145\n                 1941                   laureate: {E}, date of birth (YYYY-MM-DD):\n Year\n                  United  States,  United\n                                        name: {E}, country:,\n  Country   81    Kingdom,    Germany,                                        Roderick MacKinnon was born in United States. {E} was born in\n  of Birth           France, Sweden\n                                        name: {E}, gender:,\n Gender    4       his, male, female, her                                        David M. Lee: for his contributions in physics. {E}: for\n\nTable 3: Attributes in RAVEL. |AE| is the number of unique attribute values. In sampled prompts, {E} is a\nplaceholder for the entity.\n\n\nA.1  Details of Entities and Attributes\n\nWe first identify entity types from existing datasets such as the Relations Dataset (Hernandez et al., 2024)\nand RIPPLEEDITS (Cohen et al., 2024), where each entity type potentially contains thousands of instances.\nWe then source the entities and ground truth references for attribute values from online sources.3 4 5 6 7 8\nThese online sources are distributed under MIT, Apache-2.0, and CC-0 licenses. Compared with similar\nentity types in the Relations Dataset and RIPPLEEDITS, RAVEL has expanded the number of entities by a\nfactor of at least 10 and included multiple attributes per entity.\n  We show the cardinality of the attributes, most frequent attribute values, and random samples of prompt\ntemplates in Table 3.\n\nA.2  The RAVEL Llama2-7B Instance\n\nThe RAVEL Llama2-7B instance is used for benchmarking interpretability methods in Section 4. There\nare a total of 2800 entities in the Llama2-7B instance. Table 4 shows the number of entities, prompt\n\n   3https://github.com/kevinroberts/city-timezones\n   4https://github.com/open-dict-data/ipa-dict/blob/master/data/en_US.txt\n   5https://github.com/monolithpl/verb.forms.dictionary\n   6https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\n   7https://huggingface.co/datasets/corypaik/coda\n   8https://www.bls.gov/ooh,https://www.bls.gov/cps\n\n\n                                         13\n\n  Attributes  |AE|  Sample Values           Sample Prompts\n\n Verb\n                     take hold of, make certain,\n                                        talk: communicate by speaking; win: achieve victory; {E}:,\n  Definition  986   show, express in words,                                        like: have a positive preference; walk: move on foot; {E}:\n                make\n                  expanded, sealed, termi-  present tense: {E}, past tense:,\n  Past      986\n                     nated, escaped, answered   write: wrote; look: looked; {E}:\n Tense\n  Pronun-            k@n\"fju:z,     fI\"nIS,   bOIl,  create: kri\"eIt; become: bI\"k2m; {E}:,\n           986\n  ciation              In\"SU@r, tIp               begin: bI\"gIn; change: tSeIndZ; {E}:\n                   compensates,      kicks,\n                                        tell: tells; create: creates; {E}:,\n  Singular   986   hunts,  earns, accompa-                                        present tense: {E}, 3rd person present:\n                     nies\n\n  Physical Object\n                                        bird is a type of animal: YES; rock is a type of animal: NO;\n                      plant,  non-living  thing,      {E} is a type of animal:,\n  Category   29\n                    animal, NO, fruit         Among the categories \"plant\", \"animal\", and \"non-living thing\",\n                                            {E} belongs to \"\n                                        The color of apple is usually red. The color of violet is\n                    green,   white,   yellow,      usually purple. The color of {E} is usually,\n  Color     12\n                  brown, black             The color of apple is usually red. The color of turquoise\n                                            is usually blue. The color of {E} is usually\n                                        Among the units \"mm\", \"cm\", \"m\", and \"km\",\n                                            the size of {E} is usually on the scale of \",\n  Size      4     cm, mm, m, km                                        Given the units \"mm\" \"cm\" \"m\" and \"km\",\n                                            the size of {E} usually is in \"\n                                         hard or soft: rock is hard; towel is soft;\n                                            blackberry is soft; wood is hard; {E} is,\n  Texture    2       soft, hard                                        Texture: rock: hard; towel: soft; blackberry: soft;\n                                            charcoal: hard; {E}:\n\n Occupation\n                       treat patients, teach stu-  \"occupation\": \"photographer\", \"duties\": \"to capture\n Duty      650   dents, sell products, cre-      images using cameras\"; \"occupation\": \"{E}\", \"duties\": \"to,\n                      ate art, serve food         \"occupation\": \"{E}\", \"primary duties\": \"to\n                                        The {E} left early because\n Gender    9      he, male, his, female, she                                        The newspaper praised the {E} for\n  Bias\n                                        \"occupation\": \"sales manager\", \"industry\": \"retail\";\n                     construction, automotive,\n                                            \"occupation\": \"{E}\", \"industry\": \",\n  Industry   280   education,  health  care,                                        \"occupation\": \"software developer\", \"industry\":\n                     agriculture\n                                            \"technology\"; \"occupation\": \"{E}\", \"industry\": \"\n Work\n                       office, factory, hospital,  \"occupation\": \"software developer\", \"environment\": \"office\";\n Loca-     128\n                    construction site, studio        \"occupation\": \"{E}\", \"environment\": \"\n  tion\n\n                                 Table 3: Attributes in RAVEL, continued.\n\n\n                                            # Prompts  # Test Examples in\n                     Entity Type      # Entities                                  Accuracy (%)\n                                                Templates    Entity/Context\n\n                    City              800        90           15K/33K            97.1\n                  Nobel Laureate     600        60           9K/23K            94.3\n                  Verb              600        40           12K/20K            95.1\n                    Physical Object     400        40            4K/6K             94.3\n                   Occupation        400        30           10K/16K            96.4\n\nTable 4: Stats of RAVEL in its Llama2-7B instance, created by sampling a subset of examples where Llama2-7B has\na high accuracy in predicting attribute values.\n\ntemplates, and test examples, i.e., the number of base–source input pairs for interchange intervention in\nthe Llama2-7B instance.\n  The RAVEL Llama2-7B instance is created by filtering examples where the pre-trained Llama2-7B has\na high accuracy in predicting attribute values. For each entity type, we take the k entities with the highest\naccuracy over all prompt templates and the n prompt templates with the highest accuracy over all entities,\nwith the average accuracy over all prompts shown in Table 4. For most attributes, we directly compare\n\n\n                                         14\n\nmodel outputs against the ground truth attribute values. For “latitude” and “longitude” of a city, we relax\nthe match to be ±2 within the ground truth value. For “pronunciation” of a verb, we relax the match to\nallow variations in the transcription. For attributes with more open-ended outputs, including “definition”\nof a verb and “duty” of an occupation, we manually verify if the outputs are sensible. For “gender bias”\nof an occupation, we check for the consistency of gender bias over a set of prompts that instruct the model\nto output gender pronouns.\n\nB  Method Details\n\nB.1 PCA\n\nFor PCA, we extract the 4096-dimension hidden state representations at the target intervention site as\nthe inputs. The representations are first normalized to zero-mean and unit-variance using mean and\nvariance estimated from the training set. We use the sklearn implementation9 to compute the principal\ncomponents. We then apply L1-based feature selection10 to identify a set of dimensions that most likely\nencode the target attribute A. We undo the normalization after projecting back to the original space.\n  We vary the coefficient of the L1 penalty, i.e., the parameter “C” in the sklearn implementation, to\nexperiment with different intervention dimensions. We experiment with C ∈{0.1, 1, 10, 1000}. We\nobserve that regardless of the intervention dimension, the features selected have a high overlap with the\nfirst k principal components. For most attributes, the highest Disentangle score is achieved when using\nthe largest intervention dimension.\n\nB.2  Sparse Autoencoder\nFor the sparse autoencoder, we use a single-layer encoder-decoder model.11 The autoencoder is trained on\nWikipedia data as described below.\n\nModel  Encoder: Fully connected layer with ReLU activations, dimensions 4096 × 16384. Decoder:\nFully connected layer, dimensions 16384 × 4096. Latent dimension: 4 × 4096. The model is trained to\noptimize a combination of an L2 loss to reconstruct the representation and an L1 loss to enforce sparsity.\n\nTraining Data  For each entity type, we sample 100k sentences from the Wikipedia corpus, each\ncontaining a mention of an entity in the training set. We extract the 4096-dimension hidden states at the\ntarget intervention site as the input for training the sparse autoencoder.\n  Similar to PCA, we apply L1-based feature selection on the latent representation to identify a set of\ndimensions that most likely encode the target attribute A. We vary the coefficient C of the L1 penalty to\nexperiment with different intervention dimension. The optimal C varies across attributes.\n\nB.3 RLAP\n\nRLAP learns a set of linear probes to find the feature F. Each linear probe aims to predict the attribute value\nfrom the entity representations. Similar to PCA and sparse autoencoders, we use the 4096-dimension\nhidden state representations at the target intervention site as the initial inputs and the corresponding\nattribute value as labels. In the case of attributes with extremely large output spaces, e.g., numerical\noutputs, we approximate the output with the first token. Table 5 shows the linear classifier accuracy on\neach attribute classification task.\n  We use the official R-LACE implementation12 and extract the rank-k orthogonal matrix W from the\nfinal null projection13 as FA. For each attribute, we experiment with rank k ∈{32, 128, 512, 2048}. We\nrun the algorithm for 100 iterations and select the rank with the highest Disentangle score on the dev\nset. The optimal intervention dimension is usually small, i.e., 32 or 128, for attributes that have a high\naccuracy linear classifier.\n\n   9https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n  10https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection\n  11https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing#scrollTo=\nKn1E_44gCa-Z\n  12https://github.com/shauli-ravfogel/rlace-icml\n  13https://github.com/shauli-ravfogel/rlace-icml/blob/master/rlace.py#L90\n\n\n                                         15\n\n                                      Attribute           Entity       Context\n\n                                   City\n                                Country             0.78           1.00\n                                   Continent           0.96           1.00\n                                    Latitude             0.18           1.00\n                                 Longitude           0.13           1.00\n                               Language           0.60           1.00\n                               Timezone           0.68           1.00\n                               Nobel Laureate\n                                     Field                0.82           1.00\n                             Award Year          0.08           1.00\n                                     Birth Year           0.01           1.00\n                                Country of Birth     0.63           1.00\n                               Gender              0.93           1.00\n                                Verb\n                                     Definition           0.03           1.00\n                                     Past Tense           0.00           1.00\n                                   Pronunciation        0.00           1.00\n                                    Singular             0.00           1.00\n                                    Physical Object\n                                 Category            0.90           1.00\n                                 Color               0.49           1.00\n                                   Size                0.86           1.00\n                                   Texture             0.75           1.00\n                                 Occupation\n                              Duty                0.06           1.00\n                               Gender Bias         0.17           0.99\n                                    Industry             0.43           1.00\n                            Work Location       0.44           1.00\n\nTable 5: Accuracy of linear probes on dev splits using the Llama2-7B residual stream representations extracted\nfrom layer 7 above the last entity token. For most attribute, there exists a linear classifier with significant higher\naccuracy than random baseline on the entity dev split. For all attributes, there exists a linear classifier with close to\nperfect accuracy on the context dev split.\n\nB.4  DBM-based and DAS-based Methods\nFor DBM- and DAS-based methods, we use the implementation from the pyvene library.14 For training\ndata, both methods are trained on base–source pairs with interchange interventions.\n  For DBM and MDBM, we use a starting temperature of 1e−2 and gradually reducing it to 1e−7. The\nfeature dimension is controlled by the coefficient of the L1 loss. The optimal coefficient for the DBM\npenalty is around 0.001, while no penalty generally works better for MDBM, as the multi-task objective\nnaturally encourages the methods to select as few dimensions as possible.\n  For DAS and MDAS, we do not instantiate the full rotation matrix, but only parameterize the k\northogonal vectors that form the feature FA. The interchange intervention is defined as\n\n     II(M, FA, x, x′) = (I −W ⊤W)(GetVals(M(x), N)) + W ⊤W(GetVals(M(x′), N))\n\nwhere the rows of W are the k orthogonal vectors. We experiment with k ∈{32, 128, 512, 2048} and\nselect the dimension with the highest Disentangle score on the dev set. For most attributes, a larger\nintervention dimension, e.g., 512 or 2048, leads to a higher Disentangle score.\n\nB.5  Computational Cost\n\nAll models are trained and evaluated on a single NVIDIA RTX A6000 GPU.\n  For training, the computational cost of sparse autoencoders is the lowest, as training sparse autoencoders\ndoes not involve backpropagating through the original Llama2-7B model or computing orthogonal\nfactorization of weight matrices. Each epoch of the sparse autoencoder training, i.e., iterating over\n100k examples, takes about 100 seconds with Llama2-7B features extracted offline. The computational\ncost of RLAP- and DAS-based method largely depends on the rank of the nullspace or the intervention\ndimension, i.e., the number of orthogonal vectors. For RLAP, it takes 1 hour per 100 iterations with\n\n  14https://github.com/stanfordnlp/pyvene\n\n\n                                         16\n\n   Method    Continent   Country   Language    Latitude   Longitude   Timezone   Iso Cause   Disentangle\n\n   Entity\n   PCA       32.7 45.2   36.3 58.6   34.2 33.3   32.7 44.2   39.3 35.4   36.0 36.6   35.2  42.2        38.7\n   SAE       82.5 15.2   40.4 70.0   91.8  5.0    92.1 17.4   93.3 21.2   91.1 13.6   81.9  23.7        52.8\n   RLAP     89.4 21.0   38.2 55.8   44.6 48.0   58.1 48.2   38.2 54.0   41.1 50.0   51.6  46.2        48.9\n  DBM      65.9 70.0   44.8 70.6   42.9 54.3   45.1 59.8   44.9 57.0   72.2 54.0   52.6  61.0        56.8\n   DAS       67.3 86.4   30.1 83.8   36.3 74.0   52.7 63.2   50.3 56.6   71.0 74.0   51.3  73.0        62.1\n  MDBM    72.6 68.2   58.6 73.0   56.7 52.3   59.1 55.2   59.9 54.4   75.7 56.4   63.8  59.9        61.8\n  MDAS     92.1 69.2   82.7 65.6   86.4 51.7   91.4 47.6   93.1 46.0   92.9 62.4   89.8  57.1        73.4\n   Context\n   PCA       27.9 46.1   31.4 52.5   29.2 19.0   26.8 40.0   27.5 53.0   28.8 47.5   28.6  43.0        35.8\n   SAE       65.6 28.9   29.3 75.4   88.6  4.5    87.0 18.0   88.4 26.5   65.8 27.0   70.8  30.0        50.4\n   RLAP     86.0 21.4   22.4 84.7   36.8 43.0   46.1 55.0   28.3 72.5   34.8 51.0   42.4  54.6        48.5\n  DBM      58.7 58.6   37.9 66.0   36.4 36.0   38.3 61.4   38.9 69.0   67.4 53.5   46.3  57.4        51.8\n   DAS       58.9 84.9   17.7 89.3   27.7 54.0   33.9 77.6   40.9 72.5   64.6 73.5   40.6  75.3        58.0\n  MDBM    65.4 56.4   50.7 67.6   52.1 32.0   51.9 58.2   53.3 66.5   70.0 55.5   57.2  56.0        56.6\n  MDAS     86.6 64.9   70.5 70.7   90.3 20.0   88.0 57.0   89.8 62.0   90.0 57.5   85.9  55.4        70.6\n\n                                                      (a) Scores of city attributes.\n\n     Method   Award Year   Birth Year   Country of Birth     Field     Gender    Iso Cause   Disentangle\n\n     Entity\n    PCA       24.2  22.7    30.8  2.3    22.4    70.0      24.3 78.3    4.3 81.0   21.2  50.9        36.0\n    SAE       79.8  0.7     80.1  0.7    39.8    49.0      43.4 54.0   71.3 63.7   62.9  33.6        48.2\n    RLAP     87.3  0.3     90.3  1.0    68.0     8.7       82.5 54.0   95.3 71.0   84.7  27.0        55.8\n   DBM      91.8  0.7     98.6  0.3    61.5    32.0      71.3 57.7   92.6 71.7   83.2  32.5        57.8\n    DAS       57.1  5.0     72.7  2.3    80.9    25.3      80.1 72.7   80.8 77.7   74.3  36.6        55.5\n   MDBM    40.8  19.3    70.2  2.0    66.9    36.3      69.2 62.3   76.4 79.7   64.7  39.9        52.3\n    MDAS     83.6  4.0     85.2  2.0    88.8    28.0      86.9 58.0   93.4 78.0   87.6  34.0        60.8\n     Context\n    PCA       19.2  25.4    22.6  3.3    18.4    73.2      23.6 76.0    3.0 67.0   17.4  49.0        33.2\n    SAE       74.9  1.0     73.8  1.0    38.1    38.3      65.1 28.0   64.8 35.0   63.3  20.7        42.0\n    RLAP     88.1  0.4     90.3  0.8    54.4    67.3      77.7 67.3   94.0 61.0   80.9  39.4        60.1\n   DBM      88.1  0.2     96.9  0.0    50.6    50.2      56.1 59.3   96.8 61.7   77.7  34.3        56.0\n    DAS       42.7  18.4    13.9  7.5    37.1    72.8      30.2 82.3   88.0 72.7   42.4  50.7        46.5\n   MDBM    38.6  20.6    69.5  2.2    65.8    54.2      66.7 65.7   91.6 72.0   66.4  42.9        54.7\n    MDAS     80.2  27.4    83.9 12.3   86.6    72.8      90.2 72.0   93.4 73.0   86.9  51.5        69.2\n\n                                             (b) Scores of Nobel laureate attributes.\n\n                                         Table 6: Per-task results.\n\na feature dimension 4096 and a target rank of 128. For DAS and MDAS with the reduced parameter\nformulation, the training time for an intervention dimension of 128 (out of a feature dimension of 4096)\nover 1k intervention examples is about 50 seconds. The computational cost of DBM-based method is\nabout 35 seconds per 1k intervention examples.\n  For evaluation, the inference speed of our proposed framework is 20 seconds per 1k intervention\nexamples.\n\n\nC  Results\n\nFor all methods, we conduct hyper-parameter search on the dev set. We report a single-run test set results\nusing the set of hyper-parameters that achieves the highest score on the dev set. For intervention site, we\nchoose layer 16 for city attributes and layer 7 for the rest attributes.\n\n\nC.1  Breakdown of Benchmark Results\n\nTable 6 shows the breakdown of benchmark results in Table 2. For each method, we report a breakdown\nof the highest Disentangle score per attribute, i.e., the pair of Cause score and Iso score that add up\nto the highest Disentangle score. The final score in Table 2 is an average of the Disentangle score\nover all five entity types. For example, for PCA, the Disentangle score under the Entity setting is\n(38.7 + 36.0 + 41.3 + 43.3 + 38.1)/5 = 39.5.\n\n\n                                         17\n\n Method    Definition   Past Tense   Pronunciation    Singular   Iso Cause   Disentangle\n\n Entity\n PCA        4.9  59.5    4.6  95.3    2.1   66.5      4.2 93.3    4.0  78.6        41.3\n SAE       93.4  3.5    15.4 87.3   85.4    3.0      14.3 82.3   52.1  44.0        48.1\n RLAP     22.1 42.0   15.8 87.3   23.9   45.5     13.5 85.3   18.8  65.0        41.9\n DBM      22.0 51.0   16.3 88.7   10.2   58.0     14.2 87.0   15.7  71.2        43.4\n DAS       90.3 12.0   11.9 92.0   89.4   19.5     13.6 85.8   51.3  52.3        51.8\n MDBM    55.8 30.0   32.8 70.5   66.4   20.0     25.4 75.8   45.1  49.1        47.1\n MDAS     97.6  6.5    88.4  1.2    89.5   25.0     85.4 2.5    90.2  8.8         49.5\n Context\n PCA        9.6  57.0    8.3  84.3    4.3   44.0      9.2 78.3    7.9  65.9        36.9\n SAE       84.3 10.5   16.8 77.3   74.1    5.5      16.2 73.7   47.9  41.8        44.8\n RLAP     19.5 46.5   15.0 80.7   19.1   46.5     13.9 79.3   16.9  63.2        40.0\n DBM      21.7 53.0   16.3 84.3   12.3   52.5     14.7 81.0   16.3  67.7        42.0\n DAS       69.5 36.5    8.7  93.3   77.4   49.0      7.4 89.7   40.7  67.1        53.9\n MDBM    64.4 29.5   28.4 70.0   62.9   28.0     27.5 68.0   45.8  48.9        47.3\n MDAS     94.5 21.5   74.2 17.3   84.3   44.0     70.3 24.3   80.8  26.8        53.8\n\n                                       (c) Scores of verb attributes.\n\n   Method    Category     Color       Size      Texture    Iso Cause   Disentangle\n\n   Entity\n   PCA       45.6 49.8   35.1 63.7   27.7 50.5   26.3 47.5   33.7  52.9        43.3\n   SAE       94.2 7.9    34.2 63.2   95.0 3.0    95.3 29.0   79.6  25.8        52.7\n   RLAP     85.6 30.6   83.9 8.0    62.0 28.5   58.7 47.5   72.5  28.7        50.6\n  DBM      70.1 35.6   62.0 40.0   98.0 2.0    97.7 30.0   81.9  26.9        54.4\n   DAS       77.3 52.0   79.7 28.7   87.2 24.0   92.0 47.5   84.0  38.1        61.1\n  MDBM    59.8 48.5   53.5 59.2   74.5 27.5   81.2 49.0   67.3  46.1        56.7\n  MDAS     85.1 49.8   87.0 19.8   88.5 19.5   91.5 46.5   88.0  33.9        60.9\n   Context\n   PCA       43.1 66.8   40.3 63.3   30.8 46.5   25.4 68.0   34.9  61.1        48.0\n   SAE       39.9 70.0   43.8 62.2   91.4 6.0    90.9 34.5   66.5  43.2        54.9\n   RLAP     83.6 47.2   82.3 22.5   64.6 30.0   60.9 61.0   72.8  40.2        56.5\n  DBM      72.1 47.2   64.6 46.0   97.3 2.5    97.5 32.5   82.9  32.1        57.5\n   DAS       70.7 75.8   72.2 67.8   82.2 53.5   85.6 64.5   77.7  65.4        71.5\n  MDBM    64.3 59.0   60.6 59.7   78.6 33.0   83.2 59.5   71.7  52.8        62.2\n  MDAS     84.8 73.0   83.1 61.5   87.8 46.0   86.3 65.0   85.5  61.4        73.4\n\n                               (d) Scores of physical object attributes.\n\nMethod     Duty     Gender Bias    Industry   Work Location   Iso Cause   Disentangle\n\nEntity\nPCA       39.9 33.7   28.1  61.7    36.3 38.0   35.9   31.0      35.1  41.1        38.1\nSAE       68.9 4.0    57.1  49.0    61.7 10.5   64.3   13.0      63.0  19.1        41.1\nRLAP     62.1 17.7   93.8  44.0    58.9 18.5   62.0   18.0      69.2  24.5        46.9\nDBM      59.3 23.3   93.2  42.7    67.2 18.3   66.4   16.0      71.5  25.1        48.3\nDAS       59.8 23.0   83.7  75.7    57.9 29.3   57.9   27.0      64.9  38.7        51.8\nMDBM    52.0 35.3   81.7  66.0    57.8 29.5   59.3   24.5      62.7  38.8        50.8\nMDAS     82.5 12.0   85.0  70.0    82.5 17.5   83.7   14.5      83.4  28.5        56.0\nContext\nPCA       39.2 45.0   21.9  68.0    33.8 42.7   38.3   44.5      33.3  50.0        41.7\nSAE       66.7 7.7    47.7  61.0    58.9 14.3   65.1   14.5      59.6  24.4        42.0\nRLAP     60.3 23.0   92.5  51.0    56.7 23.3   62.3   24.0      68.0  30.3        49.1\nDBM      49.5 14.7   87.3  29.5    56.4 18.0   56.4   21.5      62.4  20.9        41.7\nDAS       46.9 49.7   79.7  85.0    44.2 55.3   46.0   46.0      54.2  59.0        56.6\nMDBM    43.6 22.7   77.7  70.5    54.2 31.3   60.9   27.0      59.1  37.9        48.5\nMDAS     78.7 32.0   81.0  85.5    70.1 38.7   74.1   27.0      75.9  45.8        60.9\n\n                                   (e) Scores of occupation attributes.\n\n                        Table 6: Per-task results, continued.\n\n\n\n\n\n                                18\n\n             (a) Cause score from RLAP.                (b) Cause score from DBM.       (c) Cause score from MDBM.\n\n          Figure 4: Additional feature disentanglement results for RLAP, DBM, and MDBM methods.\n\n\nC.2  Additional Attribute Disentanglement Results\n\nIn Figure 3, we show the feature entanglement results from DAS and MDAS. We provide additional\nresults from all other supervised methods: RLAP, DBM, and MDBM in Figure 4. Though these methods\nare trained on different objectives and identify different features FA, they show similar patterns in terms\nof entanglement between attribute representations. For all methods, representations of most attributes\nare entangled with “continent” (and “timezone”, which for most cases starts with the continent name).\nRepresentations of attributes such as “county–language” are also highly entangled.\n\n\n\n\n\n                                         19\n\n## Claim to Classify\n\"MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Interchange interventions / activation patching-based evaluation on RAVEL (II; used to compute Cause/Iso/Disentangle scores) comparing MDAS to baselines",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The supporting evidence for “achieves state-of-the-art results on RAVEL” comes from intervention-based metrics (interchange interventions) and benchmark comparisons, which are Rung 2 methods. However, the clause “demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations” is a mechanistic/general takeaway about how interpretability should work (implying distributed features are the right explanatory level), not merely that MDAS causally affects outputs under interventions. This is closer to a Rung 3 mechanistic/narrative claim (‘importance of’ distributed features) rather than a narrow causal-effect claim. No hedge language is present."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 28324,
      "completion_tokens": 205,
      "total_tokens": 28529
    },
    "finish_reason": "stop"
  }
}