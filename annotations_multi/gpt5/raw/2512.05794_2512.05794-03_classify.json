{
  "timestamp": "2026-02-19T09:08:58.942813+00:00",
  "paper_id": "2512.05794",
  "claim_id": "2512.05794-03",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2512.05794\nTitle: Mechanistic Interpretability of Antibody Language Models Using SAEs\nFull text:\n          Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n\n\n                    Rebonto Haque1  Oliver Turnbull1  Anisha Parsan2,3  Nithin Parsan2\n                                   John J. Yang2   Charlotte M. Deane1\n                               1Department of Statistics, University of Oxford, UK   2Reticular, San Francisco, USA\n            2Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (MIT), Cambridge, MA, USA\n                                                    Correspondence: deane@stats.ox.ac.uk\n\n                         Abstract                           (heavy) chain, and V and J for the VL (light) chain–give rise\n                                                                              to the final sequence diversity within the VH/VL chains. So-\n              Sparse autoencoders (SAEs) are a mechanistic                                                                   matic hypermutations, characterised by random nucleotide2025          interpretability technique that have been used to                                                                              substitutions occurring at rates considerably higher than the\n               provide insight into learned concepts within large                                                           genomic background, in the joint V(D)J segment further\n                protein language models. Here, we employ TopK                                                                          increases sequence diversity (Andreano & Rappuoli, 2021).Dec        and Ordered SAEs to investigate an autoregres-\n                 sive antibody language model, p-IgGen, and steer       The ability to bind any target antigen with high specificity\n5\n                    its generation. We show that TopK SAEs can re-        and affinity makes antibodies ideal candidates for drug dis-\n               veal biologically meaningful latent features, but         covery. As a result, antibody drugs hold a major and grow-\n              high feature–concept correlation does not guar-         ing share of the total pharmaceutical market (Crescioli et al.,\n               antee causal control over generation. In contrast,        2025). Antibody drug development pipelines need to iden-\n              Ordered SAEs impose an hierarchical structure           tify candidates which bind specifically and with high affinity\n                  that reliably identifies steerable features, but at the          to the target antigen, while also being ‘developable’ (Jarasch[cs.LG]\n              expense of more complex and less interpretable          et al., 2015). ‘Developability’ refers to properties required\n                activation patterns. These findings advance the          for a successful drug such as immunogenicity, solubility,\n               mecahnistic interpretability of domain-specific           specificity, stability, manufacturability, and storability (Ray-\n                protein language models and suggest that, while        bould & Deane, 2022).\n           TopK SAEs suffice for mapping latent features                                                                Antibody language models have been used to optimise mul-\n                to concepts, Ordered SAEs are preferable when                                                                                  tiple steps of antibody-drug development pipelines from\n                precise generative steering is required.                                                                             library generation (Turnbull et al., 2024) to humanisation\n                                                                      during lead optimisation (Chinery et al., 2024). p-IgGen is a\n                                                                   GPT-like decoder-only model trained on antibody-sequence\n          1. Introduction                                              data, consisting of 17M parameters (Turnbull et al., 2024).\n                                                         The authors released a paired model, as well as a finetuned\n          Antibodies are a key part of the body’s adaptive immune\n                                                                         version capable of generating diverse antibody libraries with\n           response. They are characterised by their ability to bind to a\n                                                                    developable properties.\n            specific antigen and subsequently neutralise it or initiate an\n        immune response. Their extensive sequence—and therefore   The lack of interpretability of machine learning models con-arXiv:2512.05794v1\n           structural—diversity enables binding to virtually any target     tributes to a lack of trust in model predictions, difficulty\n           antigen (Chiu et al., 2019).                                  determining whether biologically relevant features are being\n                                                                  used to make predictions and difficulty detecting overfitting.\n        The antigen-binding domain of antibodies is made up\n                                                                              Collectively, these pose a barrier when employing language\n           of variable heavy (VH) and variable light (VL) chains.\n                                                             models for drug discovery (Chen et al., 2023). SAEs offer\n          Antibody-antigen binding specificity and affinity are largely\n                                                                  a promising approach to identify human-interpretable con-\n          determined by structural units known as complementarity-\n                                                                       cepts learned by models and steer their generation (Chen\n          determining regions (CDRs), with each VH and VL chain\n                                                                                 et al., 2025; Templeton et al., 2024).  Prior works have\n          having 3 CDRs, making a total of 6. Within our genome,\n                                                                used SAEs to understand the inner mechanisms of PLMs\n           there are numerous V, D, and J gene segments which to-\n                                                       (Adams et al., 2025; Parsan et al., 2025b; Simon & Zou,\n           gether code for the VH and VL chains.\n                                                                    2024), and steer model output. However, to date, SAEs\n        The combinatorial assembly of the discrete gene seg-    have not been used to interrogate autoregressive protein or\n         ments–V (variable), D (diversity), and J (joining) for the VH\n\n\n                                                         1\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nantibody-specific language models.                                                                   ˆx = Wdec z + bdec                  (2)\n\nThis work advances the interpretability of antibody language                                                    where W are the weight matrices and b are the bias vectors,\nmodels, using SAEs to identify biologically relevant fea-                                                       enc and dec denote the encoder and decoder respectively,\ntures of interest learned by p-IgGen, and predictably steer\nits generation. We identify antibody-specific features, such   x is the original hidden representation, z the latent repre-\nas the complementarity-determining region (CDR) identity    sentation, and ˆx the reconstructed hidden representation.\nand germline gene identity, and use them to steer p-IgGen   ReLU activation is applied to the latent representation fol-\ngeneration for specific germline gene identities. Overall,    lowing encoding and g is a sparsification function.\nthis work shows the applicability of SAEs for incorporat-\ning rational design principles to antibody library generation.    2.2.1. TOPK SAES\nWe show that TopK SAEs can accurately identify inter-\n                                            TopK SAEs (Gao et al., 2024) limit the number of active\npretable latents underpinning model generation, whereas\n                                                                     latents to k, where k ≪din ≪dsae. din is the input hidden\nOrdered SAEs can identify steerable features capable of\n                                                          dimensions, and dsae is the latent or dictionary dimensions.\ntuning model generation.\n                                                        Equation 3 shows the loss computation.\n2. Related Work                                         L(x) =  ∥x −ˆx∥22  +       c             (3)\n                                                                                                            Reconstruction|  {z  }loss      Sparsity|{z}constraint\n2.1. Mechanistic Interpretability\n                                                The L(x) reconstruction loss compares the decoded repre-\nMechanistic interpretability refers to the approach of ex-    sentation ˆx with the original hidden representation x. When\nplaining complex machine learning systems through the    a sparsification function is not directly applied during encod-\nbehaviour of their functional units (K¨astner & Crook, 2024)     ing, a separate sparsity constraint is added in loss computa-\nby decomposing or reverse-engineering systems into their     tions, which is usually a variation of an L1 regularisation\nmore elementary computations (Rai et al., 2025). The even-    loss (Zhang et al., 2018).\ntual goal is to discover causal relationships between model\ninputs and corresponding outputs.                            2.2.2. ORDERED SAES (O-SAES)\n\nWithin the context of transformer-based language models,                                                     Ordered SAEs follow a nested SAE architecture, enabling\nthere are three main ideas relevant for mechanistic inter-                                                                  hierarchical ordering of SAE latents. Importantly, compared\npretability research: features, circuits and universality (Rai                                                                to the traditional TopK SAE architecture which arbitrarily\net al., 2025). Features refer to human-interpretable proper-                                                            orders hierarchical latents within the dictionary space, O-\nties that are encoded by model activations (Templeton et al.,                                            SAEs enforce a strict, consistent, hierarchical ordering of\n2024).  Circuits inform how these features are extracted                                                                        latents. This is because TopK SAEs enforce sparsity within\nfrom model inputs and processed to influence model out-                                                             the entire dictionary space in one go, whereas O-SAEs\nputs (Olah et al., 2020b). Finally, universality determines                                                          follow a nested approach and effectively train a number of\nwhether features and circuits identified for a specific model                                                                   individual, nested SAEs which occupy an increasing portion\nand task exist in other models and tasks (Olah et al., 2020a).                                                             of the dictionary space.\nThis paper specifically focuses on the identification of fea-\ntures from language models and using these features to steer   O-SAEs introduce two core components:  (i) per-index\nmodel generation.                                          nested grouping, and (ii) strictly decreasing truncation\n                                                          weights in order to ensure consistent ordering.\n2.2. Sparse Autoencoders                                              (i) For each truncation level m ∈{1, . . . , dsae}, the first m\n                                                     rows of the encoder and decoder are isolated:Sparse Autoencoders (SAEs) have specifically been em-\nployed in mechanistic interpretability for feature discovery.            (m)                   (m)                              W enc = [Wenc]1:m, :, Wdec = [Wdec]1:m, :    (4)\nThey are able to tackle the issue of feature superposition\nresulting in polysemantic neurons, where any given neuron    In Eq. (4) the encoder–decoder pair W enc(m) , W dec(m)  re-uses\nencodes multiple, often unrelated features. SAEs overcome    the first m rows of the full weight matrices. Because every\nthis problem by projecting dense neuron activations into    smaller autoencoder is a strict subset of the larger one, any\na sparser latent space using a sparse encoder, Equation     latent i ≤m is shared across all groups that follow. This\n1, whilst ensuring the latent representation can be recon-   “per-index nested grouping” forces early latents to model\nstructed back into the original neuron representation by a    global structure that remains useful for every deeper stage.\ndecoder following sparsification, Equation 2.                 Per-index grouping ensures non-random sampling of dic-\n                                                               tionary sizes, unlike in Matryoshka SAEs (Bussmann et al.,\n            z = g(ReLU Wenc x + benc )            (1)    2025), increasing the overall consistency of results.\n\n                                                2\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n(ii) Each partial reconstruction is weighted by a monoton-    used expansion factor r = 8, yielding a dictionary size\nically decreasing probability pM(m), so that early (low-    dsae = din × r = 768 × 8 = 6,144. Sparsity was again set\nindex) features incur a higher penalty when failing to cap-    to k = 32, ensuring the top 32 latents are used during recon-\nture coarse structure. The per-truncation loss is                 struction. All models were trained with Adam optimisers\n                                                  2             at a fixed learning rate η = 1 × 10−4. We chose a smaller\n      Lm(x) = pM(m)  x −W dec(m)⊤ W enc(m) x        (5)   maximum dictionary size for the O-SAEs to speed up train-\n                                                  2\n                                                                 ing, effectively reducing the total number of nested SAEs\nand summing over all m promotes the model to learn the                                                         being trained. Due to per-index grouping, O-SAEs need to\nmost “abstract” elements first, with progressively finer de-                                                                      train several nested SAEs based on the total dictionary size,\ntails later. Combining the decreasing probability weights                                                     whereas the regular TopK architecutre only trains a single\nwith nested latents further enforces ordering of identified                                                       model.\nlatents and maintains a stricter hierarchy.\n\n                                                                    3.2. Targeted Feature Identification using SAEs\n3. Methods\n                                                             3.2.1. TRAINING DATA\n3.1. Sparse Autoencoder Training\n                                                           Paired antibody sequence data were obtained from OAS,\nWe  adapted  TopK  Sparse  Autoencoders  (SAEs)    Coronavirus Antibody Database (CoV-AbDab) (Raybould\nfrom   the   EleutherAI/sparsify  GitHub   repository     et  al., 2021), and the Patent and Literature Antibody\n(https://github.com/EleutherAI/sparsify),  and  Ordered    Database (PLAbDab) (Abanades et al., 2024). A total\nSAEs from the Reticular GitHub repository (Parsan et al.,    of 149,069 sequences were obtained from the respective\n2025b). Training parameters were taken directly from the     datasets, based on their binding specificities to SARS-CoV2\noriginal repositories.                         RBD (binder and non-binder).\n\nWe trained both the TopK and Ordered SAEs on hidden   The data was clustered based on CDR sequence similarity\nlayer activations of p-IgGen, generated from the original    using CD-HIT (Li et al., 2001), with a 0.8 similarity thresh-\npIg-Gen training set (Turnbull et al., 2024). The training set    old on the total CDR sequence. The clusters were then\ncontained 1,800,545 VH/VL paired sequences from the Ob-   randomly split into the training-validation-test set, whilst\nserved Antibody Space database (OAS) (Olsen et al., 2022;    ensuring members of the same cluster were in only one of\nKovaltsuk et al., 2018). During training, we concatenated    the three possible splits. The splits were further stratified\nthe paired VH and VL sequences together, with appropriate    based on binding specificity to SARS-CoV2 RBD.\nstart and end tokens added, and passed them into p-IgGen to\n                                                         This specific dataset was originally prepared for a separate\ngenerate hidden activations. This generated 4 sets of hidden                                                                  project, and the SARS CoV2 RBD binding properties of\nactivations, one from each hidden layer. For Ordered SAE                                                              the antibodies are not relevant for this study. Qualitatively,\ntraining, we randomly subsampled 100,000 sequences to                                                        a dataset of equivalent size randomly sampled from OAS\ndecrease training time.                                                         should produce the same results.\n\n3.1.1. TOPK SAE                                  The following concepts were studied to identify associated\n                                                                     latents: CDR identity, which refers to whether a given\np-IgGen input dimensions din = 768 were projected onto                                                            residue lies within a specific CDR region, and V/J gene\na higher-dimensional latent/dictionary size dsae, where                                                                     identity, which refers to the germline V or J gene segment\ndsae = din × r = 768 × 32 = 24,576. r = 32 is the expan-                                                was used to code for the final antibody sequence. For the\nsion factor. ReLU activation was applied to the projection,                                                           CDR-identity, the training matrix was the latent activations\nz = ReLU Wenc x + benc  , followed by a Top-k sparsifi-                                                                for each residue. The CDR identity dataset had 7 classes\ncation with k = 32, retaining only the top 32 activations                                                            (6 CDR identities and non-CDR regions). For sequence-\nby magnitude. The resulting dictionary size was 24,576.                                                                    level concepts, V/J gene identity, the training matrix was the\nDecoder weights were initialised as the unit-normalised                                              mean pool of the residue-level latent activations in a given\ntranspose of the encoder weights to stabilise training. Train-                                                          sequence.\ning used a batch size of 8 and Adam optimisers throughout,\n                                2×10−4with a custom learning rate η = √                                                                          .                3.2.2. LINEAR PROBE                                       dsae/16,384\n\n                                      We trained a logistic regressor to act as a linear probe on\n3.1.2. ORDERED SAE\n                                                             the training-validation data. A logistic regressor (LR) was\nOrdered Sparse Autoencoders (O-SAEs) were adopted to     trained, employing 3-fold cross-validation grid search to\nretain higher-level, abstract features within our latent space    optimise hyperparameter C. In logistic regression, C is\nand hierarchically arrange the latents.  In our setup, we    the inverse of the regularisation strength: larger C applies\n\n                                                3\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nless regularisation and can overfit, while smaller C applies    within the latents, we compared the accuracies of using\nmore regularisation and can improve generalisation. Cross-    latents and hidden neurons on residue and sequence-level\nvalidation was done during training by randomly shuffling    property prediction tasks.\nand splitting the training data into 3 cross-validation sets.\n                                      We began the analysis by focusing on TopK SAEs trained\nCorrelation weights of all latents were stored and the latents\n                                                  on the final layer activations (layer 3), as seen in previous\nwith the top 500 positive correlation weights were used for\n                                                   works (Parsan et al., 2025a), since they contain the most\nfurther validation.\n                                                        complete representation of the sequence. Logistic regressor\n                                                                  training on CDR identities using latent activations obtained\n3.2.3. LATENT SELECTION\n                                                        a validation accuracy of 0.99, while using hidden neuron\nThe top correlated latents were further validated on the     activations resulted in a validation accuracy of 0.98. This\nvalidation set. Based on the strategy by Simon and Zhou    showed that residue-level CDR identity information is pre-\n(Simon & Zou, 2024), the latent activations across the vali-    served in the activated latents.\ndation set were normalised using MinMax scaling; for each\n                                                            In order to check sequence-level features, we investigated\nnormalised latent, binary latent-on/latent-off labels using\n                                                          germline gene predictions. We focused on heavy J genes for\nactivation thresholds of 0.1, 0.2, 0.5, 0.8, 0.9 were applied.\n                                                                 simplicity, given they have less allelic variation compared\nFor each latent-concept pair, a latent was defined as an\n                                                                to heavy V genes. LR training resulted in a validation F1\ninterpretable feature if its F1 score for any of the tested\n                                                    macro score of 0.93. We used an F1 score due to significant\nthresholds was greater than 0.5. At this boundary the har-\n                                                                class imbalance in heavy J genes within the training and\nmonic mean guarantees both precision and recall are at least\n                                                              validation sets. Table 1 reports the precision, recall and\n0.5, ruling out latents that are either mostly false positives\n                                                           F1-scores of the different gene identities studied and shows\nor that miss the majority of true activations.\n                                              how the model accurately predicts each IGHJ class using the\n                                       SAE latents. Overall, the high F1 macro score shows how\n3.2.4. ANTIBODY SEQUENCE ALIGNMENT\n                                                              the activated latents also preserve sequence-level features.\nAntibody sequences were aligned using ANARCI (Dunbar                                                 Our results indicate that SAE latents collectively represent\n& Deane, 2016) and the IMGT numbering (Lefranc et al.,                                                           antibody information following sparsification when applied\n2003).                                                                to an antibody language model, similar to results seen in\n                                                            previous works for general protein language models (Simon\n3.3. Steering                                 & Zou, 2024; Adams et al., 2025; Parsan et al., 2025a). This\nSteering was implemented based on the strategy by Tem-     justifies using SAE latents for further analysis to investigate\npleton et al. (Templeton et al., 2024). Each latent can be     their overall interpretability.\nrepresented by its corresponding decoder vector d(i) =\nWdec[i, :], where d(i) is the decoder vector for latent i and         Table 1. Precision, recall, and F1-score per IGHJ class.\nWdec is the decoder weight matrix. Steering is performed\n                                                               Class            Precision   Recall   F1-score\nby scaling the decoder vector and adding it to the original\nhidden state (Equation 6).                                IGHJ1            0.91      0.71      0.80\n                                                   IGHJ2            0.94      0.93      0.94\n          hl∗←hl + α · d(i)                (6)       IGHJ3            0.96      0.96      0.96\n                                                   IGHJ4            0.98      0.99      0.98\nHere, α is the steering factor and hl is the hidden state                                                   IGHJ5            0.94      0.95      0.95\nbefore the intervention and hl∗is the hidden state following                                                   IGHJ6            0.98      0.97      0.98\nthe intervention.\n                                                    Macro average     0.95      0.92      0.93\n\n4. Results\n\n4.1. TopK SAE-identified Features are Interpretable,       4.1.2. TOPK LATENT ACTIVATIONS ARE VISUALLY\n    Antibody-specific Concepts, but Not Steerable             INTERPRETABLE\n\n4.1.1. TOPK SAE LATENTS PRESERVE BIOLOGICAL       To investigate whether SAE latents provide an interpretable\n     INFORMATION FOLLOWING SPARSIFICATION          alternative to understanding model generation, we compared\n                                                             the activated patterns of latents and neurons correlated to\nTopK sparsification represents each token with far fewer la-\n                                                              properties of interest. As a baseline, we compared activa-\ntents compared to the hidden neurons, creating a possibility\n                                                                 tions of the top correlated latents and neurons for CDRH3.\nfor information loss during sparsification. To qualitatively\ncheck whether antibody-specific information is retained    Visual investigation revealed latent activations are sparse\n\n                                                4\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nand specific to CDRH3 residues, compared to neurons    concepts rather than an undefined set of all possible ones.\nwhich activate across the sequence without any immediately    Here, we examine gene identity, as it directly influences\nrecognisable pattern (Figure 1). This may be explained    antibody binding affinity and specificity(Deng et al., 2025).\nby the polysemanticity of neurons, where multiple features     Specifically, we pick IGHJ4 genes for our analysis, due to\nspecific to several unrelated residues are represented by     their widespread clinical significance underpinned by the\nthe same neuron. When investigating activation patterns,    fact that they are the most widely utilised J genes in our\nthis complicates using neurons as a tool for interpretability   immune repertoire. That is, the majority of heavy chain\nand highlights the potential greater explainability of SAE-    antibodies within any given individual originate from the\nderived latents.                                     IGHJ4 germline gene. For our analysis, we decided to focus\n\nIn order to check the utility of SAE activations for the mech-\n                                                IGHJ     Layer    Features (F1 > 0.5)  Max F1-scoreanistic interpretability of sequence-level concepts, as op-\nposed to residue-level concepts, we investigated heavy J\ngene activations as sequence-level concepts.  Similar to              Layer 0         197              0.930\nresidue-level observations, latents corresponding to heavy              Layer 1         266              0.949\n                                                 IGHJ4\nJ gene identity activate on residues representing the con-              Layer 2         189              0.930\ncept, i.e. gene identity. In this instance, the top correlated              Layer 3         85              0.752\nlatents were activated on the J domain of examined antibody\nsequences. Sequence-level representations are mean pools    Table 3. IGHJ4 feature statistics. Latents which can be used as\nof the original residue-level representations, leading to an    a binary predictor of gene identity with an F-score greater than\noverall loss of positional information. Therefore, the top     0.5, based on threshold-activation patterns are termed as ’features’.\n                                                             For each layer in p-iGgen, features corresponding to IGHJ4 werecorrelated latents also encode intrinsic positional informa-\n                                                                              identified. The maximum F-score of all the identified features from\ntion (Figure 1c). This provides an opportunity to identify\n                                                               a given layer are reported as Max F-score.\nthe residues responsible for a global, sequence-level feature,\nwith potential implications in understanding the sequence                                                   on the final layer (Table 3).\nand structural basis of antibody properties.\n                                                                        First, we looked at the absolute positional activations of\nTo quantify the predictive properties of our identified fea-                                                                     this latent across all the sequences in our validation set\ntures, we carried out an activation-threshold analysis (Table                                                   which had an IGHJ4 heavy J gene, and compared it to\n2, Methods 3.2.3). Interestingly, there was a strong prefer-                                                                activations on specific IMGT positions. Whilst activations\nence for IGHJ4 features in the final layer, with no features                                                  on absolute positions were distributed near the end of the\nbeing identified for IGHJ1 and 5. However, we identified                                                    heavy chain corresponding to the J region, the activations\nfeatures correlated to the two identities in earlier layers. This                                                   on IMGT positions were more consistent and concentrated.\nhighlighted a potential flaw in TopK SAEs; SAEs cannot                                                        This implies that the model does not base its activation\nconsistently generate monosemantic features for all the con-                                                                 pattern on the absolute sequence length alone, but rather the\ncepts represented in the hidden state. Given latents could                                                           underlying sequence alignment (Figure 2).\ncollectively be used to predict IGHJ1 and IGHJ5 2, the\nunderlying information was represented within the latent   We then chose to investigate the specific residue identities on\nspace, just not in the form of a single monosemantic latent.    which the latents were activated. Based on the heavy J gene\n                                                        sequence alignments, the top two latents activated at IMGT\n                                                              positions 120 and 119, which are a Q and G, respectively.\nTable 2. Layer 3 feature counts and maximum F-scores for IGHJ    These are conserved across all human IGHJ genes. The third\ngenes                                                             top latent activated on Y at position 117, which is unique for\n    Gene   Number of features  Max F-score                                               IGHJ4 (Scaviner et al., 1999). These results indicated that\n     IGHJ1                  0          0.366                                                            top latents encoded contextual information of the preceding\n     IGHJ2                  5          0.521                                                                 residues.\n     IGHJ3                 12          0.866\n     IGHJ4                 85          0.752         Previous studies have highlighted how highly correlated\n     IGHJ5                  0          0.486          features may be used to steer model outputs (Templeton\n     IGHJ6                 17          0.866            et al., 2024; Simon & Zou, 2024). We attempted to steer on\n                                                      each identified feature to investigate how it affects model\n                                                              generation. We positively steered on each latent, which we\n                                                         hypothesised should increase the proportion of IGHJ4 in\n4.2. Case Study of Heavy J Gene Identity for IGHJ4\n                                                          generated sequences. However, steering on these latents\nThe functional importance of SAEs for studying LLMs   was unpredictable and did not consistently increase IGHJ4\nlies in their ability to interrogate specific, domain-relevant    proportions (Figure 3).\n\n                                                5\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n(c)\n\n\n\n\n\nFigure 1. Latent activations (a) and neuron activations (b) for CDRH3 identity, and latent activations for IGHJ3 (c). The x-axis shows the\namino-acid sequence of the VH region of a test antibody; the y-axis shows normalised activation. CDRs are coloured CDRH1 (red),\nCDRH2 (blue), and CDRH3 (green). Latent activations localise to the expected regions—CDRH3 in (a) and the heavy J region in\n(c)—whereas neuron activations (b) are scattered across the sequence with no discernible pattern.\n\n\nTo check if this phenomenon was somehow exclusive for    enforcing sparsity in a dictionary consisting of hierarchical\nIGHJ4 and layer 3, we attempted to steer across all the     features. In this instance, if the identified latents correspond\nlayers for a number of different features for various gene     to only single residues within the J-domain, it essentially\nidentities, but were unable to predictably steer model gener-   becomes a residue-level feature as opposed to a sequence-\nation [data not shown]. The lack of steerability may indicate     level feature. If the feature activates on a residue specific\nhow these features individually do not contribute to the gene     to the gene identity, it may be a good predictor for the gene\nidentity, making them informative features when used for     identity, but not a steerable feature. This points to the pos-\ndownstream predictions, but not for biasing model output.      sibility that several features together confer J gene identity,\n                                                    and that these features are likely correlated to each other.\nThis may be due to feature splitting (Chanin et al., 2024)\n                                                       Hence, activating one but not the others does not necessarily\nwhich has been reported for TopK SAEs. Feature splitting\n                                                                     result in a predictable shift in model performance.\nrefers to the phenomenon where higher-order features are\nbroken down into specific contextual examples. In the case   The case study on IGHJ4 indicated that identified features re-\nof text-based language models, ’math’ may be split into     tain biologically relevant context information. Most highly\n’algebra’ and ’geometry’. These phenomena arise when    correlated features (based on LR correlation weight and f-\n\n\n                                                6\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n   (a)                                                           (b)\n\n\n\n\n\nFigure 2. Comparison of absolute positional (a) and IMGT (b) activations of top three IGHJ4 latents. The sequence/IMGT positions are\nshown on the x-axis. For the sequence positions, the amino acid sequences were end-padded to a constant length of 350. Percentage of\ntotal activations on any given position across validation IGHJ4 sequences is shown on the y-axis. The most frequent IMGT position for\nactivation is highlighted for each latent. Latent activations show a distribution near the end of the heavy chain when aligned based on\nabsolute sequence position. In contrast, latents demonstrate discrete activations when aligned based on IMGT numbering.\n\n\nscore) tend to be residue-specific. Targeted approaches such   One of the reasons behind the lack of steerability of TopK\nas this cannot easily find abstract, higher-order features, as-    latents may stem from feature splitting (Chanin et al., 2024),\nsuming they are represented within the latent space to begin    as highlighted earlier. A solution may be to use nested\nwith. Concept-specific targeted feature identification might     architectures which limit feature splitting, allowing greater\nidentify highly correlated features that are biologically infor-    steerability (Chanin et al., 2024).\nmative. For instance, two of the three top features (463 and\n4720) activate on conserved residues preceded by sequence     4.3. Ordered SAEs Identify More Steerable Features\nmotifs specific to the gene identity. The third, Latent 6276      Compared to TopK SAEs\nactivated on an IGHJ4-specific residue, which may explain\n                                                   Based on the previous case study, it is evident that the F1why this feature can be used to accurately identify IGHJ4.\n                                                           score alone is not a good metric for predicting the impor-\nHighly predictive features may be correlated with other    tance of a feature in informing model generation. Feature\nbiologically informative features. To understand whether    steering and ablation are likely to be better indicators, es-\nhighly predictive features influence model behaviour, we    pecially positively steering generation. Following unsuc-\ntried to steer along these features to increase the proportion     cessful steering with TopK SAEs, we tested Ordered SAEs,\nof IGHJ4 in generated sequences. This did not produce    which construct a hierarchical latent space capable of re-\npredictable results, making it difficult to interpret the contri-    taining both high-level and fine-grained features (Bussmann\nbution of each individual latent to model generation. Over-     et al., 2025).\nall, TopK SAEs can identify features in targeted concept\n                                      We conducted a linear probe and subsequent activation-analysis which are intuitively interpretable, however, not\n                                                            threshold analysis to identify features correlated to IGHJ4necessarily steerable.\n                                                                in layer 3. Due to the implicit hierarchy in features, we\n\n\n                                                7\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n(a)                                                               features. We identified latent 12, which was positively cor-\n                                                                related with IGHJ4, and latent 49, which was negatively\n                                                                  correlated, and steered on these features (Figure 4).\n\n                                                                 Positively steering on latent 12 increased IGHJ4 proportion\n                                                                  in model generation (Pearson’s R = 0.939, p = 6.458×10−7\n                                                    and Spearman’s correlation = 0.921, p = 2.982 × 10−6).\n                                                             Conversely, positively steering on latent 49 decreased IGHJ4\n                                                           proportion in model generation (Pearson’s R of -0.705, p-\n                                                          value = 4.89 × 10−3 and Spearman’s correlation = -0.657,\n                                                   p = 0.0106).\n\n                                                To investigate the features, we plotted their activations on\n                                                                   specific IMGT positions across a random set of IGHJ4 con-\n                                                               taining sequences (Figure 5.) The latents activate more\n                                                          broadly across the sequence, unlike TopK latents. One rea-\n(b)\n                                                        son for this may be the abstraction which enables steerability.\n                                                The latents inform downstream generation, and therefore\n                                                                 signal early on during sequence generation about the identity\n                                                              of the J-region. Given the latents communicate longer-range\n                                                            concepts, it is possible that latent activation is not linked\n                                                                to any specific residue, rather a range of residues within a\n                                                            defined length of the sequence.\n\n                                                        5. Conclusions and Future Outlook\n\n                                       We show that SAEs can successfully be used to evaluate au-\n                                                                 toregressive antibody language models and identify learned\n                                                            domain-specific features. Current SAE implementations on\n                                           pLMs for identifying features are limited by the assumption\n(c)                                                               that feature correlation is equal to biological concept cau-\n                                                                  sation. Analysis of activated latents reveal high predictive\n                                                       performance does not always correspond to steerability.\n\n                                       We found that some latents correlated to gene identity sim-\n                                                            ply mark conserved positions without a clear causal link to\n                                                     gene identity. Some highly correlated features may high-\n                                                                    light non-specific aspects of a concept, but be correlated\n                                                                to more concrete features. When we attempted to steer p-\n                                                  IgGen by amplifying these individual latents, the resulting\n                                                           antibody libraries did not show a reliable increase in IGHJ4\n                                                             usage, further underscoring that high predictive power alone\n                                                             doesn’t guarantee steerability.\n\n                                                       Ordered SAEs solve this by identifying hierarchical features\n                                                     which correspond to more abstract and higher-level concepts\n                                                                rather than simple residue-level identity.  However, this\nFigure 3. Results of IGHJ4 feature steering for latent 463 (a), 4720\n                                                 comes at the cost of intuitive interpretability of activation\n(b), 6276 (c). Y-axis shows the proportion of generated sequences.\n                                                                   patterns.Plots are coloured by heavy J gene identity. X-axis shows the\nsteering factor used (alpha). Results are for a library of 1000 p-   One of the major limitations of employing SAEs to antibody\nIgGen-generated sequences. For each latent tested (a-c), steering                                                         language models is the lack of labelled datasets, unlike gen-\ndid not result in a predictable change in library composition.                                                                   eral proteins (Suzek et al., 2007). This prevents automated\n                                                               feature identification and annotation, which has allowed\n                                                                 for their easy application for general pLMs (Simon & Zou,\nranked features with an F-score > 0.5 based on their dictio-    2024). Lack of functionally annotated data remains a prob-\nnary index, with smaller indices representing higher-level\n\n                                                8\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n  (a)                                                        (b)\n\n\n\n\n\nFigure 4. Results of IGHJ4 steering using Ordered latent 12 (a) and 49 (b). Y-axis shows the proportion of generated sequences. Plots\nare coloured by heavy J gene identity. X-axis shows the steering factor used (alpha). Results are for a library of 1000 p-IgGen-\ngenerated sequences. Latent 12—positively correlated with IGHJ4—increases IGHJ4 proportion under positive steering, whereas latent\n49—negatively correlated—decreases IGHJ4 under the same steering.\n\n\nlem within the domain of antibody language models.\n\nSeveral open problems remain to enable the use of SAEs\nfor PLM interpertability. First, exhaustive steering analyses\nshould be conducted to quantitatively measure the steerabil-\nity of Top-K and Ordered SAE features. Secondly, SAE\ntraining should be scaled using more expansive, annotated\ndatasets like FLAb (Chungyoun et al., 2024) to enable mod-\nels to identify domain-specific abstractions and facilitate\ntargeted manipulation of generative outputs. This would\nenable the generation of antibody libraries with properties\nsuch as developability and specificity, improving modern li-\nbrary generation methods and optimising the antibody drug\ndevelopment pipeline.\n\n\n\n\n\n                                                9\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\n\n\n\n\n    (a)\n\n\n\n\n\n    (b)\n\n\n\n\n\nFigure 5. IMGT activations of latent 12 (a) and 49 (b). Activation patterns of both latents show scattered distribution across the range of\nIMGT positions.\n\n\n\n\n\n                                                10\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nReferences                                  nucleic-acids/abstract/S2162-2531(23)\n                                               00039-2. Publisher: Elsevier.\nAbanades, B., Olsen, T. H., Raybould, M. I. J., Aguilar-\n  Sanjuan, B., Wong, W. K., Georges, G., Bujotzek, A.,    Chinery, L., Jeliazkov,  J. R., and Deane, C. M.  Hu-\n  and Deane, C. M.  The Patent and Literature Anti-     match  -  fast,  gene-specific  joint  humanisation  of\n  body Database (PLAbDab): an evolving reference set      antibody heavy and  light  chains, September 2024.\n  of functionally diverse, literature-annotated antibody se-    URL  https://www.biorxiv.org/content/\n  quences and structures.  Nucleic Acids Research, 52     10.1101/2024.09.16.613210v1.        Pages:\n  (D1):D545–D551, January 2024.  ISSN 0305-1048.      2024.09.16.613210 Section: New Results.\n   doi: 10.1093/nar/gkad1056. URL https://dx.doi.\n  org/10.1093/nar/gkad1056. Publisher: Oxford    Chiu, M. L., Goulet, D. R., Teplyakov, A., and Gilliland,\n  Academic.                                          G. L.   Antibody Structure and Function:  The Ba-\n                                                                         sis for Engineering Therapeutics. Antibodies, 8(4):55,\nAdams, E., Bai, L., Lee, M., Yu, Y., and AlQuraishi,     December 2019.  ISSN 2073-4468.   doi:  10.3390/\n  M. From Mechanistic Interpretability to Mechanistic      antib8040055.  URL https://www.mdpi.com/\n  Biology: Training, Evaluating, and Interpreting Sparse     2073-4468/8/4/55. Number: 4 Publisher: Multi-\n  Autoencoders on Protein Language Models, Febru-       disciplinary Digital Publishing Institute.\n  ary 2025.  URL https://www.biorxiv.org/\n  content/10.1101/2025.02.06.636901v1.      Chungyoun,  M.,  Ruffolo,  J., and Gray,  J.   FLAb:\n  Pages: 2025.02.06.636901 Section: New Results.           Benchmarking  deep  learning  methods  for   anti-\n                                                    body  fitness  prediction,  January  2024.   URL\nAndreano,  E. and  Rappuoli,  R.   Immunodominant                                            https://www.biorxiv.org/content/\n  antibody  germlines  in COVID-19.    The  Journal                                             10.1101/2024.01.13.575504v1.        Pages:\n   of Experimental Medicine, 218(5):e20210281, March                                                          2024.01.13.575504 Section: New Results.\n  2021. ISSN 0022-1007.  doi: 10.1084/jem.20210281.\n URL https://www.ncbi.nlm.nih.gov/pmc/    Crescioli, S.,   , H´el`ene, K.,   , Lin, W.,   , Jyothsna, V.,   ,\n  articles/PMC7933983/.                                 Vaishali, K., , and Reichert, J. M. Antibodies to watch\n                                                                  in 2025. mAbs, 17(1):2443538, December 2025. ISSN\nBussmann, B., Nabeshima, N., Karvonen, A., and Nanda, N.                                                        1942-0862.    doi:  10.1080/19420862.2024.2443538.\n  Learning Multi-Level Features with Matryoshka Sparse                                       URL  https://doi.org/10.1080/19420862.\n  Autoencoders, March 2025. URL http://arxiv.                                              2024.2443538. Publisher: Taylor & Francis  eprint:\n  org/abs/2503.17547. arXiv:2503.17547 [cs].                                                             https://doi.org/10.1080/19420862.2024.2443538.\nChanin, D., Wilken-Smith, J., Dulka, T., Bhatnagar, H., and\n                                                   Deng, W., Niu, X., He, P., Yan, Q., Liang, H., Wang,\n  Bloom, J. A is for Absorption: Studying Feature Split-\n                                                                      Y., Ning,  L., Lin,  Z., Zhang,  Y., Zhao, X., Feng,\n   ting and Absorption in Sparse Autoencoders, Septem-\n                                                                      L., Qu,  L., and Chen, L.  An allelic atlas of im-\n  ber 2024. URL http://arxiv.org/abs/2409.\n                                                       munoglobulin heavy chain  variable regions  reveals\n  14507. arXiv:2409.14507 [cs].\n                                                           antibody  binding  epitope  preference  resilient  to\nChen, J.-Y., Wang, J.-F., Hu, Y., Li, X.-H., Qian, Y.-R.,     SARS-CoV-2 mutation  escape.    Frontiers  in Im-\n  and Song, C.-L. Evaluating the advancements in protein      munology, 15:1471396, January 2025.  ISSN 1664-\n  language models for encoding strategies in protein      3224.    doi:  10.3389/fimmu.2024.1471396.  URL\n  function prediction: a comprehensive review. Frontiers     https://www.frontiersin.org/journals/\n   in Bioengineering and Biotechnology, 13, January 2025.    immunology/articles/10.3389/fimmu.\n  ISSN 2296-4185.  doi:  10.3389/fbioe.2025.1506508.     2024.1471396/full. Publisher: Frontiers.\n URL  https://www.frontiersin.orghttps:\n                                                      Dunbar, J. and Deane, C. M. ANARCI: antigen receptor\n  //www.frontiersin.org/journals/\n                                                      numbering and receptor classification. Bioinformatics,\n  bioengineering-and-biotechnology/\n                                                          32(2):298–300, January 2016. ISSN 1367-4803.  doi:\n  articles/10.3389/fbioe.2025.1506508/\n                                                             10.1093/bioinformatics/btv552. URL https://doi.\n  full. Publisher: Frontiers.\n                                             org/10.1093/bioinformatics/btv552.\nChen, W., Liu, X., Zhang, S., and Chen, S.  Artificial\n                                                    Gao,  L., Tour, T. D.  l., Tillman, H., Goh, G.,  Troll,   intelligence for drug discovery: Resources, methods,\n                                                                  R., Radford,  A., Sutskever,  I., Leike,  J., and Wu,  and applications.  Molecular Therapy Nucleic Acids,\n                                                                                 J.     Scaling  and  evaluating  sparse  autoencoders,  31:691–702, March 2023.  ISSN 2162-2531.   doi:\n                                                        June 2024. URL http://arxiv.org/abs/2406.  10.1016/j.omtn.2023.02.019.  URL https://www.\n                                                 04093. arXiv:2406.04093 [cs] version: 1.  cell.com/molecular-therapy-family/\n\n\n                                                11\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nJarasch,  A.,  Koll,  H., Regula,  J.  T., Bader, M., Pa-    com/doi/abs/10.1002/pro.4205.        eprint:\n  padimitriou, A., and Kettenberger, H.  Developability       https://onlinelibrary.wiley.com/doi/pdf/10.1002/pro.4205.\n  Assessment  During  the  Selection  of Novel  Ther-\n                                                            Parsan, N., Yang, D. J., and Yang, J. J.  Towards Inter-  apeutic  Antibodies.     Journal  of  Pharmaceutical\n                                                                   pretable Protein Structure Prediction with Sparse Autoen-  Sciences,  104(6):1885–1898,  June  2015.    ISSN\n                                                               coders, March 2025a. URL http://arxiv.org/  0022-3549,  1520-6017.     doi:   10.1002/jps.24430.\n                                              abs/2503.08764. arXiv:2503.08764 [q-bio]. URL    https://jpharmsci.org/article/\n  S0022-3549(15)30084-8/abstract. Publisher:                                                            Parsan, N., Yang, D. J., and Yang, J. J.  Towards Inter-\n   Elsevier.                                                                pretable Protein Structure Prediction with Sparse Au-\n                                                              toencoders.  In ICLR 2025 Workshop on GenerativeKovaltsuk, A., Leem, J., Kelm, S., Snowden, J., Deane,\n                                                     and Experimental Perspectives for Biomolecular Design  C. M., and Krawczyk, K. Observed Antibody Space: A\n                                                  (GEM), 2025b. URL https://arxiv.org/abs/  Resource for Data Mining Next-Generation Sequencing\n                                              2503.08764.  eprint: 2503.08764.  of Antibody Repertoires. The Journal of Immunology,\n  201(8):2502–2509, October 2018.  ISSN 0022-1767.                                                             Rai,  D.,  Zhou,  Y.,  Feng,   S.,  Saparov,  A.,  and\n   doi:   10.4049/jimmunol.1800708.   URL https:                                                         Yao,  Z.  A  Practical Review  of Mechanistic  In-\n  //doi.org/10.4049/jimmunol.1800708.                                                                      terpretability for Transformer-Based Language Mod-\n   eprint:    https://academic.oup.com/jimmunol/article-                                                                            els, March 2025. URL http://arxiv.org/abs/\n  pdf/201/8/2502/61431152/ji1800708.pdf.                                              2407.02646. arXiv:2407.02646 [cs].\nK¨astner, L. and Crook, B. Explaining AI through mech-\n                                                      Raybould, M.  I.  J. and Deane, C. M.   The Thera-\n   anistic interpretability. European Journal for Philoso-\n                                                               peutic Antibody ProfilerTherapeutic antibody profiler\n  phy of Science, 14(4):52, October 2024. ISSN 1879-\n                                                    (TAP) for Computational Developability Assessment.\n  4920. doi: 10.1007/s13194-024-00614-4. URL https:\n                                                              In Houen, G. (ed.), Therapeutic Antibodies: Methods\n  //doi.org/10.1007/s13194-024-00614-4.\n                                                    and Protocols, pp. 115–125. Springer US, New York,\nLefranc, M.-P., Pommi´e, C., Ruiz, M., Giudicelli, V.,     NY, 2022. ISBN 978-1-0716-1450-1.  doi: 10.1007/\n  Foulquier, E., Truong, L., Thouvenin-Contet, V., and      978-1-0716-1450-1 5. URL https://doi.org/10.\n  Lefranc, G. IMGT unique numbering for immunoglob-    1007/978-1-0716-1450-1_5.\n   ulin and T cell receptor variable domains and Ig super-\n                                                      Raybould, M.  I.  J.,  Kovaltsuk,  A., Marks,  C., and\n  family V-like domains. Developmental and Comparative\n                                                       Deane, C. M.   CoV-AbDab:  the coronavirus an-\n  Immunology, 27(1):55–77, January 2003. ISSN 0145-\n                                                             tibody  database.     Bioinformatics,  37(5):734–735,\n  305X. doi: 10.1016/s0145-305x(02)00039-3.\n                                          May 2021.   ISSN 1367-4803,  1367-4811.    doi:\nLi, W., Jaroszewski, L., and Godzik, A.  Clustering of      10.1093/bioinformatics/btaa739.    URL  https:\n  highly homologous sequences to reduce the size of large     //academic.oup.com/bioinformatics/\n  protein databases.  Bioinformatics (Oxford, England),     article/37/5/734/5893556.\n  17(3):282–283, March 2001.  ISSN 1367-4803.  doi:\n                                                            Scaviner, D.,  Barbi´e,  V., Ruiz, M., and Lefranc, M.-\n  10.1093/bioinformatics/17.3.282.\n                                                                          P.  Protein Displays of the Human Immunoglobulin\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,      Heavy, Kappa and Lambda Variable and Joining Re-\n  M., and Carter, S. Zoom In: An Introduction to Cir-      gions. Experimental and Clinical Immunogenetics, 16\n   cuits.  Distill, 5(3):10.23915/distill.00024.001, March      (4):234–240, November 1999. ISSN 0254-9670.  doi:\n  2020a. ISSN 2476-0757. doi: 10.23915/distill.00024.001.     10.1159/000019115. URL https://doi.org/10.\n  URL https://distill.pub/2020/circuits/     1159/000019115.\n  zoom-in.\n                                                      Simon, E. and Zou, J. InterPLM: Discovering Interpretable\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,      Features in Protein Language Models via Sparse Autoen-\n  M., and Carter, S. Zoom In: An Introduction to Circuits.      coders, November 2024. URL http://arxiv.org/\n   Distill, 2020b. doi: 10.23915/distill.00024.001.           abs/2412.12101. arXiv:2412.12101 [q-bio].\n\nOlsen, T. H., Boyles, F., and Deane, C. M.  Observed    Suzek, B. E., Huang, H., McGarvey, P., Mazumder, R.,\n  Antibody  Space:  A  diverse  database  of  cleaned,     and Wu, C. H.   UniRef:  comprehensive and non-\n  annotated, and translated unpaired and paired  anti-      redundant UniProt reference clusters.  Bioinformatics,\n  body sequences.    Protein Science,  31(1):141–146,      23(10):1282–1288, May 2007. ISSN 1367-4803. doi:\n  2022.  ISSN 1469-896X.   doi:  10.1002/pro.4205.      10.1093/bioinformatics/btm098. URL https://doi.\n URL       https://onlinelibrary.wiley.    org/10.1093/bioinformatics/btm098.\n\n\n                                                12\n\n                         Mechanistic Interpretability of Antibody Language Models Using SAEs\n\nTempleton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken,      Deane, C. M.  p-IgGen:  a paired antibody genera-\n   T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones,       tive language model.  Bioinformatics, 40(11):btae659,\n  A., Cunningham, H., Turner, N. L., McDougall, C.,     November 2024.  ISSN 1367-4811.   doi:  10.1093/\n  MacDiarmid, M., Freeman, C. D., Sumers,  T. R.,      bioinformatics/btae659. URL https://doi.org/\n  Rees,  E., Batson,  J., Jermyn, A., Carter,  S., Olah,     10.1093/bioinformatics/btae659.\n   C.,  and Henighan,  T.    Scaling Monosemanticity:\n                                                     Zhang,  L.,  Lu,  Y., Wang,  B.,  Li,  F.,  and Zhang,  Extracting Interpretable Features from Claude 3 Sonnet.\n                                                            Z.        Sparse   Auto-encoder   with  Smoothed  Transformer Circuits Thread, 2024.  URL https:\n                                                             $$l 1$$Regularization.     Neural  Processing  Let-  //transformer-circuits.pub/2024/\n                                                                          ters, 47(3):829–839, June 2018.  ISSN 1573-773X.  scaling-monosemanticity/index.html.\n                                                                  doi:  10.1007/s11063-017-9668-5.  URL https:\nTurnbull, O. M., Oglic, D., Croasdale-Wood, R., and     //doi.org/10.1007/s11063-017-9668-5.\n\n\n\n\n\n                                                13\n\n## Claim to Classify\n\"SAE latents collectively represent antibody information following sparsification\"\nLocation in paper: body\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "TopK/Ordered SAE latent activation analysis + linear probe performance (logistic regressor on SAE latents) comparing to hidden neurons; no intervention for this specific claim",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The supporting evidence in the surrounding text is observational: they train a logistic regressor on frozen SAE latents and report high accuracy/F1, plus qualitative activation-pattern inspection. That is Rung 1 (associational/decodability-style evidence) because it does not intervene on the model to test causal influence. The claim text uses mechanistic/representational language—“latents collectively represent antibody information”—which defaults to a Rung 3 mechanistic attribution per the codebook’s ‘represents/encodes’ decision rule (not explicitly framed as ‘decodable’). No explicit hedge words appear in the quoted claim."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 17363,
      "completion_tokens": 199,
      "total_tokens": 17562
    },
    "finish_reason": "stop"
  }
}