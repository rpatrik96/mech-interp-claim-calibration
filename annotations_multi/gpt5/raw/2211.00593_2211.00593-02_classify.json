{
  "timestamp": "2026-02-19T08:59:10.014090+00:00",
  "paper_id": "2211.00593",
  "claim_id": "2211.00593-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2211.00593\nTitle: Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\nFull text:\n           INTERPRETABILITY IN THE WILD: A CIRCUIT FOR\n           INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL\n\n\n\n                     Kevin Wang1, Alexandre Variengien1, Arthur Conmy1, Buck Shlegeris1 & Jacob Steinhardt1,2\n                   1Redwood Research\n                 2UC Berkeley\n                 kevin@rdwrs.com, alexandre@rdwrs.com,\n                 arthur@rdwrs.com, buck@rdwrs.com, jsteinhardt@berkeley.edu\n\n\n\n                                       ABSTRACT2022\n                             Research in mechanistic interpretability seeks to explain behaviors of machine\n                                 learning (ML) models in terms of their internal components.  However, most\n                               previous work either focuses on simple behaviors in small models or describesNov\n                              complicated behaviors in larger models with broad strokes.  In this work, we\n1                         bridge this gap by presenting an explanation for how GPT-2 small performs a\n                                  natural language task called indirect object identiﬁcation (IOI). Our explana-\n                                  tion encompasses 26 attention heads grouped into 7 main classes, which we\n                               discovered using a combination of interpretability approaches relying on causal\n                                  interventions.  To our knowledge, this investigation is the largest end-to-end\n                               attempt at reverse-engineering a natural behavior “in the wild” in a language\n                             model. We evaluate the reliability of our explanation using three quantitative[cs.LG]\n                                    criteria–faithfulness, completeness and minimality.  Though these criteria sup-\n                                 port our explanation, they also point to remaining gaps in our understanding.\n                          Our work provides evidence that a mechanistic understanding of large ML mod-\n                                    els is feasible, pointing toward opportunities to scale our understanding to both\n                                   larger models and more complex tasks. Code for all experiments is available at\n                      https://github.com/redwoodresearch/Easy-Transformer.\n\n\n                1  INTRODUCTION\n\n                      Transformer-based language models (Vaswani et al., 2017; Brown et al., 2020) have demonstrated\n                     an impressive suite of capabilities but largely remain black boxes. Understanding these models is\n                         difﬁcult because they employ complex non-linear interactions in densely-connected layers and op-\n                         erate in a high-dimensional space. Despite this, they are already deployed in high-impact settings\n                    (Zhang et al., 2022; Caldarini et al., 2022), underscoring the urgency of understanding and antici-\n                       pating possible model behaviors. Some researchers have argued that interpretability is critical forarXiv:2211.00593v1\n                        the safe deployment of advanced machine learning systems (Hendrycks & Mazeika, 2022).\n\n                  Work in mechanistic interpretability aims to discover, understand, and verify the algorithms that\n                   model weights implement by reverse engineering model computation into human-understandable\n                    components (Olah, 2022; Meng et al., 2022; Geiger et al., 2021; Geva et al., 2020). By understanding\n                      underlying mechanisms, we can better predict out-of-distribution behavior (Mu & Andreas, 2020),\n                         identify and ﬁx model errors (Hernandez et al., 2021; Vig et al., 2020), and understand emergent\n                      behavior (Nanda & Lieberum, 2022; Barak et al., 2022; Wei et al., 2022).\n\n                       In this work, we aim to mechanistically understand how GPT-2 small (Radford et al., 2019) im-\n                      plements a simple natural language task. We do so by using circuit analysis (R¨auker et al., 2022),\n                        identifying an induced subgraph of the model’s computational graph that is human-understandable\n                    and responsible for completing the task.\n\n                   To discover the circuit, we introduce a systematic approach that iteratively traces important compo-\n                       nents back from the logits, using a causal intervention that we call “path patching”. We supplement\n                           this mainline approach with projections in the embedding space, attention pattern analysis, and ac-\n                          tivation patching to understand the behavior of each component.\n\n\n                                                           1\n\n                     0      1    Layer    10     11\n            When                                        xnew                xorig\n            IO Mary\n                and                             Path    David                Mary\n              S1 John                             Patching   and                and\n               went                                         John               Bob\n                      to\n                   the                        GPT2-small\n                   store\n                                     ,                           Prediction               When\n                                                Mary 68.3%                   Mary\n           S2 John                                    Knock Out\n                gave                          them 11.7%                    and\n                    a                                the 4.4%                     John\n                  drink                             John 2.3%\n         END to                                her 1.9%\n\nFigure 1: Left: We isolated a circuit (in orange) responsible for the ﬂow of information connecting\nthe indirect object “Mary” to the next token prediction. The nodes are attention layers and the edges\nrepresent the interactions between these layers. Right: We discovered and validated this circuit using\ncausal interventions, including both path patching and knockouts of attention heads.\n\n\nWe focus on understanding a speciﬁc natural language task that we call indirect object identiﬁcation\n(IOI). In IOI, sentences such as “When Mary and John went to the store, John gave a drink to”\nshould be completed with “Mary”. We chose this task because it is linguistically meaningful and\nadmits an interpretable algorithm: of the two names in the sentence, predict the name that isn’t the\nsubject of the last clause.\n\nWe discover a circuit of 26 attention heads–1.1% of the total number of (head, token position)\npairs–that completes the bulk of this task. The circuit uses 7 categories of heads (see Figure 2) to\nimplement the algorithm. Together, these heads route information between different name tokens,\nto the end position, and ﬁnally to the output. Our work provides, to the best of our knowledge, the\nmost detailed attempt at reverse-engineering an end-to-end behavior in a transformer-based language\nmodel.\n\nBy zooming in on a crisp task in a particular model, we obtained several insights about the challenges\nof mechanistic interpretability. In particular:\n\n• We identiﬁed several instances of heads implementing redundant behavior (Michel et al., 2019).\n  The most surprising were “Backup Name-Mover Heads”, which copy names to the correct posi-\n   tion in the output, but only when regular Name-Mover Heads are ablated. This complicates the\n  search for complete mechanisms, as different model structure is found when some components\n  are ablated.\n• We found known structures (speciﬁcally induction heads (Elhage et al., 2021)) that were used in\n  unexpected ways. Thus mainline functionality of a component does not always give a full picture.\n• Finally, we identiﬁed heads reliably writing in the opposite direction of the correct answer.\n\nExplanations for model behavior can easily be misleading or non-rigorous (Jain & Wallace, 2019;\nBolukbasi et al., 2021). To remedy this problem, we formulate three criteria to help validate our\ncircuit explanations. These criteria are faithfulness (the circuit can perform the task as well as\nthe whole model), completeness (the circuit contains all the nodes used to perform the task), and\nminimality (the circuit doesn’t contain nodes irrelevant to the task). Our circuit shows signiﬁcant\nimprovements compared to a na¨ıve (but faithful) circuit, but fails to pass the most challenging tests.\n\nIn summary, our main contributions are: (1) We identify a large circuit in GPT-2 small that performs\nindirect-object identiﬁcation (Figure 2 and Section 3); (2) Through example, we identify useful\ntechniques for understanding models, as well as surprising pitfalls; (3) We present criteria that ensure\nstructural correspondence between the circuit and the model, and check experimentally whether our\ncircuit meets this standard (Section 4).\n\n\n2  BACKGROUND\n\n\nIn this section, we introduce the IOI task, review the transformer architecture, deﬁne circuits more\nformally, and describe a technique for “knocking out” nodes in a model.\n\n\n                                       2\n\nTask description. A sentence containing indirect object identiﬁcation (IOI) has an initial dependent\nclause, e.g “When Mary and John went to the store”, and a main clause, e.g “John gave a bottle of\nmilk to Mary”. The initial clause introduces the indirect object (IO) “Mary” and the subject (S)\n“John”. The main clause refers to the subject a second time, and in all our examples of IOI, the\nsubject gives an object to the IO. The IOI task is to predict the ﬁnal token in the sentence to be the\nIO. We use ‘S1’ and ‘S2’ to refer to the ﬁrst and second occurrences of the subject, when we want\nto specify position. We create dataset samples for IOI using 15 templates (see Appendix E) with\nrandom single-token names, places and items. We use the notation pIOI for the distribution over\nsentences generated by this procedure.\n\nTo quantify GPT-2 small’s performance on the IOI task, we use two different metrics: logit differ-\nence and IO probability. Logit difference measures the difference in logit value between the two\nnames, where a positive score means the correct name (IO) has higher probability. IO probability\nmeasures the absolute probability of the IO token under the model’s predictions. Both metrics are\naveraged over pIOI. Over 100,000 dataset examples, GPT-2 small has mean logit difference of 3.56\n(IO predicted over S 99.3% of the time), and mean IO probability of 49%.\n\nTransformer architecture. GPT-2 small is a decoder-only transformer with 12 layers and 12 at-\ntention heads per attention layer. In this work, we mostly focus on understanding the behavior of\nattention heads, which we describe below using notation similar to Elhage et al. (2021). We leave a\nfull description of the model to Appendix G.\nThe input x0 to the transformer is a sum of position and token embeddings and lies in RN×d, where\nN is the number of tokens in the input and d is the model dimension. This input embedding is the\ninitial value of the residual stream, which all attention layers and MLPs read from and write to.\nAttention layer i of the network takes as input xi ∈RN×d, the value of the residual stream at layer\ni. The attention layer output can be decomposed into the sum of attention heads hi,j. If the output\nof the attention layer is yi = Pj hi,j(xi), then the residual stream is updated to xi + yi.\nFocusing on individual heads, each head hi,j is parametrized by four matrices W Qi,j , W Ki,j , W Oi,j ∈\nRd× Hd and W Vi,j ∈R d                                                                                                  i,j                H ×d. We rewrite these parameters as low-rank matrices in Rd×d: W OV =\nW Oi,j W Vi,j and W QKi,j = W Qi,j (W Ki,j )T . The QK matrix is used to compute the attention pattern\nAi,j ∈RN×N of head (i, j), while the OV matrix determines what is written into the residual\nstream. At the end of the forward pass, a layer norm is applied before the unembed matrix WU\nprojects the residual stream into logits.\n\n\n2.1  CIRCUITS AND KNOCKOUTS\n\nIn mechanistic interpretability, we want to understand the correspondence between the components\nof a model and human-understandable concepts. A useful abstraction for this goal is circuits. If we\nthink of a model as a computational graph M where nodes are terms in its forward pass (neurons,\nattention heads, embeddings, etc.) and edges are the interactions between those terms (residual con-\nnections, attention, projections, etc.), a circuit C is a subgraph of M responsible for some behavior\n(such as completing the IOI task). This deﬁnition of a circuit is more coarse-grained than the one\npresented in Olah et al. (2020), where nodes are features (meaningful directions in the latent space\nof a model) instead of model components.\n\nJust as the entire model M deﬁnes a function M(x) from inputs to logits, we also associate each\ncircuit C with a function C(x) via knockouts. A knockout removes a set of nodes K in a compu-\ntational graph M with the goal of “turning off” nodes in K but capturing all other computations in\nM. Thus, C(x) is deﬁned by knocking out all nodes in M\\C and taking the resulting logit outputs\nin the modiﬁed computational graph.\n\nA ﬁrst na¨ıve knockout approach consists of simply deleting each node in K from M. The net effect\nof this removal is to zero ablate K, meaning that we set its output to 0. This na¨ıve approach has\nan important limitation: 0 is an arbitrary value, and subsequent nodes might rely on the average\nactivation value as an implicit bias term. Perhaps because of this, we ﬁnd zero ablation to lead to\nnoisy results in practice.\n\nTo address this, we instead knockout nodes through mean ablation: replacing them with their aver-\nage activation value across some reference distribution, similar to the bias correction method used\n\n\n                                       3\n\nin Nanda & Lieberum (2022). Mean-ablations remove the information that varies in the reference\ndistribution (e.g. the value of the name outputted by a head) but will preserve constant information\n(e.g. the fact that a head is outputting a name).\n\nIn this work, all knockouts are performed in a modiﬁed pIOI distribution called pABC. It relies on the\nsame generation procedure, but instead of using two names (IO and S) it used three unrelated random\nnames (A, B and C). In pABC, sentences no longer have a single plausible IO, but the grammatical\nstructures from the pIOI templates are preserved.\n\nWe chose this distribution for mean-ablating because using pIOI would not remove enough informa-\ntion helpful for the task. For instance, some information constant in pIOI (e.g. the fact that a name\nis duplicated) is removed when computing the mean on pABC.\n\nWhen knocking out a single node, a (head, token position) pair in our circuit, we want to preserve\nthe grammatical information unrelated to IOI contained in its activations. However, the grammatical\nrole (subject, verb, conjunction etc.) of a particular token position varies across templates. To ensure\nthat grammatical information is constant when averaging, we compute the mean of a node across\nsamples of the same template.\n\n\n3  DISCOVERING THE CIRCUIT\n\n\nWe seek to explain how GPT-2 small implements the IOI task (Section 2).  Recall the example\nsentence “When Mary and John went to the store, John gave a drink to”. The following human-\ninterpretable algorithm sufﬁces to perform this task:\n\n1. Identify all previous names in the sentence (Mary, John, John).\n2. Remove all names that are duplicated (in the example above: John).\n3. Output the remaining name.\n\nBelow we present a circuit that we claim implements this functionality. Our circuit contains three\nmajor classes of heads, corresponding to the three steps of the algorithm above:\n\n• Duplicate Token Heads identify tokens that have already appeared in the sentence. They are active\n   at the S2 token, attend primarily to the S1 token, and signal that token duplication has occurred\n  by writing the position of the duplicate token.\n• S-Inhibition Heads remove duplicate tokens from Name Mover Heads’ attention. They are active\n   at the END token, attend to the S2 token, and write in the query of the Name Mover Heads,\n   inhibiting their attention to S1 and S2 tokens.\n• Name Mover Heads output the remaining name. They are active at END, attend to previous names\n   in the sentence, and copy the names they attend to. Due to the S-Inhibition Heads, they attend to\n  the IO token over the S1 and S2 tokens.\n\n\n                                                             Legend\n             When\n             IO Mary                                                                Key / Value\n                 and\n               S1 John                                                    Query     Class of Heads   Output\n                                                                                                             Layer.Head\n             S1+1 went             Previous Token Heads\n                                                              2.2  4.11\n                       to\n                    the\n                        store,                                                             Negative Name Mover Heads\n                           Duplicate                                 Token                                    Heads   Induction                                                    Heads                                                                                                                                           10.7 11.10            S2 John\n                                                   0.1  3.0                                                           (0.10)                                                                            5.5  6.9 (5.8                                                                                               5.9)\n                 gave\n                     a                                                Name Mover Heads\n                                                                                                                                   9.9  9.6  10.0\n                   drink\n                                                                          Backup Name Mover Heads          END to                                            S-Inhibition7.3 7.9 8.6 Heads8.10\n                                                                                                                    9.0 9.7 10.1 10.2 10.6 10.10 11.2 11.9\n\n\nFigure 2: We discover a circuit in GPT-2 small that implements IOI. The input tokens on the left are\npassed into the residual stream. Attention heads move information between residual streams: the\nquery and output arrows show which residual streams they write to, and the key/value arrows show\nwhich residual streams they read from.\n\n\n                                       4\n\nA fourth major family of heads writes in the opposite direction of the Name Mover Heads, thus\ndecreasing the conﬁdence of the predictions. We speculate that these Negative Name Mover Heads\nmight help the model “hedge” so as to avoid high cross-entropy loss when making mistakes.\n\nThere are also three minor classes of heads that perform related functions to the components above:\n\n• Previous Token Heads copy information about the token S to token S1+1, the token after S1.\n• Induction Heads perform the same role as the Duplicate Token Heads through an induction mech-\n  anism. They are active at position S2, attend to token S1+1 (mediated by the Previous Token\n  Heads), and their output is used both as a pointer to S1 and as a signal that S is duplicated.\n• Finally, Backup Name Mover Heads do not normally move the IO token to the output, but take on\n   this role if the regular Name Mover Heads are knocked out.\n\nIn all of our experiments, we do not intervene on MLPs, layer norms, or embedding matrices,\nfocusing our investigation on understanding the attention heads. In initial investigations, we found\nthat knocking out each MLP individually led to good task performance, except for the ﬁrst layer.\nHowever, knocking out all MLPs after the ﬁrst layer makes the model unable to perform the task\n(Appendix J). A more precise investigation of the role of MLPs is left for future work.\n\nBelow we show how we discovered each of the seven components above, providing evidence that\nthey behave as claimed. We found that it was easiest to uncover the circuit starting at the logits and\nworking back step-by-step. Each step is divided into two parts. First, we trace the information ﬂow\nbackward from previously discovered components by ﬁnding attention heads that directly inﬂuence\nthem; we do this using a technique we introduce called path patching, which generalizes the ap-\nplication of causal mediation analysis to language models introduced in Vig et al. (2020). Second,\nwe characterize the newly identiﬁed heads by examining their attention patterns and performing\nexperiments tailored to their hypothesized function.\n\n\n3.1  WHICH HEADS DIRECTLY AFFECT THE OUTPUT? (NAME MOVER HEADS)\n\nTracing back the information ﬂow. We begin by searching for attention heads h directly affecting\nthe model’s logits. To differentiate indirect effect (where the inﬂuence of a component is mediated\nby another head) from direct effect, we designed a technique called path patching (Figure 1).\n\nPath patching replaces part of a model’s forward pass with activations from a different input. Given\ninputs xorig and xnew, and a set of paths P emanating from a node h, path patching runs a forward\npass on xorig, but for the paths in P it replaces the activations for h with those from xnew. In our\ncase, h will be a ﬁxed attention head and P consists of all direct paths from h to a set of components\nR, i.e. paths through residual connections and MLPs (but not through other attention heads); this\nmeasures the counterfactual effect of h on the members of R. The layers after the element from R\nare recomputed as in a normal forward pass. See Appendix B for full details of the method.\n\nWe will always take xorig to be a sample from pIOI, and xnew the corresponding sample from pABC\n(i.e. where the names in the sentence are replaced by three random names). We run path patching on\nmany random samples from pIOI and measure how this affects the average logit difference. Pathways\nh →R that are critical to the model’s computation should induce a large drop in logit difference\nwhen patched, since the pABC distribution removes the information needed to complete the IOI task.\n\nTo trace information back from the logits, we run path patching for the pathway h →Logits for\neach head h at the END position and display the effect on logit difference in Figure 3a. We see that\nonly a few heads in the ﬁnal layers cause a large effect on logit difference. Speciﬁcally, patching\n9.6, 9.9, and 10.0 causes a large drop (they thus contribute positively to the logit difference), while\n10.7 and 11.10 cause a large increase (they contribute negatively to the logit difference).\n\nName Mover Heads. To understand the heads positively inﬂuencing the logit difference, we ﬁrst\nstudy their attention patterns. We ﬁnd that they attend strongly to the IO token: the average attention\nprobability of all heads over pIOI is 0.59. We hypothesize that these heads (i) attend to the correct\nname and (ii) copy whatever they attend to. We therefore call these heads Name Mover Heads.\n\nTo verify our hypothesis, we design experiments to test the heads’ functionality. Let WU denote\nthe unembedding matrix, and WU[IO], WU[S] the corresponding unembedding vectors for the IO\nand S tokens. We scatter plot the attention probability against the logit score ⟨hi(X), WU[N]⟩,\nmeasuring how much head hi on input X is writing in the direction of the logit of the name N\n\n\n                                       5\n\n                                                                                                           Projection of the output of 9.9 along the name\n                                                                                        embedding vs attention probability on name\n\n                                                                                                              80                          Name Type\n                                                                                                                                          IO                                                                 Name       Embed  60                            S\n                                                                             Mover       40\n                                                                          Heads     Name  20\n                                              w\n                                                                                                                                         Dot   0\n                           Direct eﬀect on logit                               −20\n                             diﬀerence                                            0      0.2Attn Prob0.4  on0.6Name0.8     1\n                                                                                    Logit diff.\n                                            0                                   variation                   Projection of the output of 11.10 along the name\n                                                                                        embedding vs attention probability on name\n                                            2                               50%\n                                                                                                               0                          Name Type\n                                                                    25%  END h?       Logits                                                                                                                                           IO                                                                                                           −10                                                                                                                                                                                                                                     Embed                                                                                                              S\n                                            6                                                                                                           −20                                                               0%      Negative                                                                 Layer  4\n                                                                Name\n                                                                                                           −30                                                                                                                                                                                       Name                                            8                                   -25%                                                                             Mover\n                                              w                                                                                                           −40\n                                           10                                                                              -50%     Heads                                                                                                                                         Dot                                                                                                           −50\n                                              0   2   4   6   8   10                                         0      0.2     0.4     0.6     0.8     1\n                                  Head                                                              Attn Prob on Name\n\n              (a)                             (b)                                           (c)\n\nFigure 3: (a) We are searching for heads h directly affecting the logits using path patching.  (b)\nResults of the path patching experiments. Name Movers and Negative Name Movers Heads are\nthe heads that have the strongest direct effect on the logit difference. (c) Attention probability vs\nprojection of the head output along WU[IO] or WU[S] respectively.  For S tokens, we sum the\nattention probability on both S1 and S2.\n\n\n(IO or S). The results are shown in Figure 3c: higher attention probability on the IO or S token is\ncorrelated with higher output in the direction of the name (correlation ρ > 0.81, N = 500)1.\n\nTo check that the Name Mover Heads copy names generally, we studied what values are written\nvia the heads’ OV matrix.  Speciﬁcally, we ﬁrst obtained the state of the residual stream at the\nposition of each name token after the ﬁrst MLP layer. Then, we multiplied this by the OV matrix of\na Name Mover Head (simulating what would happen if the head attended perfectly to that token),\nmultiplied by the unembedding matrix, and applied the ﬁnal layer norm to obtain logit probabilities.\nWe compute the proportion of samples that contain the input name token in the top 5 logits (N =\n1000) and call this the copy score. All three Name Mover Heads have a copy score above 95%,\ncompared to less than 20% for an average head.\n\nNegative Name Mover Heads. In Figure 3b, we also observed two heads causing a large increase,\nand thus negatively inﬂuencing the logit difference. We called these heads Negative Name Mover\nHeads. These share all the same properties as Name Mover Heads except they (1) write in the\nopposite direction of names they attend to and (2) have a large negative copy score–the copy score\ncalculated with the negative of the OV matrix (98% compared to 12% for an average head).\n\n\n3.2  WHICH HEADS AFFECT THE NAME MOVER HEADS’ ATTENTION? (S-INHIBITION\n    HEADS)\n\nTracing back the information ﬂow. Given that the Name Mover Heads are primarily responsible\nfor constructing the output, we next ask what heads they depend on. As illustrated in Figure 4a,\nthere are three ways to inﬂuence attention heads: by their values, keys, or queries. In the case of\nName Mover Heads, their value matrix appears to copy the input tokens (see previous section), so\nwe do not study it further. Moreover, as the IO token appears early in the context, the Name Mover\nHeads’ key vectors for this position likely do not contain much task-speciﬁc information. Therefore,\nwe focus on the query vector, which is located at the END position.\n\nWe again use path patching, identifying the heads that affect each Name Mover Head’s query vector.\nSpeciﬁcally, for a head h we patch the path h →Name Mover Heads’ queries (Figure 4a) and report\n\n   1In GPT-2 small, there is a layer norm before the ﬁnal unembedding matrix. This means that these dot\nproducts computed are not appropriately scaled. However, empirically we found that approaches to approxi-\nmating the composition of attention head outputs with this layer norm were complicated, and resulted in similar\ncorrelation and scatter plots.\n\n\n                                       6\n\n                                                          Average attention probability\n                                  Direct eﬀect on Name                                                                           of Name Mover Heads\n                            Movers Heads' queries                     0.8\n  IO                                              0                                Logitvariationdiff.              0.7                          Attention on pIOI\n            K   V                        2                             20%            0.50.6                          AttentionS-InhibitionafterHeadspatching\n                                                                         10%                                                                                                                             from pABC                                                                                                                            0.4                                      Logits         4\n                                                                   0%END  h? Q  NameHeadsMovers                                                                                                                            0.3\n                                                    6                                                                              Layer                                    -10%           0.2\n                                                    8                                 -20%           0.1\n                                                      0   2   4   6   8   10\n                                       Head                                 0   IO    S1   S2\n\n                 (a)                                  (b)                                       (c)\n\nFigure 4: (a) Diagram of the direct effect experiments on Name Mover Heads’ queries. We patch\nin the red path from the pABC distribution. Then, we measure the indirect effect of h on the logits\n(dotted line).  All attention heads are recomputed on this path.  (b) Result of the path patching\nexperiments. The four heads causing a decrease in logit difference are S-Inhibition Heads.  (c)\nAttention of Name Mover Heads on pIOI before and after path patching of S-Inhibition Heads →\nName Mover Heads’ queries for all four S-Inhibition Heads at once (black bars show the standard\ndeviation). S-Inhibition Heads are responsible for Name Mover Heads’ selective attention on IO.\n\n\n                                                               Direct effect on\n                                                            S-Inhibition Heads' values      Logit diff.\n                                                                                                                   variation\n     h?                                                                0                                             30%\n  S2                                                                                                        20%\n            K   V    Name                               2                                             10%\n                               Movers\n                              Heads                              4                                         0%                    S-inhibition\nEND    Q      heads              Logits                                                                       -10%\n                                                                              6                                                   -20%\n\n                                                                                                                              -30%\n                                                                                0      2      4      6      8     10\n                                                                  Head\n\n                        (a)                                                              (b)\n\nFigure 5: (a) Diagram of the direct effect experiment on S-Inhibition Heads’ values. On the indirect\npath from h to the logits (dotted line), all elements are recomputed, Name Movers Heads are me-\ndiating the effect on logits. (b) Result of the path patching experiments for the S-Inhibition Heads’\nvalues.\n\n\nthe effect on logit difference in Figure 4b. Heads 7.3, 7.9, 8.6, 8.10 directly inﬂuence the Name\nMover Heads’ queries, causing a signiﬁcant drop in logit difference through this path.\n\nS-Inhibition Heads. To better understand the inﬂuence of these heads on the Name Movers Heads,\nwe visualized Name Movers Heads’ attention before and after applying path patching of all four\npreviously identiﬁed heads at once (Figure 4c). Patching these heads leads to a greater probability\non S1 (and thus less on IO). We therefore call these heads S-Inhibition Heads, since their effect is to\ndecrease attention to S1.\n\nWhat are S-Inhibition heads writing? We observed that S-Inhibition heads preferentially attend to\nthe S2 token (the average attention from END to S2 was 0.51 over the four heads). We conjectured\nthat they move information about S that causes Name Movers to attend less to S1. We discovered\ntwo signals through which S-Inhibition Heads do this. The ﬁrst, the token signal, contains the value\nof the token S and causes Name Mover Heads to avoid occurrences of that token. The second, the\nposition signal, contains information about the position of the S1 token, causing Name Mover Heads\nto avoid the S1 position no matter the value of the token at this position.\n\nTo disentangle the two effects, we designed a series of counterfactual experiments where only some\nsignals are present, and the others are inverted with respect to the original dataset (see Appendix A).\nBoth effects matter, but position signals have a greater effect than token signals. These two signals\nsufﬁce to account for the full average difference in Name Mover Heads’ attention to IO over S1.\n\n\n                                       7\n\nS-Inhibition heads output information about the position of S1 but don’t attend to it directly. They\nthus must rely on the output of other heads through value composition, as we will see next.\n\n\n3.3  WHICH HEADS AFFECT S-INHIBITION VALUES?\n\nTracing back the information ﬂow. We next trace the information ﬂow back through the queries,\nkeys, and values of the S-Inhibition Heads, using path patching as before. We found the values were\nthe most important and so focus on them here, with keys discussed in Appendix C (queries had no\nsigniﬁcant direct effect on the S-Inhibition Heads).\n\nIn more  detail,  for  each  head h  at  the S2  position we  patched  in  the  path h →\nS-Inhibition Heads’s values (Figure 5a) and reported results in Figure 5b.\n\nWe identiﬁed 4 heads with a signiﬁcant negative effect on logit difference. By looking at their\nattention pattern, we separated them into two groups. The heads from the ﬁrst group attend from S2\nto S1, while those from the second group attend from S2 to the token after S1 (S1+1 in short).\n\nDuplicate Token Heads. Two of the heads attend from S2 to S1. We hypothesize that these heads\nattend to the previous occurrence of a duplicate token and copy the position of this previous occur-\nrence to the current position. We therefore call them Duplicate Token Heads.\n\nTo partially validate this hypothesis, we investigated these heads’ attention patterns on sequences of\nrandom tokens (with no semantic meaning). We found that the 2 Duplicate Token Heads pay strong\nattention to a previous occurrence of the current token when it exists (see Appendix I).\n\nInduction Heads and Previous Token Heads. The other group of two heads attends from S2 to\nS1+1. This matches the attention pattern of Induction Heads (Elhage et al., 2021), which recognize\nthe general pattern [A] [B] ... [A] and contribute to predicting [B] as the next token. For this,\nthey act in pairs with Previous Token Heads. The Previous Token Heads write information about\n[A] into the residual stream at [B]. Induction Heads can then match the next occurrence of [A] to\nthat position (and subsequently copy [B] to the output).\n\nTo test the hypothesis that this group of heads consists of Induction Heads, we sought out the Pre-\nvious Token Heads that they rely on through key composition. To this end, we apply path patching\nto ﬁnd heads that inﬂuence logit difference through the S1+1 keys of the Induction Heads (see Ap-\npendix D). We identiﬁed two heads with signiﬁcant effect sizes. To test the induction mechanism,\nwe followed the method introduced in Olsson et al. (2022) by checking for preﬁx-matching (attend-\ning to [B] on pattern like [A] [B] ... [A]) and copying (increasing the logit of the [B] token)\nof Induction Heads on repeated random sequences of tokens. We also analyzed the attention patterns\nof Previous Token Heads on the same dataset. As reported in Appendix H, Previous Token Heads\nattend primarily to the previous token and 2 Induction Heads demonstrate both preﬁx-matching and\ncopying.\n\nLocating the position signal. Despite their differences, the attention patterns of both groups of\nheads are determined by the position of S1, suggesting that their output could also contain this\ninformation. We thus hypothesized that Duplicate Token Heads and Induction Heads are the origin\nof the position signal described in Section 3.2, which is then copied by S-Inhibition Heads.\n\nTo test this hypothesis, we applied the same technique from the previous section to isolate token and\nposition signals. We patched the output of the Induction and Duplicate Token Heads from sentences\nwith altered position signals. We observed a drop in logit difference of almost the same size as\npatching the S-Inhibition Heads (at least 88%). However, patching their output from a dataset with\na different S1 and S2 token but at the same position doesn’t change the logit difference (less than\n8% of the drop induced by patching S-Inhibition Heads).\n\nMissing validations and caveats. To fully validate the claimed function of the Duplicate Token\nand Induction Heads, we would want to perform additional checks that we omitted due to time\nconstraints.  First, for Duplicate Token Heads, we would want to study the interaction of the QK\nmatrix with the token embeddings to test if it acts as a collision detector. Moreover, we would\ninvestigate the OV matrix to ensure it is copying information from positional embeddings.  For\nInduction Heads, we would perform the more thorough parameter analysis in Elhage et al. (2021) to\nvalidate the copying and key composition properties at the parameter level.\n\n\n                                       8\n\nMoreover, in the context we study, Induction Heads perform a different function compared to what\nwas described in their original discovery. In our circuit, Induction Heads’ outputs are used as posi-\ntional signals for S-Inhibition values. We did not investigate how this mechanism was implemented.\nMore work is needed to precisely characterize these newly discovered heads and understand how\nthey differ from the original description of Induction Heads.\n\n\n3.4  DID WE MISS ANYTHING? THE STORY OF THE BACKUP NAME MOVERS HEADS\n\nEach type of head in our circuit has many copies, suggesting that the model implements redundant\nbehavior2. To make sure that we didn’t miss any copies, we knocked out all the Name Mover\nHeads at once. To our surprise, the circuit still worked (only 5% drop in logit difference). After\nthe knockout, we found the new heads directly affecting the logits using the same path patching\nexperiment from Section 3.1. These new heads compensate for the loss of Name Movers Heads by\nreplacing their role.\n\nWe selected the eight heads with the highest direct inﬂuence on logit difference after knockout and\ncall them Backup Name Mover Heads. We investigated their behavior before the knockout. We\nobserve diverse behavior: 4 heads show a close resemblance to Name Mover Heads, 2 heads equally\nattend to IO and S and copy them, 1 head pays more attention to S1 and copies it, and 1 head seems\nto track and copy subjects of clauses, copying S2 in this case. See Appendix F for further details on\nthese heads.\n\nWe hypothesize that this compensation phenomenon is caused by the use of dropout during training.\nThus, the model was optimized for robustness to dysfunctional parts. More work is needed to\ndetermine the origin of this phenomenon and if such compensation effects can be observed beyond\nour special case.\n\n\n4  EXPERIMENTAL VALIDATION\n\nIn the previous section, we provided an end-to-end explanation of how the model produces a high\nlogit difference between IO and S on the IOI task. Yet Section 3.4 shows that our experimental\nmethods do not fully account for all name-moving in the model: some new heads take on this role\nafter the main Name Mover Heads are knocked out. We therefore seek more systematic validations\nto check that our account in Section 2.1 includes all components used by the model to perform IOI.\n\nTo formalize this, we ﬁrst introduce notation to measure the performance of a circuit C on the IOI\ntask. Suppose X ∼pIOI, and f(C(X); X) is the logit difference between the IO and S tokens when\nthe circuit C is run on the input X (recall that C(X) is deﬁned by knocking out all nodes outside C,\n                                                                                                  def\nas described in Section 2.1). We deﬁne the average logit difference F(C) = EX∼pIOI [f(C(X); X)]\nas a measure of how well C performs the IOI task.\n\nUsing F, we introduce three criteria for validating a circuit C. The ﬁrst one, faithfulness, asks that\nC has similar performance to the full model M, as measured by |F(M) −F(C)|. For the circuit C\nfrom Figure 2, we ﬁnd that |F(M) −F(C)| = 0.46, or only 13% of F(M) = 3.56. In other words,\nC achieves 87% of the performance of M.\n\nHowever, as the Backup Name Mover Heads illustrate, faithfulness alone is not enough to ensure a\ncomplete account. In Section 4.1 we introduce a toy example that illustrates this issue in more detail\nand use this to deﬁne two other criteria—completeness and minimality—for checking our circuit.\n\n\n4.1  COMPLETENESS\n\nAs a running example, suppose a model M uses two identical and disjoint serial circuits C1 and C2.\nThe two sub-circuits are run in parallel before applying an OR operation to their results. Identifying\nonly one of the circuits is enough to achieve faithfulness, but we want explanations that include both\nC1 and C2, since these are both used in the model’s computation.\n\n    2It is unlikely that SGD would select for perfectly redundant modules, but redundancy is more likely when\nanalyzing a sub-distribution of all of a model’s training data or a subtask of its behavior.\n\n\n                                       9\n\n            Full circuit completeness tests                        Naive circuit completeness tests\n\n                                                                                                                               x=y\n        5                                                                             5                                            Random set\n        4                                                                             4                                                   Greedy search set\n                                                                                                                      Empty set\n   K) \\  3                                                          K)\\  3                                          Name mover\n    F(M  2                                                                                       F(M  2                                                                   S-inhibition\n                                                                                                                                                    Induction\n        1                                                                             1                                                            Duplicate token\n        0                                                                             0                                                         Previous token\n                                                                                                                                        Negative heads\n       −1  −2         0         2         4         6                              −1  −2         0         2         4         6\n                         F(C \\ K)                                                        F(C \\ K)\n\n                      (a)                                                            (b)\n\nFigure 6: Plot of points (xK, yK) = (F(C \\ K), F(M \\ K)) for (a) our circuit and (b) a naive\ncircuit. Each point is for a different choice of K. We show sets K obtained from different sampling\nstrategies: 10 uniformly randomly chosen K ⊆C, K = ∅, K a class of heads from the circuit, and\n10 K found by greedy optimization. Since the incompleteness score is |xK −yK|, we show the line\ny = x for reference.\n\nTo solve this problem, we introduce the completeness criterion:  for every subset K ⊆C, the\nincompleteness score |F(C \\ K) −F(M \\ K)| should be small. In other words, C and M should\nnot just be similar, but remain similar under knockouts.\n\nIn our running example, we can show that C1 is not complete by setting K = C1. Then C1 \\ K is\nthe empty circuit while M \\ K still contains C2. The metric |F(C1 \\ K) −F(M \\ K)| will be large\nbecause C1 \\ K has trivial performance while M \\ K successfully performs the task.\n\nTesting completeness requires a search over exponentially many subsets K ⊆C. This is computa-\ntionally intractable given the size of our circuit, hence we use three sampling methods to seek sets\nK that give large incompleteness scores:\n\n• The ﬁrst sampling method chooses subsets K ⊆C uniformly at random.\n• The second sampling method set K to be an entire class of circuit heads G, e.g the Name Mover\n  Heads. C \\ G should have low performance since it’s missing a key component, whereas M \\ G\n  might still do well if it has redundant components that ﬁll in for G.\n• Thirdly, we greedily optimized K node-by-node to maximize the incompleteness score (see Ap-\n  pendix M for details).\n\nAll results are shown in Figure 6a. The ﬁrst two methods of sampling K suggested to us that\nour circuit was complete, as every incompleteness score computed with those methods was small.\nHowever, the third resulted in sets K that had high incompleteness score: up to 3.09 (87% of the\noriginal logit difference). These greedily-found sets were usually not semantically interpretable\n(containing heads from multiple categories), and investigating them would be an interesting direction\nof future work.\n\n\n4.2  MINIMALITY\n\nA faithful and complete circuit may contain unnecessary components, and so be overly complex. To\navoid this, we should check that each of its nodes v is actually necessary. This can be evaluated by\nshowing that v can signiﬁcantly recover F after knocking out a set of nodes K.\n\nFormally, the minimality criterion is whether for every node v ∈C there exists a subset K ⊆\nC \\ {v} that has high minimality score |F(C \\ (K ∪{v})) −F(C \\ K)|.\n\nIn the running example, we can show that C1∪C2 is minimal. The proof has two symmetrical cases:\nv ∈C1 and v ∈C2. Without loss of generality, we chose v ∈C1, and then deﬁne K = C2. The\nminimality score for this choice of node v and set K is equal to |F(C1 \\ {v}) −F(C1)|, which is\nlarge since C1 is a serial circuit and so removing v will destroy the behavior.\n\nWhat happens in practice for our circuit? We need to exhibit for every v a set K such that the\nminimality score is high. For most heads, removing the class of heads G that v is a part of provides\n\n\n                                       10\n\n                                                              Name Mover Head\n\n                                                                           Backup Name Mover Head       30%\n                                                                                    Negative Name Mover Head\n\n                                                                                                    S-Inhibition Head                 difference                                                                                 Duplicate Token Head\n         logit 20%20%                                                                             Induction Head\n                                                                                     Previous Token Head\n   in\n\n\n\n       10%          Change\n\n\n\n\n        0%\n                    (9,  (10,  (9,  (10,  (11,  (8,  (7,  (8,  (7,  (5,  (5,  (6,  (5,  (0,  (0,  (3,  (4,  (2,  (11,  (10,  (10,  (10,  (9,  (10,  (11,  (9,\n             9)                    6)\n                0)                       7)      10) 9) 6) 3) 5) 9) 9) 8) 1)  10) 0)  11) 2) 2) 6)    2) 7) 1) 9) 0)                                       10)                                                                       10)\n                                        Attention head\n\nFigure 7: Plot of minimality scores |F(C \\ (K ∪{v})) −F(C \\ K)| (as percentages of F(M)) for\nall components v in our circuit. The sets K used for each component, as well as the initial and ﬁnal\nvalues of the logit difference for each of these v are in Appendix K.\n\n\na reasonable minimality score, but in some instances a more careful choice is needed; we provide\nfull details in Appendix K and display ﬁnal results in Figure 7. The importance of different nodes\nvaries, but all have a nontrivial impact (at least 1% of the original logit difference). These results\nensure that we did not interpret irrelevant nodes, but do show that the individual contribution of\nsome attention heads is small.\n\n\n4.3  COMPARISON WITH A BASELINE CIRCUIT\n\nIn the previous sections, we evaluated our circuit on three quantitative criteria. To establish a base-\nline for these criteria, we compare to a na¨ıve circuit that consists of the Name Mover Heads (but no\nBackup nor Negative Name Mover Heads), S-Inhibition Heads, two Induction Heads, two Duplicate\nToken Heads, and the Previous Token Heads. This circuit has a faithfulness score of 0.1, comparable\nto the full circuit C. However, the na¨ıve circuit can be more easily proven incomplete: by sampling\nrandom sets or by knocking-out by classes, we see that F(M \\ K) is much higher than F(C \\ K)\n(Figure 6a). Nonetheless, when we applied the greedy heuristic to optimize for the incompleteness\nscore, both circuits have similarly large incompleteness scores.\n\n\n4.4  DESIGNING ADVERSARIAL EXAMPLES\n\nAs argued in R¨auker et al. (2022), one way to evaluate the knowledge gained by interpretability\nwork is to use it for downstream applications. In this section, we do this by using knowledge of the\ncircuit to construct simple adversarial examples for the model.\n\nAs presented in Section 3, the model relies on duplicate detection to differentiate between S and\nIO. Motivated by this, we constructed passages where both the S and IO tokens are duplicated. An\nexample is “John and Mary went to the store. Mary had a good day. John gave a bottle of milk to”;\nsee Appendix L for full details. We ﬁnd that this signiﬁcantly reduces the logit difference and causes\nthe model to predict S over IO 23% of the time (Figure 8).\n\nTo ensure that the observed effect is not an artifact of the additional sentences, we included a control\ndataset using the same templates, but where the middle sentence contains S instead of IO. In these\nsentences, S appears three times in total and IO only appears once. On this distribution, the model\nhas an even higher logit difference than on pIOI, and predicts S over IO only 0.4% of the time.\n\nLimitations of the attack. Despite being inspired by our understanding of our circuit, those exam-\nples are simple enough that they could have been found without our circuit with enough effort.\n\n\n                                       11\n\n                                                                  Proportion of\n         Distribution         Logit difference  IO probability\n                                                    S logit greater than IO\n              pIOI                    3.55              0.49              0.7%\n   Additional occurrence of S\n                                     3.64              0.59              0.4%\n        (natural sentence)\n  Additional occurrence of IO\n                                     1.23              0.36              23.4%\n        (natural sentence)\n\nFigure 8: Summary of GPT-2 performance metrics on the IOI task on different datasets. In the line\norder: for pIOI, for the dataset where we added an occurrence of S (thus S appears three times in the\nsentence) and for the adversarial dataset with duplicated IO in natural sentences.\n\n\nMoreover, we do not have a full understanding of the mechanisms at play in these adversarial exam-\nples. For instance, the S-Inhibition Heads attend not only to S2, but also to the second occurrence of\nIO. As this pattern is not present in pIOI nor in pABC, it is beyond the analysis presented in Section\n3. The study of the behavior of the circuit on these adversarial examples could be a promising area\nfor future work.\n\n\n5  DISCUSSION\n\nIn this work, we isolated, understood and validated a set of attention heads in GPT-2 small composed\nin a circuit that identiﬁes indirect objects. Along the way, we discovered interesting structures\nemerging from the model internals. For instance:\n\n• We identiﬁed an example of attention heads communicating by using pointers (sharing the loca-\n   tion of a piece of information instead of copying it).\n\n• We identiﬁed heads compensating for the loss of function of other heads, and heads contributing\n  negatively to the next-token prediction. Early results suggest that the latter phenomenon occurs\n   for other tasks beyond IOI (see Appendix H).\n\nHowever, our work also has several limitations. First, despite the detailed analysis presented here,\nwe do not understand several components. Those include the attention patterns of the S-Inhibition\nHeads, and the effect of MLPs and layer norms. Second, the number of parameters in GPT-2 small\nis orders of magnitude away from state-of-the-art transformer language models. A future challenge\nis to scale this approach to these larger models.\n\nAs a starting point, we performed a preliminary analysis of GPT-2 medium. We found that this\nmodel also has a sparse set of heads directly inﬂuencing the logits. However, not all of these heads\nattend to IO and S, suggesting more complex behavior than the Name Movers Heads in GPT-2 small.\nContinuing this investigation is an exciting line of future work.\n\nMore generally, we think that zooming in on a clearly deﬁned task in a single model enables a rich\ndescription of phenomena that are likely to be found in broader circumstances. In the same way\nmodel organisms in biology enable discoveries valuable beyond a speciﬁc species, we think these\nhighly detailed (though narrow) examples are crucial to developing a comprehensive understanding\nof machine learning models.\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank Neel Nanda, Ryan Greenblatt, Lisa Dunlap, Nate Thomas, Stephen\nCasper, Kayo Yin, Dan Hendrycks, Nora Belrose, Jacob Hilton, Charlie Snell and Lawrence Chan\nfor their useful feedback.\n\n\nREFERENCES\n\nBoaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.\n  Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint\n  arXiv:2207.08799, 2022.\n\n\n                                       12\n\nTolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda B. Vi´egas, and\n  Martin Wattenberg. An interpretability illusion for BERT. CoRR, abs/2104.07143, 2021. URL\n  https://arxiv.org/abs/2104.07143.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\n  wal, Arvind Neelakantan, Pranav Shyam,  Girish Sastry, Amanda Askell, Sandhini Agar-\n  wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\n  Daniel Ziegler,  Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\n  teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\n   dlish, Alec Radford,  Ilya Sutskever, and Dario Amodei.   Language models are few-shot\n   learners.   In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-\n  vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso-\n   ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n  1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nGuendalina Caldarini, Sardar Jaf, and Kenneth McGarry. A literature survey of recent advances in\n   chatbots. Information, 13(1):41, 2022.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n  Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n  Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n  Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n  Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n  2021. https://transformer-circuits.pub/2021/framework/index.html.\n\nAtticus Geiger, Hanson Lu, Thomas F Icard, and Christopher Potts. Causal abstractions of neural\n  networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in\n  Neural Information Processing Systems, 2021. URL https://openreview.net/forum?\n  id=RmuXDtjDhG.\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\n  key-value memories. arXiv preprint arXiv:2012.14913, 2020.\n\nDan Hendrycks and Mantas Mazeika. X-risk analysis for ai research. arXiv, abs/2206.05862, 2022.\n\nEvan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob\n  Andreas. Natural language descriptions of deep visual features. In International Conference on\n  Learning Representations, 2021.\n\nSarthak Jain and Byron C. Wallace.  Attention is not Explanation.  In Proceedings of the 2019\n  Conference of the North American Chapter of the Association for Computational Linguistics:\n  Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3543–3556, Minneapolis,\n  Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357.\n  URL https://aclanthology.org/N19-1357.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.  Locating and editing factual\n  associations in gpt. arXiv preprint arXiv:2202.05262, 2022.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances\n   in neural information processing systems, 32, 2019.\n\nJesse Mu and Jacob Andreas. Compositional explanations of neurons. Advances in Neural Informa-\n   tion Processing Systems, 33:17153–17163, 2020.\n\nNeel Nanda  and Tom  Lieberum.   A  mechanistic  interpretability  analysis  of  grokking,\n  2022.  URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/\n  a-mechanistic-interpretability-analysis-of-grokking.\n\nChris Olah.  Mechanistic interpretability, variables, and the importance of interpretable bases.\n  https://www.transformer-circuits.pub/2022/mech-interp-essay,  2022.\n  Accessed: 2022-15-09.\n\n\n                                       13\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n  Zoom  in:  An introduction to  circuits.    Distill, 2020.    doi:  10.23915/distill.00024.001.\n   https://distill.pub/2020/circuits/zoom-in.\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\n  Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al.  In-context learning and induction\n  heads. arXiv preprint arXiv:2209.11895, 2022.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n  models are unsupervised multitask learners. 2019.\n\nTilman R¨auker, Anson Ho, Stephen Casper, and Dylan Hadﬁeld-Menell. Toward transparent ai:\n A survey on interpreting the inner structures of deep neural networks, 2022. URL https:\n  //arxiv.org/abs/2207.13243.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n  Łukasz Kaiser, and Illia Polosukhin.  Attention is all you need.  In Advances in Neural Infor-\n  mation Processing Systems, pp. 5998–6008, 2017.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\n   Stuart Shieber.  Investigating gender bias in language models using causal mediation analysis.\n  Advances in Neural Information Processing Systems, 33:12388–12401, 2020.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\n  gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol\n  Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.\n  ArXiv, abs/2206.07682, 2022.\n\nAngela Zhang, Lei Xing, James Zou, and Joseph C Wu. Shifting machine learning for healthcare\n  from development to deployment and from models to data. Nature Biomedical Engineering, pp.\n  1–16, 2022.\n\n\n6  APPENDIX\n\nA  DISENTANGLING TOKEN AND POSITIONAL SIGNAL IN THE OUTPUT OF\n    S-INHIBITION HEADS\n\nIn Section 3.2, we discovered that S-Inhibition Heads are responsible for the Name Mover Heads’\nspeciﬁc attention on the IO token, and in particular we discovered that their output is a signal for\nName Movers Heads to avoid the S1 token.\n\nMore precisely, we discover that they were outputting token signals (information about the value of\nthe token S) and positional signals (related to the value of the position S1). We describe here the\ndetails of this discovery.\n\nTo disentangle the two effects, we design a series of counterfactual datasets where only some signals\nare present, and some are inverted with respect to the original dataset. In this section, we conducted\nactivation patching (or simply patching). Instead of patching isolated paths, we patch the output of\na set of heads and recompute the forward pass of the model after this modiﬁcation. As shown in\nSection 3.4, the output of later (Backup) Name Mover Heads and Negative Heads depend on earlier\nheads from these classes, such that it is not possible to ignore the interactions between heads inside\na class (as it is the case in path patching in Appendix B).\n\nBy patching the output of S-Inhibition heads from these datasets, we can quantify in isolation the\nimpact of each signal on the ﬁnal logit difference.\n\nWe constructed six datasets by combining three transformations of the original pIOI distribution.\n\n• Random name ﬂip: we replace the names from a given sentence with random names, but we keep\n  the same position for all names. Moreover, each occurrence of a name in the original sentence\n   is replaced by the same random name. When we patch outputs of S-Inhibition heads from this\n\n\n                                       14\n\n                                 Original positional signal   Inverted position signal\n         Original S token signal           3.55 (baseline)                  -0.99\n      Random S token signal               2.45                       -1.96\n    S↔IO inverted token signal             1.77                       -3.16\n\nFigure 9: Logit difference after patching S-Inhibition heads from signal-speciﬁc datasets. The effect\non logit difference can be decomposed as a sum of the effects of position and token signal.\n\n\n\n                Average attention probability               Average attention probability\n                        of Name Mover Heads                         of Name Mover Heads\n\n                                                               Attention                                                         on                                                                                                                             Attention                                                                                                          on                                                                                            pIOI                                                                                                                                                                        pIOI\n                                        0.8                                                               Attention                                                                               after                                                                        patching       0.8                                                                                                                             Attention                                                                                                                                                   after                                                                                                                                 patching\n                                                                  S-Inhibition                                                         Heads                                                                                                       Heads from                                                                        from      0.7                                S-Inhibition                                        0.7                                                 random name                                                                                                          flip                                                                                       IOS2 replacement\n                                        0.6                                                                         0.6\n\n                                        0.5                                                                         0.5\n\n                                        0.4                                                                         0.4\n\n                                        0.3                                                                         0.3\n\n                                        0.2                                                                         0.2\n\n                                        0.1                                                                         0.1\n\n                                  0   IO     S1     S2                                         0   IO     S 1     S2\n\nFigure 10: Name Mover Heads’ attention probability before and after patching S-Inhibition Heads\nfrom signal-speciﬁc datasets.  Left: patching from the dataset generated by random ﬂip of name\n(same position signal, random token signal). Right: patching from the dataset generated by IO←S2\nreplacement (inverted position signal, inverted token signal).  Black bars represent the standard\ndeviation.\n\n\n  sentence, only positional signals are present, the token signals are unrelated to the names of the\n   original sequence.\n• IO↔S1 ﬂip: we swap the position of IO and S1. The output of S-inhibition heads will contain\n   correct token signals (the subject of the second clause is the same) but inverted positional signals\n  (because the position of IO and S1 are swapped)\n• IO←S2 replacement: we make IO become the subject of the sentence and S the indirect object.\n  In this dataset, both token signals and positional signals are inverted.\n\nWe can also compose these transformations. For instance, we can create a dataset with no token\nsignals and inverted positional signals by applying IO↔S1 ﬂip on the dataset with random names.\nIn total, we can create all six combinations of original, inverted, or uncorrelated token signal with\nthe original and inverted positional signal.\n\nFrom each of those six datasets, we patched the output of S-Inhibition heads and measured the logit\ndifference. The results are presented in Figure 9.\n\nThese results can be summarized as the sum of the two effects. Suppose we deﬁne the variable Stok\nto be 1 if the token signal is the original, 0 when uncorrelated and -1 when inverted. And similarly\nSpos to be 1 if the position signal is the original and -1 if inverted. Then the Figure 9 suggests that\nthe logit difference can be well approximated by 2.31Spos + 0.99Stok, with a mean error of 7%\nrelative to the baseline logit difference.\n\nFor instance, when both the positional and token signals are inverted, the logit difference is the\nopposite of the baseline. This means that the S token is predicted stronger than the IO token, as\nstrong as IO before patching. In this situation, due to the contradictory information contained in\nthe output of S-Inhibition heads, the Name Movers attend and copy the S1 token instead of the IO\ntoken (see Figure 10, right). In the intermediate cases where only one of the signals is modiﬁed,\nwe observe a partial effect compared to the fully inverted case (e.g. Figure 10, left). The effect size\ndepends on the altered signals: positional signals are more important than token signals.\n\nCan we be more speciﬁc as to what the token and positional signals are? Unfortunately, we do not\nhave a complete answer, but see this as one of the most interesting further directions of our work.\nWe expect that the majority of the positional information is about the relative positional embedding\n\n\n                                       15\n\n    A         B          C         D\n            xnew                                        xorig                                        xorig                                        xorig\n\n\n Resid    0.0          0.1         Resid    0.0          0.1         Resid    0.0          0.1         Resid    0.0          0.1\n\n Resid     MLP0            Resid     MLP0            Resid     MLP0            Resid     MLP0\n\n Resid     1.0              1.1         Resid     1.0              1.1         Resid     1.0              1.1         Resid     1.0              1.1\n\n Resid      MLP1             Resid      MLP1             Resid      MLP1             Resid      MLP1\n\n Resid    2.0           2.1         Resid    2.0           2.1         Resid    2.0           2.1         Resid    2.0           2.1\n\n Resid      MLP2            Resid      MLP2            Resid      MLP2            Resid      MLP2\n\n\n Resid     3.0           3.1         Resid     3.0           3.1         Resid     3.0           3.1         Resid     3.0           3.1\n\n         output                       output                       output                      output\n\nFigure 11: Illustration of the four forward passes in the path patching method. h = 0.1 and R =\n{2.0, 3.1}. The layer norms are not shown for clarity. In the forward pass C, we show in red all the\npaths through which h can inﬂuence the value of heads in R. Nodes in black are recomputed, nodes\nin color are patched or frozen to their corresponding values.\n\nbetween S1 and S2 (such pointer arithmetic behavior has already been observed in Olsson et al.\n(2022)). When patching in S2 Inhibition outputs from a distribution where preﬁxes to sentences are\nlonger (but the distance between S1 and S2 is constant), the logit difference doesn’t change (3.56\nbefore patching vs 3.57 after). This suggests that the positional signal doesn’t depend on the absolute\nposition of the tokens, as long as the relative position of S1 and S2 stays the same.\n\n\nB  PATH PATCHING\n\nIn this section we describe the technical details of the path patching technique we used in many\nexperiments. This technique depends on\n\n• xorig the original data point,\n• xnew the new data point,\n• h the sender attention head, and\n\n• R ⊆M the set of receiver nodes in the model’s computational graph M. In our case, R is either\n  the input (key, query or values) of a set of attention heads or the end state of the residual stream3.\n\nIt is then implemented as Algorithm 1, which can be summarized in ﬁve steps.\n\n1. Gather activations on xorig and xnew. [Lines 1-2, Figure 11A-B]\n2. Freeze4 all the heads to their activation on xorig except h that is patched to its activation on xnew.\n  [Lines 4-5, Figure 11C]\n3. Run a forward pass of the model on xorig with the frozen and patched nodes (MLPs and layer\n  norm are recomputed). [Line 7, Figure 11C]\n\n4. In this forward pass, save the activation of the model components r ∈R as if they were recom-\n  puted (but this value is overwritten in the forward pass because they are frozen). [Line 6-9, Figure\n  11C]\n\n   3The model uses the end state of the residual stream to compute the logits (after a layer norm), so we choose\nR equal to this end state when we wish to measure the direct effect of h on the logits.\n  4We use ‘freezing’ of an activation a to refer to the special case of patching when we begin a forward pass\non an input x, edit some earlier activations, and then want to patch a to its value on a forward pass of x (which\ndid not have the edits of earlier activations).\n\n\n                                       16\n\n5. Run a last forward pass on xorig patching the receiver nodes in R to the saved values. [Lines\n  12-20, Figure 11D]\n\nEach of the A variables in Algorithm 1 (Aorig, Anew and Apatch) stores activations for all nodes in\nM. Aorig and Anew contain output activations while Apatch contains the activation of the nodes in\nR (keys, queries or values of attention heads, or the end state of the residual stream).\n\nIn forward pass C of the algorithm all attention heads except h have their output frozen to their value\non xorig. This means that we remove the inﬂuence of every path including one (or more) intermediate\nattention heads p of the form h →p →r, where r ∈R. In all cases in the paper, we measure the\nlogit difference on the forward pass D compared to the logit difference of the model, as a measure\nof the importance of the path h →r. We additionally compute this logit difference as an average\nover N > 200 pairs (xorig, xnew).\n\nAlgorithm 1 Path patching\n 1: Compute all activations Anew on xnew (forward pass A)\n 2: Compute all activations Aorig on xorig (forward pass B)\n 3: for r ∈R do\n 4:    Arpatch ←Aorig\n 5:    Arpatch[h] ←Arnew[h]\n 6:     for each MLP m between h and r do\n 7:       Recompute Arpatch[m] from the activations in Arpatch (forward pass C)\n 8:    end for\n 9:    Recompute Arpatch[r] from the activations in Arpatch (forward pass C)\n10: end for\n11:\n12: Aﬁnal ←∅\n13: for c ∈M nodes of the computational graph, in topologically sorted order, do (forward pass D)\n14:       if c ∈R then\n15:         Aﬁnal[c] ←Acpatch[c]\n16:     else\n17:       Compute Aﬁnal[c] from activations in Aﬁnal\n18:    end if\n19: end for\n20: return Aﬁnal[Logits]\n\n\nC  DIRECT EFFECT ON S-INHIBITION HEADS’ KEYS\n\nIn this section, we present the direct effect analysis of the S-Inhibition Heads’ keys. The experiment\nis similar to the investigation of the S-Inhibition Heads’ values presented in Section 3.2.\n\nThe results presented in Figure 12b show that some heads signiﬁcantly inﬂuence the logit difference\nthrough S-Inhibition Heads’ keys. We observe that Duplicate Token Heads (3.0 and 0.1) appear to\nalso inﬂuence S-Inhibition Heads’ values, but their effect is reversed.\n\nMoreover, we identify 3 new heads inﬂuencing positively the logit difference: 5.9, 5.8 and 0.10.\n\nFuzzy Duplicate Token Head By looking at its attention pattern, we identiﬁed that 0.10 was paying\nattention to S1 from S2. However, the attention pattern was fuzzy, as intermediate tokens also have\nnon-negligible attention probability. We thus call it a fuzzy Duplicate Token Head. On Open Web\nText (OWT), the fuzzy Duplicate Token Head attend to duplicates in a short range (see Appendix I).\n\nFuzzy Induction Heads The heads 5.9, 5.8 are paying attention to S1+1. For 5.8, S1+1 is the token\nwith the highest attention probability after the start token. But the absolute value is small (less than\n0.1). The head 5.9 is paying attention to S1+1 but also to tokens before S. Because of these less\ninterpretable attention patterns, we called them fuzzy Induction Heads. (see Appendix H for more\ndetails about their behavior outside IOI).\n\nBy inﬂuencing the keys of the S-Inhibition Heads, we hypothesize that those heads are amplifying\nthe positional signal written by the other Induction and Duplicate Token Heads.\n\n\n                                       17\n\n                                                                     Direct effect on\n                                                                    S-Inhibition Heads' keys\n\n                                                                                                                     Logit diff.\n                                                                                     0                                         variation\n  S2     h?                                                                                               5%\n            K   V    Name                                   2                                                   2.5%                               Movers\n                              Heads                                   4                    S-Inhibition                                          Logits                                      Layer                                          0%\nEND    Q      Heads\n                                                                                     6                                                       -2.5%\n\n\n                                                                                       0      2      4      6      8     10              -5%\n                                                                       Head\n\n                        (a)                                                              (b)\n\nFigure 12: (a) Diagram of the direct effect experiment on S-Inhibition Heads’ keys at S2. (b) Result\nof the path patching experiments for direct effect experiment on S-Inhibition Heads’ keys.\n\n\n\n                                                                                    Direct effect                   Logit diff.\n                                                             on Induction Heads' key            variation10%\n\nS1+1                                                                            0                                             5%     h?\n           K   V       Name                                 2                                             0%\n                                  Movers                                                                                                                                              Layer\n                                 Heads                                                                                           -5%                   Induction\n S2    Q      Heads      S-Inhibition  Logits                        4                                                                                    0      2      4      6      8     10               -10%\n                               Heads                                     Head\n\n                        (a)                                                              (b)\n\nFigure 13: (a) Diagram of the direct effect experiment on Induction Heads’ keys at S1+1. The\neffects on logits are mediated by S-Inhibition Heads and Name Mover Heads (b) Result of the path\npatching experiments on Induction Heads’ keys at S1+1.\n\n\nD  IDENTIFICATION OF PREVIOUS TOKEN HEADS\n\n\n\nInduction Heads rely on key composition with Previous Token Heads to recognize patterns of the\nform [A] [B] ... [A]. In the context of IOI, the repeated token [A] is S2. We thus searched for\nheads directly affecting Induction Heads keys at the S1+1 position (the [B] token in the general\npattern). For this, we used path patching. The results of the experiment are visible in Figure 13.\nWe identify two main heads causing a decrease in logit difference (and thus contributing positively\nto the logit difference): 4.11 and 2.2. These heads pay primary attention to the previous token.\nThis is coherent with the Previous Token Heads we were expecting. We thoroughly investigate their\nattention patterns outside of pIOI in Appendix H.\n\nOlsson et al. (2022) also describes an induction mechanism relying on query composition in GPT-2.\nWe performed path patching to investigate heads inﬂuencing the Induction Heads queries at the S2\nposition, but did not ﬁnd any signiﬁcant effect.\n\n\n\nE  IOI TEMPLATES\n\n\n\nWe list all the templates we used in Figure 14. Each name was drawn from a list of 100 English ﬁrst\nnames, while the place and the object were chosen among a hand-made list of 20 common words.\nAll the words chosen were one token long to ensure proper sequence alignment computation of the\nmean activations.\n\n\n                                       18\n\n                                      Templates in pIOI\n               Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]\n           Then, [B] and [A] had a lot of fun at the [PLACE]. [B] gave a [OBJECT] to [A]\n       Then, [B] and [A] were working at the [PLACE]. [B] decided to give a [OBJECT] to [A]\n Then, [B] and [A] were thinking about going to the [PLACE]. [B] wanted to give a [OBJECT] to [A]\n               Then, [B] and [A] had a long argument, and afterwards [B] said to [A]\n                 After [B] and [A] went to the [PLACE], [B] gave a [OBJECT] to [A]\n        When [B] and [A] got a [OBJECT] at the [PLACE], [B] decided to give it to [A]\n    When [B] and [A] got a [OBJECT] at the [PLACE], [B] decided to give the [OBJECT] to [A]\n          While [B] and [A] were working at the [PLACE], [B] gave a [OBJECT] to [A]\n         While [B] and [A] were commuting to the [PLACE], [B] gave a [OBJECT] to [A]\n            After the lunch, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]\n             Afterwards, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]\n                Then, [B] and [A] had a long argument. Afterwards [B] said to [A]\n             The [PLACE] [B] and [A] went to had a [OBJECT]. [B] gave it to [A]\n              Friends [B] and [A] found a [OBJECT] at the [PLACE]. [B] gave it to [A]\n\nFigure 14: Templates used in the IOI dataset. All templates in the table ﬁt the ‘BABA’ pattern, but\nwe use templates that ﬁt the ‘ABBA’ pattern as well (i.e, by swapping the ﬁrst instances of [B] and\n[A] in all of the above).\n\n\nF  BACKUP NAME MOVER HEADS\n\n\nHere we discuss in more detail the discovery of the Backup Name Mover Heads. As shown in Figure\n15, knocking-out the three main Name Mover Heads surprisingly changes the behavior of the other\nheads that write in the IO-S direction (both positively and negatively). These heads compensate for\nthe loss of function from the Name Mover Heads such that the logit difference is only 5% lower.\nWe observe that the Negative Name Mover Heads have a less negative effect on logit difference,\nand 10.7 even has a positive effect on the logit difference after the knockout. The other heads that\naffected slightly positively the logit difference before the knock-out become the main contributors.\nBoth the reason and the mechanism of this compensation effect are still unclear. We think that this\ncould be an interesting phenomenon to investigate in future works.\n\nAmong the heads inﬂuencing positively the logit difference after knockout, we identiﬁed S-\nInhibition Heads and a set of other heads that we called Backup Name Mover Heads. We arbitrarily\nchose to keep the eight heads that were not part of any other groups, and affected the logit difference\nwith an effect size above the threshold of 2%.\n\nIn Figure 16 we analyze the behavior of those newly identiﬁed heads with similar techniques as\nName Mover Heads. Those can be grouped in 4 categories.\n\n\n• Four heads (9.0, 10.1, 10.10 and 10.6) that behave similarly to Name Mover Heads in their\n   attention patterns, and the scatter plots of attention vs.   dot product of their output with\n  WU[IO] −WU[S] (e.g 10.10 in Figure 16).\n\n• Two heads (10.2, 11.9) that pay equal attention to S1 and IO and write both of them (e.g 10.2 in\n  Figure 16).\n\n• One head, 11.2, that pays more attention to S1 and writes preferentially in the direction of WU[S].\n\n• One head, 9.7, pays attention to S2 and writes negatively.\n\n\nWe did not thoroughly investigate this diversity of behavior, more work can be done to precisely\ndescribe these heads. However, these heads are also the ones with the less individual importance for\nthe task (as shown by their minimality score in Figure 7). The exact choice of Backup Name Mover\nHeads doesn’t change signiﬁcantly the behavior of the circuit.\n\n\n                                       19\n\n              Direct effect on the logit difference per head            Direct effect on the logit difference per head\n                   (before Name Mover Heads KO)                           (after Name Mover Heads KO)\n\n\n\n\n       50%                                                 50%\n\n\n       25%                                                 25%                                      Name Mover Head               difference                                                                                              Backup Name Mover Head\n                                                                                                  Backup                                                                                       Name                                                                                                              Mover                                                                                                                                              (before                                                                                                                  KO)        logit\n   in  0%                                              0%                                                                                                               Negative                                                                                        Name                                                                                                               Mover                                                                                                            Head\n                                                                                                                                   S-Inhibition Heads              Variation -25%                                                        -25%\n\n\n\n       -50%                                                        -50%\n\n\n\n                                                                                                                                                      (8,                                                                                                                                                            (9,                                                                                                                                                                  (8,                    (9,\n                                                                                                9)                                                                                                        7)                                                                                                                                                      10)                                                                                                            6)    (11,2)    (11,9)   (7,3)   (9,0)    (10,7)                                       9)   (10,1)   (8,10)   (10,6)   (9,7)   (8,6)   (10,2)   (7,3)                 (10,10)    (11,10)    (10,2)    (10,6)    (10,1)   (7,             9)   (10,7)   (9,6)   (11,10)   (10,0)   (10,10)   (11,2)   (7,\n\n                                   (Layer, Head)                                                          (Layer, Head)\n\nFigure 15: Discovery of the Backup Name Mover Heads.  Left:  results from the path patching\nexperiment in Figure 3b. Right: the same path patching experiment results except computed after\nknocking out the Name Mover Heads. In both plots, the heads are ordered by decreasing order of\nthe absolute value of their effect.\n\nG  GPT-2 SMALL FULL ARCHITECTURE\n\nHere we deﬁne all components of the GPT-2 architecture, including those we don’t use in the main\ntext. GPT-2 small has the following parameters\n\n• N: number of input tokens.\n• V : length of vocabulary of tokens.\n• d: residual stream dimension.\n• L: number of layers.\n• H: number of heads per layer.\n• D: hidden dimension of MLPs.\n\nIt uses layer norm, the non-linear function\n\n                                                          def    x −¯x\n                           LN(x) =                          ,                                  (1)\n                  pPi(xi −¯xi)2\n\nwhere the mean and the difference from the mean sum are over all components of the dimension\nd vector in each sequence position. This is then followed by a learned linear transformation M\n(different for each layer norm).\n\nIn GPT-2 the MLPs all have one hidden layer of dimension D and use the GeLU non-linearity. Their\ninput is the layer normed state of the residual stream.\n\nWe addressed the parametrisation of each attention head in the main text, and cover the technical\ndetails of the WQK and WOV matrix here: the attention pattern is Ai,j = softmax(xT W QKx)i,j\n                                                                                                                                  def\nwhere the softmax is taken for each token position, and is unidirectional. We then have hi,j(x) =\nM ◦LN((Ai,j ⊗W OVi,j ).x).\nAlgorithm 2 describe how these elements are combined in the forward pass of GPT-2 small.\n\nH  VALIDATION OF THE INDUCTION MECHANISM ON SEQUENCES OF\n   RANDOM TOKENS\n\nWe run GPT-2 small on sequences of 100 tokens sampled uniformly at random from GPT-2’s token\nvocabulary. Each sequence A was duplicated to form AA, a sequence twice as long where the ﬁrst\nand second half are identical. On this dataset, we computed two scores from the attention patterns\nof the attention heads:\n\n\n                                       20\n\nBackup name               Projection of the output along the\n                                                                             Value weighted attention\n mover head         name embedding vs attention probability\n                                                                                                                                                                                                                                                                                                                       When Jacob and Scott got a drink at the school, Jacob decided to give it to Scott\n                                                                                                                                 When\n                                                                                                                                                              Jacob\n                                                                                                                                                         and\n                                                                                                                                                                                         Scott\n                                                                                                                                                                                        got\n                                                                                                                                                           a\n                                                                                                                                                                                                   drink\n  10.10                                                                                                                                                                 theat\n                                                                                                                                                                                                          school,\n                                                                                                                                                              Jacob\n                                                                                                                                                                      decided\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         give\n                                                                                                                                                                                                                                                                                                                                                          it\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         Scott\n\n                                                                                                                                                                                                                                                                                                                       When Jacob and Scott got a drink at the school, Jacob decided to give it to Scott\n                                                                                                                                 When\n                                                                                                                                                              Jacob\n                                                                                                                                                         and\n                                                                                                                                                                                         Scott\n                                                                                                                                                                                        got\n                                                                                                                                                           a\n                                                                                                                                                                                                   drink\n                                                                                                                                                                                                              at  11.2                                                                                                                                                                   the\n                                                                                                                                                                                                          school,\n                                                                                                                                                              Jacob\n                                                                                                                                                                      decided\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         give\n                                                                                                                                                                                                                                                                                                                                                          it\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         Scott\n                                                                                                                                                                                                                                                                                                                       When Jacob and Scott got a drink at the school, Jacob decided to give it to Scott\n                                                                                                                                 When\n                                                                                                                                                              Jacob\n                                                                                                                                                         and\n                                                                                                                                                                                         Scott\n                                                                                                                                                                                        got\n                                                                                                                                                           a\n                                                                                                                                                                                                   drink\n                                                                                                                                                                                                              at\n                                                                                                                                                                                        the   9.7                                                                                                                                                                                   school,\n                                                                                                                                                              Jacob\n                                                                                                                                                                      decided\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         give\n                                                                                                                                                                                                                                                                                                                                                          it\n                                                                                                                                                                                                              to\n                                                                                                                                                                                         Scott\n                                                                                                                                                                                                                                                                                                                          When Jacob and Scott got a drink at the school, Jacob decided to give it to Scott\n                                                                                                                                  When\n                                                                                                                                                               Jacob\n                                                                                                                                                          and\n                                                                                                                                                                                           Scott\n                                                                                                                                                                                          got\n                                                                                                                                                            a\n                                                                                                                                                                                                     drink\n                                                                                                                                                                                                                at\n  10.2                                                                                                                                                                                   school,the\n                                                                                                                                                               Jacob\n                                                                                                                                                                        decided\n                                                                                                                                                                                                                to\n                                                                                                                                                                                           give\n                                                                                                                                                                                                                                                                                                                                                            it\n                                                                                                                                                                                                                to\n                                                                                                                                                                                           Scott\n\n\nFigure 16: Four examples of Backup Name Mover Heads. Left: attention probability vs projection\nof the head output along WU[IO] or WU[S] respectively.  Right: Attention pattern on a sample\nsequence.\n\n\n\nAlgorithm 2 GPT-2.\nRequire: Input tokens T; returns logits for next token.\n  1: w ←One-hot embedding of T\n  2: x0 ←WEw (sum of token and position embeddings)\n  3: for i = 0 to L do\n  4:     yi ←0 ∈RN×d\n  5:     for j = 0 to H do\n  6:         yi ←yi + hi,j(xi), the contribution of attention head (i, j)\n  7:    end for\n  8:     y′i ←mi(yi), the contribution of MLP at layer i\n  9:    xi+1 ←xi + yi + y′i (update the residual stream)\n10: end for\n11: return WU ◦M ◦LN ◦xL\n\n\n\n\n                                       21\n\n             Average contribution to next token\n                  on repeated sequence\n\n\n                 25\n\n\n                 20\n             and    15                                                                                Induction Heads from our circuit\n                                                                                        Other induction heads                           output    10                                                  embedding                                                        Name Mover Heads                  head     5                                                                 Backup Name Mover Heads\n         of                            token   0                                                                         Negative Name Mover Heads\n                      prod. next                                                                           Others\n             Dot    −5\n\n                −10\n\n\n                −15\n                              7.2   9.9    7.10   6.9   9.6    10.7    11.9    10.1   5.1    10.0     11.10    10.6   8.1     10.11     10.10   5.5   9.1    11.5    10.3   9.0\n\n                                                       x\n\nFigure 17: Contribution to the next token prediction per head on repeated sequences of tokens. The\nheads are ordered by decreasing absolute values of contribution. Black contour: heads with attention\npatterns demonstrating preﬁx matching property.\n\n\n• The previous token score: we averaged the attention probability on the off-diagonal. This is the\n  average attention from the token at position i to position i −1.\n\n• The induction score: the average attention probability from Ti to the token that comes after the\n   ﬁrst occurrence of Ti (i.e. Ti−99)\n\nThese two scores are depicted in Figure 18 (center and right) for all attention heads.\n\nPrevious Token Heads. 4.11 and 2.2 are the two heads with the highest previous token score on\nsequences of random tokens. This is a strong validation of their role outside pIOI.\n\nInduction Heads. Olsson et al. (2022) deﬁne an Induction Head according to its behavior on re-\npeated sequences of random tokens. The attention head must demonstrate two properties. i) Preﬁx-\nmatching property. The head attends to [B] from the last [A] on pattern like [A] [B] ... [A] ii)\nCopy property. The head contribute positively to the logit of [B] on the pattern [A][B]...[A].\n\n5.5 and 6.9 are among the 5 heads with the highest induction score. This validates their preﬁx-\nmatching property introduced in Olsson et al. (2022).\n\nTo check their copy property, we computed the dot product ⟨hi(X), WU[B]⟩between the output of\nthe head hi on sequence X and the embedding of the token [B] on repeated sequences of random\ntokens. The results are shown in Figure 17. The two Induction Heads (5.5 and 6.9) appear in the 20\nheads contributing the most to the next token prediction. Thus validating their copying property.\n\nWe also noticed that the majority of the Negative, Backup and regular Name Mover Heads appear\nto write in the next token direction on repeated sequences of random tokens, and Negative Name\nMovers Heads contribute negatively. This suggests that these heads are involved beyond the IOI task\nto produce next-token prediction relying on contextual information.\n\n\nI  VALIDATION OF DUPLICATE TOKEN HEADS\n\n\nOn the repeated sequences of random tokens (Appendix H) we also computed the duplicate score.\nFor each token Ti in the second half of a sequence, we average the attention probability from Ti to\nits previous occurrence in the ﬁrst half of the sequence (i.e. Ti−100).\n\nThe duplicate token scores for all attention heads are depicted in Figure 18. 3.0 and 0.1 are among\nthe three heads with the highest duplicate token score (Figure 18). This is evidence of their role of\nDuplicate Token Heads outside the circuit for the IOI task.\n\n\n                                       22\n\n    Duplicate token attention prob.      Previous token attention prob.              Induction score on\n   on sequences of random tokens    on sequences of random tokens       sequences of random tokens\n\n\n\n\n\nFigure 18: Attention scores on sequences of repeated random tokens. Left: Duplicate score, the av-\nerage attention probability from a token to its previous occurrence. Center: Previous token attention\nscore, it is the average of the off-diagonal attention probability. Right: Induction score. Average\nattention probability from the second occurrence of [A] to [B] on patterns [A][B]...[A].\n\n\nHowever, the fuzzy Duplicate Head 0.10 doesn’t appear on this test. By qualitatively investigating\nits attention patterns on Open Web Text, we found that this head attends strongly to the current token.\nMoreover, when the current token is a name, and it is duplicated, the head attends to its previous\noccurrence.\n\n\nJ  ROLE OF MLPS IN THE TASK\n\n\nIn the main text, all of the circuit components are attention heads. Attention heads are the only\nmodules in transformers that move information across token positions – a crucial component of the\nIOI task – so they were our main subject of interest. However, MLPs can still play a signiﬁcant role\nin transforming the information in each residual stream. We explored this possibility by measuring\nthe direct and indirect effects of each of the MLPs in Figure 19. In these experiments, for each MLP\nin turn, we did a path patching experiment (Section 3.2) to measure the direct effect and a knock-out\nexperiment for the indirect effect.\n\nWe observe that MLP0 has a signiﬁcant inﬂuence on logit difference after knock-out (it reverses\nthe sign of the logit difference) but the other layers don’t seem to play a big role when individually\nknocked out. When all MLP layers other than the ﬁrst layer are knocked out, however, the logit\ndifference becomes −1.1 (a similar effect to the knockout of MLP0 alone).\n\n\n                                                        Logit diff.                                                            Logit diff.\n                                                           variation                                                              variation\n        Logit difference variation after           10%           Direct effect of MLP on the logit         2%\n       knock-out of MLPs at all tokens                             difference at END\n                                                    5%                                                        1%\n\n                                                    0%                                                        0%\n         0        2        4        6        8       10                               0       2       4       6       8       10\n                        Layer                                                          Layer                           -1%\n                                                               -5%\n\n                                                                                                                                      -2%\n                                                                -10%\n                                                             −115%\n\nFigure 19: Left: change in logit difference from knocking out each MLP layer. Right: change in\nlogit difference after a path patching experiment investigating the direct effect of MLPs on logits.\n\n\n\nK  MINIMALITY SETS\n\n\nThe sets that were found for the minimality tests are listed in Figure 20.\n\n\n                                       23\n\n   v             Class            K ∪{v}           F(C \\ (K ∪{v}))  F(C \\ K)\n (9, 9)      Name Mover                         [(9, 9)]                       2.26            2.62\n (10, 0)     Name Mover                    [(9, 9), (10, 0)]                   1.91            2.26\n (9, 6)      Name Mover                 [(9, 9), (10, 0), (9, 6)]                2.11            1.91\n (10, 7)         Negative                    [(11, 10), (10, 7)]                  4.07            3.13\n(11, 10)        Negative                    [(11, 10), (10, 7)]                  4.07            3.27\n (8, 10)      S Inhibition              [(7, 9), (8, 10), (8, 6), (7, 3)]             0.24            1.01\n (7, 9)       S Inhibition              [(7, 9), (8, 10), (8, 6), (7, 3)]             0.24            0.92\n (8, 6)       S Inhibition              [(7, 9), (8, 10), (8, 6), (7, 3)]             0.24            0.86\n (7, 3)       S Inhibition              [(7, 9), (8, 10), (8, 6), (7, 3)]             0.24            0.43\n (5, 5)          Induction                [(5, 9), (5, 5), (6, 9), (5, 8)]              0.97            2.12\n (5, 9)          Induction                [(11, 10), (10, 7), (5, 9)]               3.33            4.07\n (6, 9)          Induction                [(5, 9), (5, 5), (6, 9), (5, 8)]              0.97            1.46\n (5, 8)          Induction                [(11, 10), (10, 7), (5, 8)]               3.83            4.07\n (0, 1)      Duplicate Token               [(0, 1), (0, 10), (3, 0)]                0.60            1.90\n (0, 10)      Duplicate Token               [(0, 1), (0, 10), (3, 0)]                0.60            1.66\n (3, 0)      Duplicate Token               [(0, 1), (0, 10), (3, 0)]                0.60            1.05\n (4, 11)      Previous Token                   [(4, 11), (2, 2)]                   1.31            2.28\n (2, 2)       Previous Token                   [(4, 11), (2, 2)]                   1.31            1.75\n (11, 2)   Backup Name Mover   All previous NMs and backup NMs          0.95            1.37\n (10, 6)   Backup Name Mover   All previous NMs and backup NMs          1.65            1.88\n(10, 10)  Backup Name Mover   All previous NMs and backup NMs          1.88            2.11\n (10, 2)   Backup Name Mover   All previous NMs and backup NMs          1.49            1.65\n (9, 7)   Backup Name Mover   All previous NMs and backup NMs          0.81            0.95\n (10, 1)   Backup Name Mover   All previous NMs and backup NMs          1.37            1.49\n (11, 9)   Backup Name Mover   All name movers and negative heads          0.41            0.45\n (9, 0)   Backup Name Mover   All name movers and negative heads          0.41            0.45\n\n                        Figure 20: K sets for minimality for each v.\n\n\n\n\n\n                                     24\n\n                K found by greedy optimization\n                                     (9, 9), (9, 6), (5, 8), (5, 5), (2, 2)\n                               (9, 9), (11, 10), (10, 7), (8, 6), (5, 8), (4, 11)\n                                    (10, 7), (5, 5), (2, 2), (4, 11)\n                           (9, 9), (11, 10), (10, 7), (11, 2), (3, 0), (5, 8), (2, 2)\n\n         Figure 22: 4 sets K found by the greedy optimization procedure on our circuit.\n\nL  TEMPLATE FOR ADVERSARIAL EXAMPLES\n\nThe design of adversarial examples relies on adding a duplicate IO to the sentences. To this end, we\nused a modiﬁcation of the templates described in appendix E. We added an occurrence of [A] in the\nform of a natural sentence, independent of the context. The list of sentence is visible in Figure 21.\n\n                                  [A] had a good day.\n                              [A] was enjoying the situation.\n                                     [A] was tired.\n                             [A] enjoyed being with a friend.\n                              [A] was an enthusiast person.\n\nFigure 21: Templates for the natural sentences used in the generation of adversarial examples. The\nsentences were chosen to be independent of the context.\n\n\nM  GREEDY ALGORITHM\n\nThe Algorithm 3 describes the procedure used to sample sets for checking the completeness criteria\nusing greedy optimization. In practice, because the na¨ıve and the full circuit are not of the same size,\nwe chose respectively k = 5 and k = 10 to ensure a similar amount of stochasticity in the process.\nWe run the procedure 10 times and kept the 5 sets with the maximal important incompleteness score\n(including the intermediate K).\n\nAlgorithm 3 The greedy sampling procedure for sets to validate the completeness citeria.\n\n 1: K ←∅\n 2: for i to N do\n 3:    Sample a random subset V ⊆C \\ K of k nodes uniformly.\n 4:    vMAX ←arg maxv∈V |F(C \\ (K ∪{v})) −F(C \\ K)|\n 5:  K ←K ∪{vMAX}\n 6: end for\n 7: return K\n\nAs visible in Figure 22 the sets found by the greedy search contain a combination of nodes from dif-\nferent classes. This means that it is difﬁcult to interpret how our circuit is incomplete, as mentioned\nin the main text.\n\n\n\n\n\n                                       25\n\n## Claim to Classify\n\"Our explanation encompasses 26 attention heads grouped into 7 main classes\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Path patching / activation patching / knockouts (causal interventions) used to discover and validate the set of heads and their grouping",
    "method_rung": 2,
    "claim_rung": 1,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The statement is primarily a descriptive/quantitative inventory claim about what their proposed explanation contains (a set of 26 heads grouped into 7 classes), not a causal or mechanistic attribution about what those heads do. It does not assert that these heads are responsible for IOI or implement an algorithm; it just counts/components-in-explanation. The supporting methodology in the paper for identifying these heads relies on interventional causal techniques (path patching, ablations/knockouts), which are R2."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 27561,
      "completion_tokens": 166,
      "total_tokens": 27727
    },
    "finish_reason": "stop"
  }
}