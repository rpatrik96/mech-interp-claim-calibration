{
  "timestamp": "2026-02-19T09:10:26.505758+00:00",
  "paper_id": "2404.15255",
  "claim_id": "2404.15255-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2404.15255\nTitle: How to use and interpret activation patching\nFull text:\n           How to use and interpret activation patching\n\n\n\n\n                                          Stefan Heimersheim                     Neel Nanda\n                               stefan.heimersheim@gmail.com\n\n\n                                                 Abstract\n\n                               Activation patching is a popular mechanistic interpretability technique, but has2024\n                        many subtleties regarding how it is applied and how one may interpret the results.\n                     We provide a summary of advice and best practices, based on our experience using\n                                    this technique in practice. We include an overview of the different ways to applyApr                                  activation patching and a discussion on how to interpret the results. We focus on\n23                      whatmetricevidenceand associatedpatchingpitfalls.experiments provide about circuits, and on the choice of\n\n\n                1  Introduction\n\n                       1.1  What is activation patching?[cs.LG]                        Activation patching (also referred to as Interchange Intervention, Causal Tracing, Resample Ablation,\n                       or Causal Mediation Analysis) is the technique of replacing internal activations of a neural net. It is\n                        also known as Causal Tracing, Resample Ablation, Interchange Intervention or more generally as\n                     Causal Mediation Analysis. Variants of this technique have been widely used in the literature (Vig\n                           et al., 2020; Geiger et al., 2021a; Geiger, Richardson, and Potts, 2020; Soulos et al., 2019; Finlayson\n                           et al., 2021; Geiger et al., 2021b; Meng et al., 2022; Wang et al., 2022; Chan et al., 2022; Hase et al.,\n                     2023; Hanna, Liu, and Variengien, 2023; Conmy et al., 2023; Todd et al., 2023; Hendel, Geva, and\n                      Globerson, 2023; Feng and Steinhardt, 2023; Lieberum et al., 2023; Cunningham et al., 2023; Stolfo,\n                       Belinkov, and Sachan, 2023; Goldowsky-Dill et al., 2023; McDougall et al., 2023; Geva et al., 2023;\n                  Huang et al., 2023; Merullo, Eickhoff, and Pavlick, 2023; Tigges et al., 2023). Here we focus on the\n                      technique where we overwrite some activations during a model run with cached activations from a\n                       previous run (on a different input), and observe how this affects the model’s output.\n\n                       1.2  How is this related to ablation?\n\n                      Ablation is the common technique of zeroing out activations. Activation patching is more targetedarXiv:2404.15255v1                    and controlled: We replace activations with other activations rather than zeroing them out. This\n                       allows us to make targeted manipulations to locate specific model behaviours and circuits.\n\n                       1.3  An example\n\n                     For example, let’s say we want to know which model internals are responsible for factual recall in\n              ROME (Meng et al., 2022). How does the model complete the prompt “The Colosseum is in” with\n                        the answer “Rome”? To answer this question we want to manipulate the model’s activations. But the\n                   model activations contain many bits of information: This is an English sentence; The landmark in\n                       question is the Colosseum; This is a factual statement about a location.\n\n                      Ablating some activations will affect the model if these activations are relevant for any of these bits.\n                    But activation patching allows us to choose which bit to change and control for the others. Patching\n                     with activations from “Il Colosseo è dentro” locates where the model stores the language of the\n                     prompt but may use the same factual recall machinery. Patching with activations from “The Louvre is\n                        in” locates which part of the model deals with the landmark and information recall. Patching between\n\n“The Colosseum is in the city of” and “The Colosseum is in the country of” locates the part of the\nmodel that determines which attributes of an entity are recalled.\n\nA simple activation patching procedure typically looks like this:\n\n       1. Choose two similar prompts that differ in some key fact or otherwise elicit different model\n         behaviour:\n\n              E.g. “The Colosseum is in” and “The Louvre is in” to vary the landmark but control\n               for everything else.\n\n       2. Choose which model activations to patch\n\n              E.g. MLP outputs\n\n       3. Run the model with the first prompt—the source prompt—and save its internal activations\n\n              E.g. “The Louvre is in” (source)\n\n       4. Run the model with the second prompt—the destination prompt—but overwrite the selected\n          internal activations with the previously saved ones (patching)\n\n              E.g. “The Colosseum is in” (destination)\n\n       5. See how the model output has changed. The outputs of this patched run are typically\n        somewhere between what the model would output for the un-patched first prompt or second\n        prompt\n\n              E.g. observe change in the output logits for \"Paris\" and \"Rome\"\n\n       6. Repeat for all activations of interest\n\n              E.g. sweep to test all MLP layers\n\n\n1.4  What is this document about\n\nWe want to communicate useful practical advice for activation patching, and warn of common pitfalls\nto avoid. We focus on three areas in particular:\n\n       1. What kind of patching experiments provide which evidence? (Section 2)\n\n       2. How should you interpret activation patching results? (Section 3)\n\n       3. What metrics you can use, what are common pitfalls? (Section 4)\n\nFor a general introduction to mechanistic interpretability in general, and activation patching in\nparticular we refer to ARENA1 chapter 1 (in particular activation patching in chapter 1.3) as well as\nthe corresponding glossary entries on Neel Nanda’s website2.\n\n2  What kind of patching experiments should you run?\n\n2.1  Exploratory and confirmatory experiments\n\nIn practice we tend to find ourselves in one of two different modes of operation: In exploratory mode\nwe run experiments to find circuits and generate hypotheses. In confirmatory mode we want to verify\nthe circuit we found and check if our hypothesis about its function is correct.\n\nIn exploratory patching we typically patch components one at a time, often in a sweep over the\nmodel (layers, positions, model components). We do this to get an idea of which parts of a model are\ninvolved in the task in question, and may be part of the corresponding circuit.\n\nIn confirmatory patching we want to confirm a hypothesised circuit by verifying that it actually covers\nall model components needed to perform the task in question. We typically do this by patching\nmany model components at once and checking whether the task performance behaves as expected. A\nwell-known example of patching for circuit verification is Causal Scrubbing (Chan et al., 2022).\n\n   1ARENA: https://arena3-chapter1-transformer-interp.streamlit.app/\n   2https://neelnanda.io/glossary\n\n\n                                        2\n\n        2.2  Which components should you patch\n\n        Patching can be done on different levels of granularity determining the components to patch. For\n        example, we may patch the residual stream at a certain layer and position, or the output of a certain\n     MLP [layer, position] or Attention Head [layer, head, position]. At even higher granularity we could\n        patch individual neurons or sparse autoencoder features.\n\n      An even more specific type of patching is path patching. Usually, patching any component will affect\n          all model components in later layers (“downstream”). In path patching instead we let each patch\n         affect only a single target component. We call this patching the “path” between two components. For\n         details on patch patching we refer to ARENA section 1.3.4.\n\n        Path Patching can be used to understand whether circuit components affect each other directly, or\n        via mediation by another component. For example if we want to distinguish between mediation\n       (component A affects output C via component B), and amplification/calibration (component A affects\n        output C directly, but component B reads from A and also affects output C by boosting or cancelling\n        the signal to amplify or calibrate component A). These two structures look identical in default\n       component patching, but different in path patching: a direct connection (composition) between A and\n     C exists only in the second case.\n\n      As a rule of thumb, you want to start with low-granularity patching (e.g. residual stream patching),\n        then increase granularity, and finally use path patching to test which components interact with each\n         other. Fast approximations to activation patching, such as attribution patching (see Nanda, 2023, and\n         also AtP*, atpstar) can help speed up this process in large models.\n\n\n        2.3  Noising and Denoising\n\n       There are multiple ways to do activation patching. The techniques differ in what the source (source of\n         activations / model run from which the activations are copied) and destination prompt (destination that\n          is overwritten / model run in which the activations will be inserted, this is called base in Interchange\n        Interventions language, Geiger et al., 2021b) are. The use of words “source” and “destination” is\n        unrelated to their meaning in Transformer attention.\n\n       The two main methods are Denoising and Noising (see the next section for other methods).\n\n               1. Denoising: We can patch activations from a clean first prompt into a corrupted second\n               prompt “clean →corrupt”. That is running the model on the clean prompt while saving its\n                   activations, then running the model on the corrupted prompt while overwriting some of its\n                  activations with previously saved clean-prompt activations. We observe which patch restores\n                 the clean-prompt behaviour, i.e. patching which activations were sufficient to restore the\n                 behaviour.\n\n               2. Noising: Or you can patch activations from a corrupted first prompt into a clean second\n               prompt “corrupt →clean”. That is running the model on the corrupted prompt while saving\n                      its activations, then running the model on the clean prompt while overwriting some of\n                      its activations with previously saved corrupt-prompt activations. We observe which patch\n                breaks the clean-prompt behaviour, i.e. patching which activations were necessary to\n               maintain for the behaviour.\n\n      An important and underrated point is that these two directions can be very different, and are not just\n       symmetric mirrors of each other. In some situations denoising is the right tool, and in others it’s\n         noising, and understanding the differences is a crucial step in using patching correctly.\n\nTechnique                Source (saved)     Source run input   Destination / Base (over-  Destination      /   Observation\n                                                                      written)                 Base run input\n\nClean →corrupted (De-   First run  activa-  Clean tokens     Second run  activations   Corrupt tokens   What restores\nnoising, Causal Tracing2)   tions (clean)                           (corrupted)                                 behaviour\nCorrupted →clean (Nois-   First run  activa-  Corrupt tokens    Second run  activations   Clean tokens     What breaks\ning, Resample Ablation)    tions (corrupted)                        (clean)                                     behaviour\n\n       For now we round patching effects to “if I patch these activations the model performance is / isn’t\n         affected”. We discuss metrics and measuring patching effects in the last section.\n\n\n                                              3\n\n2.4  Example: AND gate vs OR gate\n\nConsider a hypothetical circuit of three components A, B, and C that are connected with an AND\nor an OR gate. They are embedded in a much larger network, and of the three just C is connected\nto the output. We run an experiment where we patch all components using the denoising or noising\ntechnique.\n\n\nAND circuit: C = A AND B\n\n        • Denoising (clean →corrupt patching): Denoising either A or B has no effect on the output,\n        only denoising C restores the output. This is because denoising A still leaves B at the\n        corrupted (incorrect) baseline, and vice versa. Denoising found only one of the circuit\n        components.\n\n        • Noising (corrupt →clean patching): Noising either A or B has an effect, as well as noising\n        C.\n\nNoising works better in this case, as it finds all circuit components in the first pass.\n\n\nOR circuit: C = A OR B\n\n        • Denoising (clean →corrupt patching): Denoising either A or B has an effect, as well as\n        denoising C.\n\n        • Noising (corrupt →clean patching): Noising either A or B has no effect on the output, only\n         denoising C restores the output. This is because noising A still leaves B at the clean (correct)\n         baseline, and vice versa. Denoising found only one of the circuit components.\n\nDenoising works better in this case, as it finds all circuit components in the first pass. These AND\nand OR structures can appear in real-world transformers as serial-dependent components (e.g. a\nlater attention head depending on an earlier one) or parallel components (such as redundant backup\nattention heads).\n\n\n2.5  Comparison to ablations and other patching techniques\n\nThere are activation patching techniques based on a single prompt. The original Causal Tracing\n(ROME, Meng et al., 2022) falls into this category, and also zero- and mean-ablation can be seen as\npatching techniques.\n\n      1. Zero ablation: Overwrite (“ablate”) the targeted activations with zeros and observe ablating\n       which component breaks the model behaviour.\n\n      2. Mean ablation: Same as above but overwrite targeted activations with their dataset mean\n        value rather than zero. This is slightly more principled than zero ablating since there is no\n         special meaning to activations being zero.\n\n      3. Gaussian noise patching (also called Causal Tracing*): This is a clean →corrupt patching\n         variant that uses as its corrupt run input the embeddings of the clean prompt with added\n        Gaussian noise. The idea is to thereby automatically generate the corrupted “prompt”. It\n       was originally used in ROME (called Causal Tracing there) but has not been used much\n          recently, especially because the corruption can sometimes be ineffective.3\n\n\n   3The success of Gaussian noise corruption is highly sensitive to the noise level. Zhang and Nanda (2023)\nthat if the noise level is just slightly lower than used in ROME, the model can recover the correct completion\ndespite the corruption.\n\n\n                                       4\n\nTechnique                Source (saved)     Source run input   Destination / Base (over-  Destination      /   Observation\n                                                                      written)                 Base run input\n\nClean →corrupted (De-   First run  activa-  Clean tokens     Second run  activations   Corrupt tokens   What restores\nnoising, Causal Tracing2)   tions (clean)                           (corrupted)                                 behaviour\nCorrupted →clean (Nois-   First run  activa-  Corrupt tokens    Second run  activations   Clean tokens     What breaks\ning, Resample Ablation)    tions (corrupted)                        (clean)                                     behaviour\nZero ablation             Zero activations   N/A             Clean run activations      Clean tokens     What breaks\n                                                                                                         behaviour\nMean ablation              Dataset mean acti-  N/A             Clean run activations      Clean tokens     What breaks\n                            vations                                                                         behaviour\nGaussian Noise patching   First run  activa-  Clean tokens     Second run  activations   Clean token em-  What restores\n(Causal Tracing2)           tions (clean)                           (corrupted from modified   bedding + Gaus-  behaviour\n                                                                  clean input)                sian noise\n\n        Generally we recommend corrupted-prompt-based techniques, noising and denoising. Their ad-\n        vantage is that one can run very precise experiments, editing some features while controlling for\n         others. They allow us to trace the difference between clean and corrupted prompt. To illustrate this\n        consider the prompts “Angela Merkel is the leader of” →“Germany” vs “Joe Biden is the leader of”\n       →“America”. Patching will find components that deal with Angela Merkel vs Joe Biden, but not\n       components that would be indifferent to this change, such as the “answer is a country circuit” or the\n         “political leader circuit”. A secondary advantage of noising and denoising is that they tend to bring\n        the model less out-of-distribution than ablation techniques (as pointed out in Chan et al. (2022), as\n        well as in e.g. Hase, Xie, and Bansal, 2021)\n\n\n        2.6  Choosing corrupted prompts\n\n       Having a corrupted prompt is great because it can tell us what model components care about, but also\n        a possible pitfall if we don’t notice what our prompts trace and don’t trace. We give some examples\n         for the Indirect Object Identification (IOI, Wang et al., 2022) demo sentence “John and Mary went to\n        the store. John gave a bottle of milk to”. Different corruptions which highlight different properties\n         the model might care about include:\n\n          Corruption                               Example                     Property traced in model\n\n        None (Clean)                               John and Mary . . . John . . .\n         Replace the value of one or multiple names,  John and Alice ... John ...  Where the model repre-\n          without changing the grammatical structure      Alice and Mary ... Alice ...   sents the name values\n                                                        Alice and Bob . . . Alice . . .\n         Change which name is direct and indirect object   John and Mary . . . Mary . . .   The value and position of\n          without changing the names or positions                                        the indirect object\n         Change the position of the names without chang-  Mary and John . . . Mary . . .   The value, but not the po-\n          ing which one is subject and indirect object                                        sition of the indirect ob-\n                                                                                                      ject (position is fixed)\n         Change a name to break the behaviour          John and Mary ... Alice ...   Specifics about IOI set-\n                                                        Alice and Mary . . . John . . .    ting (e.g. that a name is\n                                                                                         duplicated at all)\n         Change all the names                          Alice and Bob ...  Charlie   Finding and confirming\n                                                                                                  . . .                                   all relevant components\n\n\n      What kind of prompt should you choose? No matter which you choose, keep in mind what properties\n        your prompt does and does not change, and take this into account when interpreting patching results.\n      As a rule of thumb you want to choose small (narrow) variations for exploratory patching, this will\n        help you narrow down what each component is tracking. Choosing a narrow prompt distribution also\n        helps increase the (typically low) sensitivity of denoising, and decrease the (typically high) sensitivity\n        of noising. For confirmatory patching you need to choose a wide distribution of prompts that varies\n          all variables of the hypothesised circuit. Then you can noise (corrupt →clean patch) all non-circuit\n        components, and check that the model still shows the behaviour in question.\n\n\n            2Causal Tracing has been used to describe ROME-style Gaussian noise patching in particular, but also to\n         describe clean →corrupted patching in general. We recommend avoiding the name to avoid confusion.\n\n\n                                              5\n\n3  How do you interpret patching evidence?\n\nIn the previous section we said that denoising (clean →corrupt patching) tests whether the patched\nactivations are sufficient to restore model behaviour. And noising (corrupt →clean patching) tests\nwhether the patched activations are necessary to maintain model behaviour. These two are usually\nnot complements of each other, nor does one imply the other. In this section we will walk through a\nmade-up example experiment.\n\n3.1  Walkthrough a stylized example\n\nConsider the hypothetical “Nobel Peace Prize” circuit. The model correctly completes “Nobel Peace”\nwith “Prize”, using the following circuit:\n\n        • Attention head L0H0 is a “Previous Token Head” and copies the embedding of “Nobel” to\n         the position of “Peace”\n\n        • Neuron L1N42 maps the mix of Nobel and Peace embeddings to the Prize logit\n\n        • Everything else doesn’t matter (of course a real circuit is typically much messier)\n\n\n\n\n\n                           Figure 1: Toy \"Nobel Peace Price\" circuit\n\n\nNow let us run the standard patching examples, take a distribution of random English words for the\ncorrupted prompt. We would find\n\n        • Noising (corrupt →clean patching) suggests that the outputs of head L0H0, the output of\n        neuron L1N42, and the embeddings (Nobel & Peace) are all necessary components.\n        • Denoising (clean →corrupt patching) suggests that the output of neuron L1N42 is sufficient\n          to restore the circuit.\n\nWhat happened here?  Denoising finds only the neuron output L1N42, because the other two\ncomponents individually are not sufficient to restore the circuit behaviour! We’re dealing with an\nAND circuit between the attention head output and the “Peace” embedding. Noising finds all three\ncomponents here.\n\nNonetheless denoising L1N42 alone restored the model behaviour. This is a crucial intuition to keep\nin mind about denoising: If you patch component A in layer N, it has seen clean versions of every\ncomponent in layers 0 to N-1. If there’s an important component B in layer N-1 that is mediated by\ncomponent A, the model can be restored without denoising B.\n\nPatching experiments are sensitive to what precisely are the changes between the corrupt and clean\nprompt. If we created two additional corrupt distributions where we replace only either “Nobel” or\n“Peace” with a random word (i.e. distributions “X Peace” and “Nobel Y”) we could narrow down\nwhich component depends on which input.\n\n\n                                        6\n\nAlternatively we could use path patching to confirm the precise interactions. Say we want to test\nwhether the Peace embedding is necessary as an input to L0H0, as an input to L1N42, or both. For\nthis we could patch only the corresponding paths, and find that denoising (1) “Nobel →L0H0” and\n(2) “Peace →L1N42” paths is sufficient. Alternatively we might find that noising every path except\nfor (1) “Nobel →L0H0”, (2) “L0 →L1N42”, and (3) “Peace →L1N42” does not break performance.\nNote again that denoising only required restoring two paths (restoring a cross-section of the circuit)\nwhile noising required leaving 3 paths clean (the full circuit).4\n\n\n3.2  Concepts & gotchas\n\nThe walkthrough above presents a typical circuit discovery workflow. We want to highlight a couple\nof additional concepts and common issues.\n\n\nSensitivity & prompt choice: A positive patching result implies you have found activations dealing\nwith the difference between the clean and corrupt prompt. Make sure to consider all degrees of\nfreedom in a task, and consider multiple sets of corrupted prompts if necessary.\n\n\nScope of activation patching:  More generally, activation patching is always based on prompt\ndistributions, and does not make statements for model behaviour outside these specific distributions.\nFor more discussion on the limitations of patching, and the specificity of prompt-based interpretability\nin general, see Neel Nanda’s writing on What Can(’t) Activation Patching Teach Us.\n\n\nNo minimality:  Here, and in many parts of the literature, a circuit is treated as a collection of model\ncomponents that are responsible for a particular model behaviour. We typically make no claims\nthat we have found the smallest such collection of components, we only test that this collection is\nsufficient.\n\n\nBackup behaviour & OR-gates:  In some cases researchers have discovered “Backup heads”,\ncomponents that are not normally doing the task but jump into action of other components are\ndisrupted (Hydra effect, McGrath et al., 2023). For example, in IOI when one ablates a name mover\nhead (a key component of the circuit) a backup name mover head will activate and then do the task\ninstead (Wang et al., 2022).\n\nIt can be helpful to think of these as OR-gates where either component is sufficient for the model to\nwork. This does not fit well into our attempts of defining a circuit, nor plays well with the circuit\nfinding methods above. Despite the name mover heads being important, if we ablate them then, due\nto backup heads compensating, the name movers look less important. Fortunately, backup behaviour\nseems to be lossy, i.e. if the original component boosted the logits by +X, the backup compensates\nfor this by boosting less than X (the Hydra effect paper found 0.7*X). Thus these backup component\nweaken the visibility of the original component, but it is usually still visible since even 0.3*X is a\nrelatively large effect.\n\n\nNegative components:  Some work in this area (e.g. Wang et al., 2022; Heimersheim and Janiak,\n2023) noticed attention heads that consistently negatively affected performance, and noising them\nwould increase performance. This is problematic, because it makes it hard to judge the quality of a\ncircuit analysis: it may look like we’ve fully recovered (or more than fully recovered!) performance,\nby finding half the positive components but excluding all negative ones. This is an unsolved problem.\nConmy et al. (2023) propose using Kullback Leibler (KL) divergence as a metric to address this,\nwhich penalises any deviation (positive or negative), at the cost of also tracking lots of variation we\nmay not care about.\n\n\n   4This method doesn’t yet confirm which information is carried in the different paths. We can go a step further\nand noise (corrupt →clean patch) even some of the important circuit connections, namely “Nobel →L0H0 →\nL1N42” path from the “Nobel Y” distribution, and the “Peace →L1N42” path from the “X Peace” distribution.\nDoing that is essentially Causal Scrubbing (Chan et al., 2022).\n\n\n                                       7\n\n4  Metrics and common pitfalls\n\nSo far we talked about “preserving” and “restoring” performance, but in practice, model performance\nis not binary but a scale. Typically we find some components matter a lot, while others provide a\nsmall increase in performance. For the best interpretability we might look for a circuit restoring e.g.\n90% of the model’s performance, rather than reaching exactly 100% (for examples see Chan et al.,\n2022). A useful framing is the “pareto frontier” of circuit size vs. performance recovered - recovering\n80% of performance with 1% of the components is more impressive than 90% of the performance\nwith 10% of the components, but there will always be a minimum circuit size to recover a given level\nof performance.\n\nIt’s easy to treat metrics as an after-thought, but we believe that the right or wrong choice of a metric\ncan significantly change the interpretation of patching results. Especially for exploratory patching,\nthe wrong metric can be misleading. The choice of metric matters less for confirmatory patching,\nwhere you expect a binary-ish answer (“have I found the circuit or not”) and all metrics should agree.\nWe’ll go through a couple of metric choices in this section:\n\n\n  Based on                          Example\n\n  Logit difference (= Logprob difference)   Logit(Mary) - Logit(John)\n  Logarithmic probability (logsoftmax)    Logprob(Mary)\n  Probability (softmax)                  Prob(Mary)\n  Accuracy / Rank of correct answer      Rank(Mary)==0\n\n\nAn honourable mention goes to the KL divergence. Unlike the previous metrics, this metric aims\nto compare the full model output, rather than focusing on a specific task. KL divergence is a good\nmetric in such cases.\n\nIn addition to these output based metrics, in some cases it makes sense to consider some model\ninternals as metrics themselves. For example, one might use the attention paid by the name mover\nhead to the indirect object as a metric to identify the subcircuit controlling this head, or the activation\nof a key neuron or SAE feature, or the projection onto a probe (Nanda et al., 2023).\n\nIn our experience, it’s worth implementing many metrics and briefly analysing all of them. Computing\na metric is cheap (compared to the cost of the forward pass), and they all have different strengths and\nweaknesses, and can illuminate different parts of the big picture. And if they all agree that’s stronger\nevidence than any metric on its own. Where they disagree, we personally trust logit difference (or\nequivalently logprob difference) the most.\n\n4.1  The logit difference\n\nLogit difference measures to what extent the model knows the correct answer, and it allows us to\nbe specific: We can control for things we don’t want to measure (e.g. components that boost both,\nMary and John, in the IOI example) by choosing the right logits to compare (e.g. Mary vs John, or\nmultiple-choice answers). The metric also is a mostly linear function of the residual stream (unlike\nprobability-based metrics) which makes it easy to directly attribute logit difference to individual\ncomponents (“direct logit attribution”, “logit lens”). It’s also a “softer” metric, allowing us to see\npartial effects on the model even if they don’t change the rank of the output tokens (unlike e.g.\naccuracy), which is crucial for exploratory patching. We discuss problems with this and other metrics\nin the next section.\n\nIntuition for why logits and logit differences (LDs) are a natural unit for transformers: The residual\nstream and output of a transformer is a sum of components. Every component added to the residual\nstream corresponds to an addition to the LD (as the LD corresponds to a residual stream direction, up\nto layer norm). A model component can easily change the LD by some absolute amount (e.g. +1 LD).\nIt cannot easily change the LD by a relative amount (LD *= 1.5), or change the probabilities by a\nspecific amount (prob += 0.20). For example consider a model component that always outputs -1 logit\nto duplicated names (assume “John and Mary ... John ...”). This component then always writes\n+1 LD in favour of Mary, and gets a score of 1 in terms of LD. Other metrics (such as probability)\njudge this component differently, depending on what the baseline was (e.g. due to other patches). We\nwould argue that logits and logit differences are closer to the mechanistic process happening in the\n\n\n                                        8\n\ntransformer, and thus feel like a more natural unit. This is of course not a requirement, and also does\nnot hold in all places (e.g. if a component’s output depends on the input LD), but it seems to work\nwell in practice.\n\n4.2  Flaws & advantages of different metrics\n\nIt is essential to be aware of what a metric measures and is sensitive to. A key thing to track is whether\nthe metric is discrete vs continuous, and whether it’s exponential vs linear (in the logits) - continuous,\nlinear metrics are usually more accurate, which is crucial when doing exploratory patching and\nassigning “partial credit” to model components. Here we list common pitfalls of popular metrics.\n\n\n\n\n\nFigure 2: Illustration of different metrics for an example patching experiment with GPT-2 medium.\n\n        • Logit difference / logprob difference: The difference between the logit of the correct answer,\n       and the incorrect answer(s). This metric specifically measures the difference between the\n         selected logits, and is not sensitive to components which affect all of them. For example, in\n        IOI it measures the model’s confidence in Mary vs John which encapsulates the IOI-circuit\n        well without being sensitive to the “is the next token a name?”-circuit.\n        • Potential false-positive: Because the metric is a difference it may be driven by either getting\n          better at the correct answer or worse at the incorrect answer. Thus it is worth checking the\n          logits or logprobs of individual answers to confirm.\n        This is particularly concerning because the corrupted model likely puts a high probability\n       on the incorrect answer. This means that any patch that indiscriminately damages the model\n        and gets it closer to uniform will damage the incorrect answer logprob and so boost the logit\n           diff.\n        • Logprobs: This metric measures the logprob of the correct answer. It is sensitive to absolute\n        change in logarithmic probabilities (i.e. relative change in probabilities) and captures our\n          intuition for what good model performance means. We broadly think it is a good metric. It’s\n       main flaws are\n         – Saturation: Once the correct answer becomes the model’s top guess, the logprob stops\n              increasing meaningfully, even though the confidence can increase much more.\n         We can see this in Figure 2, where the green line saturates after layer 17.\n         – Unspecificity: We lose the ability to control for other properties, e.g.  in IOI we\n             cannot distinguish between components that increase both P(John) and P(Mary) from\n            components that only increase P(Mary). This can be intended, or unintended, it’s just\n             important to keep in mind.\n         – Inhibition: To increase the logprob on John, the model can either increase the John\n                 logit, or decrease other top logits, and it is hard to distinguish which is happening. This\n\n\n                                       9\n\n     may be desirable or not because the two operations likely have different mechanisms\n      and may be better tracked separately.\n\n• Probabilities: This metric measures the probability of the right answer, or the difference in\n  probabilities of different answers. The main issue with such metrics is\n\n    – Probabilities are non-linear, in the sense that they track the logits exponentially. For\n      example, a model component adding +2 to a given logit can create a 1 or 40 percentage\n       point probability increase, just depending on what the baseline was.\n     As an example of the non-linearity consider the orange line in the figure above: A\n      modest increase in logit difference around layer 17 converts to a jump in probability.\n\n    – Probabilities also inherit the problems of the logprob metric, namely saturation and\n        unspecificity.\n     The figure shows the saturation effect for the orange line at layer >18.\n\n• Binary and discrete metrics (Accuracy / top-k performance / rank / etc): These metrics round\n  off each input to a discrete metric (and then tend to average over a bunch of inputs).\n\n    – The problem with these is that generally many components contribute to a model’s\n      performance, with no single decisive contributor. Discrete metrics may suggest that\n     some significant contributors are unimportant, because they aren’t enough to cross a\n       threshold. Alternatively, these metrics may suggest that one contributor among many\n        is all that matters because it happens to be the one that pushes the model over the\n       threshold. We generally recommend using continuous metrics instead.\n     As an example consider the Figure above: The rank-based metric (red line) jumps\n      around layer 15 when the corresponding logit passes the rank 1 and 0 thresholds, while\n          it is not sensitive to any of the other changes.\n\n    – Discrete metrics can be a good fit for confirmatory patching rather than exploratory\n       patching, as in some sense accuracy is the metric we care about - can the model get the\n       question right or not?\n\n• Logits: We could just take the answer logit as a metric. This is somewhat unprincipled\n  because logits have an arbitrary baseline (adding +1 to all logits would not affect the output)\n  but tend to work in practice. Logit(John) often matches Logprob(John) without being\n  affected by the downsides of the logprob metric.\n  This metric can incorrectly pick up on components that just contribute to many logits.\n  Ensuring that the residual stream and logits have mean zero (default in TransformerLens)\n  can help address this.\n\n\n\n\n\n                                 10\n\n5  Summary\n\nIn most situations, use activation patching instead of ablations. Different corrupted prompts give you\ndifferent information, be careful about what you choose and try to test a range of prompts.\n\nThere are two different directions you can patch in: denoising and noising. These are not symmetric.\nBe aware of what a patching result implies!\n\n        • Denoising (a clean →corrupt patch) shows whether the patched activations were sufficient\n         to restore the model behaviour. This implies the components make up a cross-section of the\n          circuit.\n        • Noising (a corrupt →clean patch) shows whether the patched activations were necessary to\n        maintain the model behaviour. This implies the components are part of the circuit.\n\nBe careful when using metrics that are (i) discrete, (ii) overly sharp, or (iii) sensitive to unintended\ninformation. Ideally use a range of metrics, and try to have at least one metric that is continuous and\nroughly linear in logits such as logit difference or logprob. We recommend representing patching\nresults in a big dataframe with a column per metric and row per patching experiment, and making a\nbunch of plots from this.\n\n        • Model top-k accuracy is discrete and can overrepresent changes at thresholds and shows no\n        change for large effects that don’t cross thresholds.\n        • Most effects from patching are linear and additive in logit space. Probability is exponential\n          in logit space, so it overemphasises effects near a threshold and suppresses effects elsewhere,\n         creating overly sharp patching plots\n        • Logprob can saturate, and cannot control for a patch that boosts both the correct and incorrect\n        answer(s)\n\nAcknowledgements\n\nThanks to Arthur Conmy, Chris Mathwin, James Lucassen, and Fred Zhang for comments on a draft\nof this manuscript.\n\nReferences\n\nChan, Lawrence, Adria Garriga-Alonso, Nix Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishin-\n  skaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas (2022). Causal scrubbing: A\n  method for rigorously testing interpretability hypotheses. Alignment Forum. URL: https://www.\n  alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-\n  rigorously-testing.\nConmy, Arthur, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-\n  Alonso (Apr. 2023). “Towards Automated Circuit Discovery for Mechanistic Interpretability”.\n   In: arXiv e-prints, arXiv:2304.14997, arXiv:2304.14997. DOI: 10.48550/arXiv.2304.14997.\n  arXiv: 2304.14997 [cs.LG].\nCunningham, Hoagy, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey (Sept. 2023).\n  “Sparse Autoencoders Find Highly Interpretable Features in Language Models”. In: arXiv e-prints,\n  arXiv:2309.08600, arXiv:2309.08600. DOI: 10.48550/arXiv.2309.08600. arXiv: 2309.08600\n  [cs.LG].\nFeng, Jiahai and Jacob Steinhardt (Oct. 2023). “How do Language Models Bind Entities in Context?”\n   In: arXiv e-prints, arXiv:2310.17191, arXiv:2310.17191. DOI: 10.48550/arXiv.2310.17191.\n  arXiv: 2310.17191 [cs.LG].\nFinlayson, Matthew, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan\n  Belinkov (June 2021). “Causal Analysis of Syntactic Agreement Mechanisms in Neural Language\n  Models”. In: arXiv e-prints, arXiv:2106.06087, arXiv:2106.06087. DOI: 10.48550/arXiv.2106.\n  06087. arXiv: 2106.06087 [cs.CL].\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts (June 2021a). “Causal Abstractions\n  of Neural Networks”. In: arXiv e-prints, arXiv:2106.02997, arXiv:2106.02997. DOI: 10.48550/\n  arXiv.2106.02997. arXiv: 2106.02997 [cs.AI].\n\n\n                                       11\n\nGeiger, Atticus, Kyle Richardson, and Christopher Potts (Apr. 2020). “Neural Natural Language\n  Inference Models Partially Embed Theories of Lexical Entailment and Negation”. In: arXiv e-prints,\n  arXiv:2004.14623, arXiv:2004.14623. DOI: 10.48550/arXiv.2004.14623. arXiv: 2004.14623\n  [cs.CL].\nGeiger, Atticus, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D.\n  Goodman, and Christopher Potts (Dec. 2021b). “Inducing Causal Structure for Interpretable Neural\n  Networks”. In: arXiv e-prints, arXiv:2112.00826, arXiv:2112.00826. DOI: 10.48550/arXiv.\n  2112.00826. arXiv: 2112.00826 [cs.LG].\nGeva, Mor, Jasmijn Bastings, Katja Filippova, and Amir Globerson (Apr. 2023). “Dissecting Recall of\n  Factual Associations in Auto-Regressive Language Models”. In: arXiv e-prints, arXiv:2304.14767,\n  arXiv:2304.14767. DOI: 10.48550/arXiv.2304.14767. arXiv: 2304.14767 [cs.CL].\nGoldowsky-Dill, Nicholas, Chris MacLeod, Lucas Sato, and Aryaman Arora (Apr. 2023). “Localizing\n  Model Behavior with Path Patching”. In: arXiv e-prints, arXiv:2304.05969, arXiv:2304.05969.\n  DOI: 10.48550/arXiv.2304.05969. arXiv: 2304.05969 [cs.LG].\nHanna, Michael, Ollie Liu, and Alexandre Variengien (Apr. 2023). “How does GPT-2 compute\n  greater-than?: Interpreting mathematical abilities in a pre-trained language model”. In: arXiv\n   e-prints, arXiv:2305.00586, arXiv:2305.00586. DOI: 10.48550/arXiv.2305.00586. arXiv:\n  2305.00586 [cs.CL].\nHase, Peter, Mohit Bansal, Been Kim, and Asma Ghandeharioun (Jan. 2023). “Does Localization\n  Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in\n  Language Models”. In: arXiv e-prints, arXiv:2301.04213, arXiv:2301.04213. DOI: 10.48550/\n  arXiv.2301.04213. arXiv: 2301.04213 [cs.LG].\nHase, Peter, Harry Xie, and Mohit Bansal (June 2021). “The Out-of-Distribution Problem in\n  Explainability and Search Methods for Feature Importance Explanations”. In: arXiv e-prints,\n  arXiv:2106.00786, arXiv:2106.00786. DOI: 10.48550/arXiv.2106.00786. arXiv: 2106.00786\n  [cs.LG].\nHeimersheim, Stefan and Jett Janiak (2023). A circuit for Python docstrings in a 4-layer attention-only\n  transformer. URL: https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-\n  circuit-for-python-docstrings-in-a-4-layer-attention-only.\nHendel, Roee, Mor Geva, and Amir Globerson (Oct. 2023). “In-Context Learning Creates Task\n  Vectors”. In: arXiv e-prints, arXiv:2310.15916, arXiv:2310.15916. DOI: 10.48550/arXiv.2310.\n  15916. arXiv: 2310.15916 [cs.CL].\nHuang, Jing, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts (Sept.\n  2023). “Rigorously Assessing Natural Language Explanations of Neurons”. In: arXiv e-prints,\n  arXiv:2309.10312, arXiv:2309.10312. DOI: 10.48550/arXiv.2309.10312. arXiv: 2309.10312\n  [cs.CL].\nLieberum, Tom, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, and\n  Vladimir Mikulik (July 2023). “Does Circuit Analysis Interpretability Scale? Evidence from Multi-\n  ple Choice Capabilities in Chinchilla”. In: arXiv e-prints, arXiv:2307.09458, arXiv:2307.09458.\n  DOI: 10.48550/arXiv.2307.09458. arXiv: 2307.09458 [cs.LG].\nMcDougall, Callum, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda (Oct. 2023).\n  “Copy Suppression: Comprehensively Understanding an Attention Head”. In: arXiv e-prints,\n  arXiv:2310.04625, arXiv:2310.04625. DOI: 10.48550/arXiv.2310.04625. arXiv: 2310.04625\n  [cs.LG].\nMcGrath, Thomas, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg (July 2023).\n  “The Hydra Effect: Emergent Self-repair in Language Model Computations”. In: arXiv e-prints,\n  arXiv:2307.15771, arXiv:2307.15771. DOI: 10.48550/arXiv.2307.15771. arXiv: 2307.15771\n  [cs.LG].\nMeng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov (Feb. 2022). “Locating and Editing\n  Factual Associations in GPT”. In: arXiv e-prints, arXiv:2202.05262, arXiv:2202.05262. DOI:\n  10.48550/arXiv.2202.05262. arXiv: 2202.05262 [cs.CL].\nMerullo, Jack, Carsten Eickhoff, and Ellie Pavlick (Oct. 2023). “Circuit Component Reuse Across\n  Tasks in Transformer Language Models”. In: arXiv e-prints, arXiv:2310.08744, arXiv:2310.08744.\n  DOI: 10.48550/arXiv.2310.08744. arXiv: 2310.08744 [cs.CL].\nNanda, Neel (2023). Attribution Patching: Activation Patching At Industrial Scale. Blogpost. Section\n “How to Think About Activation Patching”. URL: https://www.neelnanda.io/mechanistic-\n  interpretability/attribution-patching.\nNanda, Neel, SenR, János Kramár, and Rohin Shah (2023). Fact Finding: Attempting to Reverse-\n  Engineer Factual Recall on the Neuron Level (Post 1). Alignment Forum. URL: https://www.\n\n\n\n                                       12\n\n  alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact- finding- attempting- to-\n  reverse-engineer-factual-recall.\nSoulos, Paul, Tom McCoy, Tal Linzen, and Paul Smolensky (Oct. 2019). “Discovering the Compo-\n   sitional Structure of Vector Representations with Role Learning Networks”. In: arXiv e-prints,\n  arXiv:1910.09113, arXiv:1910.09113. DOI: 10.48550/arXiv.1910.09113. arXiv: 1910.09113\n  [cs.LG].\nStolfo, Alessandro, Yonatan Belinkov, and Mrinmaya Sachan (May 2023). “A Mechanistic Interpre-\n   tation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis”. In: arXiv\n   e-prints, arXiv:2305.15054, arXiv:2305.15054. DOI: 10.48550/arXiv.2305.15054. arXiv:\n  2305.15054 [cs.CL].\nTigges, Curt, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda (Oct. 2023). “Linear Rep-\n  resentations of Sentiment in Large Language Models”. In: arXiv e-prints, arXiv:2310.15154,\n  arXiv:2310.15154. DOI: 10.48550/arXiv.2310.15154. arXiv: 2310.15154 [cs.LG].\nTodd, Eric, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau\n  (Oct. 2023). “Function Vectors in Large Language Models”. In: arXiv e-prints, arXiv:2310.15213,\n  arXiv:2310.15213. DOI: 10.48550/arXiv.2310.15213. arXiv: 2310.15213 [cs.CL].\nVig, Jesse, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason\n  Huang, Yaron Singer, and Stuart Shieber (Apr. 2020). “Causal Mediation Analysis for Interpreting\n  Neural NLP: The Case of Gender Bias”. In: arXiv e-prints, arXiv:2004.12265, arXiv:2004.12265.\n  DOI: 10.48550/arXiv.2004.12265. arXiv: 2004.12265 [cs.CL].\nWang, Kevin, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt (Nov.\n  2022). “Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small”.\n   In: arXiv e-prints, arXiv:2211.00593, arXiv:2211.00593. DOI: 10.48550/arXiv.2211.00593.\n  arXiv: 2211.00593 [cs.LG].\nZhang, Fred and Neel Nanda (Sept. 2023). “Towards Best Practices of Activation Patching in\n  Language Models: Metrics and Methods”. In: arXiv e-prints, arXiv:2309.16042, arXiv:2309.16042.\n  DOI: 10.48550/arXiv.2309.16042. arXiv: 2309.16042 [cs.LG].\n\n\n\n\n\n                                       13\n\n## Claim to Classify\n\"activation patching is a popular mechanistic interpretability technique but has many subtleties\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Activation patching (descriptive/meta claim about the technique; no specific experiment reported)",
    "method_rung": 2,
    "claim_rung": 1,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The sentence is a high-level descriptive claim about the technique’s popularity and that it has interpretive subtleties; it does not assert a specific causal/mechanistic fact about model internals. Thus the claim itself is not about causal effects or mechanisms (R1). The method referenced is activation patching, which is an interventional technique (R2), even though here it’s mentioned in a meta way rather than used to support an empirical internal claim."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 16318,
      "completion_tokens": 147,
      "total_tokens": 16465
    },
    "finish_reason": "stop"
  }
}