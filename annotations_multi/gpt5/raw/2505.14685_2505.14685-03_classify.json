{
  "timestamp": "2026-02-19T09:02:06.917212+00:00",
  "paper_id": "2505.14685",
  "claim_id": "2505.14685-03",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2505.14685\nTitle: Language Models use Lookbacks to Track Beliefs\nFull text:\n                   Under review as a conference paper at ICLR 2026\n\n\n         LANGUAGE MODELS USE LOOKBACKS\n          TO TRACK BELIEFS\n\n\n                       Nikhil Prakash♢, Natalie Shapira♢, Arnab Sen Sharma♢, Christoph Riedl♢,\n                    Yonatan Belinkov♠, Tamar Rott Shaham♡, David Bau♢, Atticus Geiger♣†\n\n                       ♢Northeastern University   ♠Technion  ♡MIT CSAIL   ♣Goodfire   †Pr(Ai)2R Group\n\n\n                                       ABSTRACT\n\n\n                    How do language models (LMs) represent characters’ beliefs, especially when\n                               those beliefs may differ from reality? This question lies at the heart of under-2025\n                                standing the Theory of Mind (ToM) capabilities of LMs. We analyze LMs’ ability\n                                  to reason about characters’ beliefs using causal mediation and abstraction. We\n                                 construct a dataset, CausalToM, consisting of simple stories where two charactersOct                               independently change the state of two objects, potentially unaware of each other’s\n23                           actions.lookbackOurmechanism,investigationwhichuncoversenables athepervasiveLM to recallalgorithmicimportantpatterninformationthat we callwhena\n                                               it becomes necessary. The LM binds each character-object-state triple together\n                          by co-locating their reference information, represented as Ordering IDs (OIs),\n                                  in low-rank subspaces of the state token’s residual stream. When asked about a\n                                  character’s beliefs regarding the state of an object, the binding lookback retrieves\n                                 the correct state OI and then the answer lookback retrieves the corresponding state\n                                token. When we introduce text specifying that one character is (not) visible to[cs.CL]\n                                the other, we find that the LM first generates a visibility ID encoding the relation\n                           between the observing and the observed character OIs. In a visibility lookback,\n                                    this ID is used to retrieve information about the observed character and update\n                                the observing character’s beliefs. Our work provides insights into belief tracking\n                            mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.\n\n\n                1  INTRODUCTION\n\n\n                    Theory of Mind (ToM), the ability to infer others’ mental states, is an essential aspect of social\n                    and collective intelligence (Premack & Woodruff, 1978; Riedl et al., 2021). Recent studies have\n                         established that LMs can solve some tasks requiring ToM reasoning (Street et al., 2024; Strachan et al.,\n                      2024a; Kosinski, 2024), while others have highlighted shortcomings (Ullman, 2023; Sclar et al., 2025;\n                      Shapira et al., 2024, inter alia). Previous studies primarily rely on behavioral evaluations, which do\n                       not shed light on the internal mechanisms by which LMs encode and manipulate representations ofarXiv:2505.14685v2\n                     mental states to solve (or fail to solve) such tasks (Hu et al., 2025; Gweon et al., 2023).\n\n                       In this work, we examine how LMs internally represent and track beliefs of characters, a core\n                       aspect of ToM (Dennett, 1981; Wimmer & Perner, 1983). A classic example is the Sally-Anne test\n                    (Baron-Cohen et al., 1985), which evaluates ToM in humans by assessing whether individuals can\n                        track conflicting beliefs: Sally’s belief, which diverges from reality because of missing information,\n                    and Anne’s belief, which is updated based on new observations. Our goal is to determine whether\n               LMs learn a systematic solution to such tasks or rely on superficial statistical association.\n\n               We construct CausalToM, a dataset of simple stories involving two characters, each interacting with\n                    an object to change its state, with the possibility of observing one another. We then analyze the\n                         internal mechanisms that enable Llama-3-70B-Instruct and Llama-3.1-405B-Instruct (Grattafiori\n                           et al., 2024) to reason about and answer questions regarding the characters’ beliefs about the state of\n                     each object (for a sample story, see Section 3 and for the full prompt refer to Appendix A).\n\n\n                           Correspondence to prakash.nik@northeastern.edu.\n\n\n                                                           1\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nOur findings provide strong evidence for a systematic solu-\ntion to belief tracking. We discover that LMs use a pervasive\ncomputation, which we refer to as the lookback mechanism,              Payload\nfor belief tracking. This mechanism enables the model to\nrecall important information at a later stage. In a lookback,\ntwo copies of a single piece of information are transferred to\ntwo distinct tokens. This allows attention heads at the latter\ntoken to look back at the earlier one when needed and re-\ntrieve vital information stored there, rather than transferring                                                              Address\nit directly (see Fig. 1).\nWe identify three key lookback mechanisms that collectively              Source                                                                                   Pointer\nperform belief tracking: 1) Binding lookback (Fig. 3(i)):        Source   Recalled          Lookback\nFirst, the LM assigns ordering IDs (OIs; Dai et al. 2024) that        Token    Token            Token\nencode whether a character, object, or state token appears  Figure 1: The lookback mechanism\nfirst or second. Then, the character and object OIs are copied  performs conditional reasoning; The\nto the corresponding state token and the final token residual  source token contains reference in-\nstream.  Later, when the LM needs to answer a question  formation that is copied into two in-\nabout a character’s beliefs, it uses this information to retrieve   stances, creating a pointer and an ad-\nthe answer state OI. 2) Answer lookback (Fig. 3(ii)): Uses   dress. Next to the address in the resid-\nthe answer state OI from the binding lookback to retrieve  ual stream is a payload. When neces-\nthe answer state token value. 3) Visibility lookback (Fig. 7):  sary, the model retrieves the payload\nWhen a visibility condition between characters is mentioned,  by dereferencing the pointer.  Solid\nthe model employs additional reference information called   lines represent information flow, while\nthe visibility ID to retrieve information about the observed  the dotted line indicates the attention\ncharacter, augmenting the observing character’s awareness.  “looking back” from pointer to address.\n\nOverall, this work not only advances our understanding of the internal computations in LMs that\nenable ToM but also uncovers a pervasive mechanism that plays a foundational role for in-context\nreasoning. All code and data supporting this study are available at https://belief.baulab.\ninfo.\n\n\n2  THE LOOKBACK MECHANISM\n\nOur investigation uncovers a recurring pattern of computation that we call the lookback mechanism.1\nIn lookback, a source reference is copied (via attention) into an address copy in the residual stream\nof the recalled token and a pointer copy in the residual stream of the lookback token that occurs later\nin the text. The LM places the address alongside a payload in the recalled token’s residual stream\nthat can be brought forward to the lookback token if necessary. Fig. 1 shows a generic lookback.\n\nThat is, the LM can use attention to dereference the pointer and retrieve the payload present in the\nresidual stream of the recalled token (which might contain aggregated information from previous\ntokens), bringing it to the residual stream of the lookback token. Specifically, the pointer at the\nlookback token forms an attention query vector, while the address at the recalled token forms a key\nvector. The pointer and address are not necessarily exact copies of the source reference, but they\ndo have a high dot product after being transformed by a query or key attention matrix, respectively.\nHence, a QK-circuit (Elhage et al., 2021) is established, forming a bridge from the lookback token to\nthe recalled token. The LM uses this bridge to move the payload that contains information needed to\ncomplete the subtask through the OV-circuit.\n\nTo develop an intuition for why an LM would learn to implement lookback mechanisms, consider\nthat during training, LMs process text in sequence with no foreknowledge of what might come next.\nInstead of trying to resolve every possible future question about the current context, it would be\nuseful to place addresses alongside payloads that might be useful to remember in the future when\n\n   1Although this mechanism may resemble induction heads (Elhage et al., 2021; Olsson et al., 2022), it\ndiffers fundamentally. In induction heads, information from a previous token occurrence is passed only to the\nsubsequent token, without being duplicated to its next occurrence. In contrast, the lookback mechanism copies\nthe same information not only to the location where the vital information resides but also to the target location\nthat needs to retrieve that information.\n\n\n                                       2\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n          80                                                                                        1\n                      Character\n          60        Object\n                       State\n          40                                                                                                   IIA                 Layers\n          20\n           0 :    a .   ,       .        . :     ? :    0\n                              Story  Maggy\n\n                      Bob                        andCarlaareworkingin  busyrestaurant Tocompleteanorder  MaggyBobgrabsanopaquemugandfillsitwithmilkbeer  ThenCarlagrabsanotheropaquecupandfillsitwithcoffee    Question  WhatdoesBobbelievethebottlecontains   Answer                                                                                                                              bottle\n       Figure 2: Tracing information flow of crucial input tokens using causal mediation analysis.\n\n\nperforming a variety of downstream tasks. In our setting, the LM constructs a representation of a\nstory without any certainty about the questions it may later be asked about that story, so the LM\nlocalizes pivotal information in the residual stream of certain tokens, which later become payloads\nand addresses. When the question text is reached, pointers are constructed that reference this crucial\nstory information and dereference it to find an answer to the question.\n\n\n3  EXPERIMENTAL SETUP: DATASET, MODELS, AND METHODS\n\nDataset   Existing datasets for evaluating ToM capabilities of LMs are designed for behavioral\ntesting and lack counterfactual pairs needed for causal analysis (Kim & Sundar, 2012). To address this\nproblem, we construct CausalToM, a structured dataset of simple stories, where each story involves\ntwo characters, each interacting with a distinct object causing the object to take a unique state. For\nexample: “Character1 and Character2 are working in a busy restaurant.\nTo complete an order, Character1 grabs an opaque Object1 and fills\nit with State1.  Then Character2 grabs another opaque Object2 and\nfills it with State2.” We then ask the LM to reason about one of the characters’\nbeliefs regarding the state of an object: “What does Character1 believe Object2\ncontains?” We analyze the LM’s ability to track characters’ beliefs in two distinct settings. (1)\nNo Visibility, where both characters are unaware of each other’s actions, and (2) Explicit Visibility,\nwhere explicit information about whether a character can/cannot observe the other’s actions is\nprovided,  e.g., “Bob can observe Carla’s actions.  Carla cannot observe\nBob’s actions.” We also provide general task instructions (e.g., answer unknown when a\ncharacter is unaware); refer to Appendices A & B for the full prompt and additional dataset details.\nAll subsequent experiments are conducted on 80 samples that the model answers correctly. We also\ndemonstrate generalization of the mechanism to BigToM dataset (Gandhi et al., 2024) in Appendix K.\n\nModels  Our experiments analyze Llama-3-70B-Instruct and Llama-3.1-405B-Instruct models in\nFP16 and INT8 precision, respectively, using NNsight (Fiotto-Kaufman et al., 2025). Results for\nLlama-3.1-405B-Instruct can be found in Appendix L. Both models demonstrate strong behavioral\nperformance in the no-visibility and explicit-visibility settings. We do not examine smaller models,\nas they are unable to coherently solve the CausalToM task.\n\nCausal Mediation Analysis  Our goal is to develop a mechanistic understanding of how LMs\nreason about characters’ beliefs and answer related questions (Saphra & Wiegreffe, 2024). A key\nmethod for conducting causal analysis is interchange interventions (Vig et al., 2020; Geiger et al.,\n2020; Finlayson et al., 2021), in which the LM is run on paired examples: an original input o and a\ncounterfactual input c, and certain internal activations in the LM run on the original input are replaced\nwith those computed from the counterfactual, a process also known as activation patching. We begin\nour analysis by tracing information flow from key input tokens to the final output, by performing\ninterchange interventions on the residual vectors. Specifically, we construct an intervention dataset\nwhere o contains a question about the belief of a character not mentioned in the story, while the\nstory in c includes the same queried character, as shown in Fig. 2. The expected outcome of this\nintervention is a change in the final output of o from unknown to a state token, such as beer. We\nconduct similar interchange interventions for object and state tokens (refer to Appendix C for details).\n\nFigure 2 presents the aggregated results of this experiment for the key input tokens Character1,\nObject1, and State1. The cells are color-coded to indicate the interchange intervention accuracy\n(IIA; Geiger et al., 2022). Even at this coarse level of Causal Mediation Analysis (Mueller et al.,\n\n\n                                       3\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n              Layers                                                                   coffee\n            60                                                                                                                       (ii) Answer lookback\n\n                                                                                                             coffee                                               coffee\n                                                                                                              Answer\n                                                                           2                           2\n            50\n\n                                                 1                         2                     Answer 2\n            40              (i) Binding lookback    1 1                       2 2                       StateOI    2 2\n\n\n            30       OI                OI\n                        OI      OI         OI     OI\n                                                                                               State1                                                 1                                                                                                                                                      State2 2            20                                                                                     Object1                                                                                                                                                                                       Character2              Object2                                                                                           Character1                                       1                         2\n\n            10              1                         2\n                   To complete an order , Bob grabs an opaque bottle and fills it with beer . Then Carla grabs another opaque cup and fills it with coffee . Question : What does Carla believe the cup contains ? Answer :\n\nFigure 3: Belief Tracking with no visibility between characters. We hypothesize that the LM\ntracks beliefs using two lookback mechanisms. First, in (i) Binding lookback, LM binds together\neach character-object-state triple in the state token residual stream. When asked about a specific\ncharacter-object pair, the LM looks back to the corresponding OIs to retrieve the correct state OI.\nSecond, in (ii) Answer lookback, LM dereferences that state OI (used as a pointer) to retrieve the\ntoken value of the correct state. Colors indicate information type, shapes indicate role of information\nin lookback (see Fig. 1), e.g., state OI is a payload (  ) in (i) and a pointer-address (  ) in (ii).\n\n2024; Vig et al., 2020; Meng et al., 2022), several significant insights emerge: 1) Information from\nthe correct state token (beer) flows directly from its residual stream to that of the final token in later\nlayers, consistent with prior findings (Lieberum et al., 2023; Prakash et al., 2024); 2) Information\nassociated with the query character and the query object is retrieved from their earlier occurrences\nand passed to the final token before being replaced by the correct state token.\n\nDesiderata Based Patching via Causal Abstraction  The causal mediation experiments provide a\ncoarse-grained analysis of where information flows, but do not identify what information is being\ntransferred. In a transformer, the first layer represents the input and the last layer represents the\noutput, but we wish to know: what is represented in the middle? We analyze the internal mechanism\nusing Causal Abstraction (Geiger et al., 2021; 2024); First, we hypothesize a high-level causal model\nof the computational steps from input to output (Sec. 4), and then align its variables with the LM’s\ninternal activations (Sec. 5). We test the alignment through targeted interchange interventions on\ncausal variables in the hypothesized model and hidden activations in the LM. If the LM produces the\nsame output as the causal model under these aligned interventions, it provides evidence supporting\nthe hypothesized causal model. We quantify this effect using interchange intervention accuracy (IIA;\nGeiger et al., 2022), which measures the proportion of cases where the intervened causal model and\nintervened LM agree. See Appendix D for more details.\n\nIn addition to measuring IIA on entire residual stream vectors, we also intervene on localized\nsubspaces to further isolate causal variables. To identify the subspace of a specific variable, we\nemploy Desiderata-based Component Masking (De Cao et al., 2020; Davies et al., 2023; Prakash\net al., 2024). This method learns a sparse binary mask over the activation space that maximizes the\nlogit of the hypothesized causal model output. We train a mask to select singular vectors of the\nactivation space that encode a high-level variable (see Appendix F for details). Our experiments in\nSec. 5 report both interventions on the full residual stream and on the identified subspaces.\n\n\n4  HYPOTHESIZED HIGH-LEVEL CAUSAL MODEL OF BELIEF TRACKING\n\nHere we start with an overview of our hypothesized causal model of belief-tracking when characters\nare not aware of each other’s actions. The causal model is an algorithmic process that has variables\nwith structural roles that do not refer to the details of a transformer architecture. Appendix E presents\n\n\n                                       4\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nthe full pseudocode of the causal model. In Section 5, we will present experiments to verify that the\ncausal model’s variables align with representations in the transformer.\n\nBelief tracking begins when the causal model assigns ordering IDs (OIs;    ,    ,   ) to each character,\nobject, and state token, marking their order of appearance. For instance, in the example in Fig. 3,\nBob is assigned first character OI ( 1 ), and Carla is assigned the second character OI( 2 ). Then it\nuses these OIs in two lookback mechanisms:\n\n(i) Binding lookback. The causal model creates address copies of each character OI (  ) and object\nOI (  ) that are bound to the state OI (Binding Payload,    ), creating a character–object–state triple.\nWhen a question is asked about a character and object, the causal model creates pointer copies of that\ncharacter and object OIs (Binding Pointers    ,   ) and dereferences them to retrieve the state OI.\n\n(ii) Answer lookback. The causal model creates an address copy of the state OI (Answer Address\n  ) that is bound to the state token (Answer Payload,    ). Through the binding lookback, a pointer\ncopy of this OI (  ) is created. The causal model dereferences the pointer to retrieves the correct state\ntoken payload as the final output.\n\n5  VERIFYING THE HYPOTHESIZED CAUSAL MODEL OF BELIEF TRACKING\n\nWe test our hypothesized causal model by localizing its variables within the transformer’s neural\nrepresentations. Specifically, we localize the addresses, pointers, and payloads of the (i) binding\nlookback and (ii) answer lookbacks within the LM’s internal activations. In Fig. 3, we show a trace\nof the causal model run on an input overlayed onto a schematic of a transformer architecture. This\nvisualizes the alignment between variables in the causal model and locations in the LM residual\nstream that the experiments in the remaining of this paper will support. In the binding lookback, the\ncharacter and object OI addresses are realized in the residual stream of the state token. The pointer\ncopies are brought forward to the last token residual stream where they are dereferenced via attention\nto bring forward the correct state OI payload. In the answer lookback, the address copy of the state\nOI is in the state token residual stream while the pointer copy is in the last token residual stream.\n\nEach of the following experiments localizes the presence of specific ordering IDs (OIs) and verifies\ntheir roles as hypothesized by our causal model. We do this by targeted interchange intervention\nexperiments on the causal model and the LM. We copy hidden states between identical tokens (for\nexample, replacing the representation of “:” in one context with the representation of “:” in another\ncontext, as in Fig. 4). When this intervention causes the LM’s have the same output as the causal\nmodel under an interchange intervention on OI variables, we have evidence that the OI is carrying out\nthe hypothesized role. Each experiment reports the effects of n = 80 different cases with the same\nstructure, and the effect is measured at every layer.\n\nBecause the last step of the causal model is easiest to understand, we proceed through the experiments\nin reverse order, beginning with an experiment to verify the final “answer lookback” stage. After\nthis instructive starting point, we work backward to verify the earlier steps of the model. Additional\nresults can be found in Appendix G and H.\n\n\n5.1  STEP II: ANSWER LOOKBACK – RETRIEVING THE CORRECT STATE\n\nLocalizing the Answer Payload  We first verify the presence of the correct Answer Payload\nat the deepest layer representation of final token “:”. To do so, we run an interchange intervention\nexperiment shown in Fig. 4a in which the counterfactual example c swaps the order of the characters\nand objects of the original example o and also replaces the state (drinks) tokens with new values. If\nthe Answer Payload is correctly localized, swapping it should cause the answer of the counterfactual\n(e.g., tea) to replace the answer of the original example (e.g., coffee). The gray line in Fig. 4b shows\nthat this output change is observed in every one of n = 80 cases, both when intervening on the\nfull residual stream and on the identified subspace. However, not at every layer: the information is\nonly present after layer 56, indicating that before this stage, the transformer has not yet retrieved the\ncorrect answer payload into the residual stream. That is consistent with our hypothesis that at early\nsteps, the OI has not yet been dereferenced. At an earlier stage, we expect to see an Answer Pointer.\n\nLocalizing the Pointer Information  To identify the Answer Pointer   before it is dereferenced to\nbring the payload (state token value), we examine the representations of “:” at layers earlier than 56.\n\n\n                                       5\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n                            (a) Intervention Input Example                                        (b) Layer-wise Intervention Results\n   Carla and Bob are working in a busy restaurant.  To complete         1.0                                                                                                                                      Full residual\n   an order, Carla grabs an opaque cup and fills it with tea.\n   Then Bob grabs another opaque bottle and fills it with water.        0.8    Subspace\n                                                                                                   dimension  20\n   Question:              What does Carla believe the cup contains?                                       Accuracy           Counterfactual                                                                                    0.6    Subspace            tea   Answer:\n                                                                                                   dimension  33\n   Bob and Carla are working in a busy restaurant.  To complete         0.4\n\n                                   cup   Then        Carla grabs                    another                            opaque                                       and                                           fills                                                 it with coffee.                                                                                    0.2     Original an order, Bob grabs an opaque bottle and fills it with beer.                        Carla                                          cup   Question:              What does                              believe                                      the                                              contains?                                                                                                                                                                                                                                                                                                                   Intervention\n   Answer:  coffee                                                               0.0\n    Intervention 1: Answer Pointer (   ), Causal Model Output: beer                         0   10  20  30  40  50  60  70\n   Intervention 2: Answer Payload (   ), Causal Model Output: tea                                            Layers\n\nFigure 4: Answer Lookback Pointer and Payload: The causal model predicts that if we alter the\n“Answer Payload  ” of the original to instead take the value of the counterfactual answer payload, the\noutput should change from coffee to tea; the gray curve in the line plot shows this does occur when\npatching residual vectors at the “:” token beyond layer 56, providing evidence that the answer payload\nresides in those states. On the other hand the causal model predicts that taking the counterfactual\n“Answer Pointer  ” would change the original run output from coffee to beer—a new output that\nmatches neither the original nor the counterfactual!—and we do see this surprising effect, again when\npatching layers between 34 and 52, providing strong evidence that the answer pointer is encoded at\nthose layers. These results suggest the Answer Lookback occurs between layers 52 and 56.\n\n\nOur causal model provides the hypothesis: if the Answer Pointer is present, then patching the pointer\nfrom the counterfactual run into the original run should redirect the LM to attend to the location\nof the correct counterfactual state and fetch its payload. For example, in Fig. 4a the counterfactual\npointer references the first presented state. When we patch it into the original story, we expect the\nmodel’s answer to change to beer rather than coffee. The colored line in Fig. 4b confirms that this\neffect is consistently observed when patching any layer between 34 −52 (both when patching the\nfull residual stream and the identified subspace), supporting our hypothesis that these layers encode\nthe Answer Pointer information at the final token, rather than directly transferring token values.\n\n\n5.2  STEP I: BINDING LOOKBACK – LINKING CHARACTERS, OBJECTS, AND STATES\n\nLocalizing the Address and Payload   In this experiment, we verify the presence of the address\ncopies of the character and object OIs as well as the payload (state OI) at the state token residual\nstream (recalled token, Fig. 3). As illustrated in Fig. 5a, we construct an intervention dataset where\neach example consists of an original input o with an answer that is not unknown and a counterfactual\ninput c where the character, object, and state token values are identical, except the ordering of the\ntwo story sentences is swapped while the question remains unchanged. The expected LM’s output\npredicted by our hypothesized causal model is the other state token in the original example, e.g., beer.\nThat is because patching the address and payload values at each state token, without changing the\npointer, makes the LM dereference the other state token. As a result, the model’s output should flip to\nthe other state token in the original input.\n\nWe perform the interchange intervention experiment layer-by-layer, where we replace the residual\nstream vector (or the identified subspace) of the first state token in the original run with that of the\nsecond state token in the counterfactual run and vice versa for the other state token. It is important to\nnote that if the intervention targets state token values instead of their OIs, it should not produce the\nexpected output. (This happens in the earlier layers.) As shown in Fig. 5b, the strongest alignment\noccurs between layers 33 and 38, supporting our hypothesis that the state token’s residual stream\ncontains both the address (character and object OIs) and the payload information (state OI).\n\nLocalizing the Source Reference Information   Next, we localize the source reference information,\ni.e., character and object OIs at their respective token residual stream. As illustrated in Fig. 6a,\nwe conduct an intervention experiment with a dataset where the counterfactual example, c, swaps\nthe order of the characters and objects as well as replaces the state tokens with entirely new ones\nwhile keeping the question the same as in o. Under this setup, an interchange intervention on the\nhypothesized causal model that targets the source reference should propagate changes through both\nthe address and the pointer, leaving the final output unchanged. However, if we instead freeze the\nstate token residual stream, which carries both the payload and the address, the causal model produces\nthe alternate state token (e.g., beer in Fig. 6), as the pointer refers to the other state’s address.\n\n\n                                        6\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n                               (a) Intervention Input Example                                 (b) Layer-wise Intervention Results\n   Carla and Bob are working in a busy restaurant.  To complete       1.0\n   an order, Carla grabs an opaque cup and fills it with coffee.                                              Full residual\n   Then Bob grabs another opaque bottle and fills it with beer.       0.8                        Subspace\n                                                                                                                  dimension  14\n   Question:              What does Carla believe the cup contains?                                  Accuracy 0.6            coffee        Counterfactual Answer:\n   Bob and Carla are working in a busy restaurant.  To complete       0.4\n                                                                                   0.2                                   cup   Then        Carla grabs                    another                            opaque                                       and                                           fills                                                 it with coffee.    Original an order, Bob grabs an opaque bottle and fills it with beer.                                                                                                                                                                                                                                                                                                                 Intervention                        Carla                                          cup   Question:              What does                              believe                                      the                                              contains?\n   Answer:  coffee                                                              0.0                                                                        0   10  20  30  40  50  60  70\n   Intervention: Binding Payloads and Addresses (    ,     ,   )                                                 Layers\n   Causal Model Output: beer\n\nFigure 5: Binding lookback Address and Payload: The causal model predicts that swapping\naddresses (character and object OIs;   and   ) and payloads (state OIs;   ) should cause the binding\nlookback mechanism to attend to the alternate state token and retrieve its state OI. This retrieved state\nOI is then dereferenced by the answer lookback, producing the corresponding token as the output (e.g.,\nbeer instead of coffee). The LM’s behavior matches this prediction when we perform interchange\ninterventions on the state token across layers 33–38. These findings support our hypothesis that both\naddress and payload information are encoded in the residual stream of state tokens.\n\n\n\n\n\n                               (a) Intervention Input Example                                 (b) Layer-wise Intervention Results\n   Carla and Bob are working in a busy restaurant.  To complete       1.0\n   an order, Carla grabs an opaque cup and fills it with tea.                                                  Full residual\n   Then Bob grabs another opaque bottle and fills it with water.      0.8                       Subspace\n                                                                                                                 dimension  167\n   Question:              What does Carla believe the cup contains?                                  Accuracy 0.6            tea        Counterfactual Answer:\n   Bob and Carla are working in a busy restaurant.  To complete       0.4\n                                                                                   0.2                                   cup   Then        Carla grabs                    another                            opaque                                       and                                           fills                                                 it with coffee.    Original an order, Bob grabs an opaque bottle and fills it with beer.                                                                                                                                                                                                                                                                                                                 Intervention                        Carla                                          cup   Question:              What does                              believe                                      the                                              contains?\n   Answer:  coffee                                                              0.0                                                                        0   10  20  30  40  50  60  70\n   Intervention: Binding Source (    ,   )                                                                Layers\n   Frozen: Binding and Answer Addresses+Payloads (    ,     ,    ;     ,   )\n   Causal Model Output: beer\n\nFigure 6: Source Reference Information of Binding lookback: The causal model predicts that\nswapping the source reference information (character and object OIs;     ,    ), while freezing the\naddresses and payloads of the binding lookback, should cause the binding lookback mechanism to\nattend to the alternate state token and retrieve its state OI, which would generate alternate state token\nas the final output via the answer lookback (e.g., beer instead of coffee). The LM’s behavior matches\nthis prediction when we perform interchange interventions at the character and object tokens across\nlayers 20-34. These results support our hypothesis that source reference information is encoded in\nthe residual stream of character and object tokens.\n\n\n\nIn the LM, we interchange the residual streams of the character and object tokens layer-by-layer,\nwhile keeping the residual stream of the state token fixed. As shown in Fig. 6b, this experiment\nreveals alignment between layers 20 and 34, indicating that source reference is encoded in the residual\nstreams of the character and object tokens within this layer range. Additional results are provided in\nAppendix G, where Fig. 13 shows that freezing the residual stream of the state token is necessary for\nthis alignment to emerge. These findings support our hypothesis that source reference is present in\nthe character and object tokens and is subsequently transferred to the recalled and lookback tokens.\n\nLocalizing the Pointer Information   Finally, we localize the pointer copies of the character and\nobject OIs to their corresponding tokens in the question and to the final token. See Appendices G & H\nfor details of the experiments and results.\n\nIn summary, belief tracking begins in layers 20–34, where character and object OIs are encoded in\ntheir respective token representations. These OIs are transferred to the corresponding state tokens\nin layers 33–38. When a question is asked, pointer copies of the relevant character and object OIs\nare moved to the final token by layer 34, where they are dereferenced to retrieve the correct state OI.\nAt the final token, this state OI is represented across layers 34–52, and between layers 52–56, it is\ndereferenced to fetch the answer from the correct state token, producing the final output.\n\n\n                                       7\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n             Layers                            …\n                                                                                                 Answer\n                                                                                                                   State           40       (i) Binding lookback    …                                                                                               (iii) Visibility lookback                                                                                                OI   2 2\n\n                                                                   2                   2\n                                                   Observed Info       Parsed Question\n           30          OI\n                          OI     OI                …\n                                                                                                     State2 2           Visibility ID           20                                                                                                      Character2              Object2\n                                         2\n\n           10     1            2\n                       an …                  it    …           ’s\n                            Bob grabs             Then Carla grabs another opaque cup and fills    with coffee          Bob can observe Carla        actions . Question : What does Bob believe the cup contains ? Answer :\n\nFigure 7: Visibility Lookback When one (observing) character can see another (observed) character,\nthe LM assigns a visibility ID (  ) to the visibility sentence where this relation is defined. An address\ncopy of this visibility ID remains in the visibility sentence’s residual stream. A pointer copy of\nthe visibility ID is transferred to the subsequent tokens’ residual stream. The LM dereferences this\npointer through a QK-circuit, bringing forward the payload (    ), when processing subsequent tokens.\nBased on initial evidence, this payload contains the observed character’s OI(   ). See Appendix I for\ndetails. This mechanism allows the model to incorporate the observed character’s knowledge into the\nobserving character’s belief state, enabling more complex belief reasoning.\n\n6  IMPACT OF VISIBILITY CONDITIONS ON BELIEF TRACKING MECHANISM\n\nSo far, we have demonstrated how the LM uses ordering IDs and two lookback mechanisms to track\nthe beliefs of characters that cannot observe each other. Now, we explore how the LM updates the\nbeliefs of characters when one character (observing) can observe the actions of the other (observed).\n\nHypothesized Visibility Lookback Mechanism  We hypothesize that the LM uses an additional\nlookback mechanism, which we call the Visibility Lookback, to integrate information about the\nobserved character when it is explicitly stated that one character can see another’s action. As\nillustrated in Fig. 7, we hypothesize that the LM first generates a Visibility ID (  ) at the residual\nstream of the visibility sentence, serving as the source reference information. The address copy of\nthe visibility ID remains in the residual stream of the visibility sentence, while its pointer copy gets\ntransferred to the residual stream of the subsequent tokens (lookback tokens). Then LM forms a\nQK-circuit at the lookback tokens and dereferences the visibility ID pointer to retrieve the payload.\n\nAlthough our two-character setting is unable to discern the exact semantics of the payload in the\nvisibility lookback, our observations are consistent with the payload encoding the observed character’s\nOI. Our initial observations suggest another lookback where the story sentence associated with the\nobserved character serves as the source reference, and its payload encodes information about the\nobserved character. The observed character’s OI appears to be retrieved by the lookback tokens of the\nVisibility lookback, with causal effects on the queried character’s awareness (see App. I for details).\n\n\n6.1  VERIFYING THE HYPOTHESIZED VISIBILITY LOOKBACK\n\nLocalizing the Source Reference   In this experiment, we localize the Visibility ID (   ), i.e., the\nsource reference of the Visibility lookback. We conduct an interchange intervention experiment where\nthe counterfactual is a different story in which the characters’ visibility is flipped from unobserved\nto observed (Fig. 8a), and we look for an output change from “unknown” to the answer that would\nbe observed. We intervene on the representation of all the visibility sentence tokens. As shown in\nFig. 8b (blue – line), causal effects appear between layers 10 and 23, indicating that the visibility\nID remains encoded in the visibility sentence until layer 23, after which it is split into address and\npointer copies that must be connected by dereference to have an effect. This pattern supports our\nhypothesis that the LM generates a reference to the Visibility ID.\n\nLocalizing the Payload   Next, we localize the payload (    ) information using the same counter-\nfactual dataset. However, instead of intervening on the recalled tokens, we intervene on the lookback\n\n\n                                       8\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n                               (a) Intervention Input Example                                          (b) Layer-wise Intervention Results\n   Carla and Bob are working in a busy restaurant.  To complete an         1.0                                                        Visibility\n   order, Carla grabs an opaque cup and fills it with tea.  Then Bob                                                            condition\n   grabs another opaque bottle and fills it with water.  Bob cannot        0.8                                          Question\n                             Carla   observe           Carla’s                   actions.                                   can                                       observe                                               Bob’s                                                     actions.                                                                                                                                                                                                Accuracy\n                                          bottle   Question:              What                   does Carla                              believe                                      the                                                 contains?                                                                               0.6                                                        Visibilitycondition        Counterfactual                                                                                                                                                  +Question\n   Answer:  water\n   Karen and Max are working in a busy restaurant.  To complete an         0.4\n          Karen                                flute   order,                grabs                      an opaque                                      and fills                                                it                                                   with soda.                                                               Then\n                            jar                                                           Max   Max grabs             another                     opaque                                and fills                                          it with                                                  coffee.                                                               cannot      0.2                                                                                                                                                                                                                                                                                                Intervention    Original observe Karen’s actions.  Karen cannot observe Max’s actions.   Question:  What does Karen believe the jar contains?                     0.0\n   Answer:  unknown                                                    0   10  20  30  40  50  60  70\n                                                                                           Layers   Intervention 1: Source (   ), Causal Model Output: coffee\n   Intervention 2: Payload (     ), Causal Model Output: coffee\n   Intervention 3: Address & Pointer (   ), Causal Model Output: coffee\n\nFigure 8: Visibility Lookback: We conduct three interchange intervention experiments to support\nthe Visibility Lookback hypothesis: (1) Source Alignment: We align the source reference (  ) by\nintervening on the visibility sentence, replacing it with its representation from a counterfactual run\nwhere the visibility sentence causes the queried character to become aware of the queried object’s\ncontents. We observe that source reference information aligns between layers 10 and 23, after which\nit splits into separate address and pointer components. (2) Payload Alignment: To align the payload (\n    ), we intervene on all subsequent tokens and observe alignment only after layer 31. (3) Address and\nPointer Alignment: When intervening on both the address and pointer information (   ), we observe\nalignment across a broader range of layers, particularly between layers 24 and 31, because of the\nenhanced alignment between the address and pointer copies at the recalled and lookback tokens.\n\n\ntokens, specifically the question and answer tokens. As in the previous experiment, we replace the\nresidual vectors of these tokens in the original run with those from the counterfactual run. As shown\nin Fig. 8b (red – line), alignment occurs after layer 31, indicating that the information causing the\nqueried character’s awareness is present in the lookback tokens after this layer.\n\nLocalizing Address and Pointer  The previous two experiments indicate the absence of both the\nsource and payload information between layers 24 and 31. We hypothesize that this lack of signal is\ndue to a mismatch between the address and pointer information that inhibits a lookback dereference.\nSpecifically, when intervening only on the recalled token after layer 25, the pointer is not updated,\nwhereas intervening only on the lookback tokens leaves the address unaltered, a mismatch in either\ncase. To test this hypothesis, we conduct another intervention using the same counterfactual dataset,\nbut this time, we intervene on the residual vectors of both the recalled and lookback tokens, i.e., the\nvisibility sentence, as well as the question and answer tokens. As shown in Fig. 8b (green – line),\nalignment occurs after layer 10 and remains stable, providing evidence that a lookback now occurs\nbetween layers 24 and 31. This intervention replaces both the address and pointer copies of the\nvisibility IDs, enabling the LM to form a QK-circuit and resolve the visibility lookback.\n\n\n7  RELATED WORK\n\n\nTheory of mind in LMs  Theory of mind in LMs has been widely benchmarked (Le et al., 2019;\nShapira et al., 2023; Wu et al., 2023; Kim et al., 2023; Xu et al., 2024; Jin et al., 2024; Chan et al.,\n2024; Strachan et al., 2024b). However, these benchmarks lack adequate counterfactuals for the\nbinding manipulations we need, so we made CausalToM (Section B).\n\nEntity tracking in LMs   Entity tracking and variable binding are crucial abilities for LMs to exhibit\nnot only coherent ToM capabilities, but also neurosymbolic reasoning. Many existing works have\nattempted to decipher this ability in LMs (Li et al., 2021; Davies et al., 2023; Feng & Steinhardt,\n2023; Kim & Schuster, 2023; Prakash et al., 2024; Feng et al., 2024; Dai et al., 2024; Wu et al., 2025).\nOur work builds on their empirical insights and extends the current understanding of how LMs bind\nvarious entities defined in context.\n\nMechanistic interpretability of theory of mind  Few studies explored the underlying mechanisms\nof ToM of LM (Zhu et al., 2024; Bortoletto et al., 2024; Herrmann & Levinstein, 2024). Those studies\nuse probing (Alain, 2016; Belinkov, 2022) to identify internal representations of beliefs and steering\n(Rimsky et al., 2023; Li et al., 2024) to control LMs by manipulating their activations. However, the\nmechanism by which LMs solve those tasks remains a black box, limiting our ability to understand,\npredict, and control LMs’ behaviors.\n\n\n                                       9\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n8  CONCLUSION\n\nThrough a series of interchange intervention experiments, we have mapped the end-to-end underlying\nmechanism responsible for the processing of partial knowledge and false beliefs in a set of simple\nstories. We are surprised by the pervasive appearance of a single recurring computational pattern: the\nlookback, which resembles a pointer dereference inside a transformer. The LMs use a combination\nof several lookbacks to reason about nontrivial belief states. Our improved understanding of these\nfundamental computations gives us optimism that it may be possible to fully reveal the algorithms\nunderlying not only the theory of mind, but also other capabilities in LMs.\n\n9  ETHICS STATEMENT\n\nThis work involves experiments conducted exclusively on synthetic text generated by the authors. No\nhuman subjects, personal data, or sensitive user information were collected or analyzed. The models\nwe analyze are publicly released LLMs (Llama-3-70B-Instruct and Llama-3.1-405B-Instruct). All\ninterventions and analyses were performed locally without querying proprietary APIs. We acknowl-\nedge that research on ToM in LMs may be misinterpreted as suggesting human-like cognition or\nintentionality. We explicitly caution that our findings demonstrate internal computational mechanisms,\nnot conscious reasoning, and should not be construed as evidence of sentience or moral agency in\nLMs. Our causal intervention techniques reveal latent structures within pretrained models but do not\nmodify the underlying weights or enable novel capabilities. Nevertheless, reverse-engineering latent\nbelief representations could, in principle, be misused for behavioral steering or manipulation. To\nmitigate this, we provide our code strictly for transparency and further scientific audit, rather than for\napplication in deployed systems.\n\n10  REPRODUCIBILITY STATEMENT\n\nTo facilitate reproducibility, we release the full CausalToM dataset, including all story templates\nand the code used to generate the various story instances that serve as counterfactual variants in our\nexperiments. The repository, that can be accessed at https://belief.baulab.info, contains\nall scripts required to construct the dataset, extract activations, perform interchange interventions,\nand compute causal mediation metrics, along with the hyperparameters and random seeds used for\nsubspace identification via DCM. All experiments were conducted using publicly available open-\nweight models (Llama-3-70B-Instruct and Llama-3.1-405B-Instruct). The interchange intervention\nexperiments can be reproduced either by hosting these models locally or by using remotely hosted\ninstances via NDIF (Fiotto-Kaufman et al., 2025), if sufficient local compute is not available.\n\n11  THE USE OF LARGE LANGUAGE MODELS\n\nWe used LLMs as a writing assistant to correct grammatical and typographical errors; beyond this,\nthey did not contribute to any stage of the research.\n\n12  ACKNOWLEDGEMENT\n\nThis research was supported in part by Open Philanthropy (N.P., N.S., A.S.S., D.B., A.G., Y.B.), the\nNSF National Deep Inference Fabric award #2408455 (D.B.), the Israel Council for Higher Education\n(N.S.), the Zuckerman STEM Leadership Program (T.R.S.), the Israel Science Foundation (grant No.\n448/20; Y.B.), an Azrieli Foundation Early Career Faculty Fellowship (Y.B.), a Google Academic\nGift (Y.B.), and a Google Gemma Academic Award (D.B.). This research was partly funded by the\nEuropean Union (ERC, Control-LM, 101165402). Views and opinions expressed are however those\nof the author(s) only and do not necessarily reflect those of the European Union or the European\nResearch Council Executive Agency. Neither the European Union nor the granting authority can be\nheld responsible for them.\n\n\n\n\n\n                                       10\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\nREFERENCES\n\nGuillaume Alain. Understanding intermediate layers using linear classifier probes. arXiv preprint\n  arXiv:1610.01644, 2016.\n\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a “theory of mind”?\n  Cognition, 21(1):37–46, 1985.\n\nYonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational\n   Linguistics, 48(1):207–219, 2022.\n\nMatteo Bortoletto, Constantin Ruhdorfer, Lei Shi, and Andreas Bulling. Benchmarking mental state\n  representations in language models. arXiv preprint arXiv:2406.17513, 2024.\n\nChunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye Deng, Wei Fan, Haoran Li, Xin Liu, Hongming\n  Zhang, Weiqi Wang, and Yangqiu Song. Negotiationtom: A benchmark for stress-testing machine\n  theory of mind on negotiation surrounding. arXiv preprint arXiv:2404.13627, 2024.\n\nQin Dai, Benjamin Heinzerling, and Kentaro Inui. Representational analysis of binding in language\n  models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024\n  Conference on Empirical Methods in Natural Language Processing, pp. 17468–17493, Miami,\n  Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n  2024.emnlp-main.967. URL https://aclanthology.org/2024.emnlp-main.967/.\n\nXander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Discovering\n  variable binding circuitry with desiderata, 2023. URL https://arxiv.org/abs/2307.\n  03637.\n\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions emerge\n  across layers in neural models? interpretation with differentiable masking. In Proceedings of the\n  2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3243–\n  3255, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n  emnlp-main.262. URL https://aclanthology.org/2020.emnlp-main.262.\n\nDaniel Clement Dennett. The Intentional Stance. MIT Press, 1981.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda\n  Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli,\n  Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal\n  Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris\n  Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.\n  https://transformer-circuits.pub/2021/framework/index.html.\n\nJiahai Feng and Jacob Steinhardt. How do language models bind entities in context? arXiv preprint\n  arXiv:2310.17191, 2023.\n\nJiahai Feng, Stuart Russell, and Jacob Steinhardt. Monitoring latent world states in language models\n  with propositional probes. CoRR, abs/2406.19501, 2024. doi: 10.48550/ARXIV.2406.19501. URL\n  https://doi.org/10.48550/arXiv.2406.19501.\n\nMatthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart M. Shieber, Tal Linzen, and Yonatan\n  Belinkov. Causal analysis of syntactic agreement mechanisms in neural language models. In\n  Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th An-\n  nual Meeting of the Association for Computational Linguistics and the 11th International Joint\n  Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\n   tual Event, August 1-6, 2021, pp. 1828–1843. Association for Computational Linguistics, 2021.\n   doi: 10.18653/V1/2021.ACL-LONG.144. URL https://doi.org/10.18653/v1/2021.\n  acl-long.144.\n\nJaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal,\n  Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel\n  Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha,\n\n\n                                       11\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n  Jonathan Bell, Byron C Wallace, and David Bau. NNsight and NDIF: Democratizing access to\n  open-weight foundation model internals. In The Thirteenth International Conference on Learning\n  Representations, 2025. URL https://openreview.net/forum?id=MxbEiFRf39.\n\nKanishk Gandhi, Jan-Philipp Fr¨anken, Tobias Gerstenberg, and Noah Goodman. Understanding\n   social reasoning in language models with language models. Advances in Neural Information\n  Processing Systems, 36, 2024.\n\nAtticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models\n   partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov,\n  Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the\n  Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 163–\n  173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n  blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.\n\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural\n  networks. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\n  Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual\n  Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,\n  2021, virtual, pp. 9574–9586, 2021. URL https://proceedings.neurips.cc/paper/\n  2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\n  man, and Christopher Potts.  Inducing causal structure for interpretable neural networks.  In\n  Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n   (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of\n  Proceedings of Machine Learning Research, pp. 7324–7338. PMLR, 17–23 Jul 2022. URL\n  https://proceedings.mlr.press/v162/geiger22a.html.\n\nAtticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang,\n  Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal\n   abstraction: A theoretical foundation for mechanistic interpretability, 2024. URL https://\n  arxiv.org/abs/2301.04709.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual\n  associations in auto-regressive language models, 2023. URL https://arxiv.org/abs/\n  2304.14767.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\n  Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\n  Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,\n  Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru,\n  Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak,\n  Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu,\n  Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\n   Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego\n  Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,\n  Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm´an, Frank Zhang, Gabriel\n  Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,\n  Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\n  Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\n  Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde,\n  Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\n  Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua\n  Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak,\n  Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley\n  Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\n  Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas\n  Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri,\n  Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie\n  Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes\n\n\n                                       12\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n  Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne,\n  Onur C¸elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal\n  Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\n  Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\n  Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie\n  Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana\n  Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie,\n  Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon\n  Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan,\n  Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\n  Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\n  Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti,\n   V´ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier\n  Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao\n   Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,\n  Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe\n  Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya\n  Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei\n  Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu,\n  Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit\n  Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury,\n  Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,\n  Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu,\n  Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\n   Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu\n  Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,\n  Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu,\n  Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc\n  Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\n  Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers,\n  Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank\n  Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee,\n  Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,\n  Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,\n  Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog,\n  Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James\n  Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny\n  Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings,\n  Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai\n  Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\n  Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\n  Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng\n  Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish\n   Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim\n  Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle\n  Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang,\n  Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam,\n  Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier,\n  Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia\n  Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro\n   Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,\n   Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,\n  Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\n  Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\n  Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\n  Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\n  Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang,\n  Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie\n  Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,\n\n\n                                       13\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n  Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman,\n  Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun\n  Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria\n  Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,\n  Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz,\n  Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv\n  Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\n  Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait,\n  Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The\n  llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\n\nHyowon Gweon, Judith Fan, and Been Kim. Socially intelligent machines that learn from humans\n  and help humans learn. Philosophical Transactions of the Royal Society A, 381(2251):20220048,\n  2023.\n\nDaniel A Herrmann and Benjamin A Levinstein. Standards for belief representations in llms. arXiv\n  preprint arXiv:2405.21030, 2024.\n\nJennifer Hu, Felix Sosa, and Tomer Ullman.  Re-evaluating theory of mind evaluation in large\n  language models. arXiv preprint arXiv:2502.21098, 2025.\n\nChuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman,\n  Antonio Torralba, Joshua Tenenbaum, and Tianmin Shu. MMToM-QA: Multimodal theory of\n  mind question answering. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings\n   of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n  Papers), pp. 16077–16102, Bangkok, Thailand, August 2024. Association for Computational\n   Linguistics. URL https://aclanthology.org/2024.acl-long.851.\n\nHyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten\n  Sap. Fantom: A benchmark for stress-testing machine theory of mind in interactions. arXiv\n  preprint arXiv:2310.15421, 2023.\n\nNajoung Kim and Sebastian Schuster.   Entity tracking in language models.  arXiv preprint\n  arXiv:2305.02363, 2023.\n\nYoujeong Kim and S Shyam Sundar. Anthropomorphism of computers: Is it mindful or mindless?\n  Computers in Human Behavior, 28(1):241–250, 2012.\n\nMichal Kosinski. Evaluating large language models in theory of mind tasks. Proceedings of the\n  National Academy of Sciences, 121(45), October 2024. ISSN 1091-6490. doi: 10.1073/pnas.\n  2405460121. URL http://dx.doi.org/10.1073/pnas.2405460121.\n\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel. Revisiting the evaluation of theory of mind\n  through question answering. In Proceedings of the 2019 Conference on Empirical Methods in\n  Natural Language Processing and the 9th International Joint Conference on Natural Language\n  Processing (EMNLP-IJCNLP), pp. 5872–5877, 2019.\n\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\n  language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceed-\n  ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\n  International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.\n  1813–1827, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n  2021.acl-long.143. URL https://aclanthology.org/2021.acl-long.143.\n\nKenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\n   intervention: Eliciting truthful answers from a language model. Advances in Neural Information\n  Processing Systems, 36, 2024.\n\nTom Lieberum, Matthew Rahtz, J´anos Kram´ar, Neel Nanda, Geoffrey Irving, Rohin Shah, and\n  Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice\n   capabilities in chinchilla, 2023. URL https://arxiv.org/abs/2307.09458.\n\n\n                                       14\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.  Locating and editing factual\n  associations in GPT. Advances in Neural Information Processing Systems, 36, 2022.\n\nAaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can\n  Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and\n  Yonatan Belinkov. The quest for the right mediator: A history, survey, and theoretical grounding of\n  causal interpretability, 2024. URL https://arxiv.org/abs/2408.01416.\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\n  Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,\n  Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\n   Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,\n  and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.\n  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n\nNikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning\n  enhances existing mechanisms: A case study on entity tracking.  In Proceedings of the 2024\n  International Conference on Learning Representations, 2024. arXiv:2402.14811.\n\nDavid Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and\n  Brain Sciences, 1(4):515–526, 1978. doi: 10.1017/S0140525X00076512.\n\nChristoph Riedl, Young Ji Kim, Pranav Gupta, Thomas W Malone, and Anita Williams Woolley.\n  Quantifying collective intelligence in human groups. Proceedings of the National Academy of\n  Sciences, 118(21):e2005737118, 2021.\n\nNina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner.\n  Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023.\n\nNaomi Saphra and Sarah Wiegreffe. Mechanistic? In Yonatan Belinkov, Najoung Kim, Jaap Jumelet,\n  Hosein Mohebbi, Aaron Mueller, and Hanjie Chen (eds.), Proceedings of the 7th BlackboxNLP\n  Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 480–498, Miami, Florida, US,\n  November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.blackboxnlp-1.\n  30. URL https://aclanthology.org/2024.blackboxnlp-1.30/.\n\nMelanie Sclar, Jane Yu, Maryam Fazel-Zarandi, Yulia Tsvetkov, Yonatan Bisk, Yejin Choi, and Asli\n  Celikyilmaz. Explore theory of mind: Program-guided adversarial data generation for theory of\n  mind reasoning. ICLR, 2025.\n\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. How well do large language models perform on\n  faux pas tests?  In Findings of the Association for Computational Linguistics: ACL 2023, pp.\n  10438–10451, Toronto, Canada, July 2023. Association for Computational Linguistics. URL\n  https://aclanthology.org/2023.findings-acl.663.\n\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\n  Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in\n  large language models. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th\n  Conference of the European Chapter of the Association for Computational Linguistics (Volume 1:\n  Long Papers), pp. 2257–2273, St. Julian’s, Malta, March 2024. Association for Computational\n   Linguistics. URL https://aclanthology.org/2024.eacl-long.138.\n\nPaul Smolensky. Neural and conceptual interpretation of PDP models. In James L. McClelland,\n  David E. Rumelhart, and the PDP Research Group (eds.), Parallel Distributed Processing: Explo-\n  rations in the Microstructure of Cognition: Psychological and Biological Models, volume 2, pp.\n  390–431. MIT Press, 1986.\n\nJames W. A. Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh\n  Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, Michael S. A. Graziano,\n  and Cristina Becchio. Testing theory of mind in large language models and humans. Nature Human\n  Behaviour, 8(7):1285–1295, Jul 2024a. ISSN 2397-3374. doi: 10.1038/s41562-024-01882-z.\n  URL https://doi.org/10.1038/s41562-024-01882-z.\n\n\n                                       15\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nJames WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh\n  Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al. Testing theory of\n  mind in large language models and humans. Nature Human Behaviour, 8(7):1285–1295, 2024b.\n\nWinnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben,\n  Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, and Robin I. M. Dunbar. Llms achieve\n  adult human performance on higher-order theory of mind tasks, 2024. URL https://arxiv.\n  org/abs/2405.18870.\n\nTomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv\n  preprint arXiv:2302.08399, 2023.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,\n  and Stuart Shieber.   Investigating gender bias in language models using causal mediation\n   analysis.  In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-\n  vances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran Asso-\n   ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/\n  2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.\n\nHeinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of\n  wrong beliefs in young children’s understanding of deception. Cognition, 13(1):103–128, 1983.\n\nYiwei Wu, Atticus Geiger, and Rapha¨el Milli`ere. How do transformers learn variable binding in\n  symbolic programs? arXiv preprint arXiv:2505.20896, 2025.\n\nYufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-ToM:\n A benchmark for evaluating higher-order theory of mind reasoning in large language mod-\n   els.  In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for\n  Computational Linguistics: EMNLP 2023, pp. 10691–10706, Singapore, December 2023. As-\n  sociation for Computational Linguistics.  doi: 10.18653/v1/2023.findings-emnlp.717. URL\n  https://aclanthology.org/2023.findings-emnlp.717.\n\nHainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. OpenToM: A comprehensive\n  benchmark for evaluating theory-of-mind reasoning capabilities of large language models. In\n  Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\n   of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8593–8623,\n  Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\n  //aclanthology.org/2024.acl-long.466.\n\nWentao Zhu, Zhining Zhang, and Yizhou Wang. Language models represent beliefs of self and others.\n  arXiv preprint arXiv:2402.18496, 2024.\n\n\n\n\n\n                                       16\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\nA  FULL PROMPT\n\n\n   No Visibility\n\n   Instruction:  1.  Track the belief of each character as described\n   in the story.  2.  A character’s belief is formed only when they\n   perform an action themselves or can observe the action taking place.\n   3.  A character does not have any beliefs about the container and\n   its contents which they cannot observe.  4.  To answer the question,\n   predict only what is inside the queried container, strictly based\n   on the belief of the character, mentioned in the question.  5.  If\n   the queried character has no belief about the container in question,\n   then predict ‘unknown’.  6.  Do not predict container or character\n   as the final output.\n   Story:  Bob and Carla are working in a busy restaurant.  To\n   complete an order, Bob grabs an opaque bottle and fills it with\n   beer.  Then Carla grabs another opaque cup and fills it with\n   coffee.\n   Question:  What does Bob believe the bottle contains?\n   Answer:\n\n\n    Explicit Visibility\n\n   Instruction:  1.  Track the belief of each character as described\n   in the story.  2.  A character’s belief is formed only when they\n   perform an action themselves or can observe the action taking place.\n   3.  A character does not have any beliefs about the container and\n   its contents which they cannot observe.  4.  To answer the question,\n   predict only what is inside the queried container, strictly based\n   on the belief of the character, mentioned in the question.  5.  If\n   the queried character has no belief about the container in question,\n   then predict ‘unknown’.  6.  Do not predict container or character\n   as the final output.\n   Story:  Bob and Carla are working in a busy restaurant.  To\n   complete an order, Bob grabs an opaque bottle and fills it with\n   beer.  Then Carla grabs another opaque cup and fills it with\n   coffee.  Bob can observe Carla’s actions.  Carla cannot observe\n   Bob’s actions.\n   Question:  What does Bob believe the cup contains?\n   Answer:\n\n\nB  THE CAUSALTOM DATASET\n\nWe needed to construct a new dataset because we required a task that models could reliably solve.\nIn contrast, most existing ToM datasets remain challenging for LMs. Additionally, we needed a\ndataset in which each sample is paired with multiple counterfactuals, enabling causal computations\nand the extraction of the underlying mechanism. The only dataset that met both criteria was BigToM,\nwhich we used in our study. However, even BigToM was insufficient for investigating the full range\nof factors influencing the mechanism, such as the relationship between a character and their object.\nHence, we needed to simplify the task to allow for additional counterfactuals. To test the effect of a\nspecific element, we required the ability to modify only that element without altering the rest of the\nstory or creating an incoherent scenario. For example, consider a BigToM story where a flood occurs,\nand opening a gate releases the water. In the counterfactual scenario where the gate remains closed,\nthe story’s continuation becomes unintelligible, with the occurrence of a flood.\n\nTo address this, we developed CausalToM, which features simple stories accompanied by a range\nof counterfactuals. Key features include: (1) two characters, objects, and states, (2) the ability to\nmodify each of them independently, and (3) control over whether characters witness each other’s\nactions. The dataset comprises four templates, one without visibility statements and three with\nexplicit visibility statements. Each template supports four types of questions (e.g., “CharacterX asked\n\n\n                                       17\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nabout ObjectY”). We used lists of 103 characters, 21 objects, and 23 states. For our interchange\nintervention experiments, we randomly sampled 80 pairs of original and counterfactual stories.\n\nC  CAUSAL MEDIATION ANALYSIS\n\n           Bob and Carla are working in a busy restaurant.  To\n           complete an order, Bob grabs an opaque bottle and\n           fills it with beer.  Then Carla grabs another opaque\n           cup and fills it with coffee.                                                                                             Counterfactual Question:  What does Bob believe the bottle contains?\n           Answer:  beer\n\n           David and Carla are working in a busy restaurant.  To\n           complete an order, David grabs an opaque bottle and\n           fills it with beer.  Then Carla grabs another opaque\n           cup and fills it with coffee.                                                    Original\n           Question:  What does Bob believe the bottle contains?\n           Answer:  unknown\n\n            Causal Model Output: beer\n\n\nFigure 9: Causal Mediation Analysis: The original example produces the output unknown because\nBob is not mentioned in the story, leaving the model without any information about his beliefs.\nHowever, when the residual stream vectors corresponding to Bob from the counterfactual run are\npatched into the original run, the model acquires the necessary information about that character and\nconsequently updates its output to beer.\n\nIn addition to the experiment shown in Fig.9, we conduct similar experiments for the object and\nstate tokens by replacing them in the story with random tokens, which alters the original example’s\nfinal output. However, patching the residual stream vectors of these tokens from the counterfactual\nrun restores the relevant information, enabling the model to predict the causal model output. The\nresults of these experiments are collectively presented in Fig.2, with separate heatmaps shown in\nFig. 10, 11, 12.\n\n            80                                                                                        1\n                        Character\n            60\n\n            40                                                                                                   IIA                    Layers\n            20\n             0 :    a .   ,       .        . :     ? :    0\n                                 Story  Maggy\n\n                        Bob                          andCarlaareworkingin  busyrestaurant Tocompleteanorder  MaggyBobgrabsanopaquebottleandfillsitwithbeer  ThenCarlagrabsanotheropaquecupandfillsitwithcoffee    Question  WhatdoesBobbelievethebottlecontains   Answer\n\n      Figure 10: Information flow of character input tokens using causal mediation analysis.\n\n\n\n\n\n                                       18\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n            80                                                                                        1\n                        Object\n            60\n\n            40                                                                                                   IIA                    Layers\n            20\n             0 :    a .   ,       .        . :     ? :    0\n                                 Story BobandCarlaareworkingin  busyrestaurant Tocompleteanorder Bobgrabsanopaquemugandfillsitwithbeer  ThenCarlagrabsanotheropaquecupandfillsitwithcoffee    Question  WhatdoesBobbelievethebottlecontains   Answer                                                                                                                                   bottle\n\n       Figure 11: Information flow of object input tokens using causal mediation analysis.\n\n\n            80                                                                                        1\n                         State\n            60\n\n            40                                                                                                   IIA                    Layers\n            20\n             0 :    a .   ,       .        . :     ? :    0\n                                 Story BobandCarlaareworkingin  busyrestaurant Tocompleteanorder Bobgrabsanopaquebottleandfillsitwithmilkbeer  ThenCarlagrabsanotheropaquecupandfillsitwithcoffee    Question  WhatdoesBobbelievethebottlecontains   Answer\n\n        Figure 12: Information flow of state input tokens using causal mediation analysis.\n\n\nD  DESIDERATE BASED PATCHING VIA CAUSAL ABSTRACTION\n\nCausal Models and Interventions  A deterministic causal model M has variables that take on\nvalues. Each variable has a mechanism that determines the value of the variable based on the values of\nparent variables. Variables without parents, denoted X, can be thought of as inputs that determine the\nsetting of all other variables, denoted M(x). A hard intervention A ←a overrides the mechanisms\nof variable A, fixing it to a constant value a.\n\nInterchange Interventions  We perform interchange interventions (Vig et al., 2020; Geiger et al.,\n2020) where a variable (or set of features) A is fixed to be the value it would take on if the LM were\nprocessing counterfactual input c. We write A ←Get(M(c), A) where Get(M(c), A) is the value\nof variable A when M processes input c. In experiments, we will feed a original input o to a model\nunder an interchange intervention MA←Get(M(c),A))(o).\nFeaturizing Hidden Vectors  The dimensions of hidden vectors are not an ideal unit of analysis\n(Smolensky, 1986), and so it is typical to featurize a hidden vector using some invertible function,\ne.g., an orthogonal matrix, to project a hidden vector into a new variable space with more inter-\npretable dimensions called “features”(Mueller et al., 2024). A feature intervention Fh ←f edits the\nmechanism of a hidden vector h to fix the value of features Fh to f.\nAlignment  The LM is a low-level causal model L where variables are dimensions of hidden vectors\nand the hypothesis about LM structure is a high-level causal model H. An alignment Π assigns each\nhigh-level variable A to features of a hidden vector FAh, e.g., orthogonal directions in the activation\nspace of h. To evaluate an alignment, we perform intervention experiments to evaluate whether\nhigh-level interventions on the variables in H have the same effect as interventions on the aligned\nfeatures in L.\n\nCausal Abstraction  We use interchange interventions to reveal whether the hypothesized causal\nmodel H is an abstraction of an LM L. To simplify, assume both models share an input and output\nspace. The high-level model H is an abstraction of the low-level model L under a given alignment\nwhen each high-level interchange intervention and the aligned low-level intervention result in the same\noutput. For a high-level intervention on A aligned with low-level features FAh with a counterfactual\ninput c and original input b, we write\n\n          GetOutput(LFAh ←Get(L(c),FAh ))(o)) = GetOutput(HA←Get(H(c),A))(o))           (1)\n\n\n                                       19\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nIf the low-level interchange intervention on the LM produces the same output as the aligned high-level\nintervention on the algorithm, this is a piece of evidence in favor of the hypothesis. This extends\nnaturally to multi-variable interventions (Geiger et al., 2024).\n\nGraded Faithfulness Metric  We construct counterfactual datasets for each causal variable where\nan example consists of a base prompt and a counterfactual prompt . The counterfactual label is the\nexpected output of the algorithm after the high-level interchange intervention, i.e., the right-side of\nEquation 1. The interchange intervention accuracy is the proportion of examples for which Equation 1\nholds, i.e., the degree to which H faithfully abstracts L.\n\nAligning Features to Causal Variables   In our experiments, we use Singular Vector Decomposition\n(SVD) to featurize residual stream vectors, i.e., features are the orthogonal singular vectors. For\na given transformer layer and token location, we collect the residual stream vectors across a large\nnumber of examples and compute the singular vectors. Given singular vector features Fh of a hidden\nvector h in the residual stream of the LM L, we select features to align with a causal variable A in\ncausal model H using Desiderata-based Component Masking (DCM) (De Cao et al., 2020; Davies\net al., 2023; Prakash et al., 2024). Given original input o and counterfactual input c, we train a mask\nm ∈[0, 1]|Fh| on the following objective\n\n       CE GetLogits LFh←m◦Get(L(c),Fh))(b)  , GetLogits HA←Get(H(c),A))(b)           (2)\n\n\n\n\n\n                                       20\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\nE  PSEUDOCODE FOR THE BELIEF TRACKING HIGH-LEVEL CAUSAL MODEL\n\n\nAlgorithm 2 High-level causal model for the no visibility\n  1: procedure BELIEFTRACKING(c1, o1, s1, c2, o2, s2, qc, qo)\n  2:    Ordering ID assignment\n  3:    cOI1  , oOI1  , sOI1  ←AssignOIs([c1, o1, s1], 1)\n  4:    cOI2  , oOI2  , sOI2  ←AssignOIs([c2, o2, s2], 2)\n  5:\n  6:    Binding lookback mechanism\n  7:     binding address1 ←(copy(cOI1  ), copy(oOI1  ))\n  8:     binding address2 ←(copy(cOI2  ), copy(oOI2  ))\n  9:\n10:    qOIc  ←copy({c1 : cOI1  , c2 : cOI2  }[qc])\n11:    qOIo  ←copy({o1 : oOI1  , o2 : oOI2  }[qo])\n12:     binding pointer ←(qOIc   , qOIo  )\n13:\n14:       if binding address1 = binding pointer then\n15:        binding payload ←copy(sOI1  )\n16:     else if binding address2 = binding pointer then\n17:        binding payload ←copy(sOI2  )\n18:    end if\n19:\n20:    Answer lookback mechanism\n21:    answer pointer ←binding payload\n22:    answer1 address ←sOI1\n23:    answer2 address ←sOI2\n24:       if answer1 address = answer pointer then\n25:       answer payload ←s1\n26:     else if answer2 address = answer pointer then\n27:       answer payload ←s2\n28:    end if\n29:    return answer payload\n30: end procedure\n\n\nF  DESIDERATA-BASED COMPONENT MASKING\n\n\nWhile interchange interventions on residual vectors reveal where a causal variable might be encoded\nin the LM’s internal activations, they do not localize the variable to specific subspaces. To address\nthis, we apply the Desiderata-based Component Masking technique (De Cao et al., 2020; Davies et al.,\n2023; Prakash et al., 2024), which learns a sparse binary mask m over the singular vectors of the LM’s\ninternal activations. We first cache the internal activations from 500 samples at the token positions\nspecified in the main text for each experiment. Next, we apply Singular Value Decomposition to\ncompute the singular vectors as a matrix V ∈Rd×500 where d is the dimensionality of the residual\nstream. We then masked this matrix using a learnable binary vector m ∈[0, 1]d to choose a subset of\nsingular vectors\n                                   Vmasked = V m                                        (3)\nThe chosen subset of vectors is used to construct a projection matrix Wproj ∈Rd×d.\n\n                             Wproj = VmaskedV maskedT                                        (4)\n\nThen, we perform subspace-level interchange interventions (rather than replacing the entire residual\nvector) using the following equations:\n\n\n                         hnew = Wprojhc + (I −Wproj)ho                              (5)\n\n\n                                       21\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\nwhere ho is the full residual stream of the original run, hc is the full residual stream of the counterfac-\ntual run, and hnew is the intervened vector where the chosen subspace of ho is replaced with that of\nhc.\n\nThe core idea is to first remove the existing information from the subspace defined by the projec-\ntion matrix and then insert the counterfactual information into that same subspace using the same\nprojection matrix.\n\nIn order to find the optimal subspace, we optimize m to maximize the agreement between the causal\nmodel output and the LM’s output. To do so, we train the mask for each experiment on 80 examples\nof the same counterfactual datasets specified in the main text and use another 80 samples as the\nvalidation set. We use the following objective function, which maximizes the logit of the causal\nmodel output token:\n\n             L = −logitcausal model output under intervention + λ X m                     (6)\n\nWhere λ is a hyperparameter used to control the rank of the subspace and m is the learnable mask.\nSee Appendix D for details on how the causal model output under intervention are computed. We\ntrained m for one epoch with ADAM optimizer, on batches of size 4 and a learning rate of 0.01.\nDuring training, the parameters of m are continuous and constrained to lie within the range [0, 1].\nTo enforce this constraint, we clamp their values after each gradient update. During evaluation, we\nbinarize the mask by rounding each parameter to the nearest integer, i.e., 0 or 1.\n\nG  ALIGNING CHARACTER AND OBJECT OIS\n\nAs mentioned in section 5.2, the source reference information, consisting of character and object OI,\nis duplicated to form the address and pointer of the binding lookback. Here, we describe another\nexperiment to verify that the source information is copied to both the address and the pointer. More\nspecifically, we conduct the same interchange intervention experiment as described in Fig. 6, but\nwithout freezing the residual vectors at the state tokens. Based on our hypothesis, this intervention\nwill not be able to change the state of the original run, since the intervention at the source information\nwill affect both address and pointer, hence making the model form the original QK-circuit.\n\n    Carla and Bob are working in a busy restaurant.  To complete       1.0\n    an order, Carla grabs an opaque cup and fills it with coffee.                                              Full residual\n    Then Bob grabs another opaque bottle and fills it with beer.       0.8                        Subspace\n                                                                                                                   dimension  96\n    Question:               What does Carla believe the cup contains?                                  Accuracy 0.6             coffee        Counterfactual  Answer:\n    Bob and Carla are working in a busy restaurant.  To complete       0.4\n              Bob                                  bottle       order,                  grabs                        an                           opaque                                         and                                             fills                                                   it                                                      with                                                           beer.                                                                                     0.2         Carla                                    cup               grabs                     another                             opaque                                        and                                            fills                                                  it                                                     with                                                          coffee.    Original  Thenan                                                                                                                                                                                                                                                                                                                      Intervention                         Carla                                           cup    Question:               What                    does                               believe                                       the                                               contains?\n    Answer:  coffee                                                              0.0 0   10  20  30  40  50  60  70\n    Intervention: Binding Source (    ,   )                                                                Layers\n    Causal Model Output: beer\n\nFigure 13: Source Reference Information of Binding lookback: In this interchange intervention\nexperiment, the source information, i.e., the character and object OIDs (   ,    ), is modified, while\nthe address and payload (   ,    ,   ) are recomputed based on the modified source. Since both the\naddress and pointer information are derived from the altered source, the binding lookback ultimately\nretrieves the same original state token as the payload. As a result, we do not observe high intervention\naccuracy.\n\nIn section 5.2, we identified the source of the information but did not fully determine the locations of\neach character and object OI. To address this, we now localize the character and object OIs separately\nto gain a clearer understanding of the layers at which they appear in the residual streams of their\nrespective tokens, as shown in Fig.14 and Fig.15.\n\n\n\n\n\n                                       22\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                              Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8                      Subspace\n    Question:  What does Carla believe the cup contains?                                            dimension  29.0        Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                     0.2    Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                                      Intervention\n    Answer:  unknown                                                             0.0\n                                                                         0   10  20  30  40  50  60  70\n    Intervention: Character OI (   )                                                                     Layers\n    Frozen: Object OI, Binding and Answer Addresses+Payloads (    ,     ,    ;     ,   )\n    Causal Model Output: water\n\nFigure 14: Character OI: This interchange intervention experiment swaps the character OI (   ),\nwhile freezing the object OI as well as binding lookback address and payload (   ,    ,    ). Swapping\nthe character OIs in the story tokens changes the queried character OI to the other one. Hence, the\nfinal output changes from unknown to water.\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                              Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8                      Subspace\n    Question:  What does Carla believe the cup contains?                                            dimension  58.0        Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                     0.2    Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                                      Intervention\n    Answer:  unknown                                                             0.0\n                                                                         0   10  20  30  40  50  60  70\n    Intervention: Binding Source (    ,   )                                                                Layers\n    Frozen: Binding and Answer Addresses+Payloads, Queried Character OID (    ,     ,    ;     ,   )\n    Causal Model Output: tea\n\nFigure 15: Object OI: This interchange intervention experiment swaps both the character and object\nOIs (   ,    ), while freezing the address and payload of binding lookback (   ,    ,   ) as well as queried\ncharacter OI (   ). Swapping both character and object OIs in the story tokens ensures that the queried\nobject gets the other OI. Hence, the final output changes from unknown to tea.\n\n\nH  ALIGNING QUERY CHARACTER AND OBJECT OIS\n\nIn section 5.2, we localized the pointer information of binding lookback. However, we found that this\ninformation is transferred to the lookback token (last token) through two intermediate tokens: the\nqueried character and the queried object. In this section, we separately localize the OIs of the queried\ncharacter and queried object, as shown in Fig. 16 and Fig. 17.\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                                 Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8                         Subspace\n    Question:  What does Carla believe the cup contains?                                               dimension  5        Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                     0.2    Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                                      Intervention\n    Answer:  unknown                                                             0.0\n                                                                         0   10  20  30  40  50  60  70\n     Intervention: Binding Pointer (   )                                                                  Layers\n    Causal Model Output: water\n\n\nFigure 16: Query Character OI: This interchange intervention experiment alters the OI of the\nqueried character (  ) to the other one. Hence, the final output changes from unknown to water.\n\n\nI  SPECULATED PAYLOAD IN VISIBILITY LOOKBACK\n\nAs mentioned in section 6, the payload of the Visibility lookback remains undetermined. In this\nsection, we attempt to disambiguate its semantics using the Attention Knockout technique introduced\n\n\n                                       23\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                                 Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8                         Subspace\n    Question:  What does Carla believe the cup contains?                                               dimension  7        Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                     0.2    Original  an order, Carla grabs an opaque cup and fills it with tea.                                         cup    Question:               What                    does Bob                             believe                                     the                                             contains?                                                                                                                                                                                                                                                                                                                      Intervention\n    Answer:  unknown                                                             0.0\n                                                                         0   10  20  30  40  50  60  70\n     Intervention: Binding Pointer (   )                                                                  Layers\n    Causal Model Output: water\n\n\nFigure 17: Query Object OI: This interchange intervention experiment alters the OI of the queried\nobject (  ) to the other one. Hence, the final output changes from unknown to water.\n\n\nin (Geva et al., 2023), which helps reveal the flow of crucial information. We apply this technique to\nunderstand which previous tokens are vital for the formation of the payload information. Specifically,\nwe ”knock out” all attention heads at all layers of the second visibility sentence, preventing them\nfrom attending to one or more of the previous sentences. Then, we allow the attention heads to attend\nto the knocked-out sentence one layer at a time.\n\nIf the LM is fetching vital information from the knocked-out sentence, the interchange intervention\naccuracy (IIA) post-knockout will decrease. Therefore, a decrease in IIA will indicate which attention\nheads, at which layers, are bringing in the vital information from the knocked-out sentence.  If,\nhowever, the model is not fetching any critical information from the knocked-out sentence, then\nknocking it out should not affect the IIA.\n\n                               Layer-wise IIA with Attention Knockouts\n                                      1.0\n\n                                      0.8                                                                        Accuracy                                      0.6\n\n                                      0.4                            Second sent + First vis sent\n                                                                                   First vis sent                                      0.2                                                                                                             Intervention                               Second sent\n                                      0.0                                    0    10    20    30    40    50    60    70    80\n                                                  Layer Index\n\nFigure 18: At the second visibility sentence, attention heads are restricted to retrieve information\nfrom one of three prior contexts: (1) both the second story sentence and the first visibility sentence (–\nline), (2) only the first visibility sentence (– line), or (3) only the second story sentence (– line).\n\nTo determine if any vital information is influencing the formation of the Visibility lookback payload,\nwe perform three knockout experiments: 1) Knockout attention heads from the second visibility\nsentence to both the first visibility sentence and the second story sentence (which contains information\nabout the observed character), 2) Knockout attention heads from the second visibility sentence to\nonly the first visibility sentence, and 3) Knockout attention heads from the second visibility sentence\nto the second story sentence. In each experiment, we measure the effect of the knockout using IIA.\n\nFig.18 shows the experimental results. Knocking out any of the previous sentences affects the model’s\nability to produce the correct output. The decrease in IIA in the early layers can be explained by the\nrestriction on the movement of character OIs. Specifically, the second visibility sentence mentions the\nfirst and second characters, whose character OIs must be fetched before the model can perform any\nfurther operations. Therefore, we believe the decrease in IIA until layer 15, when the character OIs\nare formed (based on the results from Section G), can be attributed to the model being restricted from\nfetching the character OIs. However, the persistently low IIA even after this layer—especially when\nboth the second and first visibility sentences are involved—indicates that some vital information is\nbeing fetched by the second visibility sentence, which is essential for forming the coherent Visibility\nlookback payload. Thus, we speculate that the Visibility payload encodes information about the\nobserved character, specifically their character OI, which is later used to fetch the correct state OI.\n\n\n                                       24\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\nJ  CORRELATION ANALYSIS OF CAUSAL SUBSPACES AND ATTENTION HEADS\n\nThis section identifies the attention heads that align with the causal subspaces discovered in the\nprevious sections. Specifically, first we focus on attention heads whose query projections are aligned\nwith the subspaces—characterized by the relevant singular vectors—that contain the correct answer\nstate OI. To quantify this alignment between attention heads and causal subspaces, we use the\nfollowing computation.\nLet Q ∈Rdmodel×dmodel denote the query projection weight matrix for a given layer:\n\nWe normalize Q column-wise:\n\n                                          Q:,j\n                                         ˜Q:,j =           for each column j                              (7)\n                                    ∥Q:,j∥\n\nLet S ∈Rdmodel×k represent the matrix of k singular vectors (i.e., the causal subspace basis). We\nproject the normalized query weights onto this subspace:\n\n                                     Qsv = ˜Q · S                                          (8)\n\nWe then reshape the resulting projection into per-head components. Assuming Qsv ∈Rdmodel×k, and\neach attention head has dimensionality dh, we write:\n\n                         Q(i)head = Q(i)sv ∈Rdh×k   for i = 1, . . . , nheads                         (9)\n\nFinally, we compute the norm of each attention head’s projection:\n\n                      head normi =  Q(i)head      for i = 1, . . . , nheads                     (10)\n                                       F\n\nWe compute the head norm for each attention head in every layer, which quantifies how strongly a\ngiven head reads from the causal subspace present in the residual stream. The results are presented\nin Fig. 19, and they align with our previous findings: attention heads in the later layers form the\nQK-circuit by using pointer and address information to retrieve the payload during the Answer\nlookback.\n\nWe perform a similar analysis to check which attention heads’ value projection matrix align with\nthe causal subspace that encodes the payload of the Answer lookback. Results are shown in Fig. 20,\nindicating that attention heads at later layers primarily align with causal subspace containing the\nanswer token.\n\nK  BELIEF TRACKING MECHANISM IN BIGTOM BENCHMARK\n\nThis section presents preliminary evidence that the mechanisms outlined in Sections 5 and 6 generalize\nto other benchmark datasets. Specifically, we demonstrate that Llama-3-70B-Instruct answers the\nbelief questions (true belief and false belief) in the BigToM dataset Gandhi et al. (2024) in a manner\nsimilar to that observed for CausalToM: by first converting token values to their corresponding OIs\nand then performing logical operations on them using lookbacks. However, as noted in Section 3,\nBigToM—like other benchmarks—lacks the coherent structure necessary for causal analysis. As\na result, we were unable to replicate all experiments conducted on CausalToM. Thus, the results\nreported here provide only preliminary evidence of a similar underlying mechanism.\n\nTo justify the presence of OIs, we conduct an interchange intervention experiment, similar to\nthe one described in Section H, aiming to localize the character OI at the character token in the\nquestion sentence. We construct an original sample by replacing its question sentence with that of a\ncounterfactual sample, selected directly from the unaltered BigToM dataset. Consequently, when\nprocessing the original sample, the model has no information about the queried character and, as\na result, produces unknown as the final output. However, if we replace the residual vector at the\n\n\n                                       25\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n                      q_proj norms on Answer lookback pointer subspace\n              60\n                                                                                          10\n              57\n              54\n              51                                                                        8\n              48\n              45                                                                        6                         Layers              42\n              39                                                                        4\n              36\n              33                                                                        2\n              30\n               0                                                                        0\n      0 5 10 15 20 25 30 35 40 45 50 55 60\n                                             Attention Heads\n\nFigure 19: Alignment between the Answer lookback pointer causal subspace and the query projection\nmatrix in Llama-3-70B-Instruct.\n\n                     v_proj norms on Answer lookback payload subspace\n                                                                                          14\n              75\n                                                                                          12\n              70\n                                                                                          10\n              65\n                         Layers 60                                                                        8                                                                                          6\n\n              55                                                                                          4\n\n              50                                                                        2\n\n               0                                                                        0\n      0 5 10 15 20 25 30 35 40 45 50 55 60\n                                             Attention Heads\n\nFigure 20: Alignment between the Answer lookback payload causal subspace and the value projection\nmatrix in Llama-3-70B-Instruct.\n\n\n\nqueried character token in the original sample with the corresponding vector from the counterfactual\nsample (which contains the character OI), the model’s output changes from unknown to the state\ntoken(s) associated with the queried object. This is because inserting the character OI at the queried\ntoken provides the correct pointer information, aligning with the address information at the correct\nstate token(s), thereby enabling the model to form the appropriate QK-circuit and retrieve the state’s\nOI. As shown in Fig. 21, we observe a high IIA between layers 9 −28—similar to the pattern seen\nin CausalToM—suggesting that the queried character token encodes the character OI in its residual\nvector within these layers.\n\nNext, we investigate the Answer lookback mechanism in BigToM, focusing specifically on localizing\nthe pointer and payload information at the final token position. To localize the pointer information,\nwhich encodes the correct state OI, we construct original and counterfactual samples by selecting two\ncompletely different examples from the BigToM dataset, each with different ordered states as the\ncorrect answer. For example, as illustrated in Fig.22, the counterfactual sample designates the first\nstate as the answer, thrilling plot, whereas the original sample designates the second state, almond\nmilk. We perform an intervention by swapping the residual vector at the last token position from the\ncounterfactual sample into the original run. The causal model outcome of this intervention is that the\n\n\n                                       26\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n    Noor is working as a barista at a busy coffee shop.  Noor\n    wants to make a delicious cappuccino for a customer who asked\n    for oat milk.  Noor grabs a milk pitcher and fills it with\n    oat milk.  A coworker, who didn’t hear the customer’s request,\n    swaps the oat milk in the pitcher with almond milk while            1.0\n    Noor is attending to another task.  Noor sees her coworker                                                Full residual\n    swapping the milk.                                                          0.8                      Subspace       Counterfactual    Question:  Does Noor believe the milk pitcher contains oat                                    dimension  380    milk or almond milk?                                                                                                                                     Accuracy 0.6\n    Answer:  almond milk\n                                                                                    0.4\n    Tariq is a fisherman in a coastal village in Indonesia.  He\n    wants to catch fish for his family’s dinner using a fishing        0.2    net that he left hanging to dry on a tree branch.  Tariq sees         Intervention\n    tat the net appears to be dry and ready to use.  A sudden           0.0\n    downpour soaks the fishing net, making it heavy and difficult       0   10  20  30  40  50  60  70   Original  to use.  Tariq notices the rain soaking the fishing net.                            Layers\n    Question:  Does Noor believe the fishing net is dry or\n    soaked?\n    Answer:  unknown\n\n    Causal Model Output: soaked\n\n\nFigure 21: Query Character OI in BigToM: This interchange intervention experiment inserts\nthe first character’s OI into the residual stream at the queried character token (   ), resulting in the\nmovement of pointer information to the last token that aligns with the address information of binding\nlookback mechanism. Consequently, the model is able to form the appropriate QK-circuit from the\nlast token to predict the correct state answer token(s) as the final output, instead of unknown.\n\n\nmodel will output the alternative state token from the original sample, oat milk. As shown in Fig.22,\nthis alignment occurs between layers 33 and 51, similar to the layer range observed for the pointer\ninformation in the Answer lookback of CausalToM.\n\n\n    Yael is in a small bookstore browsing through the science\n    fiction section.  Yael wants to find a book with a thrilling\n    plot to read during her upcoming vacation.  He notices the\n    book cover of a novel displaying an intriguing scene of a\n    futuristic cityscape.  A customer accidentally places a book\n                                bland    with         a           similar                   cover,                          but a                                      storyline,                                                 in                                                    the                                                        spot\n              thrilling                                                       Yael    where          the                        novel                              was,                                   after                                         browsing                                                  it.                                                            does       1.0    Full residual       Counterfactual  not        notice the customer putting the book back in the wrong         0.8   Subspace    spot.                                                                                                       dimension  149    Question:  Does Yael believe the book with the intriguing                  Accuracy    cover has a thrilling plot or a bland storyline?                      0.6\n    Answer:  thrilling plot                                                    0.4\n                                                         Noor    Noor is            working                    as a barista                                 at a busy                                           coffee                                                  shop.\n    wants          to             make a delicious                              cappuccino                                         for                                             a customer                                                        who                                                            asked      0.2                                                                                                                                                                                                                                                                                                                     Intervention\n                                milk pitcher    for        oat            milk.                   Noor grabs                              a                                             and                                                 fills it with\n    oat        milk.               A coworker,                           who                               didn’t                                      hear the                                               customer’s                                                          request,     0.0 0   10  20  30  40  50  60  70\n              oat milk                                           almond                                                  milk    swaps          the                       in                          the pitcher                                      with                                                       while                             Layers   Original                                        Noor    Noor is            attending                      to                         another                                 task.                                             sees her                                                      coworker\n    swapping the milk.\n    Question:  Does Noor believe the milk pitcher contains oat\n    milk or almond milk?\n    Answer:  almond milk\n\n    Causal Model Output: oat milk\n\n\nFigure 22: Answer Lookback Pointer in BigToM: This interchange intervention experiment\nmodifies the pointer information (  ) of the Answer lookback, thereby altering the subsequent QK-\ncircuit to attend to the other state (e.g., oat milk) instead of the original one (e.g., almond milk). As\na result, the model retrieves the token value corresponding to the other state to answer the question.\n\n\nFurther, to localize the payload of the Answer lookback in BigToM, we perform an interchange\nintervention experiment using the same original and counterfactual samples as mentioned in the\nprevious experiment, but with a different expected output—namely, the correct state from the\ncounterfactual sample instead of the other state from the original sample. As shown in Fig. 23,\nalignment emerges after layer 59, consistent with the layer range observed for the Answer lookback\npayload in CausalToM.\n\nFinally, we investigate the impact of the visibility condition on the underlying mechanism and\nfind that, similar to CausalToM, the model uses the Visibility lookback to enhance the observing\ncharacter’s awareness based on the observed character’s actions. To localize the effect of the visibility\n\n\n                                       27\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n    Yael is in a small bookstore browsing through the science           1.0\n    fiction section.  Yael wants to find a book with a thrilling                    Full residual\n    plot to read during her upcoming vacation.  He notices the         0.8    Subspace\n    book cover of a novel displaying an intriguing scene of a                  dimension  161                                                                                                                                                                                                              Accuracy    futuristic cityscape.  A customer accidentally places a book       0.6\n    with a similar cover, but a bland storyline, in the spot\n    where the thrilling novel was, after browsing it.  Yael does       0.4        Counterfactual  not notice the customer putting the book back in the wrong\n    spot.                                                                          0.2    Question:  Does Yael believe the book with the intriguing                            Intervention\n    cover has a thrilling plot or a bland storyline?                      0.0\n    Answer:  thrilling plot                                              0   10  20  30  40  50  60  70\n                                                                                                Layers\n    Noor is working as a barista at a busy coffee shop.  Noor\n    wants to make a delicious cappuccino for a customer who asked\n    for oat milk.  Noor grabs a milk pitcher and fills it with\n    oat milk.  A coworker, who didn’t hear the customer’s request,\n    swaps the oat milk in the pitcher with almond milk while    Original  Noor is attending to another task.  Noor sees her coworker\n    swapping the milk.\n    Question:  Does Noor believe the milk pitcher contains oat\n    milk or almond milk?\n    Answer:  almond milk\n\n    Causal Model Output: thrilling plot\n\n\nFigure 23: Answer Lookback Payload in BigToM: This interchange intervention experiment\ndirectly modifies the payload information (  ) of the Answer lookback, which is fetched from the\ncorresponding state tokens and predicted as the next token(s). Thus, replacing its value in the original\nrun, e.g. almond milk, with that from the counterfactual run, e.g. thrilling plot, causes the model’s\nnext predicted tokens to correspond to the correct answer of the counterfactual sample.\n\n\n\n    Noor is working as a barista at a busy coffee shop.  Noor\n    wants to make a delicious cappuccino for a customer who asked\n    for oat milk.  Noor grabs a milk pitcher and fills it with\n    oat milk.  A coworker, who didn’t hear the customer’s request,\n    swaps the oat milk in the pitcher with almond milk while\n    Noor         is attending                      to another task.  Noor sees her coworker         1.0    swapping             the milk.        Counterfactual\n    Question:               Does                    Noor believe the milk pitcher contains oat         0.8                                      Visibility    milk or almond                   milk?                                                                                                                                    condition    Answer:  almond milk                                                                                                                                     Accuracy                                                                                     0.6                            Question\n                                                         Noor    Noor is            working                    as a barista                                 at a busy                                           coffee                                                  shop.\n    wants          to             make a delicious                              cappuccino                                         for                                             a customer                                                        who                                                            asked      0.4                                      Visibilitycondition\n    for oat milk.  Noor grabs a milk pitcher and fills it with                                       +Question\n    oat milk.  A coworker, who didn’t hear the customer’s request,     0.2    swaps the oat milk in the pitcher with almond milk while Noor         Intervention    Original  is attending to another task.  Noor does not see her coworker      0.0                                                                         0   10  20  30  40  50  60  70\n    swapping             the                 milk.\n    Question:               Does                    Noor believe the milk pitcher contains oat                           Layers\n    milk or almond milk?\n    Answer:  oak milk\n\n    Expected Output: almond milk\n\n\nFigure 24: Visibility Lookback in BigToM: We perform three interchange interventions to establish\nthe presence of the Visibility ID, which serves as both address and pointer information. When\nintervening at the source (  )—i.e., the visibility sentence—both the address and pointer are updated,\nresulting in alignment across layers. Intervening only at the subsequent question tokens leads to\nalignment only at later layers, after the model has already fetched the payload (     ). However,\nintervening at both the visibility and question sentences results in alignment across all layers, as the\naddress and pointer remain consistent throughout.\n\n\ncondition, we perform an interchange intervention in which the original and counterfactual samples\ndiffer in belief type—that is, if the original sample involves a false belief, the counterfactual involves\na true belief, and vice versa. The expected output of this experiment is the other (incorrect) state of\nthe original sample. Following the methodology in Section 6, we conduct three types of interventions:\n(1) only at the visibility condition sentence, (2) only at the subsequent question sentence, and (3) at\nboth the visibility condition and the question sentence. As shown in Fig. 24, intervening only at the\nvisibility sentence results in alignment at early layers, up to layer 17, while intervening only at the\nsubsequent question sentence leads to alignment after layer 26. Intervening on both the visibility and\nquestion sentences results in alignment across all layers. These results align with those found in the\nCausalToM setting shown in the Fig. 8.\n\n\n                                       28\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\nPrevious experiments suggest that the underlying mechanisms responsible for answering belief\nquestions in BigToM are similar to those in CausalToM. However, we observed that the subspaces\nencoding various types of information are not shared between the two settings. For example, although\nthe pointer information in the Answer lookback encodes the correct state’s OI in both cases, the\nspecific subspaces that represent this information at the final token position differ significantly. We\nleave a deeper investigation of this phenomenon—shared semantics across distinct subspaces in\ndifferent distributions—for future work.\n\nL  GENERALIZATION OF BELIEF TRACKING MECHANISM ON CAUSALTOM TO\n   LLAMA-3.1-405B-INSTRUCT\n\nThis section presents all the interchange intervention experiments described in the main text, con-\nducted using the same set of counterfactual examples on Llama-3.1-405B-Instruct, using NDIF\nFiotto-Kaufman et al. (2025). Each experiment was performed on 80 samples. Due to computational\nconstraints, subspace interchange intervention experiments were not conducted. The results indicate\nthat Llama-3.1-405B-Instruct employs the same underlying mechanism as Llama-3-70B-Instruct to\nreason about belief and answer related questions. This suggests that the identified belief-tracking\nmechanism generalizes to other models capable of reliably performing the task.\n\n    Carla and Bob are working in a busy restaurant.  To complete       1.0\n    an order, Carla grabs an opaque cup and fills it with coffee.                                                  Full residual\n    Then Bob grabs another opaque bottle and fills it with beer.       0.8\n    Question:               What does Carla believe the cup contains?                                  Accuracy 0.6             coffee        Counterfactual  Answer:\n    Bob and Carla are working in a busy restaurant.  To complete       0.4\n                                                                                    0.2                                    cup    Then         Carla grabs                     another                             opaque                                        and                                            fills                                                  it with coffee.    Original  an order, Bob grabs an opaque bottle and fills it with beer.                                                                                                                                                                                                                                                                                                                    Intervention                         Carla                                           cup    Question:               What does                               believe                                       the                                               contains?\n    Answer:  coffee                                                              0.0                                                                         0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: beer                                                                     Layers\n                      Figure 25: Payload and address of Binding lookback\n\n\n    Carla and Bob are working in a busy restaurant.  To complete       1.0\n    an order, Carla grabs an opaque cup and fills it with tea.                                                       Full residual\n    Then Bob grabs another opaque bottle and fills it with water.      0.8\n    Question:               What does Carla believe the cup contains?                                  Accuracy 0.6             tea        Counterfactual  Answer:\n    Bob and Carla are working in a busy restaurant.  To complete       0.4\n                                                                                    0.2                                    cup    Then         Carla grabs                     another                             opaque                                        and                                            fills                                                  it with coffee.    Original  an order, Bob grabs an opaque bottle and fills it with beer.                                                                                                                                                                                                                                                                                                                    Intervention                         Carla                                           cup    Question:               What does                               believe                                       the                                               contains?\n    Answer:  coffee                                                              0.0                                                                         0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: beer                                                                     Layers\n                       Figure 26: Source Information of Binding lookback\n\n\n    Carla and Bob are working in a busy restaurant.  To complete       1.0\n    an order, Carla grabs an opaque cup and fills it with coffee.                                                  Full residual\n    Then Bob grabs another opaque bottle and fills it with beer.       0.8\n    Question:               What does Carla believe the cup contains?                                  Accuracy 0.6             coffee        Counterfactual  Answer:\n    Bob and Carla are working in a busy restaurant.  To complete       0.4\n              Bob                                  bottle       order,                  grabs                        an                           opaque                                         and                                             fills                                                   it                                                      with                                                           beer.                                                                                    0.2         Carla                                    cup               grabs                     another                             opaque                                        and                                            fills                                                  it                                                     with                                                          coffee.    Original  Thenan                                                                                                                                                                                                                                                                                                                    Intervention                         Carla                                           cup    Question:               What                    does                               believe                                       the                                               contains?\n    Answer:  coffee                                                              0.0 0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: beer                                                                     Layers\n\n\nFigure 27: Source Reference Information of Binding lookback without freezing address and\npayload\n\n\n\n\n\n                                       29\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                                   Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8\n    Question:  What does Carla believe the cup contains?       Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                    0.2   Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                                    Intervention\n    Answer:  unknown                                                            0.0\n                                                                         0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: water                                                                    Layers\n\n\n                                  Figure 28: Character OI\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                                   Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8\n    Question:  What does Carla believe the cup contains?       Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                    0.2   Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                                    Intervention\n    Answer:  unknown                                                            0.0\n                                                                         0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: tea                                                                       Layers\n\n\n                                    Figure 29: Object OI\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete       1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                                   Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.      0.8\n    Question:  What does Carla believe the cup contains?       Counterfactual  Answer:  coffee                                                                                                                                                    Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete       0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                    0.2   Original  an order, Carla grabs an opaque cup and fills it with tea.                                         cup    Question:               What                    does Bob                             believe                                     the                                             contains?                                                                                                                                                                                                                                                                                                                    Intervention\n    Answer:  unknown                                                            0.0\n                                                                         0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: water                                                                    Layers\n\n\n                                Figure 30: Query Object OI\n\n\n\n\n    Bob and Carla are working in a busy restaurant.  To complete    1.0\n    an order, Bob grabs an opaque bottle and fills it with beer.                                               Full residual\n    Then Carla grabs another opaque cup and fills it with coffee.   0.8\n    Question:  What does Carla believe the cup contains?       Counterfactual  Answer:  coffee                                                                                                                                            Accuracy 0.6\n    Carla and Bob are working in a busy restaurant.  To complete    0.4\n\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it with water.                                                                                 0.2   Original  an order, Carla grabs an opaque cup and fills it with tea.                                           bottle    Question:               What                    does Carla                               believe                                       the                                                  contains?                                                                                                                                                                                                                                                                                                        Intervention\n    Answer:  unknown                                                         0.0\n                                                                      0 10 20 30 40 50 60 70 80 90100110120\n    Causal Model Output: water                                                                 Layers\n\n\n                              Figure 31: Query Character OI\n\n\n\n\n                                       30\n\nUnder review as a conference paper at ICLR 2026\n\n\n\n\n\n    Carla and Bob are working in a busy restaurant.  To complete       1.0                                                                                             Answer Payload\n    an order, Carla grabs an opaque cup and fills it with tea.\n    Then Bob grabs another opaque bottle and fills it with water.      0.8   Binding Payload\n                                                                                    0.6       Counterfactual  Answer:Question:teaWhat does Carla believe the cup contains?                                  Accuracy\n    Bob and Carla are working in a busy restaurant.  To complete       0.4\n                                                                                    0.2                                    cup    Then         Carla grabs                     another                             opaque                                        and                                            fills                                                  it with coffee.   Original  an order, Bob grabs an opaque bottle and fills it with beer.                                                                                                                                                                                                                                                                                                                    Intervention                         Carla                                           cup    Question:               What does                               believe                                       the                                               contains?\n    Answer:  coffee                                                              0.0                                                                         0 10 20 30 40 50 60 70 80 90100110120\n     Intervention: Answer Pointer (   ), Causal Model Output: beer                                          Layers\n    Intervention: Answer Payload (   ), Causal Model Output: tea\n\n\n\n                       Figure 32: Answer Lookback Pointer and Payload\n\n\n\n\n\n    Carla and Bob are working in a busy restaurant.  To complete\n              Carla    an order,                    grabs an                             opaque                                    cup and                                            fills                                                  it                                                     with                                                          tea.\n                                  bottle    Then         Bob grabs                   another                           opaque                                         and                                             fills                                                   it                                                      with                                                           water.     1.0                                                 Visibility\n    Bob cannot observe Carla’s actions.  Carla can observe Bob’s                                                     condition                                                                          0.8                                                                                                                                  Question       Counterfactual  actions.\n    Question:               What does Carla believe the bottle contains?                     Accuracy                                                                          0.6                                                 Visibility             water    Answer:                                                                                                                                              condition\n                                                                                                                            +Question\n    Bob and Carla are working in a busy restaurant.  To complete      0.4\n   Original  Thenan order,Bob grabsCarlaanothergrabs anopaqueopaquebottlecup andand fillsfills itit withwith beer.coffee.    0.2    Bob cannot observe Carla’s actions.  Carla cannot observe                        Intervention\n    Bob’s          actions.\n    Question:               What does Carla believe the bottle contains?            0.0 0 10 20 30 40 50 60 70 80 90100110120\n    Answer:  unknown                                                                   Layers\n    Causal Model Output: coffee\n                                Figure 33: Visibility Lookback\n\n\n\n\n\n                                       31\n\n## Claim to Classify\n\"the binding lookback retrieves the correct state OI\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Interchange interventions / activation patching (causal mediation / causal abstraction with IIA) on residual stream and localized subspaces to test hypothesized variables (OIs, pointers/addresses/payloads)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "Method: The paper supports the binding-lookback story using interchange interventions (activation patching) and causal abstraction alignment measured by IIA, which is rung 2 interventional evidence. Claim: “the binding lookback retrieves the correct state OI” is a functional/mechanistic attribution (retrieves; describes what the mechanism does), not merely that intervening changes outputs, so it is rung 3. It is stated in the abstract without hedging. Slight ambiguity because it could be read as a summary of an intervention result (R2), but the phrasing attributes an internal computation (‘binding lookback’ as a mechanism) performing retrieval, which defaults to R3 per codebook."
  },
  "metadata": {
    "model": "gpt-5.2-2025-12-11",
    "usage": {
      "prompt_tokens": 35401,
      "completion_tokens": 218,
      "total_tokens": 35619
    },
    "finish_reason": "stop"
  }
}