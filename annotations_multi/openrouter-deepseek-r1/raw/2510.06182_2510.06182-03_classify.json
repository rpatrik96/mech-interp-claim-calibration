{
  "timestamp": "2026-02-19T09:53:30.275489+00:00",
  "paper_id": "2510.06182",
  "claim_id": "2510.06182-03",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2510.06182\nTitle: Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context\nFull text:\n                        Preprint\n\n\n          MIXING MECHANISMS: HOW LANGUAGE MODELS\n          RETRIEVE BOUND ENTITIES IN-CONTEXT\n\n\n                    Yoav Gur-Arieh♠♢, Mor Geva♠, Atticus Geiger♢♡\n                       ♠Blavatnik School of Computer Science and AI, Tel Aviv University\n                     ♢Pr(Ai)2R Group\n                      ♡Goodfire\n\n\n                                       ABSTRACT\n\n                  A key component of in-context reasoning is the ability of language models (LMs)\n                                  to bind entities for later retrieval. For example, an LM might represent Ann loves2025\n                                 pie by binding Ann to pie, allowing it to later retrieve Ann when asked Who loves\n                               pie? Prior research on short lists of bound entities found strong evidence that LMs\n                            implement such retrieval via a positional mechanism, where Ann is retrievedOct\n                             based on its position in context. In this work, we find that this mechanism gener-\n7                            alizes poorly to more complex settings; as the number of bound entities in context\n                                  increases, the positional mechanism becomes noisy and unreliable in middle posi-\n                                    tions. To compensate for this, we find that LMs supplement the positional mech-\n                            anism with a lexical mechanism (retrieving Ann using its bound counterpart pie)\n                            and a reflexive mechanism (retrieving Ann through a direct pointer). Through\n                                extensive experiments on nine models and ten binding tasks, we uncover a con-[cs.CL]                             sistent pattern in how LMs mix these mechanisms to drive model behavior. We\n                                leverage these insights to develop a causal model combining all three mechanisms\n                                    that estimates next token distributions with 95% agreement. Finally, we show that\n                              our model generalizes to substantially longer inputs of open-ended text interleaved\n                              with entity groups, further demonstrating the robustness of our findings in more\n                                  natural settings. Overall, our study establishes a more complete picture of how\n                     LMs bind and retrieve entities in-context.\n\n\n                1  INTRODUCTION\n\n                    Language models (LMs) are known for their ability to perform in-context reasoning (Brown et al.,\n                       2020), and fundamental to this capability is the task of connecting related entities in a text—known\n                       as binding—to construct a representation of context that can be queried for next token prediction.\n                    However, LMs are also known for struggling in reasoning tasks over long contexts (Liu et al., 2024;\n                   Levy et al., 2024). In this work, we conduct a mechanistic investigation into the internals of LMs toarXiv:2510.06182v1                          better understand how they bind entities in increasingly complex settings.\n\n                     Neural networks’ ability to bind arbitrary entities was a central issue in connectionist models of cog-\n                         nition (Touretzky & Minton, 1985; Fodor & Pylyshyn, 1988; Smolensky, 1990) and has reemerged\n                         in the era of LMs as a target phenomenon for mechanistic interpretability research (Davies et al.,\n                     2023; Prakash et al., 2024; 2025; Feng & Steinhardt, 2024; Feng et al., 2024; Wu et al., 2025). For\n                      example, to represent the text Pete loves jam and Ann loves pie, an LM will bind Pete to jam and\n                  Ann to pie. This enables the LM to answer questions like Who loves pie? by querying the bound\n                           entities to retrieve the answer (Ann). The prevailing view is that LMs retrieve bound entities using a\n                       positional mechanism (Dai et al., 2024; Prakash et al., 2024; 2025), where the query entity (pie) is\n                     used to determine the in-context position of Ann loves pie—in this case, the second clause after Pete\n                        loves jam—which is dereferenced to retrieve the answer Ann.\n\n                       In this work, we show that position-based retrieval holds only for simple settings. This mechanism\n                             is unreliable for the middle positions in long lists of entity groups—a pattern that echoes the “lost-\n                       in-the-middle” effect (Liu et al., 2024) in LMs as well as primacy and recency biases in both humans\n                      (Ebbinghaus, 1913; Miller & Campbell, 1959) and LMs (Janik, 2024). To compensate for this noise,\n\n\n                                                           1\n\nPreprint\n\n\n\n\n                                 Counterfactual Input                                                                                      pie                (A) Positional 2 , lexical\n          and reﬂexive  binding info\n                 is written to entity tokens \n                                                                                              (B) Binding info is\n                                 1                           2                           3                           4      used to retrieve             2\n                                                                                 queried entity \n\n\n\n\n\n                                  1                           2                           3                           4                           2\n\n\n\n\n\n               1                 1         2                  2        3                 3        4                 4                                                                                                                                         Patch \n\n      Joe loves ale  Ann loves pie  Pete loves jam  Tim loves tea What does Ann love   ? \n\n                                                                                                (C) Patching last\n                                       Original Input                token position                          34\n                                 1                           2                           3                           4      across examples              32\n                                                                                               splits the signals \n                                  lexical               positional 2        reﬂexive \n   … … …  … … … … … … … … … … … … … …\n\n     Ann loves ale  Joe loves jam Pete loves pie  Tim loves tea What does Tim love   ? \n\n               Original Unpatched                         Original Patched \n    Ann loves ale, Joe loves jam, Pete loves    Ann loves ale, Joe loves jam, Pete loves\n      pie, Tim loves tea. What does Tim love?      pie, Tim loves tea. What does Tim love? \n\n\n               4                                                                       34                                                     2                             34\n                        Logits   Final                                                                                                                                                                                 Logits                                     Final                             32\n\n           ale   jam   pie    tea                         ale    jam    pie    tea \n\n   Without intervention, the three mechanisms  Under intervention, the three mechanisms\n        agree with a decisive prediction            diverge in the entities predicted \n\nFigure 1: An illustration of the three mechanisms for retrieving bound entities in-context. We find\nthat as models process inputs with groups of entities: (A) binding information of three types—\npositional, lexical, reflexive—is encoded in the entity tokens of each group, (B) this binding in-\nformation is jointly used to retrieve entities in-context, and (C) it is possible to separate the three\nbinding signals with counterfactual patching. The counterfactual input is designed such that patch-\ning activations to the LM run on the original input results in the positional, lexical, and reflexive\nmechanisms predicting different entities (See §3.2).\n\n\n\n\n\nLMs supplement the positional mechanism with a lexical mechanism, where the query entity (pie)\nis used to retrieve its bound counterpart (Ann), and a reflexive mechanism, where the queried entity\n(Ann) is retrieved with a direct pointer that was previously retrieved via the query entity (pie).\n\nIn a series of ablation experiments, we show that all three mechanisms are necessary to develop\nan accurate causal model of the next token distribution (Pˆıslar et al., 2025), and that their interplay\ndepends on the positions the query entity (pie) and the retrieved entity (Ann).  This mixture of\nmechanisms is robustly present across (1) the Llama, Gemma, and Qwen model families, (2) model\nsizes within those families ranging from 2 to 72 billion parameters, and (3) ten variable binding tasks.\nBy better understanding this mechanism, we take a step toward explaining both the strengths and\nthe persistent fragilities of LLMs in long-context settings, as well as the fundamental mechanisms\nthat support in-context reasoning. We release our code and dataset of binding tasks at https:\n//github.com/yoavgur/mixing-mechs.\n\n\n                                       2\n\nPreprint\n\n\n\n\n2  PROBLEM SETUP AND PRIOR WORK\n\nEntity Binding Tasks  In our experiments, we design a number of templatic in-context reasoning\ntasks with a similar structure to the example from the introduction, i.e., Pete loves jam, Ann loves\npie. Who loves pie? Formally, a task consists of:\n\n      1. Entity Roles: Disjoint sets of entities E1, . . . , Em that will fill particular roles in a templatic\n          text. For example, the set E1 might be names of people {Ann, Pete, Tim, . . . }, and the set\n        E2 might be foods and drinks {ale, jam, pie, . . . }.\n      2. Entity Groups: An entity group is a tuple G ∈E1×· · ·×Em containing entities that will be\n        placed within the same clause in a template. For example, we could set G1 = (Pete, jam)\n       and G2 = (Ann, pie). For convenience, we define G as a binding matrix wherein Gji\n        denotes the j-th entity in the i-th entity group.\n      3. A template (T ): A function that takes as input a binding matrix G, the query entity q =\n          Gqgroupqentity, and the target entity t = Gqgrouptentity . Here qgroup is a positional index of the entity group\n         containing the target and query, and tentity ̸=  qentity index the positions of the target and\n        query entities within that group, respectively. See §A for more details and examples.\n\nContinuing our example, define\n\n                                    Who loves q?           tentity == 1\n         T (G, q, t) = G11 loves G21, G12 loves G22.\n                                         What does q love?    tentity == 2\n\nand observe that\n\n               Pete  jam\n        T                       , pie, Ann = Pete loves jam, Ann loves pie. Who loves pie?\n            Ann   pie\n\nFor our experiments, the binding matrix G will consist of distinct entities.\n\nInterchange Interventions  To probe the mechanisms an LM uses to bind and retrieve entities,\nwe employ interchange interventions (Vig et al., 2020; Geiger et al., 2020; Finlayson et al., 2021;\nGeiger et al., 2021), the standard tool for prior work on binding and retrieval (Davies et al., 2023;\nFeng & Steinhardt, 2024; Prakash et al., 2024; 2025; Wu et al., 2025). These interventions allow us\nto identify which hidden states are causally relevant for the model in entity binding, by running the\nLM on paired examples — an original input and a counterfactual input — and replacing selected\ncomponents, e.g., residual stream vectors, in the original run with those from the counterfactual.\n\nCausal Abstraction  We develop a causal model of LM internals (Geiger et al., 2021; 2025b;a)\nthat predicts the LM next token distribution using a mixture of three mechanisms (See §4). To test\nour hypotheses, we construct a dataset of paired originals and counterfactuals such that an inter-\nchange intervention on the causal model results in the positional, lexical and reflexive mechanisms\nincreasing the probability of distinct tokens. To evaluate our proposed causal model and various\nablations, we perform interchange interventions on the causal model and the LM, measuring the\nsimilarity between the next token distribution of the two models, and average across a dataset.\n\nPrior Studies of Entity Binding in LMs  Previous work paints a picture of how entity binding and\nretrieval is performed by LMs. First, LMs bind together a group of entities by aggregating informa-\ntion about all entities in the entity token at the last position in the group. By co-locating information\nabout entities in the residual stream of a single token, the LM can later on use attention to retrieve\ninformation about one bound entity conditional on a second bound entity (Feng & Steinhardt, 2024;\nFeng et al., 2024; Dai et al., 2024; Prakash et al., 2024; Wu et al., 2025), an algorithmic motif that\nPrakash et al. (2025) dub a “lookback” mechanism. We study the “pointers” used in the lookback\nmechanism that bring the next token prediction into the residual stream of the final token. We in-\nclude experiments on the “addresses” contained in the residual streams of the bound entity tokens,\nas well as the query token, in §B.5.\n\nPrior works identify a positional mechanism that is utilized in entity binding (described in detail\nin §3.1), but either evaluate it only in narrow settings (Prakash et al., 2025) or achieve low causal\n\n\n                                       3\n\nPreprint\n\n\n\n\n                                                   Patch Effects\n                           positional           lexical          reflexive        mixed        no effect\n                                     tentity = 3                                               tentity = 1\n    1.0                                                                   1.0\n                                                                                                                                     Effect\n    0.5                                                                   0.5 index=0                                                                                                               Patch\n    0.0                                                                   0.0\n       0   2   4   6   8  10  12  14  16  18  20  22  24                      tentity = 2\n    1.0                                                                   1.0\n                                                                                                                                     Effect\n    0.5                                                                   0.5 index=10                                                                                                               Patch\n    0.0                                                                   0.0\n       0   2   4   6   8  10  12  14  16  18  20  22  24                       tentity = 3\n    1.0                                                                   1.0\n                                                                                                                                              Effect\n    0.5                                                                   0.5 index=19                                                                                                                Patch\n    0.0                                                                   0.0       0   2   4   6   8  10  12  14  16  18  20  22  24        0 2 4 6 8 10 12 14 16 18\n                                  Layer                                   Patched Entity Group Index\n\nFigure 2: Results from interchange interventions on gemma-2-2b-it over a counterfactual dataset\nwith three entities per group (m = 3) (See Figure 1 and §3.2). Outputs predicted by the positional,\nlexical and reflexive mechanisms are shown in dark blue, green and orange. Left: Distribution of\neffects for three representative entity group indices (first, middle, and last) with tentity = 3. At layers\n16–18, the last token position carries binding information used for retrieval. Right: Distribution of\neffects for all indices at layer 18 for tentity ∈{1, 2, 3}, i.e., the question can be about any of the three\nentities in each clause. A U-shaped curve emerges: first and last indices rely more on the positional\nmechanism, while middle indices rely more on the lexical and reflexive mechanisms. See §B for\nreplication across models and tasks, and Figure 18 for plots using the original prompt as the x-axis.\n\n\nfaithfulness in predicting model behavior solely using this mechanism (Prakash et al., 2024; Dai\net al., 2024). (Feng & Steinhardt, 2024; Prakash et al., 2025) restrict their analysis to queries of the\nfinal token in a group (tentity = m) and to very small contexts (n ∈2, 3). Prakash et al. (2024) and\nDai et al. (2024) find a positional mechanism in longer contexts (n = 7), but with low faithfulness.\n\n\n3  THREE MECHANISMS FOR RETRIEVING BOUND ENTITIES\n\nIn this section, we define the positional mechanism and propose two alternative mechanisms (§3.1),\nall three of which make distinct claims about the causal structure of the LM. Then, we design a\ndataset with pairs of original and counterfactual inputs, such that each of the three mechanisms\nmakes distinct predictions under an interchange intervention with the pair (§3.2). Last, we perform\ninterchange interventions on different layers of the last token residual stream of the LM and visualize\nthe results so we can observe the interplay between the three mechanisms in the counterfactual\nbehavior of the LM (§3.3). In our experiments, we evaluate nine models—gemma-2-{2b/9b/27b}-\nit, qwen2.5-{3b/7b/32b/72b}-it, and llama-3.1-{8b/70b}-it—on two binding tasks, boxes and music\n(see Appendix Table 1). For gemma-2-2b-it and qwen2.5-7b-it, we evaluate on all ten binding tasks.\n\n\n3.1  THE POSITIONAL MECHANISM AND TWO ALTERNATIVES\n\nThe prevailing view is that bound entities are retrieved with a positional mechanism, but we propose\ntwo alternatives: lexical and reflexive mechanisms. The positional, lexical, and reflexive mecha-\nnisms are represented as causal models P, L, and R that each have single intermediate variables P,\nL, and R, respectively, used to retrieve an entity from context as the output.\n\n\n                                       4\n\nPreprint\n\n\n\n\n\nThe Positional Mechanism  Prior work provides evidence that a positional mechanism is used to\nretrieve an entity from a group via the group’s positional index (Dai et al., 2024; Prakash et al.,\n2024; 2025). The model P indexes the group containing the query entity (P := qgroup), and its\noutput mechanism retrieves the target entity from the group at index qgroup. In Figure 1, we have\nP = 4 when no intervention is performed on the LM and the target entity tea is retrieved from\nposition 4, but after the intervention P ←2 the entity jam at the second position is retrieved.\n\nAlthough existing evidence shows that the positional mechanism explains LM behavior in settings\nwith two or three entity groups (Prakash et al., 2025), it does not generalize. When more groups are\nintroduced, the evidence is weaker (Prakash et al., 2024; Dai et al., 2024). Our goal is to investigate\nthe failure modes of the positional mechanism as more entity groups are introduced, and to that end\nwe propose two alternative hypotheses for how LMs implement binding.\n\nThe Lexical Mechanism  The lexical mechanism is perhaps the most intuitive solution: output\nthe bound entity from the group containing the queried entity. The causal model L stores the query\nentity (L := q) and the output mechanism retrieves the target entity from the group containing q. In\nFigure 1, we have L = Tim when no intervention is performed on the LM and the output mechanism\nretrieves the entity tea from the group with Tim. However, after the intervention L ←Ann, the entity\nale is retrieved from the group with Ann.\n\nThe Reflexive Mechanism  The reflexive mechanism retrieves an entity with a direct,  self-\nreferential pointer—originating from that entity and pointing back to it. However, if this signal\nis patched into a context where the token is not present, the mechanism fails. The model R stores\nthe target entity (R := t) and the output mechanism retrieves the entity t if it appears in context. In\nFigure 1, we have R = tea when no intervention is performed and the entity tea is retrieved, but\nafter the intervention R ←pie, the entity pie is retrieved because it appears in the original input.\n\nThe reflexive mechanism is an unintuitive solution, until one considers that the architecture of an\nautoregressive LM allows attention to only look from right to left. When the query occurs after\na target in an entity group, i.e., tentity < qentity, the lexical mechanism is not possible. In the text\nTim loves tea, the entity tea cannot be copied backwards to the residual stream of Tim so that the\nlexical mechanism can answer Who loves tea? Therefore, an earlier mechanism in the LM must first\nretrieve an absolute pointer that is in turn used to retrieve the bound entity token.\n\n3.2  DESIGNING COUNTERFACTUAL INPUTS TO DISTINGUISH THE THREE MECHANISMS\n\nWe designed a dataset of paired original and counterfactual inputs such that the positional, lexical,\nand reflexive mechanisms will each make distinct predictions when an interchange intervention is\nperformed on their respective intermediate variables, P, L, and R.\n\nCounterfactual Design  Figure 1 displays a pair of original and counterfactual inputs that dis-\ntinguish our three mechanisms (further detailed in Appendix Table 1). We illustrate this with the\nfollowing example. Define the original and counterfactual binding matrices G and G′ respectively:\n                    Ann   ale         Joe    ale \n                               Joe  jam          Ann   pie            G =              G′ =                                        (1)                                       Pete   pie               Pete  jam\n                         Tim   tea           Tim   tea\nWe can then use the template T from §2 such that for these binding matrices, T (G, Tim, tea) yields\nthe original input in Figure 1 and T (G′, Ann, pie) yields the counterfactual input. Each of the three\nmechanisms produces a different output after an interchange intervention on this pair of inputs:\n\n      1. An interchange intervention on P in P would output the entity at the counterfactual query’s\n         position. Since q′group = 2 for Ann in G′, this sets P ←2, and the output becomes jam.\n      2. An interchange intervention on L in L would output the entity in the original input bound\n         to the query entity in the counterfactual input.  Since the query entity is now Ann, the\n       mechanism queries the group containing Ann in G′ and outputs the bound entity ale.\n      3. An interchange intervention on R in R would follow the direct pointer established in the\n         counterfactual input. In this case, the pointer is to the token pie, which exists in the original\n         input, and so the mechanism outputs pie.\n\n\n                                       5\n\nPreprint\n\n\n\n\n                                                                            lexical    positional    reflexive\n                                                                                  iL = 0              iL = 2              iL = 4\n  0 99 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      100            10\n  2 0 94 5  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n          1 15 44 22 9  4  3  1  1  1  0  0  0  0  0  0  0  0  0  0      80               5\n             5               36                  24                     15                         8  4                               2                                  2                                     1                                        1                                           0                                              0                                                 0                                                    0                                                       0                                                          0                                                             0                                                                0                                                                   0  4 0\n          0             7                8                  28                     23 15                            8                               4                                  3                                     1                                        1                                           1                                              0                                                 0                                                    0                                                       0                                                          0                                                             0                                                                0                                                                   0                         0\n  6 0  9 11 5 25 20 14 7  4  2  1  1  0  0  0  0  0  0  0  0      60\n\n          0             5               13                   8                      5                         6                           17                              15                                 12                                     8  5                                           3                                              2                                                 1                                                    1                                                       0                                                          0                                                             0                                                                0                                                                   0                                                     (%)     Value Index8 0  7 13 6  5 20 19 13 7  4  3  2  1  0  0  0  0  0  0  0                             iL = 6              iL = 8             iL = 11                                                                                     10          0             3               10                   8                      6                         5                            5                              15                                 14 12                                        8                                           5                                              3                                                 2                                                    1                                                       1                                                          1                                                             0                                                                0                                                                   0\n          0  3  8  7  7  6  5  5 15 14 12 7  5  2  1  1  1  1  0  0      40\n                                                                                      5   10 0  1  6  6  5  6  5  5  6 13 14 13 7  5  3  2  1  1  1  1                    Logit\n          0             1                5                   5                      5                         5                            5                               4                                  5                                     6                                       13                                          13                                             12                                                 7  5                                                       4                                                          2                                                             2                                                                1                                                                   1                                                                                      0   12          0             1                3                   3                      4                         4                            4                               4                                  5                                     5                                        8                                          15                                             13 11                                                    7                                                       6                                                          4                                                             3                                                                1                                                                   2          0  0  2  2  3  4  3  3  4  4  6  8 12 11 10 8  6  6  3  3      20 Percentage  Positional14 0  0  1  1  2  2  3  2  3  4  4  5  7 12 14 12 10 8  7  6                Mean       iL = 14            iL = 16            iL = 19\n          0  0  1  1  1  1  2  2  2  3  3  5  5  6 13 15 12 11 10 8\n   16 0  0  1  1  1  1  1  2  2  2  2  3  4  5  7 15 16 14 13 11                       10\n          0             0                0                   0                      0                         1                            1                               1                                  1                                     2                                        2                                           2                                              3                                                 4                                                    5                                                       9                                                         16                                                            21                                                               16                                                                  14                                                                                      5   18          0             0                0                   0                      1                         1                            1                               1                                  1                                     1                                        1                                           2                                              2                                                 3                                                    4                                                       6                                                         11                                                            21                                                               25                                                                  19\n          0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  2  3  8 14 69      0                0\n       0    2    4    6    8   10   12   14   16   18                    0  4  8  12 16   0  4  8  12 16   0  4  8  12 16                 Prediction Index                                                                                 Entity Group Index\n\nFigure 3: The positional mechanism is diffuse for middle entity groups. Left: Confusion matrix\n(%) of the patched positional index vs. gemma-2-2b-it’s prediction after an interchange intervention\n(as in Figure 1). Counterfactual predictions cluster near the position promoted by the positional\nmechanism, decaying with distance. Only the mixed and positional patch effects from Figure 2 are\nshown; see Figure 28 for other models and tasks. Right: Mean logit distributions with iP = 6, iR =\n14, and iL varied, illustrating interaction between the three mechanisms. The lexical and reflexive\nsignals form one-hot peaks, while the positional is broader and more diffuse. These mechanisms\nalso show additive and suppressive effects. See Figures 21, 22, and 23 for more distributions.\n\n\nEach of these three outputs is distinct from the actual output tea for the original input, which means\nthe dataset also distinguishes the three mechanisms from no intervention being performed. Let iP ,\niL, and iR be indices of the entity groups queried by the positional, lexical, and reflexive mecha-\nnisms, e.g., iP = 2, iL = 1, and iR = 3 in Figure 1 after patching. In our counterfactual datasets,\neach of the three mechanisms can predict any position in the list of entity groups from the original\ninput, i.e., iP , iL, and iR vary freely from 1 to n. For details and task templates, see §A.\n\nA relevant remaining confounder is that the reflexive mechanism predicts the output that is the\ntarget entity in the counterfactual input, meaning this dataset cannot distinguish the pointer used by\nreflexive mechanism from the actual next token prediction. In § B.7 and § C, we construct a separate\ncounterfactual dataset where the reflexive mechanism attempts to dereference a token that does not\noccur in the original input and fails. We further validate this with attention knockout experiments,\nconfirming that the model indeed relies on a reflexive mechanism for dereferencing the target entity.\n\n\n3.3  INTERVENTION EXPERIMENTS\n\nWe find experimentally that information used to retrieve a bound entity is accumulated in the last\ntoken residual stream across a subset of layers.  In Figure 2, we show the results of interchange\ninterventions on gemma-2-2b-it across the layers of the last token residual stream. We see that in\nlayers 16–18 the model accumulates binding information in the last token position. Therefore, we\nconduct our experiments on the last layer before retrieval starts, denoted as ℓ, which is different for\neach of the nine models we test, but consistent across tasks for a given model (see §B.6 for more\ndetails). We measure the next token distribution produced by the model under intervention and\ncompare it against the possible outputs for the three mechanisms. We aggregate and visualize the\nresults of these intervention experiments in Figures 2 and 3.\n\nThe positional mechanism weakens for middle positions.  We can see plainly in Figure 2 that the\npositional mechanism controls behavior solely when the positional index is at the beginning or end of\nthe sequence of n = 20 entity groups. In middle entity groups, however, its effect becomes minimal,\naccounting only for 20% of the model’s behavior. Further analysis of the cases not explained by any\nof the mechanisms—dubbed mixed in the plot—reveals that these predictions are distributed near the\npositional index (Figure 3). Additionally, when collecting the mean logit distributions across many\nsamples and fixing the positional index, we see that in the first and last positional indices it induces\n\n\n                                       6\n\nPreprint\n\n\n\n\n\na strong and concentrated distribution around that index. However, in middle indices we see this\ndistribution become wide and diffuse (Appendix Figure 9). Thus, the positional mechanism becomes\nunreliable in middle indices and cannot be used as the sole mechanism for retrieval. We show in\n§B.3 how this effect emerges as n, i.e., the total number of entity groups in context, increases, and\nin §D we disambiguate the effect of increasing n from that of increasing sequence length.\n\nThe lexical and reflexive mechanisms are modulated based on target entity position.  Observe\nin Figure 2 that when the positional mechanism is unreliable for middle positions, the lexical and\nreflexive mechanisms come into play. However, which of these two alternate mechanisms contribute\nmore depends on the location of the target entity within the entity group, denoted as tentity. When\nthe target is at the beginning of the group (tentity = 1), the reflexive mechanism is used (as discussed\nin §3.1). When the target is at the end (tentity = 3), the lexical mechanism is primarily used. Finally,\nwhen the target is in the middle (tentity = 2), both mechanisms are used to differing extents.\n\nThe three mechanisms have complex interplay.  We can see in Figure 3 the interplay between\nthe three mechanism when the positional and reflexive indices are fixed to iP = 6 and iR = 14\nwhile the lexical index iL is iterated over a range of values.  First, the logit distributions clearly\nreveal the contributions of each mechanism, with a distinct spike appearing at each index. These\nspikes, however, behave differently. In line with Figure 3, the positional index produces a wide,\ndiffuse distribution, whereas the lexical and reflexive indices produce sharp, one-hot peaks. Next,\nwe observe that the mechanisms interact through a pattern of competitive synergy, meaning that\nthey both boost and suppress one another. When the lexical index is close to the positional index,\nthe lexical contribution is amplified while the positional contribution is weakened; when they are\nfarther apart, neither affects the other. In contrast, when the lexical index is close to the reflexive\nindex, the lexical contribution is suppressed by the reflexive one.\n\nInterventions on bound entity tokens provide similar results.  To understand the residual stream\nof the bound entity tokens themselves, we design more datasets of original-counterfactual pairs and\nanalyze intervention results in §B.5. We show that binding information exists and is used in the\nentity token residual streams between layers 12 and 19 for the positional and lexical mechanisms,\nand 6-12 for the reflexive mechanism. We additionally analyze in §B.5 how this binding information\npropagates across token positions.\n\nTakeaways  These results clarify how LMs bind and retrieve entities in context. They simultane-\nously employ three mechanisms: positional, lexical, and reflexive. In the first and last entity groups,\nLMs can rely almost exclusively on the positional mechanism, where it is strongest and most con-\ncentrated. In middle groups, however, the positional signal becomes diffuse and often links entities\nto nearby groups. In these cases, the lexical and reflexive mechanisms provide sharper signals which\nrefine the positional mechanism, enabling the LM to retrieve the correct entity.\n\n\n4 A SIMPLE MODEL FOR SIMULATING ENTITY RETRIEVAL IN-CONTEXT\n\nMixing mechanisms in a causal model  We follow Pˆıslar et al. (2025) in combining together\nmultiple causal models (P, L, R) into a single causal model M that modulates between the mecha-\nnisms conditional on the input. To formalize our observations about the dynamics between the three\nmechanisms and the position of the target entity, we develop a model that approximates LM logits\nfor next token prediction, as a position-weighted mixture of terms for the positional, lexical, and\nreflexive mechanisms. The lexical and reflexive terms have separate learned weights conditioned on\ntheir respective index, i.e., iL, or iR. In accordance with the results shown in Figure 3, we model\nthe lexical and reflexive mechanisms as one-hot distributions that up-weight only the target entity in\ngroups iL and iR, respectively. The positional term is modeled as a Gaussian distribution scaled by\na single weight wpos centered at the index iP with a standard deviation that is a quadratic function\nof iP . We define a new causal model M that uses all three variables P, L, and R simultaneously to\ncompute a logit value Yi for each entity Gitentity:\n\n          Yi := wpos · N  i | iP , σ(iP )2 + wlex[iL] · 1{i = iL} + wref[iR] · 1{i = iR}        (2)\n               |     positional{zmechanism   }  |    lexical mechanism{z       }  |   reflexive {zmechanism  }\n\n\n                                       7\n\nPreprint\n\n\n\n                                                           Learned Weights\n Model                    JSS ↑                 6\n\n                                te = 1   te = 2   te = 3        4\n        Comparing against the prevailing view                                                                                                                     Weight2 M (Lone-hot; Rone-hot; PGauss)  0.95     0.96     0.94\n  Pone-hot (prevailing view)     0.42     0.46     0.45                                                            0                        wlex  wref wpos\n          Modifying the positional mechanism                                   p(p) M w/ Poracle                0.96     0.98     0.96\n M w/ Pone-hot                0.86     0.85     0.85         6\n             Ablating the three mechanisms                    4\n M \\ {PGauss}               0.67     0.68     0.67       STD\n M \\ {Lone-hot}              0.94     0.91     0.75         2\n M \\ {Rone-hot}              0.69     0.87     0.92                                                            0\n M \\ {Rone-hot, Lone-hot}       0.69     0.84     0.74\n                                                               0  2  4  6  8  10 12 14 16 18\n M \\ {PGauss, Rone-hot}        0.12     0.27     0.48                        Entity Group Index\n M \\ {PGauss, Lone-hot}        0.55     0.41     0.20\n Uniform                    0.44     0.57     0.49\n\nFigure 4: Results for training our full model M  (Lone-hot, Rone-hot,PGauss), in addition to vari-\nants, baselines and ablations. Left: JSS scores for modeling the LM next token distribution over\niP , iL, iR. Evaluated on gemma-2-2b-it for the music binding task, with te =  tentity. Our model\nattains near-perfect JSS, slightly below the oracle. KL values (Table 3) show the same trend. All\nCIs are < 0.02; for M and M w/ oracle they are < 0.002. Right: Learned weights wlex, wref, wpos\nand σ curve, for tentity = 2. Observe σ widens for middle indices and narrows toward the end.\n\nWhere σ(iP ) = α( iPn )2 + β iPn + γ. We learn wpos, wlex, wref, α, β, γ from data.\n\nLearning how the mechanisms are mixed  To generate data for training our causal model we per-\nformed 150 interchange interventions per combination of 1 ≤iP , iL, iR ≤n using the original and\ncounterfactual inputs designed to distinguish the three mechanisms (see Figure 1 and Section 3.2).\nWe collected the logit distributions per index combination, and averaged them into mean probability\ndistributions by first applying a softmax over the entity group indices and then taking the mean. This\nyields n3 = 8, 000 probability distributions, which serve as our data for training and evaluation. We\nused 70% of the data for learning the causal model parameters and split the remainder evenly be-\ntween validation and test sets. The loss used is the Jensen–Shannon divergence (JSD) between our\nmodel’s predicted probability distribution and the target, chosen for its symmetry.\n\nWe evaluate M alongside a range of baselines, variants, and ablations to characterize our model’s\nperformance and understand the contribution of the different mechanisms. Experiments are run with\ngemma-2-2b-it on the music task (n = 20, tentity ∈[3]).  In §B.8 we report the same setup for\nthis model as well as qwen2.5-7b-it on additional tasks, with similar trends. We measure similarity\nbetween the predicted and target distributions using Jensen–Shannon similarity (JSS), defined as\n1 −JSD, calculated with log2 to yield values in [0, 1]. See Appendix Table 3 for KL divergences.\n\nWe compare our model with: (1) The prevailing view – a one-hot distribution at the positional index,\n(2) a variant of M which uses a one-hot distribution at iP instead of a gaussian, (3) ablations of M\nthat use only a subset of the mechanisms (e.g., M\\{Lone-hot} is M without the lexical mechanism),\nand (4) a uniform distribution. Finally, as an upper bound, we evaluate an oracle variant, where the\nlexical and reflexive components are learned as usual, but the positional component is swapped with\nthe actual logit distributions of the model, as a function of iP (see Figure 9).\n\nResults  In Figure 4 we show the results. We can see that our model achieves near perfect perfor-\nmance, only slightly below the oracle, at an average JSS of 0.95. In contrast, the model representing\nthe prevailing view of how entity binding works achieves an average JSS of 0.44, well below even\nthe uniform distribution baseline with 0.5. Next, we see that modeling the positional mechanism\nas a one-hot as opposed to a gaussian significantly hurts performance, dropping to 0.85 JSS. The\n\n\n                                       8\n\nPreprint\n\n\n\n\n 0   1000 Tokens        4000 Tokens\n 3                                       1.0\nIndex6                                                                             100\n 9                                                                                                                                                      Effect                                                                                     80      0.8\n  12                                                                                                                              Positional\n  15                                                                             60                                                                      Lexical                                                 (%)    0.6                                                      ReflexivePositional18\n                                                                                     40                                                           Mixed\n 0   7000 Tokens       10000 Tokens           0.4                                        No Effect\n                                                                                                                                     Invalid                                                                                                                                                                   Percentage ProportionIndex36                                                                             20                                                                                                               Accuracy 9                                       0.2\n\n  12\n  15                                                                             0       0.0Positional18                                   0  2000 4000 6000 8000 10000\n     0  3  6  9  12 15 18   0  3  6  9  12 15 18                   #Filler Tokens\n        Prediction Index        Prediction Index\n\nFigure 5: Padding results for gemma-2-2b-it on the boxes task. Left: Confusion matrix between\nthe model’s predicted index and the positional index patched in from the counterfactual. This gets\nincreasingly fuzzy for early tokens as padding is increased. Right: Distribution of effects as padding\nis increased, showing the positional mechanism strengthens at the expense of the lexical mechanism.\n\n\nablations further reveal how mechanisms are employed: for instance, when tentity = 1, ablating the\nlexical mechanism has nearly no effect, while for tentity = 3 this is true for the reflexive mechanism.\n\nWe can also see in Figure 4 the learned parameters of the model for tentity = 2. We see that in this\nsetting, the lexical and reflexive mechanisms behave similarly, being weaker in the beginning, flat\nin the middle, and with an uptick at the end. The reflexive mechanism is slightly more dominant\nhere, in keeping with the table results for tentity = 2. For the positional mechanism we can see that it\nstarts off very concentrated, becoming wider in middle indices, and finally becoming more narrow\ntowards the end, mirroring previous results.\n\n\n5  INTRODUCING FREE FORM TEXT INTO THE TASK\n\nTo test our model’s generalization to more realistic inputs, we modify our prompt templates T such\nthat they include filler sentences between each entity group. To this end, we create 1,000 filler\nsentences that are “entity-less”, meaning they do not contain sequences that signal the need to track\nor bind entities, e.g. “Ann loves ale, this is a known fact, Joe loves jam, this logic is easy to follow...”.\nThis enables us to evaluate entity binding in a more naturalistic setting, containing much more noise\nand longer sequences. We evaluate different levels of padding by interleaving the entity groups with\nan increasing number of filler sentences, for a maximum of 500 tokens between each entity group.\n\nThe results, shown in Figure 5 for gemma-2-2b-it on the boxes task, show that our model at first\nremains remarkably consistent in more naturalistic settings, across even a ten-fold increase in the\nnumber of tokens. However, as the amount of filler tokens increases, we see that the magnitude of\nthe mechanisms’ effects changes. The lexical mechanism declines in its effect, while the positional\nand mixed effects slightly increase. We can also see that the mixed effect remains distributed around\nthe positional index, but that it slowly becomes more diffuse. Thus, when padding with 10,000\ntokens, we get that other than the first entity group, the positional information becomes nearly non-\nexistent for the first half of entity tokens, while remaining stronger in the latter half. This suggests\nthat a weakening lexical mechanism relative to an increasingly noisy positional mechanism might\nbe a mechanistic explanation of the “lost-in-the-middle” effect (Liu et al., 2024).\n\n\n6  CONCLUSION\n\nIn this paper, we challenge the prevailing view that LMs retrieve bound entities purely with a posi-\ntional mechanism. We find that while the positional mechanism is effective for entities introduced\nat the beginning or end of context, it becomes diffuse and unreliable in the middle. We show that\nin practice, LMs rely on a mixture of three mechanisms: positional, lexical, and reflexive. The lex-\nical and reflexive mechanisms provide sharper signals that enable the model to correctly bind and\n\n\n                                       9\n\nPreprint\n\n\n\n\n\nretrieve entities throughout. We validate our findings across 9 models ranging from 2-72B, and 10\nbinding tasks, establishing a general account of how LMs retrieve bound entities.\n\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by the Alon scholarship, the Israel Science Foundation grant\n1083/24, and a grant from Open Philanthropy. Figure 1 uses images from www.freepik.com.\n\n\nREFERENCES\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\n  wal, Arvind Neelakantan, Pranav Shyam,  Girish Sastry, Amanda Askell, Sandhini Agar-\n  wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\n  Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\n  Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n  Radford, Ilya Sutskever, and Dario Amodei.  Language models are few-shot learners.   In\n  H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\n   ral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates,  Inc.,\n  2020.  URL https://proceedings.neurips.cc/paper_files/paper/2020/\n  file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nQin Dai, Benjamin Heinzerling, and Kentaro Inui.  Representational analysis of binding in lan-\n  guage models.   In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceed-\n  ings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp.\n  17468–17493, Miami, Florida, USA, November 2024. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2024.emnlp-main.967. URL https://aclanthology.org/2024.\n  emnlp-main.967/.\n\nXander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Discovering\n   variable binding circuitry with desiderata, 2023. URL https://arxiv.org/abs/2307.\n  03637.\n\nHermann Ebbinghaus. Memory: A Contribution to Experimental Psychology. Teachers College,\n  Columbia University, New York, NY, US, 1913.\n\nJiahai Feng and Jacob Steinhardt. How do language models bind entities in context? In The Twelfth\n  International Conference on Learning Representations, 2024. URL https://openreview.\n  net/forum?id=zb3b6oKO77.\n\nJiahai Feng, Stuart Russell, and Jacob Steinhardt.  Monitoring Latent World States in Language\n  Models with Propositional Probes, June 2024.\n\nMatthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan\n  Belinkov.  Causal analysis of syntactic agreement mechanisms in neural language models.  In\n  Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th An-\n  nual Meeting of the Association for Computational Linguistics and the 11th International Joint\n  Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1828–1843, Online,\n  August 2021. Association for Computational Linguistics.  doi: 10.18653/v1/2021.acl-long.144.\n  URL https://aclanthology.org/2021.acl-long.144/.\n\nJerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive architecture: A critical anal-\n   ysis. Cognition, 28(1-2):3–71, 1988. doi: 10.1016/0010-0277(88)90031-5.\n\nAtticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models\n   partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov,\n  Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the\n  Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 163–\n  173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n  blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16/.\n\n\n                                       10\n\nPreprint\n\n\n\n\n\nAtticus Geiger, Hanson Lu, Thomas F Icard, and Christopher Potts. Causal abstractions of neural\n  networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in\n  Neural Information Processing Systems, 2021. URL https://openreview.net/forum?\n  id=RmuXDtjDhG.\n\nAtticus Geiger, Jacqueline Harding, and Thomas Icard. How causal abstraction underpins computa-\n   tional explanation, 2025a. URL https://arxiv.org/abs/2508.11214.\n\nAtticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang,\n  Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal\n   abstraction: A theoretical foundation for mechanistic interpretability. Journal of Machine Learn-\n  ing Research, 26(83):1–64, 2025b. URL http://jmlr.org/papers/v26/23-0058.\n  html.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.  Dissecting recall of factual\n  associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali\n   (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-\n   cessing, pp. 12216–12235, Singapore, December 2023. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/2023.\n  emnlp-main.751/.\n\nRomuald A. Janik. Aspects of human memory and large language models, 2024. URL https:\n  //arxiv.org/abs/2311.03839.\n\nMosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length\n  on the reasoning performance of large language models.  In Lun-Wei Ku, Andre Martins, and\n  Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-\n  putational Linguistics (Volume 1: Long Papers), pp. 15339–15353, Bangkok, Thailand, August\n  2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.acl-long.818. URL\n  https://aclanthology.org/2024.acl-long.818/.\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\n  Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the\n  Association for Computational Linguistics, 12:157–173, 2024. doi: 10.1162/tacl a 00638. URL\n  https://aclanthology.org/2024.tacl-1.9/.\n\nNorman Miller and Donald Campbell. Recency and primacy in persuasion as a function of timing\n  of speeches and measurement. Journal of abnormal psychology, 59:1–9, 07 1959. doi: 10.1037/\n  h0049330.\n\nTheodora-Mara Pˆıslar, Sara Magliacane, and Atticus Geiger. Combining causal models for more ac-\n  curate abstractions of neural networks. In Fourth Conference on Causal Learning and Reasoning,\n  2025. URL https://openreview.net/forum?id=mVftlEi1CD.\n\nNikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning\n  enhances existing mechanisms: A case study on entity tracking.  In The Twelfth International\n  Conference on Learning Representations, 2024. URL https://openreview.net/forum?\n  id=8sKcAWOf2D.\n\nNikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott\n  Shaham, David Bau, and Atticus Geiger. Language models use lookbacks to track beliefs, 2025.\n  URL https://arxiv.org/abs/2505.14685.\n\nPaul Smolensky.   Tensor product variable binding and the representation of symbolic struc-\n   tures in connectionist systems.  Artificial Intelligence, 46(1-2):159–216, 1990.  doi: 10.1016/\n  0004-3702(90)90007-M.\n\nDavid S. Touretzky and Geoffrey E. Minton. Symbols among the neurons: details of a connectionist\n  inference architecture. In Proceedings of the 9th International Joint Conference on Artificial In-\n   telligence - Volume 1, IJCAI’85, pp. 238–243, San Francisco, CA, USA, 1985. Morgan Kaufmann\n  Publishers Inc. ISBN 0934613028.\n\n\n                                       11\n\nPreprint\n\n\n\n\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\n   Stuart Shieber.  Investigating gender bias in language models using causal mediation analysis.\n  Advances in neural information processing systems, 33:12388–12401, 2020.\n\nYiwei Wu, Atticus Geiger, and Rapha¨el Milli`ere. How do transformers learn variable binding in\n  symbolic programs?   In Forty-second International Conference on Machine Learning, 2025.\n  URL https://openreview.net/forum?id=kVtyv7bpnw.\n\n\nA  LIST OF BINDING TASKS\n\nWe define ten different binding tasks spanning domains, syntaxes, and subjects: one with m = 2 and\nnine with m = 3. Table 1 lists the entity sets for each task, along with an example instantiation for\n                                                                                andn = 2 and different values of qentity. Note that when m = 3, we use two query entities: Gqgroupqentity1\nGqgroup  These are the two entities in the entity group which aren’t the target entity. For example,    qentity2.\nwhen tentity = 2 we set qentity1 = 1 and qentity2 = 3.\n\nFigures 10 and 11 show the results of the TargetRebind interchange intervention on gemma-\n2-2b-it and qwen2.5-7b-it across all tasks and all values of tentity. Our findings are consistent: the\npositional mechanism dominates for early and late entity groups, while the lexical and reflexive\nmechanisms take over in the middle. We also replicate the effect in Figure 2 and Figure 4: reflexive\nis more present when tentity = 1 (first), lexical when tentity = 3 (last), and both are balanced when\ntentity = 2 (middle).\n\nB  MORE EXPERIMENTS\n\nIn this section we discuss experiments that further strengthen our model of how LMs perform entity\nbinding and retrieval, that couldn’t be included in the main section.\n\n\nB.1  REPLICATING RESULTS ACROSS MODELS\n\nTo validate robustness, we evaluated 9 models across 3 families, spanning 2–72B parameters. As\nshown in Figures 6 and 7 for the boxes and music tasks with tentity ∈[3], our findings transfer\nconsistently across models. The positional mechanism dominates for the first and last entity groups,\nwhile in middle positions lexical and reflexive take over, with a mixed effect distributed around\nthe positional index. In the Qwen family, we also observe that positional efficacy strengthens with\nmodel size. Overall, these results point to a universal strategy used by LMs to solve entity binding\ntasks.\n\n\nB.2  ENCODING OF POSITIONAL INFORMATION\n\nThroughout our experiments (notably Figures 2, 4, 6 and 10), we show that the model does not rely\nsolely on the positional mechanism. One possible explanation is that, as illustrated in Figure 3, the\npositional signal becomes diffuse and weak for middle entity groups. This may reflect the model’s\nlimited ability to encode entity group positions in a linearly separable manner. To test this hypothe-\nsis, we collected hidden state activations at entity token positions as well as at final token positions at\nevery layer, and assessed their separability using PCA and a multinomial logistic regression probe.\nFigure 19 shows the results: PCA projections for entity token positions with n = 20, and linear\nprobe accuracies for both entity and final positions across n ∈{5, 10, 15, 20}.  Consistent with\nour broader findings, the first and last entity groups are readily separable, while middle groups ex-\nhibit substantial overlap. We also observe a clear dependence on n: smaller contexts yield better\nseparation, whereas larger contexts make positions increasingly indistinguishable.\n\n\nB.3  EFFECT OF n\n\nPrevious work has described model behavior faithfully using only the positional mechanism (Feng\n& Steinhardt, 2024; Prakash et al., 2025), but these analyses were limited to small contexts (n ∈\n\n\n                                       12\n\nPreprint\n\n\n\n\n\n Name                    Entity Sets Sample       Binding Example\n  Filling Liquids           E1 = {John, Mary}        John and Mary are working at a busy restaurant.\n                         E2 = {cup, glass}        To fulfill an order, John fills a cup with beer and\n                         E3 = {wine, beer}       Mary fills a glass with wine. Who filled a cup with\n                                                      beer?\n\n People and Objects        E1 = {John, Mary}        John put the cup in the office and Mary put the toy\n                         E2 = {toy, medicine}       in the kitchen. What did Mary put in the kitchen?\n                         E3 = {kitchen, office}\n\n Programming Dictionary   E1 = {a, b}             The  following  are  dictionary  variables   in\n                         E2 = {John, Mary}        Python: a={‘name’:‘Mary’, ‘Country’:‘Canada’},\n                         E3 = {US, Canada}       b={‘name’:‘John’, ‘Country’:‘US’}. What is the\n                                                      country in variable a where ‘name’ == ‘John’?\n\n Music                   E1 = {John, Mary}       At the music festival, John performed rock music\n                         E2 = {rock, pop}         on the piano, and Mary performed pop music on\n                         E3 = {guitar, piano}       the guitar. What music did Mary play on the gui-\n                                                               tar?\n\n Biology Experiment       E1 = {John, Mary}         In a biology laboratory experiment, Mary placed\n                         E2 = {serum, enzyme}      the serum in a vial, and John placed the enzyme in\n                         E3 = {beaker, vial}        a beaker. Who placed the serum in a vial?\n\n Chemistry Experiment     E1 = {John, Mary}         In a chemistry laboratory experiment, Mary added\n                         E2 = {ethanol, acetone}    the acetone to a crucible, and John added the\n                         E3 = {crucible, funnel}    ethanol to a funnel. What did John add to a fun-\n                                                        nel?\n\n  Transportation            E1 = {John, Mary}         In a city transportation system, John, drove the\n                         E2 = {truck, taxi}          truck to the mall, and Mary drove the taxi to the\n                         E3 = {mall, park}          park. Where did Mary drive the taxi?\n\n  Sports Events            E1 = {John, Mary}         In a sports competition, Mary played hockey at the\n                         E2 = {hockey, cricket}     stadium, and John played cricket at the field. Who\n                         E3 = {stadium, field}      played hockey at the stadium?\n\n Space Observations       E1 = {John, Mary}        During an astronomy study, John observed an as-\n                         E2 = {planet, asteroid}     teroid with a radar, and Mary observed a planet\n                         E3 = {telescope, radar}    with a telescope. What did John observe with a\n                                                        radar?\n\n Boxes                   E1 = {toy, medicine}     The toy is in box B, and the medicine is in Box A.\n                         E2 = {box A, box B}      Which box is the medicine in?\n\nTable 1: List of all binding tasks we evaluate in our experiments. We show entity sets composed\nof only two entities per set for brevity. We also only show examples for n = 2 but evaluate over\nn ∈[3, 20]\n\n\n{2, 3}).  In this work, we show extensively that this finding doesn’t hold for larger values of n.\nTo evaluate exactly the relationship between the efficacy of the positional mechanism and n, we\nconduct the TargetRebind interchange intervention on gemma-2-2b-it and qwen2.5-7b-it for\nall n ∈[3, 20]. We see that the trend seen in all experiments holds across values of n: the positional\nmechanism is effective in the first and last entity groups, but not in middle ones.  Its efficacy for\nmiddle entity groups declines as n increases. This trend is consistent with the separability analysis\nin Figure 19, which shows that hidden states from middle entity groups become increasingly difficult\nto classify by position as n grows.\n\n\nB.4  MECHANISM AGREEMENT\n\nIn the TargetRebind interchange intervention used to produce the results in Figure 2 (and others\nthroughout the paper), we explicitly make sure to have different values for the positional, lexical and\nreflexive indices, so that we can know which mechanism most affected the model’s output. However,\nas shown in Figure 3, these mechanisms behave additively, and we suspect that when they agree,\nthey overwhelmingly drive model behavior. To evaluate this, we conduct two experiments, one for\n\n\n                                       13\n\nPreprint\n\n\n\n\n\n                                          Patch Effects                                                                                      Patch Effects\n                           positional        mixed          reflexive           lexical                                                 positional        mixed          reflexive           lexical\n    1.0   gemma-2-2b-it | Layer 18       gemma-2-9b-it | Layer 28      gemma-2-27b-it | Layer 25         1.0   gemma-2-2b-it | Layer 18       gemma-2-9b-it | Layer 28      gemma-2-27b-it | Layer 25\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0                                                                                                                    0.0\n    1.0 Qwen2.5-3B-Instruct | Layer 31  Qwen2.5-7B-Instruct | Layer 22 Qwen2.5-32B-Instruct | Layer 51      1.0 Qwen2.5-3B-Instruct | Layer 31  Qwen2.5-7B-Instruct | Layer 22 Qwen2.5-32B-Instruct | Layer 51\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0                                                                                                                    0.0\n    1.0Qwen2.5-72B-Instruct | Layer 62 Llama-3.1-8B-Instruct | Layer 16 Llama-3.1-70B-Instruct | Layer 35      1.0Qwen2.5-72B-Instruct | Layer 62 Llama-3.1-8B-Instruct | Layer 16 Llama-3.1-70B-Instruct | Layer 35\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0 0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18        0.0 0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18\n        Patched Entity Group Index     Patched Entity Group Index     Patched Entity Group Index            Patched Entity Group Index     Patched Entity Group Index     Patched Entity Group Index\n\nFigure 6: Evaluation of the TargetRebind interchange intervention in 9 different models across\n3 model families spanning 2-72B parameters, for the boxes binding task and tentity ∈[2]. We see\nthat the results remain remarkably consistent.\n\n                                          Patch Effects\n                          positional        mixed          reflexive           lexical\n\n        gemma-2-2b-it | Layer 18       gemma-2-9b-it | Layer 28      gemma-2-27b-it | Layer 25\n    1.0\nEffect0.80.6\nPatch0.40.2\n    0.0\n      Qwen2.5-3B-Instruct | Layer 31  Qwen2.5-7B-Instruct | Layer 22 Qwen2.5-32B-Instruct | Layer 51\n    1.0\nEffect0.80.6\nPatch0.40.2\n    0.0\n     Qwen2.5-72B-Instruct | Layer 62 Llama-3.1-8B-Instruct | Layer 16 Llama-3.1-70B-Instruct | Layer 35\n    1.0\nEffect0.80.6\nPatch0.40.2\n    0.0      0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18\n       Patched Entity Group Index     Patched Entity Group Index     Patched Entity Group Index\n\nFigure 7: Evaluation of the TargetRebind interchange intervention in 9 different models across\n3 model families spanning 2-72B parameters, for the music binding task and tentity = 3.\n\n\ntentity = 1 where the positional mechanism agrees with the reflexive one, and one for tentity = 3\nwhere the positional mechanism agrees with the lexical one. The results, shown in Figure 16, show\nthat this is indeed the case.\n\n\nB.5  BINDING SIGNALS IN ENTITY TOKENS\n\nIn our main experiments, we focus on interchange interventions for the last token position, showing\nthat it encodes positional, lexical and reflexive signals. In this section, we conduct experiments to\n\n\n                                       14\n\nPreprint\n\n\n\n\n\n                                          Patch Effects                                                                                      Patch Effects\n                           positional        mixed          reflexive           lexical                                                 positional        mixed          reflexive           lexical\n    1.0   gemma-2-2b-it | Layer 19       gemma-2-9b-it | Layer 29      gemma-2-27b-it | Layer 26         1.0   gemma-2-2b-it | Layer 20       gemma-2-9b-it | Layer 30      gemma-2-27b-it | Layer 27\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0                                                                                                                    0.0\n    1.0 Qwen2.5-3B-Instruct | Layer 32  Qwen2.5-7B-Instruct | Layer 23 Qwen2.5-32B-Instruct | Layer 52      1.0 Qwen2.5-3B-Instruct | Layer 33  Qwen2.5-7B-Instruct | Layer 24 Qwen2.5-32B-Instruct | Layer 53\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0                                                                                                                    0.0\n    1.0Qwen2.5-72B-Instruct | Layer 63 Llama-3.1-8B-Instruct | Layer 17 Llama-3.1-70B-Instruct | Layer 36      1.0Qwen2.5-72B-Instruct | Layer 64 Llama-3.1-8B-Instruct | Layer 18 Llama-3.1-70B-Instruct | Layer 37\n Effect0.80.6                                                                                                                                                                                    Effect0.80.6\n Patch0.40.2                                                                                                                                                      Patch0.40.2\n    0.0 0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18        0.0 0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18   0  2  4  6  8 10 12 14 16 18\n        Patched Entity Group Index     Patched Entity Group Index     Patched Entity Group Index            Patched Entity Group Index     Patched Entity Group Index     Patched Entity Group Index\n\nFigure 8: Evaluation of the TargetRebind interchange intervention at 1 and 2 layers after the\nevaluation in Figure 6, for tentity = 2. We see that the model shifts from aggregating binding infor-\nmation to retrieving the entities.\n\n              iP = 0             iP = 1             iP = 2             iP = 3             iP = 4\n    20\n    10\n     0\n              iP = 5             iP = 6             iP = 7             iP = 8             iP = 9\n    20\n    10Value\n     0\n             iP = 10           iP = 11           iP = 12           iP = 13           iP = 14Logit\n    20\n    10Mean 0\n             iP = 15           iP = 16           iP = 17           iP = 18           iP = 19\n    20\n    10\n     0\n       0 3 6 9 12 15 18  0 3 6 9 12 15 18  0 3 6 9 12 15 18  0 3 6 9 12 15 18  0 3 6 9 12 15 18\n                              Entity Group Index\n\nFigure 9: The mean logit distribution as a function of the positional index (iP ), for qwen2.5-7b-it\non the boxes task with tentity = 2. We can see the the positional binding signal induces a strong\nand concentrated signal for entity groups in the beginning and the end, while inducing a weak and\ndiffuse one for middle groups.\n\n\n\nverify the existence of these signals in the entity token positions themselves, as well as identify the\nmovement of these signals across token positions.\n\nFirst, we conduct the PosSwap , LexSwap and RefSwap interchange interventions, described\nin Table 2, with the results shown in Figure 17. We see that they achieve nearly identical interchange\nintervention accuracies as when performing TargetRebind with the last token position. Addi-\ntionally, wee see that for the positional and lexical mechanisms, the crucial layers where the binding\ninformation is contained in the entity tokens and used for retrieval is layers 12-19, while for reflexive\nit’s 6-12.\n\nTo further trace how binding signals flow through the model, we apply attention knockout (Geva\net al., 2023). We first identify a minimal set of layers where blocking attention from the last token\n\n\n                                       15\n\nPreprint\n\n\n\n\n\n                                                                                      Patch Effects\n                                                                            positional        mixed          reflexive           lexical\n       1.0          filling liquids [tentity = 1]                      filling liquids [tentity = 2]                      filling liquids [tentity = 3]            music performance [tentity = 1]         music performance [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0    music performance [tentity = 3]          people and objects [tentity = 1]          people and objects [tentity = 2]          people and objects [tentity = 3]      programming people dict [tentity = 1]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0 programming people dict [tentity = 2]    programming people dict [tentity = 3]         lab experiments [tentity = 1]              lab experiments [tentity = 2]              lab experiments [tentity = 3]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0  chemistry experiments [tentity = 1]      chemistry experiments [tentity = 2]      chemistry experiments [tentity = 3]           transportation [tentity = 1]               transportation [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0       transportation [tentity = 3]                sports events [tentity = 1]                sports events [tentity = 2]                sports events [tentity = 3]            space observations [tentity = 1]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0    space observations [tentity = 2]         space observations [tentity = 3]               boxes [tentity = 1]                    boxes [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n       0.0 0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18\n                Patched Entity Group Index                 Patched Entity Group Index                 Patched Entity Group Index                 Patched Entity Group Index\n\n\nFigure 10: Results of the TargetRebind interchange intervention for gemma-2-2b-it across all\ntasks and possible values of tentity.\n\n\n\nposition to the query entity token (e.g., which box is the medicine in?)  degrades performance.\nAcross all values of tentity, this occurs in layers 11–16, dropping accuracy from 98% to 37%, aligning\nwith the layers where binding information resides in entity tokens. Knockout becomes even more\neffective when applied to both the query token and the token immediately after it, reducing accuracy\nto 8%. This suggests that some of the query signal is copied forward. Consistent with this, blocking\nattention from the last token position only to the token following the query token decreases accuracy\nby just one point. However, when we block attention both from the last position to the query token\nand from its following token, accuracy drops to 6%, confirming that crucial binding information\nreaches the last position via the query token.\n\nFinally, we test whether binding information propagates from entity tokens to the query token. The\nlexical mechanism may not require such propagation, since its signal can be generated directly from\nthe query token. By contrast, the reflexive signal in the entity tokens originates from the answer\ntoken, so the query token must retrieve it in order for the signal to reach the last token position. To\nevaluate this, we block attention from the query token to different entity tokens. For the reflexive\nsignal (setting tentity = 1), we block attention to the entity token identical to the query token—\nwhere our RefSwap intervention localized the signal—and to the token immediately after it. This\nintervention is most effective in layers 8–12, reducing accuracy to 6%, and matching the layers\nwhere entity tokens use this signal (Figure 17). Blocking attention to other entity tokens in the\nqueried entity group has no effect. In contrast, for the lexical signal (setting tentity = 2), blocking\nattention from the query token to the correct answer entity token reduces accuracy only to 86%,\neven when applied across all layers. Moreover, blocking attention from the query token to all entity\ntokens at all layers still leaves accuracy at 90%. These results support our hypothesis: the lexical\nsignal can be derived locally from the query token, while the reflexive signal must be retrieved from\n\n\n                                       16\n\nPreprint\n\n\n\n\n\n                                                                                      Patch Effects\n                                                                            positional        mixed          reflexive           lexical\n       1.0          filling liquids [tentity = 1]                      filling liquids [tentity = 2]                      filling liquids [tentity = 3]            music performance [tentity = 1]         music performance [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0    music performance [tentity = 3]          people and objects [tentity = 1]          people and objects [tentity = 2]          people and objects [tentity = 3]      programming people dict [tentity = 1]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0 programming people dict [tentity = 2]    programming people dict [tentity = 3]         lab experiments [tentity = 1]              lab experiments [tentity = 2]              lab experiments [tentity = 3]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0  chemistry experiments [tentity = 1]      chemistry experiments [tentity = 2]      chemistry experiments [tentity = 3]           transportation [tentity = 1]               transportation [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0       transportation [tentity = 3]                sports events [tentity = 1]                sports events [tentity = 2]                sports events [tentity = 3]            space observations [tentity = 1]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n\n       0.0\n       1.0    space observations [tentity = 2]         space observations [tentity = 3]               boxes [tentity = 1]                    boxes [tentity = 2]\n\n       0.8\n      Effect 0.6\n     Patch 0.4\n       0.2\n       0.0 0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18    0   2   4   6   8   10  12  14  16  18\n                Patched Entity Group Index                 Patched Entity Group Index                 Patched Entity Group Index                 Patched Entity Group Index\n\n\nFigure 11: Results of the TargetRebind interchange intervention for qwen2.5-7b-it across all\ntasks and possible values of tentity.\n\n\n\n\n                                                                                                                                      Patch Effects\n                                           gemma-2-2b-it | boxes                     positional     mixed      reflexive       lexical\n    1.0      n=3             n=4             n=5             n=6             n=7             n=8\n Effect\n    0.5\n Patch\n    0.0      0           1           2   0       1       2       3   0     1     2     3     4   0    1    2    3    4    5   0   1   2   3   4   5   6   0  1  2  3  4  5  6  7\n    1.0      n=9             n=10             n=11             n=12             n=13             n=14\n Effect\n    0.5\n Patch\n    0.0      0  1  2  3  4  5  6  7  8   0  1  2  3  4  5  6  7  8  9   0    2    4    6    8   10  0   2   4   6   8   10    0   2   4   6   8  10  12  0   2   4   6   8  10  12\n    1.0      n=15             n=16             n=17             n=18             n=19             n=20\n Effect\n    0.5\n Patch\n    0.0      0  2  4  6  8  10 12 14  0  2  4  6  8  10 12 14    0  2  4  6  8 10 12 14 16  0  2  4  6  8 10 12 14 16    0  2  4  6  8 10 12 14 16 18  0  2  4  6  8 10 12 14 16 18\n          Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index\n\nFigure 12: Results for the TargetRebind interchange intervention on gemma-2-2b-it for n ∈\n[3, 20] and tentity = 3 on the boxes task. We see a trend where, the more entity groups need to be\nbound in context, the worse the positional mechanism is at binding those in the middle.\n\n\n\n\nentity tokens. This also explains why the model appears to produce the reflexive binding signal\nearlier than in the lexical or positional mechanisms – it requires an additional stage of retrieval.\n\n\n                                       17\n\nPreprint\n\n\n\n\n\n                                                                                                                                      Patch Effects\n                                           gemma-2-2b-it | music                     positional     mixed      reflexive       lexical\n    1.0      n=3             n=4             n=5             n=6             n=7             n=8\n Effect\n    0.5\n Patch\n    0.0      0           1           2   0       1       2       3   0     1     2     3     4   0    1    2    3    4    5   0   1   2   3   4   5   6   0  1  2  3  4  5  6  7\n    1.0      n=9             n=10             n=11             n=12             n=13             n=14\n Effect\n    0.5\n Patch\n    0.0      0  1  2  3  4  5  6  7  8   0  1  2  3  4  5  6  7  8  9   0    2    4    6    8   10  0   2   4   6   8   10    0   2   4   6   8  10  12  0   2   4   6   8  10  12\n    1.0      n=15             n=16             n=17             n=18             n=19             n=20\n Effect\n    0.5\n Patch\n    0.0      0  2  4  6  8  10 12 14  0  2  4  6  8  10 12 14    0  2  4  6  8 10 12 14 16  0  2  4  6  8 10 12 14 16    0  2  4  6  8 10 12 14 16 18  0 2 4 6 8 10 12 14 16 18\n          Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index\n\nFigure 13: Results for the TargetRebind interchange intervention on gemma-2-2b-it for n ∈\n[3, 20] and tentity = 3 on the music task. We see a trend where, the more entity groups need to be\nbound in context, the worse the positional mechanism is at binding those in the middle.\n\n\n                                                                                                                                      Patch Effects\n                                           Qwen-2.5-7B-Instruct | boxes                positional     mixed      reflexive       lexical\n    1.0      n=3             n=4             n=5             n=6             n=7             n=8\n Effect\n    0.5\n Patch\n    0.0      0           1           2   0       1       2       3   0     1     2     3     4   0    1    2    3    4    5   0   1   2   3   4   5   6   0  1  2  3  4  5  6  7\n    1.0      n=9             n=10             n=11             n=12             n=13             n=14\n Effect\n    0.5\n Patch\n    0.0      0  1  2  3  4  5  6  7  8   0  1  2  3  4  5  6  7  8  9   0    2    4    6    8   10  0   2   4   6   8   10    0   2   4   6   8  10  12  0   2   4   6   8  10  12\n    1.0      n=15             n=16             n=17             n=18             n=19             n=20\n Effect\n    0.5\n Patch\n    0.0      0  2  4  6  8  10 12 14  0  2  4  6  8  10 12 14    0  2  4  6  8 10 12 14 16  0  2  4  6  8 10 12 14 16    0  2  4  6  8 10 12 14 16 18  0 2 4 6 8 10 12 14 16 18\n          Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index\n\nFigure 14: Results for the TargetRebind interchange intervention on qwen2.5-7b-it for n ∈\n[3, 20] and tentity = 3 on the boxes task. We see a trend where, the more entity groups need to be\nbound in context, the worse the positional mechanism is at binding those in the middle.\n\n\nB.6  FINDING THE TARGET LAYER\n\nWe seek to identify for each model what the last layer before retrieval is, so that we can perform our\ninterchange interventions on that layer. Indeed in Figure 2 we see that there are a subset of layers\nwhere the last token position contains the binding information, after which it contains the retrieved\nanswer. Thus, for each model we identify the last layer where patching the last token position does\nnot copy the retrieved token. The intervention on this layer ℓis shown in Figure 6 for tentity ∈[2],\nand in Figure 8 we show this same intervention for ℓ+ 1 and ℓ+ 2 with tentity = 2. We see clearly\nthat for ℓ, the percentage of cases where the answer post-intervention is the retrieved entity from\nthe counterfactual example is at or below random chance. However, for ℓ+ 1 and ℓ+ 2 this effect\nbecomes the majority, showing that the model has shifted to retrieval. We also see that this layer is\nconsistent across tasks in Figures 6 and 7.\n\n\nB.7  REMOVING TARGETED ENTITY TOKENS\n\nIn §3.1 we detail how the lexical and reflexive mechanisms are pointers that get dereferenced to\nthe queried entity. To strengthen these claims, in this section, we evaluate what happens when\nwe modify the TargetRebind interchange intervention, such that the entities targeted by those\nmechanisms do not exist in the original prompt. Thus, for the example in Figure 1, to test the lexical\n\n\n                                       18\n\nPreprint\n\n\n\n\n\n                                                                                                                                      Patch Effects\n                                           Qwen-2.5-7B-Instruct | music                positional     mixed      reflexive       lexical\n    1.0      n=3             n=4             n=5             n=6             n=7             n=8\n Effect\n    0.5\n Patch\n    0.0      0           1           2   0       1       2       3   0     1     2     3     4   0    1    2    3    4    5   0   1   2   3   4   5   6   0  1  2  3  4  5  6  7\n    1.0      n=9             n=10             n=11             n=12             n=13             n=14\n Effect\n    0.5\n Patch\n    0.0      0  1  2  3  4  5  6  7  8   0  1  2  3  4  5  6  7  8  9   0    2    4    6    8   10  0   2   4   6   8   10    0   2   4   6   8  10  12  0   2   4   6   8  10  12\n    1.0      n=15             n=16             n=17             n=18             n=19             n=20\n Effect\n    0.5\n Patch\n    0.0      0  2  4  6  8  10 12 14  0  2  4  6  8  10 12 14    0  2  4  6  8 10 12 14 16  0  2  4  6  8 10 12 14 16    0  2  4  6  8 10 12 14 16 18  0 2 4 6 8 10 12 14 16 18\n          Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index        Patched Entity Group Index\n\nFigure 15: Results for the TargetRebind and tentity = 3 interchange intervention on qwen2.5-\n7b-it for n ∈[3, 20] on the music task. We see a trend where, the more entity groups need to be\nbound in context, the worse the positional mechanism is at binding those in the middle.\n\n\n\n     1.0                                                            1.0\n                                           Patch Effects                                                                    Patch Effects\n                               positional+lexical        mixed          reflexive                                    positional+reflexive        mixed           lexical\n     0.8                                                            0.8\n\n     0.6                                                            0.6   Effect                                                                                                               Effect\n  Patch0.4                                                                                        Patch0.4\n     0.2                                                            0.2\n\n     0.0                                                            0.0      0    2    4    6    8   10   12   14   16   18          0    2    4    6    8   10   12   14   16   18\n                    Patched Entity Group Index                                  Patched Entity Group Index\n\n\nFigure 16: We evaluate gemma-2-2b-it’s behavior when aligning the mechanisms for the music\ntask. We align the positional and reflexive mechanisms for tentity = 1, and the positional and lexical\nmechanisms for tentity = 3. We see that when the mechanisms point at the same entity for retrieval,\nthe model consistently responds with the correct entity.\n\n\nmechanism we’d change the counterfactual such that Ann is replaced with a different new name\nMax, and for the reflexive we’d change pie to cod (separately). We see in Figure 25 that this leads\nthe model to rely solely on the positional mechanism, since the others have pointers that cannot be\ndereferenced. In Figure 26 we see that in this case, relying on the positional mechanism yields a\nnoisy distribution around the positional index.\n\nA possible alternative explanation for why the model isn’t retrieving the entity pointed to by these\ntwo mechanisms, is that there might be some other mechanism that prevents the model from an-\nswering with entities that do not exist in the context. To evaluate this, we conduct the same exact\ninterventions, but for layer ℓ+ 1, where the retrieval is already taking place (see §B.6). Thus, if\nsuch a mechanisms exists, we’d expect to see the same results, where the model relies solely on\nthe positional mechanism. Otherwise, we’d expect the model to respond with the retrieved answer\nfrom the counterfactual. We can see in Figure 27 that the model indeed responds with the retrieved\nanswer from the counterfactual, falsifying this alternative explanation. Thus, we conclude that the\nmodel indeed relies on the lexical and reflexive mechanisms as pointers for dereferencing.\n\n\nB.8  ADDITIONAL CAUSAL MODELS\n\nWe report the the KL divergence scores for gemma-2-2b-it on the music task in Table 3. We ad-\nditionally report all metrics for gemma-2-2b-it on the sports task in Table 4, and qwen2.5-7b-it on\n\n\n                                       19\n\nPreprint\n\n\n\n\n                   Positional                IIA                   Lexical                 IIA                 Reflexive                IIA\n    0\n                                                                                                                                                               0.40\n    4                                                 0.8                                                    0.5                                                  0.35\n    8                                                                                                        0.4                                                  0.30\n                                                         0.6\n   12                                                                                                        0.3 Layer                                                                                                                                                          0.25                                                                                                                                                               0.20\n   16                                                 0.4                                                    0.2                                                  0.15\n   20                                                 0.2                                                                                                      0.10                                                                                                                0.1\n                                                                                                                                                               0.05\n   24\n                                                         0.0                                                    0.0                                                  0.00\n      0  2  4  6  8 10 12 14 16 18         0  2  4  6  8 10 12 14 16 18         0  2  4  6  8 10 12 14 16 18\n                Positional Index                           Lexical Index                          Reflexive Index\n\nFigure 17: Results for the PosSwap  (left), LexSwap (middle) and RefSwap  (right) inter-\nchange interventions on gemma-2-2b-it for the boxes task. Each square shows the interchange in-\ntervention accuracy (IIA) for a given layer and positional, lexical or reflexive index. We see that\npositional and lexical binding information exists in entity tokens in layers 12-19, while reflexive\nbinding information does in layers 6-12.\n\n\n\n      1.0\n                                             Patch Effects\n      0.8                                  positional          reflexive                                    mixed              lexical\n      0.6   Effect\n  Patch0.4\n      0.2\n\n      0.0        0   2   4   6   8   10  12  14  16  18   0   2   4   6   8   10  12  14  16  18\n               Original Entity Group Index                       Lexical Index\n\n\nFigure 18: We show results of the TargetRebind interchange intervention on gemma-2-2b-it\nfor the boxes task with different indices on the x-axis. Left: using the index of the queried entity\ngroup. This has little effect overall, except for dips at the first and last indices in the positional effect.\nUnder TargetRebind , the positions of queried entity groups cannot coincide between the coun-\nterfactual and original prompts. Thus, when the original query targets the first or last groups—where\npositional information is strongest—these groups are never patched, slightly weakening results on\naverage. Right: using the lexical index. Here the pattern mirrors Figure 3, with weaker effects at\nthe edges and stronger ones in the middle.\n                                                                              .\n\n\nboth tasks in Tables 5 and 6. For training, we use Adam (β1 = 0.9, β2 = 0.999) with learning rate\n0.05, run for up to 2,000 epochs with a batch size of 512 and early stopping after 200 epochs.\n\n\nC  THE REFLEXIVE MECHANISM\n\nIn §3.1, we describe the reflexive binding mechanism, where a direct pointer originating from an\nentity token is used to point back at itself.  In this section we provide further evidence for the\nexistence of this mechanism as described.\n\nWe can see in Figure 10 that when querying the first entity in a tuple, the lexical patch effect is com-\npletely superseded by the reflexive one, wherein the patching results in the model answering with\nthe queried entity itself from the counterfactual example (see the reflexive patch effect in Table 2).\nOn first glance it seems as if this means that the last token position at this layer already contains the\nanswer, since patching it yields the answer from the counterfactual. However, we show in §B.7 that\n\n\n                                       20\n\nPreprint\n\n\n\n\n\n Name               Original           Counterfactual     Patch Positions     Patch Effects\n  TargetRebind   The  bottle  is  in   The  bottle  is  in   Last token position   Q: Positional\n                    box C, the pen is   box Q, the ball is                     A: Lexical\n                         in box A, the ball   in box A, the pen                      C: Reflexive\n                             is in box Q, and    is in box C, and                     N: No effect\n                        the rock is in box   the rock is in box\n                    N. Which box  is   N. Which box  is\n                        the rock in?         the pen in?\n\n  PosSwap         The pen is in box   The ball is in box  A→A  Q→Q     A: Patched tokens\n                A and the ball is in  Q and the pen is in                      encode positional\n                    box Q. Which box   box A. Which box                        binding  used by\n                             is the ball in?          is the ball in?                              the model\n                                                                        Q: Patched tokens\n                                                                        do   not  encode\n                                                                                            positional binding\n                                                                               used by the model\n\n  LexSwap         The pen is in box   The ball is in box  A→A  Q→Q     A: Patched tokens\n              A and the ball is in  A and the pen is in                      encode     lexical\n                    box Q. Which box   box Q. Which box                        binding  used by\n                             is the ball in?          is the ball in?                              the model\n                                                                        Q: Patched tokens\n                                                                        do   not  encode\n                                                                                               lexical    binding\n                                                                               used by the model\n\n  RefSwap         The pen is in box   The ball is in box  A→A  Q→Q     A: Patched tokens\n              A and the ball is in  A and the pen is in                      encode   reflexive\n                    box Q. What is in   box Q. What is in                        binding  used by\n                  Box Q?          Box Q?                                    the model\n                                                                        Q: Patched tokens\n                                                                        do   not  encode\n                                                                                              reflexive  binding\n                                                                               used by the model\n\n          Table 2: Original/counterfactual pair examples for interchange interventions.\n\n\n\nwhen using an interchange intervention where the entity pointed to by the reflexive mechanism does\nnot exist in the original input, the model resorts to solely relying on the positional mechanism rather\nthan responding with the answer from the counterfactual input. This shows that the intervention is\nnot copying the retrieved answer from the counterfactual to the original, but rather copying a pointer\nto that answer.\n\nWe further validate this by running the original interchange intervention, but this time knocking out\nattention from the last token position to the target entity (Geva et al., 2023), shown in Figure 20.\nAgain we see that the model does not respond with the counterfactual target entity unless it can find\nit in context, which we prevent by blocking attention to it. Conversely, blocking attention at a layer\nwhen the model has already retrieved the answer has no effect on the model’s answer. Thus, we can\nconclude that the model indeed relies on a reflexive mechanism for binding and retrieving entities in\ncontext.\n\n\nD  CONTEXT LENGTH ABLATION STUDY\n\n\nIn our evaluations, we show the effect of the number of entities that need to be bound in-context\non LMs’ use of the positional, lexical and reflexive mechanisms. However, a confounding factor is\nthat as the number of entities increases, so does the length of the sequence itself. To disentangle\nthese effects, we pad contexts with n ∈[3, 19] so that all sequences match the length of those with\nn = 20. Padding is done using “entity-less” sentences, as described in §5. The results are shown\nin Figure 24. If the effects of increasing n were due to increasing sequence length, we’d expect all\nresults to be identical to when setting n = 20, and to each other. However, we see that, while the\n\n\n                                       21\n\nPreprint\n\n\n\n\n\n                                                                                                      Entity Pos | N=5  Entity Pos | N=10  Entity Pos | N=15  Entity Pos | N=20\n                    Layer 1         Layer 2         Layer 3         Layer 4         Layer 5         0\n                                                                                       2\n\n    1                                                                                       4\n                                                                                       6                                                                                                                                        F1 1.0             PCA                                                                                       8\n  Tuple Index                                                                             10\n       0                    Layer 6         Layer 7         Layer 8         Layer 9         Layer 10        Layer 1214       1\n       2 1                                                                         16                                                                                                                   0.8\n       3             PCA                                                                         1820       4\n       5                                                                               22\n       6           Layer 11        Layer 12        Layer 13        Layer 14        Layer 15        24\n       7                                                                                          0                                                                                              1  2                                                                                                    3  4   0                                                                                                              2  4                                                                                                                    6  8   0                                                                                                                               3  6                                                                                                                                     9  12   0 3                                                                                                                                                  6 9                                                                                                                                                      12 15 18         0.6\n       8 1                                                                                                        Tuple                                                                                                        Index                                                                                                                            Tuple                                                                                                                           Index                                                                                                                                               Tuple                                                                                                                                             Index                                                                                                                                                                  Tuple                                                                                                                                                                Index\n       9\n       10   PCA                                                                                 Last Pos | N=5    Last Pos | N=10   Last Pos | N=15   Last Pos | N=20\n       11                                                                               0\n       12                   Layer 16        Layer 17        Layer 18        Layer 19        Layer 20         24                                                                                                                   0.4       13\n       14 1                                                                          6\n       15                                                                               8\n       16   PCA                                                                         10\n       17                                                                                                                                                                                                                                                             0.2                                                                                                                                                 Layer 1214       18                   Layer 21        Layer 22        Layer 23        Layer 24        Layer 25                                                                                      16       19\n    1                                                                         18\n                                                                                      20\n             PCA                                                                         22\n                                                                                      24\n                 PCA 0         PCA 0         PCA 0         PCA 0         PCA 0             0  1  2  3  4   0  2  4  6  8   0  3  6  9  12   0 3 6 9 12 15 18\n                                                                                                        Tuple Index         Tuple Index         Tuple Index         Tuple Index\n\nFigure 19: Separability of hidden states for entity token positions and the last token position, across\nlayers and values of n. PCA projections (left) and multinomial logistic regression probes (right)\nshow that first and last entity groups are linearly separable, while middle groups overlap substan-\ntially. Separability decreases as the number of entities n increases.\n\n Model                          KLt||p ↓                         KLp||t ↓\n                                    tentity = 1    tentity = 2    tentity = 3    tentity = 1    tentity = 2    tentity = 3\nM                     0.22        0.17        0.26        0.31        0.21        0.41\n\nM w/ Oracle Pos        0.14        0.08        0.14        0.32        0.11        0.24\n\nM w/ One-Hot Pos      0.71        0.67        0.71        1.00        0.88        0.88\n\n Only  One-Hot  At     6.41        5.61        5.95        3.41        2.39        2.78\n Pos\n\nM \\ {P}               1.75        1.52        1.71        4.51        2.37        3.17\n\nM \\ {L}               0.39        0.73        1.76        0.3         0.37        1.42\n\nM \\ {R}               2.14        1.08        0.61        2.10        0.54        0.44\n\nM \\ {L, R}             2.10        1.22        1.82        2.13        0.73        1.50\n\nM \\ {P, R}             9.19        7.35        5.32        10.7        5.55        4.34\n\nM \\ {P, L}             4.66        6.18        8.45        4.28        2.92        5.40\n\n Uniform                2.71        1.96        2.44        7.57        3.49        4.84\n\nTable 3: KL divergence results for modeling an LM’s behavior contingent on the positional, lexical\nand reflexive indices.  Evaluated on gemma-2-2b-it for the music binding task. Our full model\nachieves the best performance, only slightly below the oracle.\n\n\ndistribution of patch effects is slightly affected by padding, the results and trends align closely with\nour results without padding. This indicates that model behavior is governed primarily by the number\nof entities that must be bound, rather than by sequence length.\n\nE LLM USAGE\n\nIn this work, the authors relied on LLMs solely to assist with implementing specific helper functions.\n\n\n\n\n\n                                       22\n\nPreprint\n\n\n\n\n                         Layer                                          Layer + 1    1.0                                                     1.0\n    0.8                                                     0.8                                 positionalPatch Effects  reflexive\n                                                                                                   mixed              lexical\n    0.6                                                     0.6   Effect                                                                                                            Effect\n   Patch0.4                                                                                     Patch0.4\n    0.2                                                     0.2\n\n    0.0                                                     0.0      0   2   4   6   8   10  12  14  16  18       0   2   4   6   8   10  12  14  16  18\n                     Patched Entity Group Index                                Patched Entity Group Index\n\n\nFigure 20: Patch effects under TargetRebind  for gemma-2-2b-it while blocking attention to\nthe target entity. Left: blocking attention when the model is accumulating binding information in\nthe last token position leads to it not being able to dereference the reflexive pointer. Had the patch\ncontained the retrieved answer, this plot would be fully orange. Right: patching at the following\nlayer and blocking attention to the target entity. Here the plot is fully orange since the entity has\nalready been retrieved.\n\n                 lexical    positional    reflexive                       lexical    positional    reflexive\n            iP = 0             iP = 2             iP = 4                iR = 0            iR = 2            iR = 4\n    20\n    10                                                                              10\n\n     0                                                                               0\nValue20       iP = 6             iP = 8            iP = 10      Value       iR = 6            iR = 8           iR = 11\n\n                                                                                    10    10Logit                                                                              Logit\nMean 0      iP = 14           iP = 16           iP = 19     Mean 0     iR = 14          iR = 16          iR = 19\n    20\n\n                                                                                    10\n    10\n\n     0                                                                               0\n     0  4  8  12 16   0  4  8  12 16   0  4  8  12 16       0  4  8  12 16   0  4  8  12 16   0  4  8  12 16\n                      Entity Group Index                                       Entity Group Index\n\nFigure 21: Mean logit distributions under TargetRebind for gemma-2-2b-it on the music task.\nLeft: fixing iL = 8, iR = 16 and varying iP . Right: fixing iP = 6, iL = 14 and varying iR.\n\n                 lexical    positional    reflexive                       lexical    positional    reflexive\n            iL = 0              iL = 2              iL = 4                iR = 0            iR = 2            iR = 4\n    25                                                                              25\n    20                                                                              20\n    15                                                                              15\n    10\nValue25       iL = 6              iL = 8             iL = 11      Value25      iR = 6            iR = 8           iR = 11\nLogit2015                                                                             Logit2015\nMean10      iL = 14            iL = 16            iL = 19     Mean      iR = 14          iR = 16          iR = 19\n    25                                                                              25\n    20                                                                              20\n    15                                                                              15\n    10\n     0  4  8  12 16   0  4  8  12 16   0  4  8  12 16       0  4  8  12 16   0  4  8  12 16   0  4  8  12 16\n                      Entity Group Index                                       Entity Group Index\n\nFigure 22: Mean logit distributions under TargetRebind for qwen2.5-7b-it on the music task.\nLeft: fixing iP = 6, iR = 14 and varying iL. Right: fixing iP = 6, iL = 14 and varying iR.\n\n\n\n                                       23\n\nPreprint\n\n\n\n\n\n                 lexical    positional    reflexive                       lexical    positional    reflexive\n            iP = 0             iP = 2             iP = 4                iR = 0            iR = 2            iR = 4\n                                                                                    25\n    30\n                                                                                    20\n    20                                                                              15\nValue        iP = 6             iP = 8            iP = 10      Value25      iR = 6            iR = 8           iR = 11\n    30\nLogit20                                                                             Logit2015\nMean       iP = 14           iP = 16           iP = 19     Mean      iR = 14          iR = 16          iR = 19\n                                                                                    25\n    30\n                                                                                    20\n    20                                                                              15\n\n     0  4  8  12 16   0  4  8  12 16   0  4  8  12 16       0  4  8  12 16   0  4  8  12 16   0  4  8  12 16\n                      Entity Group Index                                       Entity Group Index\n\nFigure 23: Mean logit distributions under TargetRebind for qwen2.5-7b-it on the sports task.\nLeft: fixing iL = 8, iR = 16 and varying iP . Right: fixing iP = 8, iL = 19 and varying iR.\n\n\n\n\n\n    8\n                                                                                                                 patch_effect\n                                                                                                                             positional\n    7                                                                                                                             lexical\n                                                                                                                               reflexive\n                                                                                                   mixed\n    6 Effect\n\n    5 Patch\n    4\n\n    3        3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n                              n\n\nFigure 24: Mean patch effects per number of entities in context (n). For each n, we report the\nstandard mean patch effects (right) alongside results from padded sequences (left), where sequence\nlength is fixed to match n = 20. While padding slightly shifts the distribution of patch effects, the\noverall patterns remain consistent: model behavior is primarily controlled by the number of entities\nin context, rather than sequence length.\n\n\n\n\n\n                                       24\n\nPreprint\n\n\n\n\n\n                                       tentity = 1                                                                  tentity = 2    1.0                                                     1.0\n    0.8                                                     0.8                                 positionalPatch Effects  reflexive\n                                                                                                   mixed              lexical\n    0.6                                                     0.6   Effect                                                                                                            Effect\n   Patch0.4                                                                                     Patch0.4\n    0.2                                                     0.2\n\n    0.0                                                     0.0      0   2   4   6   8   10  12  14  16  18       0   2   4   6   8   10  12  14  16  18\n                    Patched Entity Group Index                                Patched Entity Group Index\n\n\nFigure 25: Left:  results for TargetRebind interchange intervention on gemma-2-2b-it with\ntentity = 1, where the query entity in the counterfactual does not exist in the original. Right: results\nfor TargetRebind interchange intervention on gemma-2-2b-it with tentity = 2, where the target\nentity in the counterfactual does not exist in the original. We can see in both plots that when the\nmodel can’t use the lexical and reflexive mechanisms since the entities they point to don’t exist, the\nmodel falls back to solely using the positional mechanism (distribution showed in Figure 26).\n\n\n\n\n\n                                        tentity = 1                                                                         tentity = 2\n                                                                                                                                                  100 0 97 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 100 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1 3 83 10 2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 0 94 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 2 1 12 45 25 10 4  1  1  0  0  0  0  0  0  0  0  0  0  0  0  2 1 14 41 18 11 5  3  1  1  1  1  0  0  0  0  0  0  0  0  0        80\n 3 1  6 21 30 20 11 6  3  1  1  0  0  0  0  0  0  0  0  0  0  3 0  8 30 23 12 9  4  3  2  2  2  1  1  1  1  0  0  0  0  0\n 4 1  6 10 22 25 18 7  4  3  2  2  1  0  0  0  0  0  0  0  0  4 1  7 24 17 16 11 7  4  4  2  2  1  0  1  0  0  1  0  0  0        60\n 5 1  6 12 9 24 22 14 6  3  2  1  1  1  0  0  0  0  0  0  0  5 0  8 21 14 15 11 7  6  5  3  4  1  1  1  0  0  0  0  0  0\n 6 1  4 12 7  9 23 18 11 4  4  3  1  1  1  1  0  0  0  0  0  6 1  6 15 16 13 12 9  8  5  6  4  2  1  0  0  1  0  1  0  0\n                         8 0  3  9  9 11 13 11 7  7  6  6  5  3  3  3  1  1  1  1  1         (%)Index7 0  3  8  9  8  6 22 13 11 6  4  3  2  1  1  1  0  0  0  0  7 0  3 13 12 12 14 11 8  6  5  5  4  2  1  1  1  1  0  1  1        40 8 1  1  4  6  7  7  8 21 13 10 7  4  4  2  1  1  0  0  0  0  Index\n 9 0  1  3  4  6  6  4  7 23 14 12 7  5  3  1  2  1  0  0  0  9 0  1  8  9 11 10 10 7  9  6  6  5  5  2  1  2  1  2  1  1\n  10 0  0  2  2  4  6  6  5  7 26 13 10 7  5  3  2  1  1  0  1   10 0  0  3  5  8 10 11 8  9  8  9  7  7  3  3  3  2  2  1  1\n  11 0  0  2  2  2  3  2  4  4  6 27 13 10 7  5  4  3  2  1  1   11 0  0  2  4  7  9  7  8 10 9  7  8  7  5  3  4  3  2  2  2        20                                                                                                                                                                                                                                                                                                                                                                                                                                                             PercentagePositional12 0  1  1  1  2  2  2  3  5  4  6 28 14 10 7  4  4  2  1  2    Positional12 0  0  2  3  3  6  7  7  8  9  9  8  8  7  5  4  4  3  2  4\n  13 1  1  1  0  1  1  2  1  4  4  5  5 29 11 10 7  7  5  2  2   13 0  0  1  2  4  5  6  7  8  7 10 9  6  7  6  5  6  5  4  4\n  14 0  0  1  1  0  1  1  1  1  3  3  4  7 32 13 12 7  5  5  3   14 0  0  1  0  2  4  3  5  6  6  7  9  9  6  9  7  7  7  6  5\n  15 1  0  1  0  0  0  1  1  1  2  2  4  3  6 31 17 12 9  6  5   15 0  0  1  1  2  1  3  4  4  7  6  8  9  9  7  9 10 8  6  6\n  16 0  0  0  0  0  0  0  0  0  1  1  2  4  3  6 35 15 13 10 8   16 0  0  0  1  1  1  1  3  3  4  5  6  7  7  9  8 10 12 11 11\n  17 0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  6 34 23 17 11   17 0  0  0  0  1  1  1  2  2  4  4  4  6  7  7  9  9 14 13 14\n  18 0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  2  9 43 26 15   18 0  0  0  0  0  1  1  2  2  3  3  4  6  5  7 10 9 14 15 17\n  19 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  2  9 34 51   19 0  0  0  0  0  0  0  0  0  1  1  1  2  2  4  6  7  9 11 57\n                                                                                                                                                  0\n       0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19        0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19\n                          Prediction Index                                               Prediction Index\n\nFigure 26:   Left:  confusion matrix  for  non-lexical and  reflexive patch  effects under  the\n TargetRebind interchange intervention on gemma-2-2b-it with tentity = 1, where the query\nentity in the counterfactual does not exist in the original.  Right: results for non-lexical and re-\nflexive patch effects under the TargetRebind interchange intervention on gemma-2-2b-it with\ntentity = 2, where the queried entity in the counterfactual does not exist in the original. We can\nsee that when the model can’t use the lexical and reflexive mechanisms, it falls back on the noisy\npositional mechanism.\n\n\n\n\n\n                                       25\n\nPreprint\n\n\n\n\n\n                                       tentity = 1                                                                  tentity = 2    1.0                                                     1.0\n    0.8                                                     0.8                                 positionalPatch Effects  reflexive\n                                                                                                   mixed              lexical\n    0.6                                                     0.6   Effect                                                                                                            Effect\n  Patch0.4                                                                                     Patch0.4\n    0.2                                                     0.2\n\n    0.0                                                     0.0      0   2   4   6   8   10  12  14  16  18       0   2   4   6   8   10  12  14  16  18\n                    Patched Entity Group Index                                Patched Entity Group Index\n\n\nFigure 27: Results for TargetRebind interchange interventions on gemma-2-2b-it with tentity ∈\n[2], where the query entity (left) or queried entity (right) in the counterfactual do not exist in the\noriginal, patching at layer ℓ+ 1. We see that the model copies the retrieved answer from the coun-\nterfactual, showing that no mechanism exists to suppresses answering with entities that do not exist\nin the original prompt.\n\n\n\n\n\n     0    gemma-2-2b-it          gemma-2-9b-it         gemma-2-27b-it                    0    gemma-2-2b-it          gemma-2-9b-it         gemma-2-27b-it\n     2                                                                                                     2\n\n                                                                                                           6 Index 468                                                                                                                                         Index 4                                                                                                           8\n    10                                                                                                    10\n    12                                                                                                    12\n    14                                                                                                    14 Positional16                                                                                                                                                                                                                                                                                 Positional16\n    18                                                                                          100       18                                                                                          100\n     0  Qwen2.5-3B-Instruct     Qwen2.5-7B-Instruct    Qwen2.5-32B-Instruct     80         0  Qwen2.5-3B-Instruct     Qwen2.5-7B-Instruct    Qwen2.5-32B-Instruct     80\n     2                                                                                                     2\n\n                                                                                                           6                                                                                                                                                                                                       60 (%) Index 468                                                                                          60 (%)  Index 4                                                                                                           8\n    10    12                                                                                          40        1012                                                                                          40\n    14                                                                                                    14\n                                                                                                          16 Positional16    18                                                                                          20 Percentage   Positional                                                                                                          18                                                                                          20 Percentage\n     0 Qwen2.5-72B-Instruct    Llama-3.1-8B-Instruct   Llama-3.1-70B-Instruct                0 Qwen2.5-72B-Instruct    Llama-3.1-8B-Instruct   Llama-3.1-70B-Instruct\n     2                                                                                                     2\n\n                                                                                                           6 Index 468                                                                                          0        Index 4                                                                                          0                                                                                                           8\n    10                                                                                                    10\n    12                                                                                                    12\n    14                                                                                                    14 Positional16                                                                                                                                                                                                                                                                                 Positional16\n    18                                                                                                    18\n  024Prediction681012Index141618 024Prediction681012Index141618 024Prediction681012Index141618      024Prediction681012Index141618 024Prediction681012Index141618 024Prediction681012Index141618\n\nFigure 28: Confusion matrix for non-lexical and reflexive patch effects under the TargetRebind\ninterchange intervention for all models, showing the diffuse distribution around the positional index.\nLeft: tentity = 1. Right: tentity = 2.\n\n\n\n\n\n                                       26\n\nPreprint\n\n\n\n\n\n Model               JSS ↑               KLt||p ↓              KLp||t ↓\n                           t = 1   t = 2   t = 3   t = 1   t = 2   t = 3   t = 1   t = 2   t = 3\nM         (Lone-hot,   0.94    0.95    0.93    0.3     0.21    0.31    0.35    0.28    0.39\n  Rone-hot,PGauss)\nM w/ Poracle          0.96    0.97    0.96    0.13    0.11    0.13    0.32    0.19    0.21\nM w/ Pone-hot         0.85    0.87    0.87    0.77    0.59    0.62    1.2     0.81    0.73\n  Pone-hot  (prevailing   0.4     0.46    0.43    6.5     5.74    6.03    3.72    2.54    2.9\n view)\n\nM \\ {P}             0.67    0.69    0.71    1.75    1.53    1.49    4.91    3.04    1.88\n\nM \\ {L}             0.93    0.89    0.75    0.41    0.8     1.99    0.37    0.53    1.25\n\nM \\ {R}             0.69    0.84    0.9     1.83    1.28    1.04    2.52    0.71    0.47\n\nM \\ {L, R}          0.68    0.79    0.74    1.84    1.52    2.06    2.54    1.05    1.36\n\nM \\ {P, R}          0.11    0.31    0.47    9.25    7.19    5.32    10.9    6.11    3.61\n\nM \\ {P, L}          0.55    0.45    0.23    4.71    5.95    8.16    4.64    3.06    4.49\n\n Uniform              0.45    0.54    0.54    2.66    2.13    2.22    8      4.78    2.93\n\nTable 4: Results for modeling gemma-2-2b-it’s behavior on the sports binding task, contingent on\nthe positional, lexical and reflexive indices. Here t denotes tentity.\n\n\n\n\n\n Model               JSS ↑               KLt||p ↓              KLp||t ↓\n                           t = 1   t = 2   t = 3   t = 1   t = 2   t = 3   t = 1   t = 2   t = 3\nM         (Lone-hot,   0.94    0.92    0.92    0.27    0.34    0.37    0.35    0.48    0.45\n  Rone-hot,PGauss)\nM w/ Poracle          0.98    0.98    0.98    0.07    0.07    0.07    0.1     0.1     0.1\nM w/ Pone-hot         0.87    0.89    0.88    0.58    0.47    0.53    1.09    0.74    0.69\n  Pone-hot  (prevailing   0.56    0.55    0.53    4.66    4.65    5.01    1.84    1.88    2.04\n view)\n\nM \\ {P}             0.62    0.66    0.66    1.85    1.68    1.73    5.05    3.29    2.55\n\nM \\ {L}             0.93    0.85    0.78    0.51    1.14    1.77    0.33    0.74    1.09\n\nM \\ {R}             0.86    0.9     0.9     0.9     0.89    1.0     0.68    0.54    0.5\n\nM \\ {L, R}          0.86    0.84    0.78    0.92    1.24    1.79    0.71    0.83    1.11\n\nM \\ {P, R}          0.16    0.34    0.42    8.43    6.69    5.93    9.04    5.6     4.4\n\nM \\ {P, L}          0.35    0.24    0.17    6.64    7.8     8.57    6.8     5.3     5.32\n\n Uniform              0.55    0.58    0.54    2.11    1.95    2.21    5.92    4.09    3.54\n\nTable 5: Results for modeling qwen2.5-7b-it’s behavior on the music binding task, contingent on the\npositional, lexical and reflexive indices. Here t denotes tentity.\n\n\n\n\n\n                                       27\n\nPreprint\n\n\n\n\n\n Model               JSS ↑               KLt||p ↓              KLp||t ↓\n                           t = 1   t = 2   t = 3   t = 1   t = 2   t = 3   t = 1   t = 2   t = 3\nM         (Lone-hot,   0.95    0.93    0.92    0.24    0.31    0.36    0.28    0.39    0.47\n  Rone-hot,PGauss)\nM w/ Poracle          0.98    0.98    0.97    0.07    0.08    0.11    0.09    0.11    0.18\nM w/ Pone-hot         0.87    0.89    0.88    0.62    0.52    0.55    1.14    0.68    0.84\n  Pone-hot  (prevailing   0.57    0.55    0.51    4.58    4.72    5.18    1.73    1.84    2.21\n view)\n\nM \\ {P}             0.61    0.66    0.66    1.88    1.75    1.73    5.11    2.9     3.35\n\nM \\ {L}             0.94    0.87    0.77    0.53    1.09    1.64    0.27    0.64    1.27\n\nM \\ {R}             0.87    0.89    0.91    0.89    1.05    0.85    0.57    0.54    0.48\n\nM \\ {L, R}          0.87    0.83    0.77    0.92    1.33    1.65    0.6     0.82    1.29\n\nM \\ {P, R}          0.17    0.31    0.44    8.39    7.01    5.75    9.04    5.54    4.78\n\nM \\ {P, L}          0.33    0.32    0.14    6.77    7.27    8.77    6.97    3.88    7.27\n\n Uniform              0.54    0.56    0.53    2.12    2.06    2.25    6.01    3.9     4.71\n\nTable 6: Results for modeling qwen2.5-7b-it’s behavior on the sports binding task, contingent on\nthe positional, lexical and reflexive indices. Here t denotes tentity.\n\n\n\n\n\n                                       28\n\n## Claim to Classify\n\"causal model combining all three mechanisms that estimates next token distributions with 95% agreement\"\nLocation in paper: body\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Interchange interventions and causal abstraction",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim describes a causal model combining positional, lexical, and reflexive mechanisms to estimate next token distributions with 95% agreement. The method involves interchange interventions (a form of activation patching) and causal abstraction, which are Rung 2 interventional techniques per the codebook. The claim asserts a mechanistic explanation ('how LMs mix these mechanisms') and functional attribution ('the model uses these components'), with definitive language ('THE mechanism') implying uniqueness and underlying computation. This aligns with Rung 3 (counterfactual/mechanistic claims). No hedging is present, and confidence is high due to explicit experimental validation across models and tasks."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 34891,
      "completion_tokens": 1372,
      "total_tokens": 36263
    },
    "finish_reason": "stop"
  }
}