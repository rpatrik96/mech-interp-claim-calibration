{
  "timestamp": "2026-02-19T10:12:36.789401+00:00",
  "paper_id": "2506.18167",
  "claim_id": "2506.18167-04",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2506.18167\nTitle: Understanding Reasoning in Thinking Language Models via Steering Vectors\nFull text:\n                      Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n          UNDERSTANDING REASONING IN THINKING LAN-\n          GUAGE MODELS VIA STEERING VECTORS\n\n\n                      Constantin Venhoff∗                                              Iv´an Arcuschin∗\n                         University of Oxford                                           University of Buenos Aires\n                       United Kingdom                                             Argentina\n                 constantin@robots.ox.ac.uk                  iarcuschin@dc.uba.ar\n\n\n                        Philip Torr                   Arthur Conmy            Neel Nanda\n                         University of Oxford\n2025                 United Kingdom\n                                       ABSTRACT\nOct                       Recent advances in large language models (LLMs) have led to the development of\n                                 thinking language models that generate extensive internal reasoning chains before\n                              producing responses. While these models achieve improved performance, control-22\n                                  ling their reasoning processes remains challenging. This work presents a steering\n                             approach for thinking LLMs by analyzing and manipulating specific reasoning\n                              behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on\n                           500 tasks across 10 diverse categories, we identify several reasoning behaviors ex-\n                                  hibited by thinking models, including expressing uncertainty, generating examples\n                                  for hypothesis validation, and backtracking in reasoning chains. We demonstrate[cs.LG]                            that these behaviors are mediated by linear directions in the model’s activation\n                             space and can be controlled using steering vectors. By extracting and applying\n                                these vectors, we provide a method to modulate specific aspects of the model’s\n                                reasoning process, such as its tendency to backtrack or express uncertainty. Our ap-\n                              proach offers practical tools for steering reasoning processes in thinking models in\n                               a controlled and interpretable manner. We validate our steering method using three\n                              DeepSeek-R1-Distill models, demonstrating consistent control across different\n                           model architectures.\n\n\n                1  INTRODUCTION\n\n              A recent trend in the development of large language models (LLMs) has been thinking LLMs, which\n                       generate extensive internal reasoning chains before producing responses (Reynolds & McDonell,\n                     2021; Nye et al., 2021; Wei et al., 2022). Examples include OpenAI’s o1 (OpenAI, 2024) and\n                    DeepSeek’s R1 (DeepSeek-AI, 2025). These models have achieved remarkable improvements inarXiv:2506.18167v4               performance (Chollet, 2024), yet controlling and understanding their internal reasoning processes\n                      remains challenging.\n\n                   To address this challenge, we develop a steering approach for thinking LLMs by analyzing reasoning\n                      behaviors in DeepSeek-R1-Distill models. We focus on several reasoning behaviors exhibited by\n                       these models, including their tendency to express uncertainty, backtrack in reasoning chains, and\n                       generate examples for hypothesis testing. While these behaviors may not constitute a complete\n                   taxonomy of reasoning mechanisms, they provide a practical foundation for developing steering\n                     methods.\n\n               We investigate whether these reasoning behaviors of thinking LLMs can be directly controlled\n                      using steering vectors, a method that has been shown to allow precise behavioral control in LLMs\n                    (Subramani et al., 2022; Turner et al., 2023; Zou et al., 2023; Panickssery et al., 2023; Templeton\n                           et al., 2024; Arditi et al., 2024a). By extracting and applying steering vectors, we provide a means to\n                     modulate the internal reasoning dynamics of thinking LLMs in a controlled manner.\n\n                           *Equal contribution\n\n\n                                                           1\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nFigure 1: Steering on DeepSeek-R1’s backtracking feature vector changes the model’s behavior.\nDepending on whether we add or subtract this vector to the activations at inference time, the model\nincreases or decreases its tendency to abandon its current approach and explore alternative strategies\nfor the task at hand. Highlighted sections indicate instances of this behavior.\n\n\n\nIn summary, our work presents the following key contributions:\n\n      1. We develop a method to extract steering vectors for specific reasoning behaviors in DeepSeek-\n         R1-Distill models, enabling precise control over aspects of their thinking process, such as\n        backtracking and uncertainty estimation.\n      2. We demonstrate the effectiveness of our steering approach through an empirical evaluation\n       on over 500 tasks across 10 diverse categories, showing consistent control across multiple\n        DeepSeek-R1-Distill model architectures and sizes.\n\nOur steering approach provides a practical method for controlling reasoning processes in thinking\nmodels in an interpretable manner, opening new possibilities for fine-grained control of model\nbehavior. To facilitate reproducibility and enable further research, we make our complete experimental\ncodebase and datasets publicly available.1\n\n\n2  PRELIMINARIES\n\n2.1  ATTRIBUTION PATCHING\n\nA fundamental challenge in analyzing the behavior of large language models (LLMs) is identifying\nthe specific components and layers responsible for a given behavior. A widely used technique for\naddressing this challenge is activation patching (Meng et al., 2022). Activation patching works by\nreplacing the activations of a specific model component with those from a counterfactual example,\nwhich differs only in a specific aspect of the behavior being analyzed. If this intervention significantly\nalters the model’s output with respect to the observed behavior, the modified component can be\nmarked as playing a key role in implementing this behavior. The patching effect is quantified as the\nchange in a relevant output metric:\n\n              ∆L = L(xclean | do(a = apatch)) −L(xclean),\n\nwhere L is a metric measuring the difference in model outputs (e.g., KL-divergence), a is the original\nactivation, and apatch is the counterfactual activation.\n\nSince activation patching is computationally expensive, a more efficient linear approximation known\nas attribution patching (Nanda, 2023; Syed et al., 2023) is often used, which utilizes the gradients of\nthe model’s activations with respect to the metric:\n\n                                     ∂\n           ∆L ≈(apatch −aclean)T ·        L(xclean | do(a = aclean)).\n                                                  ∂aclean\n\n  1https://github.com/cvenhoff/steering-thinking-llms\n\n\n                                       2\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n2.2  COMPUTING STEERING VECTORS\n\nThe Difference of Means method is a widely used technique for extracting steering vectors in LLMs\n(Turner et al., 2024; Arditi et al., 2024b). This technique is based on constructing contrastive datasets\nthat differ in a specific concept and computing the difference in their mean activations of a model.\nFormally, let D+ and D−be two datasets where samples in D+ exhibit a given concept, while\nsamples in D−do not. Given a model component, we compute the Difference of Means vector as:\n\n                            1                1\n                 u =  X  a(pi) −  X  a(pj)\n                          |D+|             |D−|\n                                pi∈D+            pj∈D−\n\nwhere a(pi) and a(pj) represent the activations of the model components over the prompts from the\nrespective datasets. This vector u captures the primary direction in activation space that differentiates\nthe two datasets with respect to the target concept. In cases where explicitly matched counterfactuals\nare unavailable, a common heuristic is to define D+ as the set of all samples exhibiting the target\nbehavior, while D−consists of the full dataset. In this scenario, the Difference of Means vector\nis computed by subtracting the overall mean activation from the mean activation of the behavior-\nassociated examples. This isolates the direction in the activation space most associated with the target\nbehavior while reducing the influence of general model biases.\n\n\n2.3  DEEPSEEK THINKING MODELS\n\nAs mentioned in the introduction, thinking models are a type of language model designed to generate\nlong chains of internal reasoning before arriving at a final answer. Examples of this type of models\ninclude QwQ (Qwen Team, 2024), Gemini 2.0 Flash Thinking (GDM, 2024), o1 (OpenAI, 2024),\nand DeepSeek-R1 (DeepSeek-AI, 2025).\n\nIn this work, we focus on characterizing the thinking mechanisms of DeepSeek-R1, a recent thinking\nmodel that has achieved a similar performance to o1-preview on the ARC-AGI-Pub dataset (Knoop,\n2025; Chollet, 2024). DeepSeek-R1 is a language model trained through a multi-stage process that\ncombines large-scale reinforcement learning (RL) with the strategic use of supervised fine-tuning\n(SFT). The model’s architecture uses a Mixture-of-Experts (MoE) approach with 37B activated\nparameters and 671B total parameters.\n\nThe DeepSeek team has distilled R1’s reasoning capabilities into smaller, dense models ranging from\n1.5B to 70B parameters, based on both Qwen and Llama architectures. These distilled models achieve\nsimilar or better performance than frontier production models like GPT-4o and Claude 3.5 Sonnet at\nseveral math and coding benchmarks (DeepSeek-AI, 2025). We use the Qwen-14B and Llama-8B\ndistilled models of DeepSeek R1 for our analysis.\n\n\n3  IDENTIFYING REASONING BEHAVIORS FOR STEERING\n\nTo identify reasoning behaviors suitable for steering, we examined 100 reasoning chains generated\nby DeepSeek-R1 and 100 answers produced by GPT-4o, using tasks across diverse categories (see\nSection 4.1). Consistent with prior findings (DeepSeek-AI, 2025), we observe that one of the key\ndifferences is that the DeepSeek-R1 distills tend to explore many different approaches when solving a\ntask, whereas the base models follow a more linear reasoning trajectory. Notably, the thinking models\ncan express uncertainty about its current approach, generate examples or scenarios to test hypotheses,\nand backtrack to intermediate steps when revising its reasoning.\n\nBased on these observations, we identify the following behavioral patterns in the DeepSeek-R1 distill\nreasoning processes as targets for our steering approach:\n\n        • Initialization: The model rephrases the task and articulates initial thoughts, typically at the\n        beginning of the reasoning chain.\n        • Deduction: The model derives conclusions based on its current approach and assumptions.\n        • Knowledge Augmentation: The model incorporates external knowledge to refine its rea-\n         soning.\n        • Example Testing: The model generates examples or scenarios to validate its working\n         hypothesis.\n\n\n                                       3\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n                           Comparison of Reasoning Patterns Fractions and Total Sentences\n                                                                                                            Avg: 57%                                                       Llama-3.3-70B-Instruct  60%        Deepseek-R1-Distill-Llama-70B                                                                                                         30   Avg: 27.6\n                Deepseek-R1-Distill-Llama-8B                                                        Llama-3.1-8B-Instruct                            Avg: 52%\n  50%        Deepseek-R1-Distill-Qwen-32B         Qwen2.5-32B-Instruct                                          25\n                                                     Qwen2.5-14B-Instruct\n                Deepseek-R1-Distill-Qwen-1.5B                                                      Qwen2.5-1.5B-Instruct                                                                        CountFraction40%        Deepseek-R1-Distill-Qwen-14B                                                                                                         20            Avg: 14.4\n  30%                                                                 Avg: 24%                                   15Sentence                                                                                                                                                                                                                                                                                                         Sentence\n  20%                                                         Avg: 15%                                            Avg10Avg\n                            Avg: 9%                                              Avg: 6% Avg: 5%                     Avg: 7% Avg: 9%                     5  10%  Avg: 4%                  Avg: 1%           Avg: 2%\n   0%                                                                                                   0           Backtracking       Uncertainty        Example          Adding             Initializing         Deduction                        Total\n                                Estimation           Testing         Knowledge                                                   Sentences\n\nFigure 2: Comparison of behavioral patterns across five DeepSeek-R1-Distill models and five baseline\nmodels on 100 randomly selected tasks from our dataset (cf. Section 4.1). The plot on the left shows\nthe fraction of sentences annotated with each behavioral category. The plot on the right shows the\naverage number of sentences per response. Thinking models generate substantially longer responses\n(27.6 vs 14.4 sentences on average) and exhibit a higher fractions of backtracking, uncertainty\nestimation and example testing behaviors, but lower fractions of knowledge augmentation.\n\n\n\n\n        • Uncertainty Estimation: The model explicitly states its confidence or uncertainty regarding\n            its reasoning.\n        • Backtracking: The model abandons its current approach and explores an alternative strategy.\n\nAn annotated example of these behaviors is provided in Appendix B. To systematically annotate\nreasoning chain in our experiments, we employ GPT-4o (see Appendix A for details). To quantify\nthe prevalence of these reasoning behaviors, we generate reasoning chains using five DeepSeek-R1-\nDistill models (Qwen-14B, Qwen-32B, Llama-70B, Qwen-1.5B, and Llama-8B) and compare them\nagainst five baseline models (Qwen2.5-Math-1.5B, Qwen2.5-32B-Instruct, Qwen2.5-14B-Instruct,\nLlama-3.3-70B-Instruct, and Llama-3.1-8B-Instruct) across 100 diverse tasks (see Section 4.1). We\nthen automatically annotate each reasoning chain and compute the fraction of sentences annotated\nwith each behavioral category. Results are presented in Figure 2.\n\nThe results reveal several key distinctions between thinking and baseline models. Most notably,\nthinking models generate substantially longer responses, averaging 27.6 sentences vs 14.4 for baseline\nmodels. Both model types devote similar fractions of their responses to initialization and deduction,\nsuggesting that these processes are comparable across model types. In contrast, baseline models\nshow lower fractions of backtracking, uncertainty estimation, and, to a lesser extent, example testing,\ninstead allocating more of their responses to knowledge augmentation.\n\nThese patterns suggest that uncertainty estimation and backtracking are prominent behaviors that\ndistinguish thinking models, followed by example testing and knowledge addition. Given that\ninitialization and deduction do not seem to be behaviors specific to thinking models, steering on these\nseems conceptually ill-defined; initialization occurs once at the start and deduction is fundamental to\nany LLM, therefore we omit them from the empirical study on steering vectors.\n\n\n\n4  EXTRACTING AND EVALUATING STEERING VECTORS\n\n\nIn this section, we demonstrate that the reasoning behaviors of the DeepSeek-R1-Distill models\ncharacterized in Section 3 are mediated by linear directions, the steering vectors. We assess the causal\neffect of these vectors by comparing the model’s original reasoning chains to those generated under\npositive and negative steering (adding and subtracting the steering vectors). Our findings indicate\nthat the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process.\nAdditionally, our steering vectors provide an efficient way to influence these models’ reasoning\nbehavior, for example, increasing their tendency to backtrack or modulate their inherent uncertainty\nin their own reasoning.\n\n\n                                       4\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n   Category                Description\n\n   Mathematical Logic      Problems requiring formal logical operations, mathematical\n                               proofs, and numerical reasoning\n    Spatial Reasoning         Tasks involving visualization, geometric manipulation, and under-\n                              standing spatial relationships\n   Verbal Logic             Problems focused on language-based reasoning, syllogisms, and\n                              verbal analogies\n    Pattern Recognition       Questions requiring identification and continuation of sequences\n                              or abstract patterns\n    Lateral Thinking         Problems that require creative, non-linear approaches to reach\n                             unconventional solutions\n   Causal Reasoning         Tasks involving understanding cause-and-effect relationships and\n                         making causal inferences\n    Probabilistic Thinking     Problems requiring reasoning about uncertainty, probability, and\n                                     statistical concepts\n   Systems Thinking         Questions about complex systems, interconnected components,\n                          and emergent behaviors\n   Creative Problem Solv-  Open-ended problems requiring novel approaches and innovative\n   ing                         solutions\n    Scientific Reasoning      Tasks involving hypothesis formation, experimental design, and\n                             evidence evaluation\n\n                 Table 1: Task categories used to analyze reasoning behaviors.\n\n\n4.1  EXPERIMENTAL SETUP\n\nFor our experiments, we generate a dataset of 500 tasks across 10 categories using Claude 3.5 Sonnet\n(see Table 1). We conduct our experiments on three DeepSeek-R1-Distill models: Qwen-14B,\nQwen-1.5B, and Llama-8B. When generating a reasoning chain, we use greedy decoding and\n1000 max tokens per response.\n\n\n4.2  LOCATING CAUSALLY RELEVANT ACTIVATIONS\n\nTo extract robust steering vectors, we first identify the activations where these vectors are linearly\nrepresented within the model. We focus on the residual stream activations, i.e., the outputs of each\ntransformer layer. Given a reasoning chain generated by a DeepSeek-R1-Distill model, we identify\nboth the token positions and layers where the steering vector is active. This process consists of two\nkey steps:\n\n      1. Identifying relevant token positions: Determine which tokens in the reasoning chain\n        correspond to a specific behavioral category.\n\n      2. Determining causally relevant layers: Use attribution patching (Section 2) to evaluate\n       which layers contribute causally to the behavior in question.\n\nTo obtain token positions associated with each behavioral category, we generate 500 reasoning chains\nwith the tasks introduced in Section 4.1, using both DeepSeek-R1-Distill models and then annotate\nthem automatically with GPT-4o. Since the DeepSeek-R1-Distill models are autoregressive, we\nconsider for each category both the token position preceding the start of a token-sequence annotated\nwith the current category and the annotated sequence itself as the causally relevant token positions\n(up to 10 tokens). This ensures that we capture both the decision point where the model transitions\ninto the behavior and the behavior’s execution phase.\n\nTo identify the causally relevant layers for each behavioral category, we first extract a steering vector\ncandidate from every layer using the Difference of Means method (Section 2.2):\n\n         1                 1                                 1\n  ucℓ=  X  ¯acℓ(pi) −  X  acℓ(pj),   with   ¯acℓ(pi) =    X   aℓ(t).\n                        |D−|       |D+|\n            pi∈D+                             pj∈D−                            |seqc(pi)| t∈seqc(pi)\n\n\n                                       5\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n                                DeepSeek-R1-Distill-Llama-8B\n            Backtracking             Uncertainty Estimation          Example Testing            Adding Knowledge\n    5                                                                                                      5\n                                      4\n    4                                                                    4                                 4\n    3                                      3                                 3                                 3KL-Divergence 2                                 2                                 2                                 2\nMean 1                                 1                                 1                                 1\n     0         10        20        30   0         10        20        30   0         10        20        30   0         10        20        30\n                   Layer                              Layer                              Layer                              Layer\n\n                              DeepSeek-R1-Distill-Qwen-1.5B\n            Backtracking             Uncertainty Estimation          Example Testing            Adding Knowledge\n    0.4                                      0.4                                                                                0.8\n                                                                                       0.4\n    0.3                                      0.3                                                                                0.6                                                                                       0.3\n                                                                                                                                0.4KL-Divergence 0.2                                      0.2                                      0.2\nMean 0.1                                      0.1                                      0.1                                      0.2\n      0    5    10   15   20   25      0    5    10   15   20   25      0    5    10   15   20   25      0    5    10   15   20   25\n                   Layer                              Layer                              Layer                              Layer\n\n                               DeepSeek-R1-Distill-Qwen-14B\n             Backtracking             Uncertainty Estimation          Example Testing            Adding Knowledge\n                                              0.6\n                                                                                       0.5                                      0.5    0.6                                      0.5\n                                                                                       0.4                                      0.4                                              0.4\n    0.4                                      0.3                                      0.3                                      0.3KL-Divergence\nMean 0.2                                      0.20.1                                      0.20.1                                      0.20.1\n      0     10    20    30    40       0     10    20    30    40       0     10    20    30    40       0     10    20    30    40\n                    Layer                              Layer                              Layer                              Layer\n\n\nFigure 3: Causal impact of candidate steering vectors across model layers. The y-axis represents the\nabsolute mean KL-divergence for the next-token logit distribution when removing the steering vector\nat each layer. The steering vectors for all reasoning mechanisms have similar peaks in the middle\nlayers of the respective models.\n\n\n\nwhere:\n\n        • aℓ(t) represents the residual stream activation at layer ℓfor token position t.\n\n        • seqc(p) is the set of all token sequences within prompt p that are annotated with category c,\n         including the preceding token position.\n\n        •  ¯acℓ(pi) denotes the mean activation across all token positions within the annotated sequences\n         of category c at layer ℓ.\n\n        • D+ consists of prompts containing at least one sequence labeled with category c, while D−\n         represents the full dataset.\n\nThe resulting vector ucℓserves as a candidate steering vector for each layer. To ensure consistent\nscaling across different sequence lengths and behaviors, we normalize each steering vector to have\nthe same magnitude as the mean overall activation:\n\n                                                                       ∥¯aoverallℓ   ∥                                       uc,normℓ   = ucℓ·\n                                            ∥ucℓ∥\n\nwhere ¯aoverallℓ      is the mean activation across all tokens in the dataset at layer ℓ. This normalization\nhelps account for varying sequence lengths and ensures steering vectors have comparable magnitudes\nacross different behavioral categories.\n\n\n                                       6\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n        DeepSeek-R1-Distill-Llama-8B    DeepSeek-R1-Distill-Qwen-1.5B    DeepSeek-R1-Distill-Qwen-14B\n\n   70%        Original(%)                 Positive Steering   60%              Negative Steering\n   50%Fraction\n   40%\n\n   30%Sentence\nAverage 20%10%\n    0%\n                                                                                                        testingaddingknowledge        backtrackinguncertaintyestimationexample                           testingaddingknowledge  backtrackinguncertaintyestimationexample                                                                  testingaddingknowledge  backtrackinguncertaintyestimationexample\n\nFigure 4: Effect of applying the steering vector for each reasoning behavior across different distill\nmodels. The y-axis shows the change in the fraction of tokens exhibiting each behavior when\napplying positive or negative steering. Positive steering increases behaviors such as backtracking and\nuncertainty estimation, while negative steering suppresses or significantly reduces them, confirming\nthe causal influence of our extracted vectors.\n\n\n4.3  SELECTING FINAL STEERING VECTORS\n\nTo determine the final steering vectors, we apply attribution patching (Section 2.1) to quantify the\ncausal relevance of each vector in its respective layer. Specifically, we consider the following patching\nexperiment: Given a candidate steering vector ucℓfor a specific behavioral category, we add it to the\nresidual stream activation preceding a token-sequence annotated with one of the relevant behaviors.\nTherefore, we define the patched activation as:\n\n                                               apatchedℓ   = aℓ+ ucℓ.\n\nIf this intervention leads to a significant change in the KL divergence of the next-token prediction,\nthen the steering vector in layer ℓis causally relevant for the given behavior. We approximate the\npatching effect for this experiment with:\n\n                                ∂\n              ∆L ≈(ucℓ)T ·     L(xclean | do(aℓ= aclean)),\n                                  ∂aℓ\nwhere ucℓ= (apatchedℓ    −aℓ). We average the absolute patching effect for each category over all\ncategory-sequences in all 500 reasoning chains. The results are shown in Figure 3. Based on\nthese results, we can select the causally most relevant steering vectors from the layers where the\npatching scores are highest. However, we avoid selecting early layers that show excessive correlation\nwith embedding tokens, as these layers primarily capture token-specific representations rather than\nbehavioral patterns (Appendix D).\n\n\n4.4  EVALUATION OF STEERING VECTORS\n\nTo evaluate the effectiveness of our extracted steering vectors, we apply them at the selected layers\n(see Table 2) and observe their influence on the model’s reasoning process. Steering is implemented by\nadding or subtracting the extracted steering vectors ucℓto the residual stream activations at inference\ntime. By applying this intervention, we can increase or decrease behaviors such as backtracking,\nuncertainty estimation, and example testing, providing a direct mechanism for manipulating the\nmodel’s reasoning process.\n\n\n4.5  RESULTS OF STEERING\n\nWe apply each steering vector to 50 unseen reasoning tasks and analyze how the model’s reasoning\nbehavior changes. The results, presented in Figure 4, demonstrate that our extracted vectors effectively\n\n\n                                       7\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                         DeepSeek-R1-Distill   DeepSeek-R1-Distill   DeepSeek-R1-Distill\n Behavioral Category       Llama-8B          Qwen-1.5B         Qwen-14B\n\n Uncertainty Estimation          12                  18                  29\n Example Testing               12                  15                  29\n Backtracking                  12                  17                  29\n Adding Knowledge             12                  18                  24\n\nTable 2: Selected layers for each behavioral category based on attribution patching results. For\neach model, we select the layer with the maximum score from the attribution patching experiments,\nignoring early layers that are highly correlated with embedding tokens.\n\n\ncontrol the model’s reasoning patterns. Appendix E includes a full example of positive and negative\nsteering on the “Adding Knowledge” vector for the DeepSeek-R1-Distill-Qwen-14B model.\n\nAs shown in Figure 4, positive steering increases behaviors such as backtracking, uncertainty es-\ntimation, and example testing, while negative steering reduces them. These effects are consistent\nacross both DeepSeek-R1-Distill models, reinforcing the hypothesis that Thinking LLMs encode\nthese reasoning mechanisms as linear directions in their activation space. Our findings confirm that\nsteering vectors provide a reliable and efficient method for interpreting and controlling the internal\nreasoning dynamics of thinking large language models. Further analysis of the cosine similarity\nbetween different steering vectors reveals that most behavioral categories correspond to distinct\ndirections in the model’s activation space (see Appendix C).\n\n\n5  RELATED WORK\n\nRecent work has explored methods for steering and interpreting language models by identifying\nmeaningful directions or features within their internal representation spaces. Subramani et al. (2022)\nshow that extracting latent steering vectors from pretrained language models can systematically\nalter the generated text. Similarly, Turner et al. (2023) propose activation engineering, modifying\nmodel activations at inference time to control outputs, in contrast to prompt engineering or fine-\ntuning. Extending this line of research, Panickssery et al. (2023) introduce Contrastive Activation\nAddition (CAA), which derives a “contrastive” vector by averaging differences in residual stream\nactivations between positive and negative examples of a target behavior. Adding this vector to a\nmodel’s activations elicits more desirable outputs without retraining. Beyond these methods, Zou\net al. (2023) propose Representation Engineering, offering a top-down approach to refining and\nanalyzing internal representations for greater transparency.\n\nApplying these methods for fine-grained control of language models through systematic manipulation\nof their internal representations Li et al. (2023) propose an inference-time intervention to encourage\nmore truthful responses without additional training. Additionally, Arditi et al. (2024a) has shown\nthat refusal behavior can be localized to a single direction in latent space, that minimally affect other\ncapabilities, enabling targeted interventions to encourage refusal or jailbreak the model.\n\nA related line of work focuses on leveraging these internal representations for reasoning and chain-\nof-thought. Zhao et al. (2025) found a steering vector to efficiently elicit long chain-of-thought in\nlanguage models. Dutta et al. (2024) analyzed the neural sub-structures within Llama-2 7B that facili-\ntate multistep reasoning over fictional ontologies and found that the model deploys multiple parallel\npathways for step-by-step reasoning. In concurrent work, Hazra et al. (2025) train sparse autoencoders\non DeepSeek’s 671B-parameter R1 reasoning model, uncovering internal features associated with\nreasoning behaviors, such as backtracking, which can be used for steering interventions.\n\n\n6  CONCLUSION AND FUTURE WORK\n\nThis work presents a steering approach for controlling reasoning behaviors in thinking LLMs, with\na specific focus on DeepSeek-R1-Distill models. We demonstrate that several reasoning behaviors\nexhibited by these models, including expressing uncertainty, backtracking, and example testing,\ncan be effectively controlled using steering vectors. While our analysis does not claim to provide\n\n\n                                       8\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\na complete taxonomy of reasoning mechanisms, it establishes a practical framework for steering\nspecific aspects of model behavior. Our key findings indicate that:\n\n        • Several reasoning behaviors in thinking models can be isolated to specific directions in the\n        model’s activation space, enabling precise control through steering vectors.\n        • Our steering approach is effective across a diverse set of 500 tasks, demonstrating robust\n         control over targeted reasoning behaviors.\n        • The method generalizes across different model architectures within the DeepSeek-R1-Distill\n         family, showing consistent steering effects.\n\nThese results provide practical tools for modulating reasoning capabilities in thinking models. The\nability to adjust specific aspects of the reasoning process through steering vectors opens new possibil-\nities for adapting these models to different tasks and requirements.\n\nDespite these promising results, our work has several limitations that suggest directions for future\nresearch. The current automated annotation process using GPT-4o, while efficient, has occasionally\nproduced false positives and false negatives in identifying reasoning patterns. Future work should\nfocus on developing more robust annotation methods, potentially incorporating multiple models or\nhuman validation to improve accuracy. Additionally, while our analysis centers on DeepSeek-R1-\nDistill models, the generalization of these findings to other models that have undergone reward-based\nRL training instead of being fine-tuned on thinking models (e.g., Qwen’s QwQ) remains an open\nquestion. Extending this research to a broader range of models would provide deeper insights into the\nuniversality of thinking mechanisms and their practical applications. By addressing these limitations,\nfuture research can further advance the understanding and control of thinking models, paving the way\nfor more reliable and adaptable AI systems.\n\nACKNOWLEDGEMENTS\n\nWe would like to thank the ML Alignment & Theory Scholars (MATS) program for supporting this\nresearch, and in particular John Teichman and Cameron Holmes for being great research managers.\nWe would also like to thank reviewers from the Workshop on Reasoning and Planning for Large\nLanguage Models at ICLR 2025 for extremely helpful feedback on early drafts of this paper.\n\nREFERENCES\n\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel\n  Nanda. Refusal in Language Models Is Mediated by a Single Direction. arXiv, June 2024a. doi:\n  10.48550/arXiv.2406.11717.\n\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel\n  Nanda. Refusal in language models is mediated by a single direction. arXiv, abs/2406.11717,\n  2024b. URL https://arxiv.org/abs/2406.11717.\n\nFranc¸ois Chollet. OpenAI o3 Breakthrough High Score on ARC-AGI-Pub, December 2024. URL\n  https://arcprize.org/blog/oai-o3-pub-breakthrough.\n\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement ‘learning,\n  2025.\n\nSubhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty. How to think\n   step-by-step: A mechanistic understanding of chain-of-thought reasoning, 2024. URL https:\n  //arxiv.org/abs/2402.18312.\n\nGDM. Gemini flash thinking: Gemini 2.0 Flash Thinking Experimental, 2024. URL https:\n  //deepmind.google/technologies/gemini/flash-thinking/.\n\nDron Hazra, Max Loeffler, Murat Cubuktepe, Levon Avagyan, Liv Gorton, Mark Bissell, Owen Lewis,\n  Thomas McGrath, and Daniel Balsam. Under the hood of a reasoning model, 2025. URL https:\n  //www.goodfire.ai/blog/under-the-hood-of-a-reasoning-model.    Ac-\n   cessed: 2025-06-22.\n\n\n                                       9\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\nMike Knoop. R1-Zero and R1 Results and Analysis, January 2025. URL https://arcprize.\n  org/blog/r1-zero-r1-results-analysis.\n\nKenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-\n  Time Intervention: Eliciting Truthful Answers from a Language Model. arXiv, June 2023. doi:\n  10.48550/arXiv.2306.03341.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.  Locating and editing factual\n   associations in gpt. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.),\n  Advances in Neural Information Processing Systems, volume 35, pp. 17359–17372. Curran Asso-\n   ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n  2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf.\n\nNeel  Nanda.     Attribution  patching:   Activation  patching  at  industrial  scale.    2023.\n   https://www.neelnanda.io/mechanistic-interpretability/attribution-patching.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\n  Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\n  Augustus Odena. Show your work: Scratchpads for intermediate computation with language\n  models, 2021.\n\nOpenAI.  Learning to reason with LLMs, 9 2024. URL https://openai.com/index/\n  learning-to-reason-with-llms/.\n\nNina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt\n  Turner.  Steering Llama 2 via Contrastive Activation Addition.  arXiv, December 2023.  doi:\n  10.48550/arXiv.2312.06681.\n\nQwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 11 2024. URL https:\n  //qwenlm.github.io/blog/qwq-32b-preview/.\n\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the\n  few-shot paradigm, 2021.\n\nNishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting Latent Steering Vectors from\n  Pretrained Language Models. arXiv, May 2022. doi: 10.48550/arXiv.2205.05124.\n\nAaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit\n  discovery. In NeurIPS Workshop on Attributing Model Behavior at Scale, 2023. URL https:\n  //openreview.net/forum?id=tiLbFR4bJW.\n\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen,\n  Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L\n  Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers,\n  Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan.\n  Scaling monosemanticity:  Extracting interpretable features from claude 3 sonnet.  Trans-\n  former Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/\n  scaling-monosemanticity/index.html.\n\nAlexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini,\n  and Monte MacDiarmid. Steering Language Models With Activation Engineering. arXiv, August\n  2023. doi: 10.48550/arXiv.2308.10248.\n\nAlexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and\n  Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv,\n  abs/2308.10248, 2024. URL https://arxiv.org/abs/2308.10248.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n  Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\n  models. In Proceedings of the 36th International Conference on Neural Information Processing\n  Systems, NIPS ’22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088.\n\n\n                                       10\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nZekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, and Biwei Huang. Activation\n  control for efficiently eliciting long chain-of-thought ability of language models, 2025. URL\n  https://arxiv.org/abs/2505.17697.\n\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,\n  Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J.\n  Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson,\n   J. Zico Kolter, and Dan Hendrycks. Representation Engineering: A Top-Down Approach to AI\n  Transparency. arXiv, October 2023. doi: 10.48550/arXiv.2310.01405.\n\n\n\n\n\n                                       11\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\nA  DETAILS ON THE ANNOTATION PROCESS\n\nWe use the following prompt to automatically annotate LLM responses:\n\nPlease split the following reasoning chain of an LLM into\nannotated parts using labels and the following format [\"label\n\"]...[\"end-section\"]. A sentence should be split into multiple\nparts if it incorporates multiple behaviours indicated by the\nlabels.\n\nAvailable labels:\n0. initializing -> The model is rephrasing the given task and\nstates initial thoughts.\n1. deduction -> The model is performing a deduction step based on\nits current approach and assumptions.\n2. adding-knowledge -> The model is enriching the current approach\n with recalled facts.\n3. example-testing -> The model generates examples to test its\ncurrent approach.\n4. uncertainty-estimation -> The model is stating its own\nuncertainty.\n5. backtracking -> The model decides to change its approach.\n\nThe reasoning chain to analyze:\n{thinking_process}\n\nAnswer only with the annotated text. Only use the labels outlined\nabove. If there is a tail that has no annotation leave it out.\n\n\nB  ANNOTATED EXAMPLE\n\nRiddle: What has cities, but no houses; forests, but no trees; and rivers, but no water?\n\nDeepSeek R1 Response:\n\nOkay, so I came across this riddle. At first glance, it seems\ntricky, but I can break it down.\nLet me analyze each part separately. The first part: \"What has\ncities, but no houses.\" The answer must be something that contains\n cities but lacks houses. Cities have buildings, roads, and\ninfrastructure, but without houses, it must be something that\nrepresents a city rather than an actual one.\nMaybe it’s a city without residential buildings, but that doesn’t\nfit perfectly.\nWait, perhaps it refers to something that *displays* cities rather\n than being a real city.\nWhat about the second part: \"forests, but no trees\"? Forests are\nmade of trees, so if there are no trees, it isn’t really a forest.\n But perhaps it’s something that symbolizes a forest without\nactually being one.\nFinally, \"rivers, but no water.\" Rivers are defined by flowing\nwater, so if there’s no water, it might be referring to a\nrepresentation of a river rather than a real one.\nConsidering all three parts together, the answer must be something\n that includes symbolic representations of cities, forests, and\nrivers but lacks their real-world components. A map fits this\ndescription-it has cities, forests, and rivers marked on it, but\nthey are not real.\n\n\n\n                                       12\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nAutomatically annotated response: (colored by assigned label)\n\n[\"initializing\"]Okay, so I came across this riddle. At first\nglance, it seems tricky, but I can break it down.[\"end-section\"]\n[\"deduction\"]Let me analyze each part separately. The first part:\n\"What has cities, but no houses.\" The answer must be something\nthat contains cities but lacks houses.[\"end-section\"]\n[\"adding-knowledge\"]Cities have buildings, roads, and\ninfrastructure, but without houses, it must be something that\nrepresents a city rather than an actual one.[\"end-section\"]\n[\"example-testing\"]Maybe it’s a city without residential\nbuildings, but that doesn’t fit perfectly.[\"end-section\"]\n[\"backtracking\"]Wait, perhaps it refers to something that\n*displays* cities rather than being a real city.[\"end-section\"]\n[\"deduction\"]What about the second part: \"forests, but no\ntrees\"?[\"end-section\"]\n[\"adding-knowledge\"]Forests are made of trees, so if there are no\ntrees, it isn’t really a forest.[\"end-section\"]\n[\"example-testing\"]Unless it’s an area that used to be a forest\nbut was deforested-but that wouldn’t be called a forest\nanymore.[\"end-section\"]\n[\"uncertainty-estimation\"]That might be a stretch,\nthough.[\"end-section\"]\n[\"deduction\"]The third part: \"rivers, but no\nwater.\"[\"end-section\"]\n[\"adding-knowledge\"]Rivers are defined by flowing water. If\nthere’s no water, it might be referring to a representation of a\nriver rather than a real one.[\"end-section\"]\n[\"example-testing\"]Maybe it’s a dry riverbed, but then it’s not\ntechnically a river anymore.[\"end-section\"]\n[\"deduction\"]Putting it all together, the answer must be something\nthat includes symbolic representations of cities, forests, and\nrivers but lacks their real-world components.[\"end-section\"]\n[\"deduction\"]A map fits this description-it has cities, forests,\nand rivers marked on it, but they are not real.[\"end-section\"]\n\n\nC  COSINE SIMILARITY BETWEEN FEATURE VECTORS\n\nTo better understand the relationships between different reasoning behaviors in thinking models, we\nanalyze the cosine similarity between the extracted steering vectors for different behavioral categories.\nThis analysis provides insights into how distinct these reasoning mechanisms are in the model’s\nrepresentational space and whether certain behaviors share similar underlying features.\n\nWe compute pairwise cosine similarities between the steering vectors for five key behavioral cate-\ngories: initializing, backtracking, uncertainty-estimation, adding-knowledge, and deduction. For each\npair of behavioral categories, we calculate the cosine similarity between their corresponding steering\nvectors at the layers identified as causally relevant through attribution patching (Section 4.3).\nThe cosine similarity between two feature vectors uc1 and uc2 is computed as:\n                                                uc1 · uc2\n                                sim(uc1, uc2) =\n                                                      |uc1||uc2|\n\nFigure 5 presents the cosine similarity heatmaps for both DeepSeek-R1-Distill models. Several key\nobservations emerge from this analysis:\n\n      1. Distinct reasoning mechanisms: Most behavioral categories show relatively low cosine\n          similarities with each other, indicating that they correspond to distinct directions in the\n        model’s activation space. This supports our hypothesis that different reasoning behaviors\n         are mediated by separate linear directions.\n\n\n                                       13\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                    Cosine Similarity Between Feature Vectors                                    Cosine Similarity Between Feature Vectors\n                                                                                                            1.00                                                                                                                      1.00\n\n               initializing       1.00           0.26           0.29           0.17           0.08            0.75                          initializing       1.00           0.36           0.32           0.23           0.25            0.75\n\n\n                                                                                                            0.50                                                                                                                      0.50\n          backtracking       0.26           1.00           0.68           0.23           0.56                                 backtracking       0.36           1.00           0.78           0.36           0.70\n\n                                                                                                            0.25                                                                                                                      0.25\n uncertainty-estimation       0.29           0.68           1.00           0.12           0.49            0.00    Similarity      uncertainty-estimation       0.32           0.78           1.00           0.24           0.64            0.00    Similarity\n                                                                                                             0.25Cosine                                                                                                                   0.25Cosine\n\n     adding-knowledge       0.17           0.23           0.12           1.00           0.26                           adding-knowledge       0.23           0.36           0.24           1.00           0.47\n                                                                                                             0.50                                                                                                                      0.50\n\n             deduction       0.08           0.56           0.49           0.26           1.00              0.75                    deduction       0.25           0.70           0.64           0.47           1.00              0.75\n\n\n                                                                                                             1.00                                                                                                                      1.00\n                             initializing  backtrackinguncertainty-estimationadding-knowledge    deduction                                                              initializing  backtrackinguncertainty-estimationadding-knowledge    deduction\n\n          (a) DeepSeek-R1-Distill-Llama-8B                     (b) DeepSeek-R1-Distill-Qwen-14B\n\nFigure 5: Cosine similarity heatmaps between steering vectors for different reasoning behaviors. The\nheatmaps show pairwise similarities between feature vectors extracted for five behavioral categories.\nValues range from -1 (completely opposite) to 1 (identical direction), with colors indicating the\nstrength and direction of similarity. Most behaviors show low to moderate similarities, indicating\nthey represent distinct reasoning mechanisms in the model’s activation space.\n\n\n      2. Uncertainty and backtracking correlation: Both models show moderate positive correlation\n        between uncertainty-estimation and backtracking behaviors, which aligns with our intuition\n          that models often express uncertainty before deciding to change their reasoning approach.\n         Despite this cosine similarity overlap, our steering experiments (see Section 4.5) demonstrate\n          that these represent fundamentally different mechanisms in the model’s reasoning process.\n      3. Model-specific patterns: While the overall structure is similar across models, there are\n         notable differences in the specific similarity values, suggesting that different model architec-\n         tures may encode these reasoning behaviors with varying degrees of separation.\n\nThese results demonstrate that the extracted steering vectors capture meaningful and largely orthogo-\nnal directions in the model’s representational space, validating our approach for isolating specific\nreasoning mechanisms in thinking language models.\n\nD  COSINE SIMILARITY WITH EMBEDDING AND UNEMBEDDING VECTORS\n\nTo better understand the relationship between our extracted steering vectors and the model’s embed-\nding space, we analyze the cosine similarity between steering vectors and the model’s embedding\nand unembedding matrices across different layers. This analysis helps explain why certain layers are\nmore effective for steering than others.\n\nD.1  METHODOLOGY\n\nWe compute  the  cosine  similarity between each  steering  vector  at each  layer and  the\nmodel’s embedding  matrix (model.embed tokens.weight) and unembedding  matrix\n(model.lm head.weight).  For each steering vector ucℓat layer ℓand behavioral category\nc, we calculate:\n\n                               simembed(ucℓ) = max cos(ucℓ, Ei)                               (1)\n                                                                          i\n                               simunembed(ucℓ) = max cos(ucℓ, Uj)                              (2)\n                                                               j\n\nwhere Ei represents the i-th row of the embedding matrix and Uj represents the j-th row of the\nunembedding matrix.\n\n\n                                       14\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n               Feature Vector Embedding Similarities Across Layers                                                 Feature Vector Embedding Similarities Across Layers\n\n     0.225                                                                                               backtrackinguncertainty-estimation                                                                                                                backtrackinguncertainty-estimation\n     0.200                                                                                           adding-knowledgededuction                       0.12                                                                                            adding-knowledgededuction\n     0.175                                                                                                                      initializingexample-testing                  0.10                                                                                                                       initializingexample-testing\n\n     0.150\n                                                                                                                                         0.08  Similarity 0.125                                                                                                                                                                                                                                                                                                                                                                                                                   Similarity\n Cosine 0.100                                                                                                                                                                                                                                                  Cosine 0.06\n     0.075                                                                                                                               0.04\n\n     0.050\n                                                                                                                                         0.02\n     0.025\n          0          5          10         15         20         25         30                                             0              10             20             30             40\n                                      Layer Index                                                                                                  Layer Index\n\n        (a) Embedding similarities - Llama-8B                 (b) Embedding similarities - Qwen-14B\n\n\n\n             Feature Vector Unembedding Similarities Across Layers                                               Feature Vector Unembedding Similarities Across Layers\n     0.18\n                                                                                                         backtracking                                                                                                                                                                                                                                              backtracking                                                                                                                                         0.18\n                                                                                                               uncertainty-estimation                                                                                                                                                                                                                                                            uncertainty-estimation\n     0.16                                                                                                    adding-knowledge                                                                                                                                                                                                                                   adding-knowledge                                                                                                                                         0.16\n                                                                                                        deduction                                                                                                                                                                                                                                            deduction\n     0.14                                                                                                                       initializing\n                                                                                                            example-testing                  0.14                                                                                                                       initializingexample-testing\n     0.12                                                                                                                                0.12  Similarity 0.10                                                                                                                                                                                                                                                                                                                                                                                                                      Similarity 0.10\n Cosine 0.08                                                                                                                                                                                                                                                    Cosine 0.08\n     0.06                                                                                                                                0.06\n     0.04                                                                                                                                0.04\n                                                                                                                                         0.02\n         0          5          10         15         20         25         30                                             0              10             20             30             40\n                                      Layer Index                                                                                                   Layer Index\n\n       (c) Unembedding similarities - Llama-8B               (d) Unembedding similarities - Qwen-14B\n\nFigure 6: Layer-wise cosine similarities between steering vectors and embedding/unembedding\nmatrices. The Llama model shows high embedding similarity in early layers, indicating strong\ncorrelation with token representations. This pattern is not observed in the Qwen model, explaining\nthe different layer selection strategies for effective steering.\n\n\nD.2  RESULTS AND ANALYSIS\n\nFigure 6 shows the layer-wise similarity patterns for both models. The analysis reveals distinct\narchitectural differences:\n\nDeepSeek-R1-Distill-Llama-8B: Shows high similarity with embedding vectors in the first layers,\nexplaining the high KL divergence observed at the beginning of the attribution plots (Figure 3). This\nsuggests that Llama retains token representation information in early layers, making steering vectors\nin these layers too correlated with specific tokens to be effective for behavioral control.\n\nDeepSeek-R1-Distill-Qwen-14B: Does not exhibit this pattern, with more uniform similarities across\nlayers. This architectural difference allows for more flexibility in layer selection for steering.\n\nThese findings inform our layer selection strategy: we prioritize layers with high KL divergence in\nattribution plots while avoiding early layers that show excessive correlation with embedding tokens.\nThis approach ensures that our steering vectors capture behavioral patterns rather than token-specific\nrepresentations.\n\nE  STEERED EXAMPLE (ADDING KNOWLEDGE)\n\nTask:\n\nWhat’s the probability of drawing exactly two hearts when dealing\nthree cards from a standard deck?\n\nOriginal Response:\n\n[\"initializing\"]\nOkay, so I need to figure out the probability of drawing exactly\ntwo hearts when dealing three cards from a standard deck.\n[\"end-section\"]\n\n\n\n                                       15\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n[\"uncertainty-estimation\"]\nHmm, let’s see.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nI remember that probability problems often involve combinations,\n[\"end-section\"]\n\n[\"deduction\"]\nso maybe I should start by recalling how combinations work.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nFirst, a standard deck has 52 cards, right? And there are 13\nhearts in the deck, one for each rank.\n[\"end-section\"]\n\n[\"deduction\"]\nSo, if I’m drawing three cards, I want exactly two of them to be\nhearts and one to be a non-heart. That makes sense.\n[\"end-section\"]\n\n[\"deduction\"]\nI think the way to approach this is to calculate the number of\nfavorable outcomes (i.e., drawing exactly two hearts and one non-\nheart) and then divide that by the total number of possible\noutcomes when drawing three cards.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nSo, probability is generally favorable over total.\n[\"end-section\"]\n\n[\"initializing\"]\nLet me write that down:\n[\"end-section\"]\n\n[\"deduction\"]\nProbability = (Number of ways to choose 2 hearts and 1 non-heart)\n/ (Total number of ways to choose 3 cards from the deck). Okay, so\n I need to compute both the numerator and the denominator.\n[\"end-section\"]\n\n[\"deduction\"]\nStarting with the denominator, the total number of ways to choose\n3 cards from 52.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThat’s a combination problem because the order doesn’t matter when\n you’re dealing cards.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThe formula for combinations is C(n, k) = n! / (k! * (n - k)!),\nwhere n is the total number, and k is the number you’re choosing.\n[\"end-section\"]\n\n[\"deduction\"]\n\n\n                                       16\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nSo, the denominator is C(52, 3). Let me compute that:\n[\"end-section\"]\n\n[\"deduction\"]\nC(52, 3) = 52! / (3! * (52 - 3)!) = (52 * 51 * 50) / (3 * 2 * 1) =\n (52 * 51 * 50) / 6\n[\"end-section\"]\n\n[\"initializing\"]\nLet me calculate that:\n[\"end-section\"]\n\n[\"uncertainty-estimation\"]\n52 divided by 6 is approximately 8.666..., but maybe I should\ncompute it step by step.\n[\"end-section\"]\n\n[\"deduction\"]\nFirst, 52 * 51 = 2652\n[\"end-section\"]\n\n[\"deduction\"]\nThen, 2652 * 50 = 132600\n[\"end-section\"]\n\n[\"deduction\"]\nNow, divide by 6: 132600 / 6 = 22100\n[\"end-section\"]\n\n[\"deduction\"]\nSo, the total number of ways is 22,100. Got that.\n[\"end-section\"]\n\n\nPositively Steered Response\n\n[\"initializing\"]\nOkay, so I need to figure out the probability of drawing exactly\ntwo hearts when dealing three cards from a standard deck.\n[\"end-section\"]\n\n[\"uncertainty-estimation\"]\nHmm, probability problems can be tricky,\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nbut I remember that probability often involves combinations and\nthe concept of favorable outcomes over total possible outcomes.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nLet me start by recalling that a standard deck has 52 cards,\ndivided into four suits: hearts, diamonds, clubs, and spades.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nHearts are one of the four suits, and there are 13 cards in each\nsuit.\n[\"end-section\"]\n\n\n\n                                       17\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n[\"deduction\"]\nSo, in a standard deck, there are 13 hearts, and the rest of the\ncards are non-hearts, which would be 52 minus 13, so 39 non-heart\ncards.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nWhen dealing with probabilities involving card draws, especially\nwithout replacement, the concept of combinations is really\nimportant.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nCombinations are used to determine the number of ways to choose a\nsubset of items from a larger set, and the formula for\ncombinations is C(n, k) = n! / (k! * (n - k)!), where n is the\ntotal number of items, and k is the number of items you’re\nchoosing.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nIn this problem, we’re dealing with hypergeometric distribution,\nwhich deals with successes and failures in draws without\nreplacement.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThe hypergeometric probability formula is used to calculate the\nprobability of k successes (in this case, drawing hearts) in n\ndraws (here, n is 3 because we’re dealing three cards).\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThe formula is P(X = k) = (C(K, k) * C(N - K, n - k)) / C(N, n),\nwhere:\n- N is the total number of items in the population, which in this\ncase is the deck of 52 cards.\n- K is the number of success states in the population, which here\nis the number of hearts, so K = 13.\n- n is the number of draws, which is 3 for this problem.\n- k is the number of observed successes, which is the number of\nhearts we want to draw. In this case, we’re interested in exactly\ntwo hearts, so k = 2.\n[\"end-section\"]\n\n[\"deduction\"]\nSo, for exactly two hearts in three card draws, we need to\ncalculate the probability of drawing exactly 2 hearts and 1 non-\nheart card.\n[\"end-section\"]\n\n[\"deduction\"]\nThe formula for this specific case would be: P(X = 2) = [C(13, 2)\n* C(39, 1)] /\n[\"end-section\"]\n\n\nNegatively Steered Response\n\n[\"initializing\"]\n\n\n                                       18\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nFirst, I need to determine the number of ways to draw exactly two\nhearts from the deck.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThere are 13 hearts in the deck, and I want to choose 2 of them.\n[\"end-section\"]\n\n[\"deduction\"]\nThe number of ways to do this is 13 choose 2.\n[\"end-section\"]\n\n[\"adding-knowledge\"]\nThen, I need to choose 1 card from the remaining 27 non-heart\ncards.\n[\"end-section\"]\n\n[\"deduction\"]\nThe number of ways to do this is 27 choose 1.\n[\"end-section\"]\n\n[\"deduction\"]\nFinally, I multiply these two numbers together to get the total\nnumber of favorable outcomes.\n[\"end-section\"]\n\n[\"deduction\"]\nThen, I divide by the total number of possible ways to draw 3\ncards from the deck, which is 52 choose 3.\n[\"end-section\"]\n\n[\"deduction\"]\nThis gives me the probability of drawing exactly two hearts.\n[\"end-section\"]\n\n\n\n\n\n                                       19\n\n## Claim to Classify\n\"Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors\"\nLocation in paper: conclusion\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Steering vectors",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The paper uses steering vectors (Rung 2 interventional method) to extract vectors and control behaviors. The claim states that reasoning behaviors are isolated to specific directions (a mechanistic representation claim, Rung 3) and that this enables control. The claim is stated as a fact without hedging. Confidence is 4 due to minor ambiguity in the strength of the representation claim versus the interventional evidence."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 18222,
      "completion_tokens": 1317,
      "total_tokens": 19539
    },
    "finish_reason": "stop"
  }
}