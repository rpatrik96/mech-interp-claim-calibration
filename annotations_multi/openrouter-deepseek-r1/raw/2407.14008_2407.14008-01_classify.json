{
  "timestamp": "2026-02-19T09:45:30.085948+00:00",
  "paper_id": "2407.14008",
  "claim_id": "2407.14008-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2407.14008\nTitle: Investigating the Indirect Object Identification circuit in Mamba\nFull text:\n               Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n                                             Danielle Ensign 1  Adri`a Garriga-Alonso 2\n\n\n                         Abstract                          previous work has shown other interpretability techniques\n                                                                         apply. For example, Sharma et al. (2024) locate and edit          How well will current interpretability techniques\n                                                                              factual information with ROME (Rank One Model Editing)               generalize to future models? A relevant case\n                                                  Meng et al. (2023). Ali et al. (2024) extract hidden atten-               study is Mamba, a recent recurrent architecture\n                                                                           tion matrices, and Torres (2024); Grazzi et al. (2024) use              with scaling comparable to Transformers. We\n                                                                             linear probes to identify capabilities. Additionally, Paulo               adapt pre-Mamba techniques to Mamba and par-2024                                                                          et al. (2024) showed that Contrastive Activation Addition                   tially reverse-engineer the circuit responsible for\n                                                           (Rimsky et al., 2024), Tuned Lens (Belrose et al., 2023)                the Indirect Object Identification (IOI) task. Our\n                                                             and probes to elicit latent knowledge (Mallen et al., 2024)Jul         techniques provide evidence that 1) Layer 39 is a\n                                                                              transfer to the Mamba architecture.\n22        keynamesbottleneck,one position2) Convolutionsforward, andin 3)layerThe39nameshift        This work focuses on applying techniques from circuit-\n                  entities are stored linearly in Layer 39’s SSM. Fi-        based mechanistic interpretability to Mamba to see how\n                  nally, we adapt an automatic circuit discovery tool,        well these techniques transfer to new architectures. In par-\n                positional Edge Attribution Patching, to identify           ticular, we study state-spaces/mamba-370m, a 370-million-\n              a Mamba IOI circuit. Our contributions provide        parameter Mamba model pretrained (Gu & Dao, 2023) on\n                   initial evidence that circuit-based mechanistic in-       The Pile (Gao et al., 2020). We chose this model as it is[cs.LG]          terpretability tools work well for the Mamba ar-         the smallest Mamba model with good performance (∼96%\n                 chitecture.                                            accuracy on our templates) on the Indirect Object Identifica-\n                                                                             tion (IOI) task (Wang et al., 2023).\n\n                                                                       In particular, for the IOI task, we:          1. Introduction\n\n             If we care about using interpretability on new models, we       1. Show multiple lines of evidence suggesting layer 39 is\n          should know: Will interpretability techniques generalize to         a bottleneck:\n        new architectures?                                                       (a) Zero and Resample ablation (Section 3.1.4) ex-\n                                                                             periments point to layer 39 and layer 0.        To investigate this question, we apply existing mechanis-\n             tic interpretability techniques to a new model developed           (b) We compute greedy minimal subsets of layers\n            after most interpretability techniques: Mamba. Mamba is             allowed to transfer information across tokens (“to-\n          a State Space Model (SSM), a type of recurrent neural net-            ken cross-talk”). These always include layer 39\n        work (Gu & Dao, 2023). Mamba is the result of years of               (but layer 0 only 18% of the time).\n        work on language modeling with state space models (Gu                                                                                 2. Provide evidence that the convolution on layer 39 shiftsarXiv:2407.14008v2            et al., 2020; 2022; Fu et al., 2023), and is one of many                                                           name data to the next token position.\n        new RNN-like architectures (Beck et al., 2024; Peng et al.,\n          2023; Gu & Dao, 2023; Lieber et al., 2024). These RNNs       3. Modify the representations used by layer 39 using aver-\n          have scaling competitive with Transformers, unlike LSTMs         ages of activations, resulting in overwriting one output\n         (Kaplan et al., 2020). Because it is a recurrent network, it       name with another (Rimsky et al., 2024). These re-\n          only needs to store hidden states from the previous token,          sults suggest that in the SSM of layer 39, entity names\n           resulting in faster inference. The recurrence is also linear         are linearly represented, with different representations\n          (and thus associative) over token position, which permits          for the first and second time (or sentence) the names\n            further optimizations. See Appendix for architecture details.        appear in IOI.\n\n         While we are the first to focus on finding circuits in Mamba,      4. Provide multiple lines of evidence that layer 39 writes\n                                                                           outputs into only the final token position:\n              1Independent 2FAR AI.\n                                                                                       (a) Resample ablation on the hidden state and the\n                                                                                values added to the residual stream.\n\n                                                         1\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n                                                                    2.1. State Space Model (SSM)\n\n                                                Mamba’s SSM block can be written as mapping a 1D space\n                                                                 to a N-dimensional state space, then back to a 1D space:\n\n\n                                                                                   [N]   [N,N] [N]     [N,1] [1]\n                                                                         ht = A ht−1 + B xt              (1)\n\n                                                                                             [N]                                                                                               [1,N]                                                                                                                                 [1,1]                                                                                                                  [1]\n                                                                                 yt =                                                                                  ht +                                                 C                                               D  [1]x t                 (2)\n\n\n                                                                                                                       [N]\n                                                            In Mamba A is diagonal, so we will just write it as A and\n                                                   do an element-wise product “⊙”.\n\n                                                  Each layer does E of these in parallel. A has a separate\n                                                          value for each e, and is encoded as an [E, N] matrix. We\n                                                       can denote Ae as the N-sized entry for stream e, giving us,\n\n\n                                                                                [N]   [N]     [N]      [N,1] [1]\n                                                                                   ht,e =Ae ⊙ ht−1,e + B xt,e            (3)\n\n                                                                                            [N]                                                                                             [1,N]                                                                                                                                [1,1]                                                                                                              [1]\n                                          =                                                +                                                                                        yt,e                                                C  ht,e                                               D  [1]x t,e                (4)\n\n\n                                                                  Finally, Ae, B, and C depend on the SSM input, and so\n                                                          gain a subscript t. B also gains a subscript e through the\n                                                                                                                      [1]\n                                                               variable time-step, ∆t,e. The final SSM expressions are:\n\n\n                                                                                [N]    [N]      [N]      [N,1] [1]\n                                                                                  ht,e =At,e ⊙ ht−1,e + Bt,e xt,e           (5)\n\n                                                                                           [N]                                                                                             [1,N]                                                                                                                                [1,1]                                                                                                             [1]\n                                          =                                               +                                                                                       yt,e                                                                  Ct ht,e                                              D  [1]x t,e,                (6)\nFigure 1. Our hypothesis for the role of Layer 39. The representa-\ntions of n1–n3 and n4–n5 are interchangeable over positions.       where\n\n                                                                          [N]                  [1]\n                                                                    At,e = exp(− ∆t,e exp(Alog)e),               (7)\n      (b) A slight modification of the results of EAP gives\n                                                                          [N]      [1] [N]                    [N,E] [E]         us a subgraph that is capable of doing IOI while\n                                                                    Bt,e =∆t,e Bt,     with [N], Bt = WB xt,     (8)\n         only leaving layer 39’s final token’s outputs un-\n          patched.                                                        [N]   [N,E] [E]\n                                                         Ct = WC xt,                                  (9)\n                                                                                                      [1]               [E]   [E]       [1]\nIn addition, we show that ACDC and Edge Attribution Patch-           ∆t,e = softplus( xt · W e∆ + B∆e ),            (10)\ning (Syed et al., 2023) both result in sparse graphs when\napplied to IOI on Mamba, and provide the resulting compu-         [E,E]  [E]  [N,E] [N,E] [N,E]\ntational graphs.                                           with W ∆, B∆, WB , WC , Alog being learned parameters,\n                                                      and softplus(x) = log(1 + ex). This parameterization guar-\n                                                            antees that A < 1, and thus the hidden state does not ex-\n2. The Test Subject: Mamba                                                              plode.\n\nHere we provide a brief overview of the Mamba architecture.\nWe refer the reader to Ensign et al. (2024) for a more detailed     2.2. Architecture\n                        [A,B]\nintroduction. We use  v   to denote that variable v has   Mamba has multiple layers which each add to a residual\nshape [A, B].                                                stream. Each layer does:\n\n                                                2\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n                   [B,L,D]   [B,L,E]                           3.1.2. METRIC\n   • Project input  resid  to  x\n                   [B,L,D]   [B,L,E]                       There are many choices of metrics: KL-Divergence, Logit\n   • Project input  resid  to  skip                              Diff, Accuracy, Probability of the correct answer, etc. The\n                                                              best metric to use in general is an open question and may\n   • Conv over the time dimension, with a different filter                                                     be task specific. For IOI, Zhang & Nanda (2024) suggest\n     for each e ∈[E] (x = conv(x))                                                             the Normalized Logit Diff metric, as that helps propagate\n   • Apply non-linearity (silu) (x = silu(x))                 information missed by accuracy.  See the Appendix for\n                                                                 further details.\n   • y = SSM(x)\n                                                             3.1.3. COMPUTATIONAL GRAPH   • Gating: y = y ∗silu(skip)\n                                      We use the MambaLens library (see also Nanda & Bloom             [B,L,E]   [B,L,D]\n   • Project   y    to output                               (2022)) to intervene at different locations per experiment.\n\nWhere B is batch size, L is context length, D is embedding        • Section 4.1.1 “Resample Ablation” uses\ndimension, and silu(x) = x ∗sigmoid(x). See Figure 2.        blocks.{layer}.hook layer input.\n\n                                                                            • Section 4.1.2 “Layer Removal” uses\n3. Circuit-based Mechanistic Interpretability       blocks.{layer}.hook out proj.\n\nTo understand how large language models (LLMs) imple-        • Section 4.1.3 “Removing Cross Talk” uses\nment their emergent capabilities (Wei et al., 2022), we focus      blocks.{layer}.hook in proj.\non finding human-interpretable algorithms (Olah, 2022).\nThis involves representing models as computational graphs        • Section 4.2 “Layer 39 Uses Conv to Shift Names One\nand identifying circuits that are subsets of that computa-         Position Forward” uses\ntional graph. Ideally, each subgraph would also be anno-      blocks.{layer}.hook in proj and\ntated to describe the role of each component (Geiger et al.,      blocks.{layer}.hook conv.\n2021).\n                                                                            • Section 4.3 “Controlling model output by modifying\nFinding circuits that capture the behavior on all inputs is         representations on layer 39” uses\nintractable for large language models. Therefore, we study      blocks.{layer}.hook ssm input. For the\nbehavior on specific tasks.                                       cosine sim plots, it uses further hooks inside the ssm,\n                                                                described in Appendix.\n3.1. Problem Description\n                                                                            • Section 4.4 “Layer 39 moves information into only\nFollowing (Geiger et al., 2021; Conmy et al., 2023): we         the last token position” uses\nhave a behavior (task) that we would like to study, a metric      blocks.{layer}.hook h.{token pos},\nfor evaluating performance, and a coarse-grained compu-      blocks.{layer}.hook out proj, and the\ntational graph of the neural network on which we express        hooks used in Section D\nexplanations. We would like to find the minimal subgraph\n                                                                            • Both Section D “EAP” runs usethat attains a high enough metric score (where target metric\n                                              blocks.{layer}.hook layer input andscore is a hyperparameter), with an explanation of what\n                                              blocks.{layer}.hook layer output. Theyvariations in the data each graph component captures.\n                                                                    also use hook embed (described the next section) as\n                                                                   the input node, and3.1.1. IOI TASK\n                                              blocks.47.hook resid post as the output\nWe are studying the IOI task, initially examined by (Wang         node.\net al., 2023). Consider an example data point:\n                                                                            • The “ACDC” (Section D.2) run following the (non\nFriends Isaac, Lucas and Lauren went to          positional) EAP run uses all the hooks from the EAP\nthe office. Lauren and Isaac gave a                runs. It also uses blocks.{layer}.hook skip,\nnecklace to                                   blocks.{layer}.hook conv, and\n                                              blocks.{layer}.hook ssm input.\nThe model is asked to predict the next token, and the correct\nanswer is “ Lucas”. “ Lucas” is the Indirect Object    Note that blocks.{layer}.hook layer input is\nwe are trying to identify. See the Appendix for detailed    the residual stream before normalization.  If we patched\ndata-generation templates and corruption information.         directly on these, it would modify downstream values as\n\n                                                3\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n                                                                 Figure 3. Fully connected causal graph, using the additivity of\n                                                                    the residual stream. This is an example network with 4 layers,\n                                                              so the output node is blocks.3.hook resid post.  The\n                                                                                full network we study has 48 layers, so the output node is\n                                               blocks.47.hook resid post\n\n\n\n\n                                                               well. Thus, to patch only a single layer’s input, we clone\n                                                                      this value first.\n\n                                                             3.1.4. ABLATIONS\n\n                                                To identify which nodes and edges are important, we take\n                                                                  inspiration from causal inference (Pearl, 2009): ablate nodes\n                                                            of our computational graph and observe changes in the\n                                                               output.\n\n                                                        Replacing activations with zero (Olsson et al., 2022; Cam-\n                                                       marata et al., 2021) or the mean over many data points\n                                               (Wang et al., 2023) was initially used. However, these can\n                                                                    result in activations that are out of distribution (Chan et al.,\n                                                            2022). Resample ablation (Geiger et al., 2021), also known\n                                                            as interchange interventions and causal tracing, is a com-\n                                                 monly used alternative (Hanna et al., 2023; Heimersheim\n                                & Janiak, 2023; Wang et al., 2023; Conmy et al., 2023).\n                                                    Resample ablation begins by running a forward pass with aFigure 2. A single layer in the Mamba architecture, with hook\npoints listed in all the locations we intervene. Note that the SSM    corrupted prompt, then substitutes those corrupted activa-\ncontains further hook points, described in Section 4.3, “Controlling     tions into a forward pass run on the uncorrupted prompt.\nmodel output”. The “SSM” and “conv” components are affected\n                                                              In addition to resample ablation, in Mamba (and Transform-\nby previous time steps.\n                                                                          ers), the residual stream is a sum of outputs from every layer.\n                                                          This allows us to create an edge between every layer (Elhage\n                                                                      et al., 2021), see Figure 3.\n\n                                                To patch an edge this causal graph (Elhage et al., 2021)\n                                                        going from layer i to layer j, we can do:\n\n                                                4\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n                                                   where k ∈[0, . . . , ITERS −1], then compute the average\n                                                            of all these scores (ITERS is an int hyperparameter that\n     [B,L,D]      [B,L,D]   [B,L,D]        [B,L,D]\n patched inputj = inputj − outputi + corrupted outputi       determines how fine grained our approximation is, usually\n                                                        5-10 is large enough). The attribution is computed in using                                                     (11)\n                                                       Equation 12 like before, however, the forward pass for a\n                                                          given αk only “partially” applies every patch as follows:\nWe give the dimensions of our tensors in square brackets\n[ ] above the term.  Here, B is batch size, L is context\nlength, and D is embedding dimension. The outputi is        [B,L,D]       [B,L,D]     [1] [B,L,D]        [B,L,D]\nblocks.i.hook out proj,  computed  during  the    patched inputj = inputj − αk(outputi + corrupted outputi)\nsame forward pass  as  the  inputj  and patched inputj                                                         (13)\n(which  both  use blocks.j.hook layer input).\ncorrupted outputi  are  stored  values  from  a  sepa-                                                 Once we have these attribution scores, we can sort all edges\nrate  forward  pass  using  the  corrupted  prompt  and\n                                                  by their attribution and perform a binary search to find the\nblocks.i.hook out proj.\n                                                     minimal set of edges that achieves our desired metric.\n\n3.2. Semi-automatic Circuit Discovery                 The major downside of these automated methods is that\n                                                               (aside from token-level attributions) they do not yet assign\nInitially, finding circuits had to be done by hand: patch-                                                                  interpretations to nodes.\ning subsets of nodes and edges (known as path patching\n(Goldowsky-Dill et al., 2023)) until a circuit emerges. Sev-\neral methods have since been developed to automate this    4. Findings\nprocess. Subnetwork probing learns a mask over the graph                                                                    4.1. Layer 39 is Important\nusing gradient descent (Cao et al., 2021). Automated Circuit\nDisCovery (ACDC) starts from sink nodes and works back-  We have three lines of evidence suggesting layer 39 is im-\nwards to reconstruct the causal graph (Conmy et al., 2023).    portant. While two of these lines of evidence also suggest\nACDC requires a separate forward pass for every edge, and    layer 0 is important, we also provide evidence that token\nthis can be very time-consuming. Head Importance Score     cross-talk in layer 0 is not usually needed.\nfor Pruning (Michel et al., 2019), and more recently EAP\n(Edge Attribution Patching) (Syed et al., 2023), use the    4.1.1. RESAMPLE ABLATION\ngradient to approximate the contribution of all edges simul-\n                                                 To determine which layers are important, we will resample\ntaneously. In particular, EAP approximates the attribution\n                                                                ablate blocks.{layer}.hook layer input. To de-\nscores of an edge between layer i and layer j via:\n                                                         termine which tokens matter, we do this patch separately\n                                                                 for each (layer, token position) pair. This forces us to limit\n                                                                 to the three templates that share name token positions (one\n  [B,L,D]     [B,L,D]         [B,L,D]        [B,L,D]\n                                                     (12)    could use more templates and use semantic labels instead  attri7→j = (−outputi + corrupted outputi)∇ inputj\n                                                             of token positions, but that is left to future work).\n\nWhere ∇inputj is the gradient given from the backward    Because each corruption affects different positions, averag-\nhook made in these steps:                                   ing over them does not make sense. Thus, we show results\n                                                             separately for each corruption. We focus on 3-name tem-\n                                                                     plates. While 2-name templates are simpler, we find results  1. For  every  layer,  create  a  backward  hook  on\n                                                   from 2-name templates to be misleading as the task is too   blocks.j.hook layer input\n                                                             simple.\n  2. Run a forward pass that patches every edge                                                           Figure 4 shows that normalized logit diff changes most when\n                                                           patching layer 0 and 39.  3. Compute the metric on the resulting logits, and call\n   backward on the metric’s value.\n                                                             4.1.2. LAYER REMOVAL\n\nTo get an attribution for each edge, we sum attri7→j over the    Each  layer adds  to  the  residual  stream.    This  al-\nL and D axes, then mean over the B axis.                 lows us  to “remove” a  layer by  setting  this added\n                                                          value   to   zero,    i.e.,   zero-ablating   layer  outputs\nThis approximation can be improved by using integrated\n                                            (blocks.{layer}.hook proj out).    We   plot\ngradients (Marks et al., 2024; Sundararajan et al., 2017):\n                                                              probability of the correct answer, as there is no corrupted                   [B,L,D]  [1]            [1]\ncompute a separate attri7→j(αk) for an αk = k/(ITERS−1)    answer to compare to.\n\n                                                5\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 4. Displayed is 1 - (Normalized logit diff) for each (layer, position) patch, averaged over 80 data points. 0 corresponds to acting\nlike the uncorrupted forward pass, and 1 corresponds to acting like the corrupted forward pass. The y-axis is Layer, and the x-axis is\ntoken position. The corruptions can be observed by inspecting the token position labels. Each of the five plots correspond to different IOI\npatches.\n\n\n                                                             convolutional (conv) layer, and the SSM block. (For clarity,\n                                                                  in transformers, attention is where “token cross-talk” occurs,\n                                                            as that is where information can flow between different\n                                                         token positions)\n\n                                                              Putting corrupted data into the conv will also put corrupted\nFigure 5. Relative probability of the correct token when zero-    data into the SSM, as it is downstream of the conv. Thus,\nablating each layer’s outputs. Relative probability is the softmax    to remove a specific layer’s ability to have token cross-\nover the 4 logits from prompt and corruption names. The clean     talk, we can apply resample ablation to that layer’s conv\nmodel gets 83%.                                              inputs (blocks.{layer}.hook in proj) at all posi-\n                                                                      tions. Because we also patched convs in previous positions,\n                                                             the SSM will only have information about the corrupted\nIn Figure 5, we again see that, layers 0 and 39 are crucial     input.\nparts of the circuit that cannot be removed.\n                                                                           If we patch every layer before L in the manner above, this\nWe also find that by repeatedly removing the layer that de-   removes any information about previous tokens at layer\ncreases accuracy the least, about half of the layers can be    L. However, if we only patch some previous layers, the\nremoved with minimal impact on accuracy. We replicated    previous tokens can have influence: a previous layer could\nthis layer removal robustness on GPT-2-Small (Radford   move two tokens into the same position, and then a later\net al., 2019). This might be seen as evidence for the residual     layer could process those token interactions in place.\nstream having a privileged basis that is consistent between\n                                                            Also, note that this does not completely remove “cross talk”,layers. However, it is also consistent with there being multi-\n                                                                                             it only removes cross talk that is specific to the uncorruptedple distinct spaces (for example, embed-0, 0-39, 39-out), or\n                                                    prompt (i.e., cross talk that is needed for outputting thelayers being simultaneously compatible with multiple dif-\n                                                                correct answer). Cross talk that occurs in both uncorruptedferent spaces. See Belrose et al. (2023) for more discussion\n                                                    and corrupted prompts will still occur. This is somewhaton this “privileged basis” perspective.\n                                                           acceptable because we only care about task-relevant cross\n                                                                        talk.4.1.3. REMOVING TOKEN CROSS-TALK\n                                                    Given these two disclaimers, we still feel this is a usefulIt would be useful to know where information travels be-\n                                                      proxy for “removing cross talk”.tween tokens, as opposed to just modifying the representa-\ntions in place. We conduct an experiment to find a small set    Now, start with patching all layers’ cross talk, then “unpatch”\nof layers that do this “token cross-talk”.                       the layer that improves accuracy the most. This is repeated\n                                                                    until accuracy is about 0.9, resulting in a “minimal crossThere are two ways in which a layer at a specific token\n                                                                     talk circuit” that can perform the task. We do this separatelyposition can affect future positions (“token cross-talk”): the\n\n\n                                                6\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 6. Out of all (corruption, template) pairs, the proportion of\ntimes a given layer was in the minimal cross talk circuit.\n\n\nfor each (corruption, template) pair.\n\nIn Figure 6, we see that Layers 39 and 15 appear in every\nminimal circuit found. Layer 15 seems worthy of investi-\ngation in future work, as these two also stand out in EAP.\nInspecting the logs, Layer 39 is always the first layer added\nand has a large effect.\n\nIn 82% of these minimal circuits, Layer 0 did not appear.\nThis is strong evidence that for the majority of (corruption,\ntemplate) pairs, computation Layer 0 does is in-place and    Figure 7. Cosine Similarity between the current token’s contribu-\nnot cross talk.                                                                     [B,E,N][B,E,1]\n                                                                         tion to h (which is  Bi¯     xi    , each i is on the x axis), and the\nIn Transformers, it is suspected that layer 0 is responsible                 [B,E,N]\n                                                              hidden state (  hj   each j is on the y axis)\nfor multi-token embeddings (Nanda et al., 2023). These\nresults suggest something else happens in Mamba. However,\nbecause all of our prompts use single token names, it is        • If hypothesis 2 were true, we should see the -1 conv\npossible that these capabilities are simply not needed for          slice at token position + 1 have a large value.\nthis task (but still exist).\n\n                                                          Figure 8 supports hypothesis 2, that Layer 39 uses conv to\n4.2. Layer 39 Uses Conv to Shift Names One Position\n                                                                       shift names one position forward.\n   Forward\n                                      We did some tests to investigate multi-token names and\nWhen examining the hidden state, we can display the cosine                                                     found the cosine similarity plots always have lines at the\nsimilarity of a token’s contribution to the current state with                                                              position after the first token of the name (possibly other\nfuture (and previous) hidden states. This allows us to see                                                               layers handle multi-token names). It is also worth nothing\nhow much the value was “kept around” (see Appendix for                                                                    that we see the horizontal lines for entities, not just names.\nmore information on the hooks used here).\n                                       We do not yet know why this shifting behavior occurs, and\nAs this is not causal, it should not be relied on too heavily.                                                             leave that question for future work.\nThe structure seems to be name-dependent; we show three\nrepresentative examples in Figure 7.\n                                                                    4.3. Controlling Model Output by Modifying\nWhat stands out is that the horizontal lines are one token        Representations on Layer 39\nafter each name. This could either mean that 1) A previous\n                                      We hypothesize that the representations in the SSM are\nlayer shifted the tokens over, or 2) Layer 39 shifted the\n                                                                 linear because, on a single layer, the mechanism it has to\ntokens over using the conv.\n                                                     add or remove information from tokens is linear in h.\nTo distinguish between these, we can do resample ablation                                      [1]\non the individual conv “slices”: The conv can be seen as   We tried to visualize ∆t,e adding or removing information\nfour E-sized “slice” vectors for each (-3,-2,-1,0) relative     to various parts but did not find it very insightful. Instead,\ntoken position, that are multiplied (element-wise) by the     to investigate whether the internal representation of Layer\ncorresponding E-sized token representations.              39 SSM is linear, we do the following:\n\n\n   • If hypothesis 1 were true, we should see the 0 conv       1. Create  a  large  IOI  dataset.    For  each  data\n      slice at token position + 1 have a large value.                   point,   store  the  activations  of  each  name’s\n\n                                                7\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n                                                                 Figure 9. Proportion of data where logit of the corrupted name\n                                                                                   is higher than the logit of original name, using the two methods\n                                                                 described in Section 4.3. The x-axis is the position the average\n                                                      was computed from, the y-axis is the position being substituted.\n                                                      To substitute into the fourth and fifth positions, we substitute the\n                                                                       correct answer (instead of a patched name).\n\n\n                                                     Having a separate average for each position also lets us test\n                                                                                  if token position is an important part of the representation.\nFigure 8. This is 1 - Normalized Logit Diff when patching on     If it is, we should expect that “John” at name position 2\nthe given conv slice. 0 corresponds to acting like uncorrupted, 1    should not be easily substituted for “Mary” at name position\ncorresponds to acting like corrupted. The x-axis is conv slices (-2,     0.\n-1, then 0) for layer 39. The y-axis is token position; observe the\nlabels to see which corruption was used.                            Instead, in Figure 9 we find that the first three name positions\n                                                                are compatible, while the fourth and fifth positions are much\n                                                                    less compatible.\n    blocks.39.hook ssm input.   We  use  the\n     activation at the token position one after the name,    4.3.1. COMPATIBILITY OF FIRST THREE NAME\n     because of the shifting behavior we observed earlier.          POSITIONS\n\n  2. For each name, average the representations. Store a   The compatibility of the first three name’s representations\n     separate average for, say, “John” in the first position,    could either suggest:\n    “John” in the second position, etc. We use enough\n     data points that each (name, position) pair gets 50-100        • The IOI circuit does not store positional information in\n     values to average over.                                        the first three names, or\n\n  3. Replace Method: To write a different name, simply sub-        • There are circuits to handle incorrectly encoded posi-\n      stitute the SSM input at that position with the averaged          tional information, which got activated and handled\n     value from a different name.                               our patching well despite having incorrect positional\n                                                                  data\n  4. Subtract and Add Method Instead of substituting, sub-\n      tract the current name’s average and add the substituted                                                            Distinguishing between these is left for future work.\n     name’s average.\n\n                                                             4.3.2. INCOMPATIBILITY OF FOURTH AND FIFTH NAME\nWe find that the Replace Method works adequately, while         POSITIONS\nthe Subtract and Add Method works surprisingly well,\n                                                        Consider one of our data points: “Friends Isaac, Lucas and\nchanging the logits to the desired output more than 95%\n                                                      Lauren went to the office. Isaac and Lucas gave a necklace\nof the time.\n                                                              to” (answer is “ Lauren”)\nOne thing to note: It was possible that the SSM was using\n                                        We see that when names occur in the fourth or fifth position,\nthe representations from the name’s token position, as well\n                                                                                            it is the second time they occur in the prompt.\nas the name’s token position + 1. Patching on conv slices\nwas initial evidence this did not occur, and the efficacy of   One hypothesis is that the conv sees a period and encodes\nthis replacement procedure provides further evidence that     that in the name representation. However, while the model\nthis is not the case.                              we study (mamba-370m) has four conv slices, we find that\n\n                                                 8\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\nthe conv slice attending to the -3 position is always zero,    Instead, we will just sum over the D dimension and mean\nlikely due to a bug in the Mamba training code. Thus, it    over the B dimension, giving us an attribution for every\ncan only attend to the previous 2 positions in practice (the    (edge, position). See Appendix D.\nthird conv slice is for attending to the current position). This\n                                                The results of EAP further emphasize the importance of\nmeans that the name in the fifth position’s representation is\n                                                     Layer 39. However, there is also significant activity else-\nnot distinguishable from the name in the third position by\n                                                    where that merit further analysis.\nthe conv.\n\nThus, some token cross-talk must be happening in a layer                                                        6. Future Work\nbefore 39. As mentioned above, for 82% of (corruption,\ntemplate) pairs, cross-talk in layer 0 is not needed. So while    There are still many open questions we have about the IOI\nthese experiments provide strong evidence that layer 39 is a     circuit in mamba-370m. Future work can focus on:\nbottleneck, more circuit analysis is needed.\n\n                                                                            • Analysis of what cross talk is done before layer 39\n4.4. Layer 39 moves information into only the last token\n    position                                                             • Analysis of what the later layers are doing to decode\n                                                                   the answer encoded in the final token position\nWe can do resample ablation on the ssm hidden state via\nblocks.{layer}.hook h.{token pos}.                     • Training Sparse Autoencoders (SAEs) and using EAP\n                                                                     to make a feature circuit capable of doing the task, to\nFigure 10 shows that it only uses the hidden states one after\n                                                                  get a more fine grained analysis (similar to work in\nthe ablated token, in line with 4.2. We also see that hidden\n                                                      Marks et al. (2024))\nstate values are used all the way to the last token position.\nThis tells us that the answer-relevant information is moved        • Conducting similar analysis on other tasks (such as\ninto the last token position. However, it is possible that         docstring (Heimersheim & Janiak, 2023) or greater\ninformation is also sent to other, earlier positions as well.          than (Hanna et al., 2023))\n\nTo  test  for  this, we  can do  resample  ablation on\nblocks.{layer}.hook proj out, which is the value    7. Reproducibility\nadded to the residual stream at the end of each layer.                                                          All   code   for   experiments   can   be   found   at\nIn Figure 11 we see that only the last index is used.            https://github.com/Phylliida/investigating-mamba-ioi.\n                                                           All experiments were conducted on a RTX A6000.\nIn addition, positional EAP (Section D.3.5) suggests that\nother (non-last token) connections are important, as they\nare preserved in the set of edges that get 85%. However    8. Credits\ntheir attribution scores are very low. Manually removing                                                 The authors would like to thank the ML Alignment & The-\nall the non-last token connections going out from layer 39                                                          ory Scholars (MATS) program for providing a workspace to\nonly reduces accuracy from 85.2% to 83.8%, and reduces                                                         conduct this research, FAR AI Labs for compute, and LTFF\nnormalized logit diff from to 0.877 to 0.873. This suggests                                                                for funding. We would also like to thank Niels uit de Bos,\nthat either there is backup behaviour activated when those                                                                        Iv´an Arcuschin Moreno, Rohan Gupta, Thomas Kwa, Scott\npositions are patched, or that these connections are mostly                                                               Neville, Gonc¸alo Paulo and Joseph Bloom for the helpful\nspurious and not essential parts of the circuit.                                                              conversations.\nThese three lines of evidence together strongly suggest that\nthe task-relevant information provided by layer 39 is stored   References\nonly in the last token position.\n                                                                   Ali, A., Zimerman, I., and Wolf, L. The hidden attention of\n                                             mamba models, 2024.\n5. Positional Edge Attribution Patching\n  (Positional EAP)                                  Beck, M., P¨oppel, K., Spanring, M., Auer, A., Prudnikova,\n                                                               O., Kopp, M., Klambauer, G., Brandstetter, J., and\nHere we describe a simple modification to EAP that allows                                                               Hochreiter, S. xlstm: Extended long short-term mem-\nus to have token-level edge attributions.                                                                    ory, 2024. URL https://arxiv.org/abs/2405.\n                                      [B,L,D]                04517.\nTypically, in EAP, after we compute attri7→j we sum over\nthe L and D dimensions, then take the mean over the B    Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky,\ndimension to get an attribution for each edge.                             I., McKinney, L., Biderman, S., and Steinhardt, J. Elicit-\n\n\n                                                9\n\n                               Investigating the Indirect Object Identification circuit in Mamba\n\n  ing latent predictions from transformers with the tuned     Geiger, A., Lu, H., Icard, T., and Potts, C. Causal ab-\n   lens, 2023.                                                      stractions of neural networks, 2021. URL https:\n                                             //arxiv.org/abs/2106.02997.\nCammarata, N., Goh, G., Carter, S., Voss, C., Schubert, L.,\n  and Olah, C. Curve circuits. Distill, 2021. doi: 10.23915/    Goldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A.\n   distill.00024.006. https://distill.pub/2020/circuits/curve-      Localizing model behavior with path patching, 2023.\n   circuits.\n                                                              Grazzi, R., Siems, J., Schrodi, S., Brox, T., and Hutter, F. Is\nCao, S., Sanh, V., and Rush, A. Low-complexity prob-      mamba capable of in-context learning?, 2024.\n  ing via finding subnetworks. In Proceedings of the\n  2021 Conference of the North American Chapter of       Gu, A. and Dao, T. Mamba: Linear-time sequence model-\n  the Association for Computational Linguistics: Hu-          ing with selective state spaces, 2023.\n  man Language Technologies, pp. 960–966, Online,\n  2021. Association for Computational Linguistics.        Gu, A., Dao, T., Ermon, S., Rudra, A., and Re, C. Hippo:\n   doi: 10.18653/v1/2021.naacl-main.74. URL https:       Recurrent memory with optimal polynomial projections,\n  //aclanthology.org/2021.naacl-main.74.      2020.\n\nChan, L., Garriga-Alonso, A., Goldowsky-Dill, N.,         Gu, A., Goel, K., and R´e, C. Efficiently modeling long\n  Greenblatt, R., Nitishinskaya, J., Radhakrishnan,            sequences with structured state spaces, 2022.\n  A., Shlegeris, B., and Thomas, N.  Causal scrub-\n                                                    Hanna, M., Liu, O., and Variengien, A. How does gpt-  bing: A method for rigorously testing interpretabil-\n                                                    2 compute greater-than?: Interpreting mathematical   ity hypotheses.  Alignment Forum, 2022. URL\n                                                                           abilities in a pre-trained language model, 2023.  https://www.alignmentforum.org/posts/\n  JvZhhzycHu2Yd57RN/causal-scrubbing-a-\n                                                     Heimersheim, S. and Janiak, J. A circuit for Python doc-\n  method-for-rigorously-testing.\n                                                                    strings in a 4-layer attention-only transformer, 2023.\nConmy, A., Mavor-Parker, A. N., Lynch, A., Heimer-       URL https://www.alignmentforum.org/\n                                            posts/u6KXXmKFbXfWzoAXn/a-circuit-  sheim, S., and Garriga-Alonso, A. Towards automated\n                                           for-python-docstrings-in-a-4-layer-   circuit discovery for mechanistic interpretability, 2023.\n                                             attention-only.\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,\n  N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T.,    Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\n  DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds,       Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and\n   Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L.,        Amodei, D. Scaling laws for neural language models,\n  Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan,      2020.\n   J., McCandlish, S., and Olah, C. A mathematical frame-\n                                                              Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J.,  work for transformer circuits. Transformer Circuits\n                                                        Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y.,  Thread, 2021. URL https://transformer-\n                                                           Shalev-Shwartz, S., Abend, O., Alon, R., Asida, T.,  circuits.pub/2021/framework/index.\n                                                     Bergman, A., Glozman, R., Gokhman, M., Manevich,  html.\n                                                               A., Ratner, N., Rozen, N., Shwartz, E., Zusman, M.,\nEnsign, D., Paulo, G., and Garriga-alonso, A. Ophiol-        and Shoham, Y. Jamba: A hybrid transformer-mamba\n  ogy (or, how the mamba architecture works), 2024.          language model, 2024.\n URL https://www.lesswrong.com/posts/\n  TYLQ8gAMAmpeFcwXN/ophiology-or-how-       Mallen, A., Brumley, M., Kharchenko, J., and Belrose,\n  the-mamba-architecture-works. Accessed:       N. Eliciting latent knowledge from quirky language\n  2024-04-09.                                             models, 2024.\n\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,    Marks, S., Rager, C., Michaud, E. J., Belinkov, Y., Bau,\n  and R´e, C. Hungry hungry hippos: Towards language        D., and Mueller, A. Sparse feature circuits: Discovering\n  modeling with state space models, 2023.                  and editing interpretable causal graphs in language\n                                                           models, 2024.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\n   Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,    Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Lo-\n   Presser, S., and Leahy, C. The Pile: An 800gb dataset        cating and editing factual associations in gpt, 2023.\n  of diverse text for language modeling. arXiv preprint     URL https://arxiv.org/abs/2202.05262.\n  arXiv:2101.00027, 2020.                                  Accessed: 2024-7-8.\n\n                                                10\n\n                               Investigating the Indirect Object Identification circuit in Mamba\n\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads      Rimsky, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger,\n   really better than one? In Wallach, H. M., Larochelle,         E., and Turner, A. M. Steering llama 2 via contrastive\n  H., Beygelzimer, A., d’Alch´e-Buc, F., Fox, E. B., and         activation addition, 2024.\n  Garnett, R. (eds.), Advances in Neural Information\n                                                     Sharma, A. S., Atkinson, D., and Bau, D. Locating and\n  Processing Systems 32: Annual Conference on Neural\n                                                                  editing factual associations in mamba, 2024.\n  Information Processing Systems 2019, NeurIPS 2019,\n  December 8-14, 2019, Vancouver, BC, Canada, pp.        Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribu-\n  14014–14024, 2019. URL https://proceedings.      tion for deep networks, 2017.\n  neurips.cc/paper/2019/hash/\n                                                       Syed, A., Rager, C., and Conmy, A. Attribution patching  2c601ad9d2ff9bc8b282670cdd54f69f-\n                                                           outperforms automated circuit discovery, 2023.  Abstract.html.\n                                                                 Torres, A. Othello Mamba: Evaluating the mamba architec-\nNanda, N. and Bloom, J.  Transformerlens. https:                                                                  ture on the othellogpt experiment, 2024. URL https:\n  //github.com/TransformerLensOrg/                                            //github.com/alxndrTL/othello_mamba.\n  TransformerLens, 2022.\n                                                Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B.,\nNanda, N., Rajamanoharan, S., Kram´ar, J., and               and Steinhardt, J. Interpretability in the wild: a circuit\n  Shah, R.   Fact finding: Do early layers spe-                  for indirect object identification in GPT-2 small. In\n   cialise in local processing? (post 5), 2023. URL          The Eleventh International Conference on Learning\n  https://www.lesswrong.com/posts/               Representations, 2023. URL https://openreview.\n  xE3Y9hhriMmL4cpsR/fact-finding-do-        net/forum?id=NpsVSN6o4ul.\n  early-layers-specialise-in-local-\n                                                      Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,  processing. Accessed: 2023-12-22.\n                                                        Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D.,\nOlah, C. Mechanistic interpretability, variables, and the        Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O.,\n  importance of interpretable bases. https://www.        Liang, P., Dean, J., and Fedus, W. Emergent abilities of\n  transformer-circuits.pub/2022/mech-           large language models, 2022.\n  interp-essay, 2022.                                                     Zhang, F. and Nanda, N. Towards best practices of activa-\n                                                                   tion patching in language models: Metrics and methods,Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,\n                                                          2024.  N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,\n  A., et al.  In-context learning and induction heads,\n  2022. URL https://transformer-circuits.\n  pub/2022/in-context-learning-and-\n  induction-heads/index.html.        Appendices\n\nPaulo, G., Marshall, T., and Belrose, N. Does transformer\n   interpretability transfer to rnns?, 2024.               A. IOI Task Details\n\nPearl, J.  Causality.  Cambridge University Press,       We use the 4 prompt templates from (Conmy et al., 2023):\n  2 edition, 2009. ISBN 978-0-521-89560-6.  doi:\n                                          Then, [NAME], [NAME] and [NAME] went to\n  10.1017/CBO9780511803161.\n                                          the [PLACE]. [NAME] and [NAME] gave a\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcad-    [OBJECT] to\n  inho, S., Biderman, S., Cao, H., Cheng, X., Chung,\n  M., Grella, M., GV, K. K., He, X., Hou, H., Lin, J.,      Afterwards [NAME], [NAME] and [NAME]\n  Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H.,   went to the [PLACE]. [NAME] and [NAME]\n  Mantri, K. S. I., Mom, F., Saito, A., Song, G., Tang, X.,   gave a [OBJECT] to\n  Wang, B., Wind, J. S., Wozniak, S., Zhang, R., Zhang,\n   Z., Zhao, Q., Zhou, P., Zhou, Q., Zhu, J., and Zhu, R.-J.   When [NAME], [NAME] and [NAME] arrived\n  Rwkv: Reinventing rnns for the transformer era, 2023.   at the [PLACE], [NAME] and [NAME] gave\n  URL https://arxiv.org/abs/2305.13048.    a [OBJECT] to\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,      Friends [NAME], [NAME] and [NAME] went\n  and Sutskever, I. Language models are unsupervised    to the [PLACE]. [NAME] and [NAME] gave\n  multitask learners. 2019.                        a [OBJECT] to\n\n                                                11\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 10. Displayed is 1 - (Normalized logit diff) for each (layer, position) patch, averaged over 80 data points. 0 corresponds to acting\nlike the uncorrupted forward pass, and 1 corresponds to acting like the corrupted forward pass. The y-axis is Layer, and the x-axis is\ntoken position. The corruptions can be observed by inspecting the token position labels. Each of the five plots correspond to different IOI\npatches.\n\n\n\n\n\n                          Figure 11. Same as Figure 10, but for blocks.{layer}.hook proj out\n\n\n\n\n\n                                                12\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\nIn resample ablation, there are many ways to corrupt a        • unpatched is a forward pass without our intervention\nprompt. We create a dataset choosing randomly from all          (the baseline forward pass)\npossible corruptions and locations of names that:\n                                                                            • corrupted is a forward pass without our intervention,\n                                                       where the prompt is modified to make B the correct\n  1. Replace all instances of a single name with another\n                                                          answer\n    name, and\n\n                                                                            • patched is a forward pass that patches edges (as de-  2. Change the output\n                                                                  scribed above)\n\nWhile we could patch two names at the same time, 1 simpli-\nfies the number of things being changed at the same time. 2    This results in a 1 when the model acts like the unpatched\nis necessary to determine if the patch had any effect.          forward pass, and 0 when the model acts like the corrupted\n                                                         forward pass. Note that it is possible to obtain scores outside\nThis results in the following 5 corruptions:                                                              the [0,1] range.\n\nCAB AB C                                       The abs is a novel addition by us. For data points where\nDAB AB D                                                 the model is incorrect, maximizing normalized logit diff\n                                                  would result in the model becoming more incorrect. This\nACB AB C                                              abs modification fixes that issue.\nADB AB D\n                                           C. SSM Hooks used\nABC AB C\nABD AB D                                            For     the     cosine     similarity      plots,    we\n                                                      used                 blocks.39.hook B bar,\nABC AB C                                  blocks.39.hook ssm input,                and\nABC AC B                                  blocks.39.hook h.{pos}\n\n                                                     See figure C for a detailed overview of the SSM nternals.\nABC AC B\nABC BC A\n                                            D. Automated Circuit Discovery Results\nIn each of these, the top line represents the uncorrupted\n                                       We use the following edges:\nprompt, and the bottom line represents the corrupted prompt.\nLetters correspond to names: the first three are the first three\nnames, the second two are the fourth and fifth names, and        • embed   7→    layer   input   (hook embed   7→\nthe last is the output. If two letters are the same, that means         blocks.i.hook layer input)\nthat those places share the same name. Otherwise, the names\n                                                                            • layer output 7→later layer input (blocks.i.hook proj outare different.\n                                                              7→blocks.j.hook layer input)\n\nB. Normalized Logit Diff                                       • layer output 7→output (blocks.i.hook proj out 7→\n                                                               blocks.47.hook resid post)\nNormalized Logit Diff is defined as:\n\n                                                                            • embed     7→     output    hook embed     7→\nmin_diff =\n                                                               blocks.47.hook resid post)\n    A_logit_corrupted - B_logit_corrupted\nmax_diff =\n    A_logit_unpatched - B_logit_unpatched Where output is the residual stream after the final layer has\npossible_range = abs(max_diff - min_diff) added its layer output.\n# prevent divide by zero                                       We do a few separate experiments.\npossible_range[possible_range == 0] = 1\nlogit_diff =                                                            D.1. EAP\n    A_logits_patched - B_logits_patched\nnormalized_logit_diff =                       At the most high level, we can run (integrated gradient) EAP\n    (logit_diff-min_diff)/possible_range   without positions. Using binary search to find the minimum\n                                                  number of edges to give us at least 85% accuracy results in\nwhere                                                        the following adjacency matrix:\n\n                                                13\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n                 Figure 12. Internals of the SSM block, we restrict this diagram to only the parts we are interested in\n\n\n                                                            D.2. ACDC for layer information\n\n                                       We run EAP without positions using the edges listed above,\n                                                    and use binary search to find the least edges needed to get\n                                         85% accuracy. We take the resulting graph and run ACDC\n                                                  on it with a thresh of 0.0001, with (non-positional) edges\n                                                                  for individual conv slices, the ssm, the skip connection, and\n                                                                            all the edges above.\n\n                                                          This allows us to get a hint at what parts each layer is using.\n\n                                                        This resulting circuit has an average normalized logit diff\n                                                            of 0.84 and achieves 88% accuracy on a held-out test set,\n                                                         so there is little loss in performance from doing this further\n                                                       prune (because the thresh is so low, most edges pruned\n                                                              are those that decrease ability to do the task, which is why\n                                                         accuracy has gone up).\nFigure 13. Integrated gradients EAP, minimum set of edges. A\n                                                 Of note, this reproduces the “Convs of layer 39 shift names\nblue dot means the edge is present. The y-axis is the input node,\n                                                                 to the next position” result from above.the x-axis is the output node\n                                                               Ideally we could do this with EAP and no longer need\n                                          ACDC, but but leave that for future work.\n\nWe also present the edge attributions here, and the corre-    D.3. EAP With Positions\nsponding actual effects on normalized logit diff (determined\n                                                             In the following:during the ACDC run below)\n\nThis is not very insightful, so we clamp values to let us see        • n1 means the first name in the prompt, n2 means the\nmore (the thresholds chosen by hand)                           second name in the prompt, etc.\n\n                                                14\n\n                    Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 14. Edge Attributions                                          Figure 15. Effects on normalized logit diff\n\n\n\n\n\n                                       15\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 16. ACDC results from inside layers. Each node is 1 if the edge is present, 0 if it is not. 0,-1, and -2 are the corresponding conv\nslices\n\n\n\n\n\n                                                16\n\n                         Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 17. Same as the above figure, however, each cell shows the decrease in normalized logit diff if that edge is patched\n\n\n\n\n\n                                           17\n\nInvestigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n   Figure 18. Same as above, however, values are clamped to 0.1\n\n\n\n\n\n                        18\n\nInvestigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n    Figure 19. Same above, however, values are clamped to 0.05\n\n\n\n\n\n                        19\n\n                               Investigating the Indirect Object Identification circuit in Mamba\n\n   • pos0 means the first token, pos1 means the second    D.3.3. LAYERS THAT ARE MISSING NAMES\n     token, etc. (these are used for non-name tokens)\n                                                                           If we consinder a layer as “having” a name if it received it\n                                                    from embed or layer 0, the following layers have n1-n5:\n   • out means the final token, where the answer is gener-\n     ated                                      And these are ones that are missing names:\n\n\nFor reference, here is a prompt:                                          Missing n1: 1, 5, 28, 32, 33, 34\n                                          Missing n2: 31, 33\npos0 <|endoftext|>                                          Missing n3: 31, 33\npos1 Then                                          Missing n4: 32, 34, 3, 40, 9\npos2 ,                                          Missing n5: 9, 31, 34, 40\nn1 Sally                                          Missing n1-n5: 41-47\npos4 ,\nn2 Martha\npos6 and                                        Of those, 1, 3, 5, 9 are only connected to 0/embed and\nn3 Edwin                                                 39. In particular, we have:\npos8 went\npos9 to\n                                          1: missing n1pos10 the\n                                             n1,n2,n3,n5 -> 39pos11 restaurant\n                                          3: missing n4pos12 .\n                                             n1-n5 -> 39n4 Edwin\n                                          5: missing n1pos14 and\n                                             n1-n5 -> 39n5 Sally\n                                          9: missing n4,n5pos16 gave\n                                             n1-n5 -> 39pos17 a\npos18 drink\nout to\n                                                          Otherwise, we have\n\nD.3.1. CONNECTIONS FROM EMBED\n                                          Missing n1: 28, 32, 33, 34\nEvery layer receives n1-n5, except:\n                                          Missing n2: 31, 33\n                                          Missing n3: 31, 33Missing n1: layers 1, 5, 8, 25, 28, 29,\n                                          Missing n4: 32, 34, 40     30, 32, 34, 36\n                                          Missing n5: 31, 34, 40Missing n2: 30, 31, 36\n                                          Missing n1-n5: 41-47Missing n3: layers 25, 27, 29, 30, 31,\n     34, 36, 38\nMissing n4: layers 3, 9, 25, 32, 34, 40                                          28, 31, 32, 33, 34, 40 all seem to be involved in\nMissing n5: layer 9, 25, 27, 31, 34, 37,                                                         a complex circuit, and have inputs from other layers.\n     38, 40\nMissing n1-n5: 33, 41-47                            Just examining the missing terms:\nTODO: Output?\n\nD.3.2. CONNECTIONS OF LAYER 0\n\nLayer 0 takes as input n1-n5, and sends n1-n5 to every layer,\nexcept:\n\nMissing n1: 1, 2, 3, 28, 31, 32, 33, 34\nMissing n2: 6, 7, 31, 33\nMissing n3: 2, 31, 33                     28 is missing n1\nMissing n4: 1, 2, 3, 9, 18, 21, 32, 34, 40\nMissing n5: 3, 9, 18, 31, 32, 34, 40      28 does not receive n1 from anyone\nMissing n1-n5: 4, 5, 41-47                    outputs n1 to 39\n\n\n                                                20\n\n                               Investigating the Indirect Object Identification circuit in Mamba\n\n                                          34 receives n4 from 32\n                                              does not output it\n\n                                          34 receives n5 from 28, 31\n                                              does not output it\n\n\n\n\n\n31 is missing n2, n3, n5\n\n31 does not receive n2 or n3 from anyone\n    output n2-n3 to 39\n                                          40 is missing n4, n5\n31 receives n5 from 29\n    outputs n5 to 34\n                                          40 does not receive n4-n5\n                                              does not output n4-n5\n\n                                                        D.3.4. CONNECTIONS TO 39\n\n                                              As expected, layer 39 stands out as noteworthy. Every layer\n                                                            before 39 has a connection to 39 for every name, with these\n                                                             exceptions:\n\n                                          Missing n1: layer 2\n32 is missing n1, n4                      Missing n2: layer 34\n                                          Missing n3: layer 34\n32 does not receive n1                    Missing n4: layers 1, 31, 32, 34\n   outputs n1 to 39                       Missing n5: layers 2, 31, 32, 34\n32 receives n4 from 28, 30\n   outputs n4 to 34                                 In addition, there are these extra connections to layer 39\n\n                                          pos6: layer 34\n                                          pos14: layers 31, 32, 34\n                                          out: layers  28, 29, 30, 33, 35, 37, 38\n\n                                               Where\n\n33 is missing n1-n3                                           • pos6 is the “ and” between n2 and n3\n\n                                                                            • pos14 is the “ and” between n4 and n5\n33 does not receive n1-n3\n   outputs n2-n3 to 39\n                                                        D.3.5. CONNECTIONS FROM 39\n   outputs n1 to 35\n                                          n1,n2,n3: layer 40\n                                          pos12: layer 43\n                                          pos14: layer 40\n                                          pos16: layers 40,41,43,44,45,46,47\n                                          pos18: layer 40\n                                          out: layers 43,45,46,47, and output\n\n\n                                                                            • pos12 is the “.”\n34 is missing n1, n4, n5\n                                                                            • pos14 is the “ and” between n4 and n5\n34 does not receive n1 from anyone\n                                                                            • pos16 is the “ gave” after n5\n    outputs n1 to 39\n                                                                            • pos18 is the object (for example, “ drink”)\n\n                                                21\n\n                               Investigating the Indirect Object Identification circuit in Mamba\n\nD.3.6. GRAPH AFTER HIDING 39, EMBED, AND 0\n\nKeeping the above in mind, once we hide those three nodes\nthe graph is quite readable. If we hide 35, it is even more\nreadable. We also plot layer 35, for reference.\n\n\n\n\n\n                                                22\n\n                                Investigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\nFigure 20. Positional EAP After hiding Embed, Layer 0, and Layer 39. Numbers correspond to names, pos14 means token in position 14,\nout means the final token\n\n\n\n\n\n                                                23\n\nInvestigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n            Figure 21. Same as above, but also hide 35\n                        24\n\nInvestigating the Indirect Object Identification circuit in Mamba\n\n\n\n\n\n            Figure 22. Layer 35 Positional EAP results\n\n\n\n\n\n                        25\n\n## Claim to Classify\n\"partially reverse-engineer the circuit responsible for the Indirect Object Identification task\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "ACDC and Edge Attribution Patching (EAP)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The phrase 'reverse-engineer the circuit responsible' uses definite article 'the' and functional attribution ('responsible for'), indicating R3 mechanistic claim. Methods used (ACDC/EAP) are R2 interventional techniques. 'Partially' doesn't hedge the mechanistic claim itself. Classic R2→R3 overclaim pattern matching codebook's 'THE circuit' example."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 19746,
      "completion_tokens": 336,
      "total_tokens": 20082
    },
    "finish_reason": "stop"
  }
}