{
  "timestamp": "2026-02-19T10:55:45.101222+00:00",
  "paper_id": "2601.03595",
  "claim_id": "2601.03595-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** ‚Üí Does context make clear the author means \"is linearly decodable from\"?\n     - YES ‚Üí Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO ‚Üí Code as **R3** (default mechanistic reading)\n   - **YES** ‚Üí Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result ‚Üí Code as **R2**\n     - Mechanism ‚Üí Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** ‚Üí Code as **R3**\n   - **NO** ‚Üí Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper ‚Üí Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications ‚Üí Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** ‚Üí Does the paper claim the component is the *unique* controller?\n     - YES ‚Üí Code as **R3**\n     - NO ‚Üí Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** ‚Üí Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing ‚Üí \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching ‚Üí \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering ‚Üí \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE ‚Üí \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention ‚Üí \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation ‚Üí \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" ‚Üí still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" ‚Üí code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2‚Üí3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2‚ÜíR3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2‚ÜíR3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2‚ÜíR3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2‚ÜíR3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2‚ÜíR3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 ‚Üí Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2‚ÜíR3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2‚ÜíR3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2‚ÜíR3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2‚ÜíR3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2‚ÜíR3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2‚ÜíR3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1‚ÜíR3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kram√°r et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) ‚Üí R3\n   - Check for uniqueness language (the, only) ‚Üí R3\n   - Check for storage/encoding language ‚Üí R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model ‚Üí R1\n   - If intervention but not per-instance counterfactual ‚Üí R2\n   - If establishes unique/necessary mechanism ‚Üí R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") ‚Üí still R3, note hedge in confidence\n   - Multi-method papers ‚Üí use highest-rung method that directly supports claim\n   - Implicit claims from narrative ‚Üí code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2601.03595\nTitle: Controllable LLM Reasoning via SAE-Based Steering\nFull text:\n            Controllable LLM Reasoning via Sparse Autoencoder-Based Steering\n\n\n       Yi Fang1,2*, Wenjie Wang1, Mingfeng Xue3, Boyi Deng1*, Fengli Xu2,4, Dayiheng Liu3, Fuli Feng1\n                                  1University of Science and Technology of China,\n                      2Zhongguancun Academy, 3Alibaba Group, 4Tsinghua University\n\n\n\n\n\n                          Abstract                                                                                    Next Reasoning Steps\n                Large Reasoning Models (LRMs)  exhibit               Problem: In triangle ùê¥ùêµùê∂, ùê¥ùê∑     No controlling     Wait,the calculationlet‚Äôs checkiswhetherright ‚Ä¶\n                 human-like cognitive reasoning strategies (e.g.,                      isFindthe thealtitudelengthfromof ùê¥ùêµùê∑to andùêµùê∂ ‚Ä¶      + Problem        Wait, the problem asks for\n                   backtracking, cross-verification) during reason-               area of triangle ùê¥ùêµùê∂.             Understanding     both area and lengths ‚Ä¶\n                  ing process, which improves their performance                    Reasoning Process           + Procedural      Wait, to check the answer,2026                                                                                                                                                               First, let‚Äôs use the                  Planning              I next need to ‚Ä¶               on complex tasks. Currently, reasoning strate-                     Heron‚Äôs formula to find\n                    gies are autonomously selected by LRMs them-                       theAreaarea= ‚Ä¶‚Ä¶                 + Multi-Perspective   Wait, let‚Äôs go back to the\n                    selves. However, such autonomous selection                     == 84.sqrt(7056)                      Verification       last steps ‚Ä¶Jan                                                                                                                                Therefore, the answer\n                   often produces inefficient or even erroneous rea-                 may be 84.                  + Backtracking     Wait, let‚Äôs solve it in\n7                                                                                                                                    Wait, ‚Ä¶                                          another way ‚Ä¶\n                  soning paths. To make reasoning more reliable\n                and flexible, it is important to develop methods                      Error: requirement.Missed the 'length'        + ReasoningHypothesis      Wait,‚Ä¶ thesupposeanswer wouldwe have‚Ä¶\n                   for controlling reasoning strategies. Existing\n                methods struggle to control fine-grained rea-\n                 soning strategies due to conceptual entangle-       Figure 1: An illustration of reasoning strategy control.[cs.AI]          ment in LRMs‚Äô hidden states. To address this,      By deliberately controlling the LRM‚Äôs strategy selection,\n             we leverage Sparse Autoencoders (SAEs) to de-      we can flexibly intervene and correct its reasoning path\n                compose strategy-entangled hidden states into a      when a flaw emerges.\n                   disentangled feature space. To identify the few\n                    strategy-specific features from the vast pool of\n             SAE features, we propose SAE-Steering, an ef-                                                          2025; Marjanovi¬¥c et al., 2025; Pan et al., 2025).\n                      ficient two-stage feature identification pipeline.\n                                                       Such reasoning strategies improve the accuracy and\n                 SAE-Steering first recalls features that amplify\n                                                               robustness of LRMs on challenging tasks (Snell                   the logits of strategy-specific keywords, filter-\n                  ing out over 99% of features, and then ranks        et al., 2025; Zaremba et al., 2025). These LRMs\n                   the remaining features by their control effec-      autonomously select reasoning strategies during\n                    tiveness. Using the identified strategy-specific       reasoning. However, such autonomous reasoning\n                   features as control vectors, SAE-Steering out-       often produces inefficient or even erroneous rea-\n                 performs existing methods by over 15% in con-                                                              soning paths (Chen et al., 2025; Wang et al., 2025).\n                       trol effectiveness. Furthermore, controlling rea-\n                                                     To improve the reliability and flexibility of reason-arXiv:2601.03595v1           soning strategies can redirect LRMs from er-\n                                                                      ing, external guidance is promising. For example,                 roneous paths to correct ones, achieving a 7%\n                   absolute accuracy improvement.                    as illustrated in Figure 1, if an LRM misinterprets\n                                                                   the problem but pursues a flawed verification path,\n          1  Introduction                                   external guidance can redirect it to re-examine the\n                                                         problem statement, correcting the error. Therefore,            Large Reasoning Models (LRMs), such as GPT-\n                                                               developing methods for deliberate control over rea-           o1 (OpenAI, 2025) and DeepSeek-R1 (Guo et al.,\n                                                             soning strategies is crucial.             2025), employ a ‚Äúthink-then-answer‚Äù paradigm,\n              explicitly generating intermediate reasoning pro-      Existing control methods fall into two categories:\n             cesses before deriving final answers.   Within   prompt-based and activation-based. Prompt-based\n             these reasoning processes, LRMs exhibit human-   methods control the LRM‚Äôs reasoning by incor-\n              like cognitive reasoning strategies such as self-   porating instructions either in the  initial user\n             correction and cross-verification (Gandhi et al.,   prompt (Zhou et al., 2024) or during intermedi-\n                                                                      ate reasoning stages (Wu et al., 2025; Zhang et al.,                *Work done when Yi Fang and Boyi Deng were interns at\n               Alibaba Group.                                       2025). However, these methods lack direct control\n\nover the LRM‚Äôs internal generative process, which    ing candidates on a small validation set, selecting\nresults in frequent instruction-following failures, es-   the most effective features for final application. Ex-\npecially when reasoning context is long or instruc-    tensive evaluations demonstrate that SAE-Steering\ntions conflict with pre-trained behaviors (Qi et al.,    consistently outperforms baselines by over 15% in\n2025). Activation-based methods offer more direct    control effectiveness across various reasoning tasks\ncontrol by deriving a control vector to modify the   and LRM architectures. Moreover, SAE-Steering\nLRM‚Äôs hidden states during generation (Venhoff   can correct erroneous reasoning paths in LRMs,\net al., 2025). This control vector is typically com-   improving absolute accuracy by 7%, highlighting\nputed as activation differences between contrastive    the potential of strategic control. In summary, the\npairs exhibiting or lacking a target behavior (Tang    contributions of this work are threefold:\net al., 2025). However, curating contrastive pairs                                                              ‚Ä¢ We leverage SAEs to disentangle and identify\nthat cleanly isolate a single strategy is difficult. As                                                         strategy-specific features, overcoming the con-\na result, the derived control vectors are prone to                                                     cept entanglement problem inherent in control-\nconcept entanglement (Elhage et al., 2022; Yang                                                          ling reasoning strategies.\net al., 2025b), inadvertently capturing features of                                                              ‚Ä¢ We   propose   SAE-Steering   to   identify\nmultiple strategies and hindering precise control.                                                         strategy-specific   features,   addressing   the\n  To overcome this limitation, we propose lever-                                                    challenge  of  efficient and  effective  feature\naging Sparse Autoencoders (SAEs) (Huben et al.,                                                         selection from the massive set of SAE features.\n2024) to decompose the LRM‚Äôs hidden states into                                                              ‚Ä¢ Extensive experiments validate SAE-Steering‚Äôs\na sparse set of interpretable and monosemantic fea-                                                       effectiveness and robustness in controlling rea-\ntures (Bricken et al., 2023). Specifically, a well-                                                 soning strategies and demonstrate its potential\ntrained SAE projects the low-dimensional, strategy-                                                  use in correcting erroneous reasoning paths.\nentangled hidden states of an LRM into a high-\ndimensional, disentangled feature space. This pro-\n                                       2  Preliminary\njection aims to isolate strategy-specific features in\nthe high-dimensional space, thereby providing dis-   Strategy Selection.  LRMs employ a diverse\nentangled control vectors for reasoning strategy                                                 range of cognitive reasoning strategies during their\ncontrol. However, the high-dimensional feature                                                   reasoning processes, making a comprehensive eval-\nspace introduces a new challenge: identifying the                                                    uation of control over each one impractical. There-\nfew strategy-specific features from tens of thou-                                                         fore, we focus on five representative reasoning\nsands of learned SAE features. Existing selection                                                        strategies that are frequent, effective, and widely\nmethods (Galichin et al., 2025), which rely on dif-                                                    studied in prior work (Gandhi et al., 2025; Zhong\nferential activation strength across contrastive pairs,    et al., 2024). As illustrated in Figure 1, the five\nface the same difficulty in constructing clean con-                                                         strategies we selected are:\ntrastive pairs. Furthermore, high activation does\n                                                              ‚Ä¢ Problem Understanding: rephrasing the prob-\nnot guarantee effective control, leading to the se-\n                                            lem statement, clarifying its constraints and in-\nlection of many spurious or ineffective features.\n                                                          terpreting the given information.\n  To address this, we propose identifying effec-\n                                                              ‚Ä¢ Procedural Planning: defining a sub-task or\ntive features by directly assessing their capacity\n                                                        outlining a plan for the subsequent reasoning.\nto steer target strategy generation.  Considering\n                                                              ‚Ä¢ Backtracking: identifying a mistake in previousexhaustively evaluating all features is computa-\n                                                     reasoning and attempting to correct it or revert totionally infeasible, we introduce SAE-Steering,\n                                                  a prior step.a two-stage pipeline for efficiently identifying and\n                                                              ‚Ä¢ Multi-Perspective Verification: verifying a con-selecting effective strategy control features, bal-\n                                                      clusion by applying a different method or exam-ancing cost and precision. As shown in Figure 2,\n                                                      ining specific cases.SAE-Steering first employs a low-cost, high-recall\n                                                              ‚Ä¢ Hypothesis Reasoning: making an assumptioncriterion to rapidly filter out over 99% of irrelevant\n                                                     or posing a \"what if\" scenario to explore possi-features by identifying those that amplify the logits\n                                                                  bilities or test certain conditions.of strategy-specific keywords‚Äîa strong indicator\nof control potential.  It then applies a more com-      Importantly, this selection is purely for evalua-\nputationally intensive evaluation to quantitatively    tion convenience; our method is general and appli-\nassess and rank the control effectiveness of remain-   cable to control other reasoning strategies as well.\n\n                (a) Architecture of SAE                       (b) Feature Identification Pipeline of SAE-Steering\n\n\n                                                                                Multi-Perspective Verification\n                                                                            Corpus\n                                                                                                                            Strategy Keywords\n                                                                                           1. Another idea ‚Ä¶                  Extraction\n        Layer ùíç+1                                                                        2. Let‚Äôs try different approach ‚Ä¶\n                                 ‚Ä¶\n                                                                Encoder ‚Ä¶    Decoder                                 Stage1                     Stage2\n         Layer ùíç    ùíô                            #ùíô                                Amplify the logit of                Controlling reasoning\n                       ‚Ä¶      strategy keywords?                 strategy effectively?\n                                Strategy-specific Feature\n                                                                           (30k ‚Üí<100)   ‚Ä¶       (<100 ‚Üítop k)\n                          SAE\n\n                                                                        All SAE Features                 Recalled Features                  Strategy-specific\n                                                                                                                                 Features\n\nFigure 2: (a) Overview of the SAE architecture. (b) Feature identification pipeline of SAE-Steering. Numbers below\nthe arrows indicate the approximate count of features retained.\n\n\nTask Formulation. We next formalize the task    in Figure 2a, an SAE is an encoder‚Äìdecoder archi-\nof controlling reasoning strategies. In a standard    tecture trained to represent an input activation as a\nautoregressive setting, an LRM generates the next    sparse linear combination of learned feature direc-\ntoken yt based on the prefix Y<t = {y1, . . . , yt‚àí1}.    tions. Given a residual stream activation x ‚ààRN,\nThe LRM processes Y<t through its L transformer      it encodes x into a sparse feature activation vector\nlayers, producing a sequence of residual stream   z ‚ààRM (M ‚â´N) and reconstructs it as ÀÜx:\nactivations {x1t , x2t , . . . , xLt }. In vanilla decoding,\n                                                 z = œÉ (Wenc(x ‚àíbdec) + benc) ,     (2)these activations remain unmodified. Strategy con-\n                                                                                  ÀÜx = Wdecz + bdec,                    (3)trol departs from this by injecting a control vector\n‚àÜx‚Ñìat a specific layer ‚Ñì:                                           where Wenc ‚ààRM√óN, benc ‚ààRM, Wdec ‚àà\n                      ‚Ñì                       RN√óM, bdec ‚ààRN, and œÉ is an activation func-               x‚Ä≤t = x‚Ñìt + Œ± ¬∑ ‚àÜx‚Ñì,            (1)\n                                                            tion.\nwhere Œ± ‚ààR is a coefficient controlling the steer-     The SAE is trained to satisfy a dual objective: (1)\ning strength.  The activation x‚Ä≤t ‚Ñì then replaces   minimizing the reconstruction error ‚à•x ‚àíÀÜx‚à•22 and\nxt‚Ñì and is propagated through the remaining lay-   (2) enforcing a sparsity restriction, which dictates\ners, influencing the final generation. By repeat-    that the reconstruction must be constructed from\ning this intervention for T consecutive tokens,   only a few active latent directions1. This training\nthe LRM produces a steered trajectory Y ‚Ä≤ =    process enables the SAE to approximate x as a\n{y‚Ä≤t, y‚Ä≤t+1, . . . , y‚Ä≤t+T‚àí1}. Given a pre-specified rea-   sparse linear combination of the decoder columns:\nsoning strategy s, the goal of reasoning strategy                M\ncontrol is to construct ‚àÜx‚Ñìsuch that the steered           x ‚âàbdec + X zi(x)fi           (4)\ntrajectory Y ‚Ä≤ exhibits the desired strategy s.                                  i=1\n\n                                             where each column fi of Wdec corresponds to a dis-3  Method\n                                                  entangled and interpretable latent direction, which\nThis section details our method in two parts. First,  we refer to as a feature throughout the paper. The\nwe describe how we control reasoning strategies by    scalar zi(x) is the i-th component of the activation\nmanipulating strategy-specific features identified    vector z, indicating the activation strength of each\nin the SAE (Section 3.1). Second, we introduce    feature for the input x.\nSAE-Steering, a two-stage pipeline developed to   A key benefit of this decomposition is that\neffectively identify these features from the vast    the sparsity objective encourages monosemantic-\nSAE feature pool (Section 3.2).                           ity (Bricken et al., 2023): each learned feature tends\n                                                         to capture a single concept, significantly mitigating\n3.1  Strategy Control with SAE Features\n                                                     the concept entanglement (Huben et al., 2024).\nWe  train SAEs  to  disentangle  and  identify\n                                                  1We enforce sparsity via a Top-K activation function,\nstrategy-specific features, which then serve as the                                                       which only retains the K largest activation values and sets the\ncontrol vectors for strategy control. As illustrated     rest to zero, following (Gao et al., 2025).\n\n  We then identify the strategy-specific feature fs    a strategy-specific corpus by manually identifying\n(one of the learned fi directions) that is associated    reasoning segments in the LRM‚Äôs responses. We\nwith the target reasoning strategy s (see identifica-   then extract the most frequent words from each cor-\ntion methods in Section 3.2). By using fs as the   pus to serve as strategy keywords (see Appendix B\ncontrol vector ‚àÜx in Eq. 1, we steer the LRM‚Äôs    for the keywords list and identification details).\nreasoning strategy by repeatedly injecting fs into      Next, we estimate all SAE features‚Äô potential\nthe residual stream activations at the SAE-trained    logit contribution to strategy keywords using logit\nlayer ‚Ñìfor the next T tokens generation:              lens (nostalgebraist, 2020). Logit lens is a method\n                                        commonly used to estimate the logit contribution\n  x‚Ä≤t+k‚Ñì = x‚Ñìt+k + Œ± ¬∑ fs,  k = 0, 1, . . . , T ‚àí1      of hidden state activations to each token in the\n                                                 (5)    vocabulary. We adapt it to SAE features as follows:\n                                                    Formally, let U ‚ààRN√óV be the LRM‚Äôs unem-\nwhere Œ± is the steering strength. The selection of Œ±\n                                              bedding matrix (i.e., the weight matrix of the LM\nis a trade-off: excessively large values cause repet-\n                                                    head), mapping hidden activations to logits over a\nitive outputs (Fu et al., 2021), while excessively\n                                                vocabulary of size V . Let Wdec ‚ààRN√óM be the\nsmall values fail to control effectively. For each\n                                  SAE decoder matrix. As described in Section 3.1,\nfeature, we determine Œ± by searching downwards\n                                              each column of Wdec corresponds to a disentan-\nfrom an empirically chosen high value, iteratively\n                                                 gled feature direction fi ‚ààRN. We compute the\ndecreasing it until repetitive generation is elimi-                                                          logit contribution matrix L ‚ààRM√óV for all fea-\nnated (see Appendix A for details).\n                                                        tures via:\n3.2  Identification of Strategy-specific Features\n                                          L = W‚ä§decU,                (6)\nTo efficiently identify the few critical, strategy-\nspecific features from tens of thousands of learned   where the i-th row Li,: gives the logit contributions\nSAE features, we introduce SAE-Steering, a two-   of feature fi across the vocabulary. This compu-\nstage pipeline designed for both efficiency and pre-    tation requires only a single matrix multiplication,\ncision. The first stage employs a low-cost, high-   making it low-cost and efficient.\nrecall criterion to rapidly construct a compact can-    We aim to recall features that specifically and\ndidate set, while the second stage applies a more    significantly amplify strategy keywords, while\ncomputationally intensive, high-fidelity evaluation    avoiding those that amplify irrelevant tokens more\nto select the most effective features. As shown in    strongly than the keywords. To achieve this, we\nFigure 2b, SAE-Steering first recalls features that    extract the top-10 tokens with the highest logit con-\namplify the logits of strategy-specific keywords.    tribution for each feature and recall features sat-\nThis stage is low-cost and highly-efficient, filtering    isfying: (i) at least n of these tokens are strategy\nout 99% irrelevant features. Subsequently, SAE-   keywords, and (ii) each such keyword‚Äôs logit con-\nSteering evaluates and ranks the control effective-    tribution exceeds a threshold œÑ. This recall step\nness of remaining candidates on a small validation     is highly selective, narrowing the candidate pool\nset, selecting top-ranked feature for application.     from tens of thousands of features to several tens.\n\nStage 1: Recall based on logit estimation.  In   Stage 2: Rank based on Control Effectiveness.\nthe first stage, we efficiently distill a small set of    In the second stage, we evaluate and rank the can-\npromising candidates from tens of thousands of    didate features from Stage 1 to identify those with\nSAE features by selecting those that positively in-   the highest control effectiveness. This ranking is\nfluence the logits of strategy keywords. The guid-   based on their empirical performance on a small\ning hypothesis is that features which substantially    validation set P.\nincrease these keyword logits are more likely to      Formally, for each problem p ‚ààP with a given\nsteer the LRM toward the corresponding reasoning   response prefix Y<t, we generate two distinct T-\nstrategy.                                        token continuations2: (i) a baseline trajectory Y0,\n   Specifically, we first extract strategy keywords    generated via standard decoding, and (ii) a steered\nfollowing the approach of Galichin et al. (2025).    trajectory Y (j), generated using the candidate fea-\nThese keywords serve as a computationally effi-    ture fj as the control vector. An LLM judge then\ncient proxy to identify features potentially corre-                                                  2We set the sampling temperature to 0 to eliminate ran-\nlated with the target strategy. Briefly, we first create    domness as a confounding factor in our evaluation.\n\nassesses whether Y (j) more explicitly demonstrates   ‚ó¶Vector Steering (Venhoff et al., 2025), which\nthe target strategy s than Y0 3, yielding binary judg-      uses an LLM to annotate reasoning strategies for\nment Jp,j ‚àà{0, 1}. The control effectiveness of a      constructing contrastive datasets, then extracts\nfeature fj is then calculated as the control success      control vectors via contrast pairs.\nrate over the validation set:\n                                               Evaluation Protocol.  We evaluate control effec-\n                        1                       tiveness following the procedure described in Stage\n        Effectiveness(fj) = X Jp,j.      (7)\n                          |P|                 2 of Section 3.2.  Importantly, for feature selec-\n                           p‚ààP\n                                                        tion in Stage 2 of SAE-Steering, we use only GPT-\n                                            4o (OpenAI, 2024) as the judge. For test evaluation,This empirical ranking allows us to select the top-\n                                     we employ three LLM judges‚ÄîGPT-4o (OpenAI,ranked feature as fs for the target strategy s.\n                                                 2024), Gemini-2.5-flash (Comanici et al., 2025),\n4  Experiments                             and Deepseek-V3.2 (Liu et al., 2024)‚Äîto vote as\n                                                    judges. This majority voting mitigates individual\nIn this section, we conduct experiments to address                                                judge biases and ensures more reliable evaluation.\nthe following research question:                                We also test the agreement between LLM judges\n‚Ä¢ RQ1: Can our SAE-based steering method, lever-   and human annotators, which achieves a high agree-\n  aging the identified features, reliably control   ment rate of 0.82 (see Appendix C for details), con-\n LRMs‚Äô reasoning strategies?                      firming the reliability of LLM judges.\n‚Ä¢ RQ2:  How  effective  is SAE-Steering  for\n                                           Implementation   Details. We   train  TopK-\n  strategy-specific feature identification?\n                                     SAEs (Gao et al., 2025) (with K = 50) on the\n‚Ä¢ RQ3: Can we correct an LRM‚Äôs erroneous rea-\n                                                              last layer of DeepSeek-R1-Distill-Llama-8B (Guo\n  soning path by deliberately controlling its reason-\n                                                             et al., 2025) (hereafter referred to as R1-Llama-8B)\n  ing strategies?\n                                             and Qwen3-8B (Team, 2025b). For SAE-Steering\n                                                 hyperparameters, we set n = 2 and œÑ = 0.1 in4.1  Experiment Setup\n                                                 Stage 1, and continuation length T = 512 in Stage\nDatasets. We   train  our  SAEs  on   acti-                                                           2. For sampling, we set the temperature to 0 during\nvations  from  a  mixed  corpus  combining                                                    control  effectiveness  evaluations  to  eliminate\nLMSYS-CHAT-1M  (Zheng   et    al.,  2024)                                              confounding effects from sampling stochasticity.\nand OPENTHOUGHTS-114K  (Team,  2025a),                                              For error correction experiments, we adopt the\nfollowing prior work (Galichin et al., 2025). For                                                             officially recommended temperature of 0.6 and set\nthe evaluation of reasoning strategy control, we                                                     the maximum token length to 32,768.\nfirst randomly sample 50 responses from past\nAIME competitions (1983‚Äì2023) (AIME, 2025)    4.2  Control Effectiveness of SAE-Based\nas the validation set. We then evaluate control         Steering (RQ1)\neffectiveness on 200 randomly sampled responses                                          SAE-based steering outperforms baselines.  We\nfrom AIME‚Äô24 and 25 (AIME, 2025) and 200                                                       report the control effectiveness of different methods\nresponses from GPQA (Rein et al., 2023). GPQA                                                      in Table 1, from which we make the following\nis a science reasoning dataset spanning biology,                                                     observations:\nphysics, and chemistry, which we use to assess\nthe out-of-domain generalization capability of our    (1) Activation-based methods (Vector Steering\nstrategy-specific features.                          and SAE-Steering) consistently outperform\n                                                    prompt-based methods (Think Intervention) ex-\nBaselines.  We compare SAE-Steering with three                                                        cept in some cases within Hypothesis Reason-\nrepresentative control methods:                                                            ing, which demonstrates the superiority of di-\n‚ó¶Logit Boosting, which directly boosts the logits        rectly intervening in hidden states.\n  of strategy-specific keywords;                                                        (2) SAE-Steering significantly outperforms Vector\n‚ó¶Think Intervention (Wu et al., 2025), which in-                                                            Steering, with an average improvement of 15%.\n   serts human-crafted instructions into the middle                                    We attribute this to the disentangling properties\n  of the reasoning process;                                                       of SAEs, which mitigate the conceptual en-\n                                                    tanglement present in control vectors, thereby   3We provide the prompt and validate the reliability of LLM\nJudges in Appendix C.                                     enabling more precise strategy control.\n\n                                  R1-Llama-8B                 Qwen3-8B\n Dataset  Method                                                                        Average\n                     PU   PP  BK  MV  HR  PU   PP  BK  MV  HR\n\n           Logit Boosting      0.21   0.49   0.30   0.27   0.32   0.44   0.61   0.39   0.49   0.56    0.41\n          Think Intervention   0.56   0.49   0.21   0.21   0.39   0.62   0.81   0.12   0.23   0.61    0.43\n AIME\n           Vector Steering      0.69   0.82   0.67   0.48   0.34   0.74   0.85   0.55   0.51   0.52    0.62\n          SAE-Steering       0.88   0.86   0.69   0.76   0.41   0.92   0.92   0.78   0.70   0.65    0.76\n\n           Logit Boosting      0.28   0.68   0.29   0.39   0.56   0.43   0.79   0.40   0.47   0.63    0.49\n          Think Intervention   0.66   0.69   0.35   0.23   0.57   0.68   0.83   0.17   0.16   0.77    0.51\n GPQA\n           Vector Steering      0.77   0.90   0.61   0.52   0.51   0.89   0.89   0.80   0.55   0.72    0.72\n          SAE-Steering       0.94   0.90   0.78   0.93   0.70   0.94   0.95   0.81   0.82   0.89    0.87\n\nTable 1: Control effectiveness evaluation across five reasoning strategies: Problem Understanding (PU), Procedural\nPlanning (PP), Backtracking (BK), Multi-Perspective Verification (MV), and Hypothesis Reasoning (HR).\n\n\n(3) SAE features identified in the math domain    No steering:\n    demonstrate comparable effectiveness on sci-     Wait, m=132 is the first one with four primes. But let me check\n     entific reasoning tasks (GPQA), indicating the    m=132 again. Is there a smaller m?\n    generalizability of SAE features across differ-     Wait, m=105: 10931=17√ó643. Two primes.\n    ent reasoning domains.              ‚Ä¶\n                                                           Logit Boosting:\n(4) All methods achieve better control effective-                                                                       Wait, m=132 is the first one with four primes. But let me check\n    ness on GPQA than on AIME. This may be-    m=another value. For example, m=another number where the\n                                                                  expression could be composite with more factors.\n    cause the reasoning length of GPQA is much                                ‚Ä¶\n    shorter than AIME (7k vs. 15k tokens), which\n                                                       SAE-Steering:\n   makes control easier.                                       Alternatively, maybe I can think of another approach. Maybe\n                                                                     the original problem is similar to the Euler's prime-generating\nSAE features function beyond keyword amplifi-     polynomial but with a different constant term                                ‚Ä¶\ncation.  Importantly, although our SAE features\nare recalled by identifying features that amplify                                                       Figure 3: Case study: SAE-Steering changes reasoning\nthe logits of strategy-specific keywords, they en-    behavior while Logit Boosting only boosts keywords.\ncode deeper strategy concepts beyond keyword\npromotion.  This is evidenced by SAE-Steering\nachieving over 35% better control effectiveness    tion strength on keywords compared to other tokens\nthan Logit Boosting (Table 1). To illustrate this    in the reasoning context. We use ReasonScore to\nmore explicitly, we present a case study in Figure 3.    recall the same number of features as our Stage\nIn this case, boosting the logits of strategy key-   1 (143 for R1-Llama-8B and 357 for Qwen3-8B)\nwords like ‚Äúanother‚Äù does not genuinely change   and evaluate the precision of recalled features, i.e.,\nthe reasoning strategy. The LRM generates ‚Äúan-   the proportion of recalled features that successfully\nother‚Äù but still continues to verify its answer by    control reasoning strategies. As shown in Table 2,\ntesting different values of m. In contrast, SAE-   SAE-Steering outperforms ReasonScore by 28%\nSteering successfully guides the LRM to adopt the    in precision, demonstrating the superiority of logit-\nMulti-Perspective Verification strategy. Additional   based over activation-based feature identification.\nsteering examples are available in Figure 6.          Logits directly measure causal effects on outputs,\n                                                         better reflecting features‚Äô actual control capability\n4.3  Effectiveness of SAE-Steering for Feature    than activation strength.\n      Identification (RQ2)\n\nLogit-based  recall   is  more  precise  than\n                                                           R1-Llama-8B  Qwen3-8B\nactivation-based.  We compare the effectiveness\nof identifying strategy-specific features of SAE-     ReasonScore        0.33          0.27\nSteering with existing activation-based methods,     SAE-Steering       0.61          0.52\nspecifically ReasonScore (Galichin et al., 2025).\nReasonScore recalls features based on their activa-           Table 2: Precision of recalled features.\n\n    120      Problem Understanding\n             Procedural Planning                                                             0.9\n    100       Backtracking\n              Multi-Perspective Verification\n     80       Hypothesis Reasoning                                                          0.8Features\nof 60                                                                                                                                                                                                                                                                                   Effectiveness0.7\n     40                                                                                                     Problem Understanding\n                                                                                                                    Procedural Planning\n                                                                                                     0.6                                   Backtracking     20Number                                                                                                                                             Control                                       Multi-Perspective Verification\n                                                                                                                 Hypothesis Reasoning      0                                                                                             0.5\n          0        5       10       15       20       25       30       35                  0        5       10       15       20       25       30       35\n                         Layer                                                 Layer\n\n      Figure 4: Recalled features across layers.               Figure 5: Control effectiveness across layers.\n\n\nLayer-wise analysis of feature identification.    sert a ‚Äúwait‚Äù token at the end of the initial, flawed\nIn the main experiments, we train SAEs on the last    reasoning to induce further thinking. During this\nlayer of LRMs. Here we further investigate how   extended reasoning phase, we apply SAE-Steering\nthe identification of strategy-specific features varies    to control the LRM‚Äôs subsequent reasoning strat-\nacross layers. Due to computational constraints, we    egy. To select the most appropriate strategy for\nlimit this analysis to Qwen3-8B. We first examine    different problems, we train a strategy router (Ap-\nthe presence of strategy-specific features across lay-   pendix E). We compare our approach with two\ners by measuring the number of features recalled   common self-correction baselines: (1) Budget Forc-\nby Stage 1 of SAE-Steering. As shown in Figure 4,   ing (Muennighoff et al., 2025), which only extends\nstrategy-specific features are rare in shallow layers    reasoning without strategic guidance; and (2) Self-\n(0, 3, 7, 11) but prevalent in deeper layers (23, 27,   Reflection (Shinn et al., 2023), which prompts the\n31, 35), which is consistent with prior findings that  LRM to reflect on its previous answer and generate\nabstract reasoning concepts are primarily encoded    a new response.\nin the deeper layers of LRMs (Yun et al., 2021; Shi\n                                                  Results.  The error correction results are shown\net al., 2025).\n                                                      in Table 3, from which we make the following\n  We next investigate the control effectiveness of                                                     observations:\nthese features across layers by reporting the aver-\nage control effectiveness of the top-3 features. As    (1) The highest correction rate is only 33%, with\nshown in Figure 5, shallow layers exhibit poor con-     MATH500 achieving the highest rate and\ntrol effectiveness, while layers beyond 20 demon-     AIME the lowest. This demonstrates the diffi-\nstrate strong and relatively stable control effective-       culty of error correction, and harder tasks are\nness. This suggests that reasoning strategy control        also more difficult to correct.\nshould be applied to middle-to-late layers for opti-                                                        (2) Budget Forcing outperforms Self-Reflection\nmal results.                                              on all datasets except GPQA on Qwen3-8B,\n                                                      demonstrating the advantage of continuous rea-\n4.4  Correcting Erroneous Reasoning Paths\n                                                       soning. By continuing from the current state\n     via Strategy Control (RQ3)\n                                                           rather than reprocessing the entire reasoning\nSetup.  To demonstrate the practical value of strat-        process, Budget Forcing maintains better focus\negy control, we test whether controlling reason-      on error correction.\ning strategies can correct errors even after the\nLRM has already generated a wrong answer‚Äî     Model        Method       MATH500  AIME25  GPQA\na more challenging setting than simple genera-                       Self-Reflection     0.1394     0.0123   0.0262\ntion. Specifically, we sample incorrect LRM re-     R1-Llama-8B  Budget Forcing    0.2121     0.0123   0.0626\nsponses on the MATH500 (Lightman et al., 2023),                   SAE-Steering     0.3313     0.0552   0.1196\nAIME25 (AIME, 2025), and GPQA (Rein et al.,                       Self-Reflection     0.0993     0.0411   0.0749\n                                                        Qwen3-8B     Budget Forcing    0.1773     0.0685   0.0484\n2023), and attempt to correct them during an ex-                   SAE-Steering     0.2411     0.1370   0.1154\ntended reasoning process (See Appendix D for\nsample details and dataset statistics). Following    Table 3:  Error correction rates across methods and\nBudget Forcing (Muennighoff et al., 2025), we in-    datasets.\n\n(3) SAE-Steering consistently outperforms Budget    to learn monosemantic features in an unsupervised\n    Forcing across all LRMs and datasets, with an   way, eliminating annotation requirements while\n    average absolute accuracy improvement of 7%.    better disentangling conceptually-entangled hidden\n    This suggests that deliberately controlling rea-    states.\n    soning strategies enables more effective error\n    correction.                                 Sparse Autoencoders.  Mechanistic interpretabil-\n                                                            ity seeks to understand the internal workings of\n5  Related Work                     LRMs by analyzing the structure and function of\n                                                          their learned representations (Singh et al., 2024;\nReasoning Strategies in LRMs.  Early studies\n                                                     Gantla, 2025). A primary tool in this field is SAEs,\nattempt to improve LLM performance on com-\n                                           which decompose high-dimensional LRM activa-\nplex tasks by designing prompts to guide reasoning\n                                                      tions into a sparse set of latent features (Bricken\nprocesses (Shinn et al., 2023; Zhou et al., 2024).\n                                                             et al., 2023; Huben et al., 2024). These features of-\nRecent research demonstrates that LLMs trained\n                                                      ten correspond to human-interpretable concepts, en-\nwith rule-based reinforcement learning can unsu-\n                                                     abling researchers to probe and manipulate specific\npervisedly develop human-like cognitive reason-\n                                                     aspects of LRM behavior (Deng et al., 2025; Yang\ning strategies such as self-reflection and backtrack-\n                                                            et al., 2025b). For example, (Galichin et al., 2025)\ning (Liu et al., 2024). These advancements have\n                                                    leveraged SAEs to identify features associated with\nled to the emergence of current LRMs. During in-\n                                                    reasoning. In their method, reasoning features are\nference, LRMs produce long Chains-of-Thoughts\n                                                     selected as those that activate more strongly on\n(CoTs) that explore diverse reasoning paths while\n                                                    reasoning-related keywords (e.g., ‚Äòwait‚Äô, ‚Äòalterna-\ncontinuously verifying previous steps (Marjanovi¬¥c\n                                                          tively‚Äô) than on other tokens. However, high acti-\net al., 2025). In this process, LRMs employ di-\n                                                   vation strength does not necessarily indicate con-\nverse human-like cognitive reasoning strategies\n                                                               trol capacity, causing such methods to recall many\nsuch as backtracking and multi-perspective veri-\n                                                       features that show superficial correlations with rea-\nfication. The use of these reasoning strategies im-\n                                                soning behaviors but lack the ability to effectively\nproves their accuracy and robustness in solving\n                                                    control fine-grained reasoning strategies. Instead,\ncomplex problems (Gandhi et al., 2025; Snell et al.,\n                                     we recall features through their direct logit contri-\n2025; Muennighoff et al., 2025).\n                                                   butions to strategy-specific tokens, enabling more\nControllable LLM Reasoning.  Many works at-   precise recall of features with genuine control ef-\ntempt to control LRM reasoning behavior. These    fectiveness.\nmethods can be categorized into prompt-based and\nactivation-based. Prompt-based methods (Wu et al.,   6  Conclusion\n2025; Yang et al., 2025a; Zhang et al., 2025) insert\nhuman-scripted instructions into intermediate rea-   In this work, we leverage strategy-specific features\nsoning steps, mimicking the LRM‚Äôs style to seam-   of SAEs to achieve fine-grained control over LRMs‚Äô\nlessly steer its reasoning trajectory.  Activation-   reasoning strategies. SAEs decompose strategy-\nbased methods directly modify hidden states using    entangled hidden states into disentangled strategy-\ncontrol vectors derived from contrastive activation    specific features. To identify these strategy-specific\nanalysis. For example, many works (Sheng et al.,    features from the vast pool of SAE features, we\n2025; Tang et al., 2025; Lin et al., 2025) obtain    propose SAE-Steering, a two-stage feature identifi-\ncontrol vectors by contrasting activations between    cation pipeline that balances efficiency and preci-\nshort and long CoT responses. However, such pairs    sion. SAE-Steering first employs a logit estimation\nfail to isolate individual strategies, causing control   method to rapidly recall candidate features that\nvectors to suffer from concept entanglement and    amplify strategy-specific keywords, then ranks the\nonly enable coarse-grained control (e.g., reason-    control effectiveness of remaining features through\ning length) rather than fine-grained strategy control.    intervention experiments on a validation set. Exten-\nVenhoff et al. (2025) address this by using LLM    sive experiments demonstrate the effectiveness and\njudges to annotate each reasoning step with fine-   robustness of our identified features in controlling\ngrained strategy labels, then contrasting activations    reasoning strategies. Furthermore, we demonstrate\nacross labels. However, accurate step-level annota-    that controlling reasoning strategies can redirect\ntion is challenging. Conversely, we leverage SAEs  LRMs from erroneous paths to correct ones.\n\nLimitations                                     Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\n                                                       Carol Chen, Roger B. Grosse, Sam McCandlish,\nWhile SAE-Steering demonstrates promising re-      Jared Kaplan, Dario Amodei, Martin Wattenberg,\nsults, several limitations remain to be addressed      and Christopher Olah. 2022. Toy models of superpo-\n                                                                      sition. arXiv preprint arXiv:2209.10652.in future work.  First, due to computational con-\nstraints, we only evaluated five representative strate-                                                  Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei\ngies to demonstrate our method‚Äôs effectiveness. Fu-      Shi. 2021. A theoretical analysis of the repetition\nture work could investigate controlling other rea-      problem in text generation. In AAAI.\nsoning strategies. Second, we only demonstrate\n                                               Andrey V. Galichin, Alexey Dontsov, Polina Druzhin-the application of controlling reasoning strategies\n                                                                  ina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tu-\nin error correction scenarios. Future work could                                                                 tubalina, and Ivan V. Oseledets. 2025. I have covered\nexplore applying such control to a wider range of        all the bases here: Interpreting reasoning features\napplications. Third, we only attempted to correct       in large language models via sparse autoencoders.\nerroneous reasoning paths by enforcing LRMs to      arXiv preprint arXiv:2503.18878.\ncontinue reasoning and controlling subsequent rea-\n                                                 Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh,\nsoning strategies. Future work could explore guid-                                                  Nathan Lile, and Noah D. Goodman. 2025. Cogni-\ning the LRM at earlier stages‚Äîeither at the begin-       tive behaviors that enable self-improving reasoners,\nning or during intermediate steps‚Äîto dynamically        or, four habits of highly effective stars. Second Con-\n                                                            ference on Language Modeling.adjust the reasoning trajectory.\n\n                                                Sandeep Reddy Gantla. 2025. Exploring mechanistic\n                                                                    interpretability in large language models: Challenges,\nReferences                                                          approaches, and insights. In 2025 International Con-\n                                                           ference on Data Science, Agents & Artificial Intelli-AIME. 2025. Aime problems and solutions.\n                                                     gence (ICDSAAI).\nTrenton Bricken, Adly Templeton, Joshua Batson,\n   Brian Chen, Adam Jermyn, Tom Conerly, Nick   Leo Gao, Tom Dupr√© la Tour, Henk Tillman, Gabriel\n   Turner, Cem Anil, Carson Denison, Amanda Askell,     Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan\n   Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas       Leike, and Jeffrey Wu. 2025. Scaling and evaluating\n   Schiefer, Tim Maxwell,  Nicholas Joseph, Zac       sparse autoencoders. In ICLR.\n   Hatfield-Dodds, Alex Tamkin, Karina Nguyen, and\n  6 others. 2023. Towards monosemanticity: Decom-   Daya Guo, Dejian Yang, Haowei Zhang, Junxiao\n   posing language models with dictionary learning.      Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\n  Transformer Circuits Thread.  Https://transformer-      rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\n   circuits.pub/2023/monosemantic-                      Deepseek-r1: Incentivizing reasoning capability in\n   features/index.html.                                    llms via reinforcement learning.  arXiv preprint\n                                                        arXiv:2501.12948.\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He,\n   Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi                                                     Robert Huben, Hoagy Cunningham, Logan Riggs Smith,\n   Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang,                                                   Aidan Ewart, and Lee Sharkey. 2024. Sparse autoen-\n  Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. Do                                                         coders find highly interpretable features in language\n  NOT think that much for 2+3=? on the overthink-                                                       models. In ICLR.\n   ing of long reasoning models. In ICML. OpenRe-\n   view.net.                                                     Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\n                                                        Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nGheorghe Comanici, Eric Bieber, Mike Schaekermann,\n                                                     Wen-tau Yih. 2020. Dense passage retrieval for open-\n   Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Mar-\n                                                   domain question answering. In EMNLP (1). Associa-\n   cel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and\n                                                                tion for Computational Linguistics.\n  1 others. 2025. Gemini 2.5: Pushing the frontier with\n  advanced reasoning, multimodality, long context, and\n                                                               Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin,   next generation agentic capabilities. arXiv preprint\n                                           Roman Soletskyi, Shengyi Costa Huang, Kashif Ra-   arXiv:2507.06261.\n                                                                      sul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin,\nBoyi Deng, Yu Wan, Baosong Yang, Yidan Zhang, and      Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lam-\n   Fuli Feng. 2025. Unveiling language-specific fea-       ple, and Stanislas Polu. 2024. Numinamath.\n   tures in large language models via sparse autoen-\n   coders. In ACL (1), pages 4563‚Äì4608. Association    Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri\n   for Computational Linguistics.                        Edwards, Bowen Baker, Teddy Lee, Jan Leike,\n                                                   John Schulman, Ilya Sutskever, and Karl Cobbe.\nNelson Elhage,  Tristan Hume,  Catherine Olsson,      2023.  Let‚Äôs verify step by step.  arXiv preprint\n   Nicholas Schiefer, Tom Henighan, Shauna Kravec,      arXiv:2305.20050.\n\nZhengkai Lin, Zhihang Fu, Ze Chen, Chao Chen, Liang   Noah Shinn, Federico Cassano, Ashwin Gopinath,\n  Xie, Wenxiao Wang, Deng Cai, Zheng Wang, and      Karthik Narasimhan, and Shunyu Yao. 2023. Re-\n   Jieping Ye. 2025. Controlling thinking speed in rea-       flexion: language agents with verbal reinforcement\n  soning models. NeurIPS.                                  learning. In NeurIPS.\n\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,   Chandan Singh, Jeevana Priya Inala, Michel Galley,\n  Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi      Rich Caruana, and Jianfeng Gao. 2024. Rethinking\n  Deng, Chenyu Zhang, Chong Ruan, and 1 others.       interpretability in the era of large language models.\n  2024. Deepseek-v3 technical report. arXiv preprint      arXiv preprint arXiv:2402.01761.\n  arXiv:2412.19437.\n                                                        Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Avi-\nSara Vera Marjanovi¬¥c, Arkil Patel, Vaibhav Adlakha,       ral Kumar. 2025. Scaling LLM test-time compute\n  Milad Aghajohari, Parishad BehnamGhader, Mehar       optimally can be more effective than scaling parame-\n   Bhatia, Aditi Khandelwal, Austin Kraft, Benno Kro-       ters for reasoning. In ICLR. OpenReview.net.\n   jer, Xing Han L√π, Nicholas Meade, Dongchan Shin,\n  Amirhossein Kazemnejad, Gaurav Kamath, Marius   Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min,\n  Mosbach, Karolina Sta¬¥nczak, and Siva Reddy. 2025.     Xin Zhao, Binbin Hu, Ziqi Liu, and Zhiqiang Zhang.\n  Deepseek-r1 thoughtology: Let‚Äôs think about llm rea-      2025. Unlocking general long chain-of-thought rea-\n   soning. arXiv preprint arXiv:2504.07128.               soning capabilities of large language models via rep-\n                                                             resentation engineering.  In ACL (1), pages 6832‚Äì\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xi-      6849. Association for Computational Linguistics.\n  ang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\n   Zettlemoyer, Percy Liang, Emmanuel J. Cand√®s, and    Openthoughts Team. 2025a.   Openthoughts:  Data\n   Tatsunori Hashimoto. 2025.  s1: Simple test-time       recipes for reasoning models.\n   scaling. arXiv preprint arXiv:2501.19393.\n                                        Qwen Team. 2025b. Qwen3 technical report.\nnostalgebraist. 2020. Interpreting gpt: The logit lens.\n                                                A√§ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.  Less-Wrong (blog).\n                                                         Representation learning with contrastive predictive\n                                                          coding. arXiv preprint arXiv:1807.03748.OpenAI. 2024.   Gpt-4o system card.    Preprint,\n  arXiv:2410.21276.\n                                                        Constantin Venhoff, Iv√°n Arcuschin, Philip Torr, Arthur\n                                              Conmy, and Neel Nanda. 2025. Understanding rea-\nOpenAI. 2025.  Openai o1 system card.  Accessed:\n                                                        soning in thinking language models via steering vec-\n  2025-02-21.                                                                       tors. CoRR, abs/2506.18167.\n\nQianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shil-                                              Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu\n   ian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min                                                     Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li,\n  Zhang, Yulan Wu, and Liang He. 2025. A survey                                                  Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao\n   of slow thinking-based reasoning llms using rein-                                                   Mi, and Dong Yu. 2025. Thoughts are all over the\n   forced learning and inference-time scaling law. arXiv                                                             place: On the underthinking of o1-like llms. arXiv\n   preprint arXiv:2505.02665.                                                             preprint arXiv:2501.18585.\n\nYunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng   Tong Wu, Chong Xiang, Jiachen T. Wang, and Pra-\n   Liu, Bin Xu, Lei Hou, and Juanzi Li. 2025. AGEN-      teek Mittal. 2025. Effectively controlling reasoning\n  TIF: benchmarking instruction following of large      models through thinking intervention. arXiv preprint\n  language models in agentic scenarios.   CoRR,      arXiv:2503.24370.\n  abs/2505.16944.\n                                              Chenxu Yang, Qingyi Si, Mz Dai, Dingyu Yao, Mingyu\nDavid Rein, Betty Li Hou, Asa Cooper Stickland,      Zheng, Minghui Chen, Zheng Lin, and Weiping\n  Jackson Petty, Richard Yuanzhe Pang, Julien Di-     Wang. 2025a. Test-time prompt intervention.\n   rani, Julian Michael, and Samuel R. Bowman. 2023.\n  GPQA: A graduate-level google-proof q&a bench-   Jingyuan Yang, Rongjun Li, Weixuan Wang, Ziyu Zhou,\n  mark. CoRR, abs/2311.12022.                       Zhiyong Feng, and Wei Peng. 2025b. Lf-steering:\n                                                         Latent feature activation steering for enhancing se-\nLeheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao,      mantic consistency in large language models. arXiv\n  Changshuo Shen, Yi Zhang, Xiang Wang, and Tat-       preprint arXiv:2501.11036.\n  Seng Chua. 2025. On reasoning strength planning in\n   large reasoning models. NeurIPS.                 Zeyu Yun, Yubei Chen, Bruno A. Olshausen, and\n                                               Yann LeCun. 2021. Transformer visualization via\nWei Shi, Sihang Li, Tao Liang, Mingyang Wan, Gojun       dictionary learning: contextualized embedding as\n  Ma, Xiang Wang, and Xiangnan He. 2025. Route      a linear superposition of transformer factors.  In\n   sparse autoencoder to interpret large language mod-     DeeLIO@NAACL-HLT, pages 1‚Äì10. Association for\n   els. CoRR, abs/2503.08200.                           Computational Linguistics.\n\nWojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak,   then extract the top-20 most frequent words and\n   Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel    then perform a manual curation to select the key-\n   Dias, Eric Wallace, Kai Xiao, Johannes Heidecke,                                           words we identified as most representative of the\n  and 1 others. 2025.  Trading inference-time com-\n   pute for adversarial robustness.   arXiv preprint    target reasoning strategy. The final keywords lists\n   arXiv:2501.18841.                                  are shown in Table 4.\n\nXingsheng Zhang, Luxi Xing, Chen Zhang, Yanbing\n                                                                      Reasoning Strategy               High-Frequency Keywords\n   Liu, Yifan Deng, Yunpeng Li, Yue Hu, and Chenxu\n   Niu. 2025. Can we steer reasoning direction by think-       Problem Understanding        problem, question, statement, reads, says\n   ing intervention? In Findings of EMNLP 2025.              Procedural Planning                    let, need, planning, decomposition\n                                                                              Backtracking                              earlier, previous, initial, back\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle        Multi-Perspective Verification       another, example, case, approach\n   Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,       Hypothesis Reasoning           maybe, perhaps, assume, suppose, if\n  Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\n   Ion Stoica, and Hao Zhang. 2024. Lmsys-chat-1m:   Table 4: High-frequency keywords corresponding to\n A large-scale real-world LLM conversation dataset.   each reasoning strategy.\n   In ICLR. OpenReview.net.\n\nQihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu,\n  Liang Ding, and Bo Du. 2024. Achieving> 97% on\n                           C  Reliability of LLM Judges  gsm8k: Deeply understanding the problems makes\n   llms better solvers for math word problems. Frontiers\n   of Computer Science.                        To validate LLM judge reliability, we conducted\n                                                a human annotation study.  Specifically, we ran-\nPei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen,\n                                         domly sampled 200 steered outputs (40 per strat-  Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny\n  Zhou, Swaroop Mishra, and Huaixiu Steven Zheng.   egy) alongside their unsteered baselines. We then\n  2024. SELF-DISCOVER: large language models   asked three human annotators (Krippendorff‚Äôs al-\n   self-compose reasoning structures. In NeurIPS.                                             pha = 0.78) to evaluate whether the steered output\n                                         more explicitly demonstrates the target strategy\nA  Selection of Steering Strength\n                                                 than the baseline.  Taking human judgments as\nThe hyper-parameter Œ± determines the steering   ground truth, we evaluate the accuracy of LLM\nstrength during strategy control. An overly large Œ±    judges. As shown in Table 5, LLM judges achieve\ncan cause the LRM to generate repetitive outputs,   0.82 agreement with human annotations, indicating\nwhile an Œ± that is too small may yield negligible    reliable performance.\ncontrolling effects. We thus select an Œ± value that\nis as large as possible without inducing repetitive                                              Reasoning Strategy         Agreement\noutputs. Specifically, we use the validation set to\n                                               Problem Understanding            0.85determine Œ± for each feature. For each validation\n                                                    Procedural Planning               0.75sample, we first steer the feature with Œ± = 15 and\n                                                   Backtracking                     0.83check for repetitive outputs. If repetition occurs,\n                                                      Multi-Perspective Verification      0.85we decrease Œ± by one and re-steer. We repeat this\n                                                  Hypothesis Reasoning              0.8process until no repetition is detected. We then\nuse the average Œ± across validation samples as the     Average                          0.82\nsteering strength for the test set. The starting value\nof 15 was chosen empirically, as we found that    Table 5: Agreement between human annotators and\nhigher values frequently lead to repetitive outputs  LLM judges.\nfor most features.\n\nB  Extraction of Strategy Keywords\n                           D  Curation of Error Correction Dataset\nTo extract strategy keywords for each reasoning\nstrategy, we first construct a corpus for each reason-   To  sample  incorrect LRM  responses  from\ning strategy by sampling the responses of the LRM   MATH500, AIME25, and GPQA, we sample eight\nto a diverse set of problems and manually identify-   responses for each problem in these datasets and\ning the segments corresponding to each reasoning    retain only the incorrect ones. The final dataset\nstrategy. From each strategy-specific corpus, we     statistics are shown in Table 6.\n\n Model      MATH500  AIME25  GPQA    E.2  Implementation Details\n R1-Llama-8B     495       163      878      For each reasoning strategy, we select the top three\n Qwen3-8B        141        73      641       strategy-specific features with the best control ef-\n                                                     fectiveness on the validation set, yielding a total\n    Table 6: Statistics of Error Correction Dataset.       of fifteen features for the strategy router to choose\n                                                from. We include three rather than one feature\n                                                  per strategy because different features may be ef-\nE  Strategy Router                                                       fective in different contexts, providing the router\n                                                  with more flexibility to adapt to different reasoning\nE.1  Methods\n                                                      scenarios.\nTo steer LRMs‚Äô reasoning strategies from erro-     For the training of our strategy router, we use a\nneous paths to correct ones, we need to select ap-    training set composed of 919 problems from past\npropriate strategies based on the current reasoning  AIME competitions (1983‚Äì2023) (AIME, 2025)\ncontext. Reasoning strategies can be controlled ei-   and 4,000 problems from the ‚Äòaops_forum‚Äô source\nther manually or by an automatic strategy router.   of the NUMINAMATH-1.5 dataset (LI et al., 2024).\nHere we train a lightweight router via contrastive   For each problem, we sample eight initial re-\nlearning (van den Oord et al., 2018) to automati-   sponses. To empirically evaluate the effectiveness\ncally select effective strategies based on the current    of a feature fj on an incorrect response yi, we ap-\nreasoning context, thereby eliminating the need for    ply reasoning steering with fj to generate eight\nmanual intervention.                               responses and measure the proportion of them that\n   Specifically, we instantiate the strategy router as    successfully correct the initial error. This training\na bi-encoder architecture (Karpukhin et al., 2020).   data is strictly separated from our test sets in Sec-\nA context encoder, Ec(¬∑), embeds the current rea-    tion 4.4, ensuring no data leakage. Notably, GPQA\nsoning state (represented by the final token of the    represents an out-of-domain scenario, demonstrat-\nresponse prefix Y<t), and a feature encoder, Ef(¬∑),   ing our method‚Äôs generalization capability.\nprojects each strategy-specific feature fs into the\nsame representation space. The effective scores be-\ntween the context and a feature are then computed\nas the dot product of their respective embeddings:\n\n      score(Y<t, fs) = ‚ü®Ec(Y<t), Ef(fs)‚ü©    (8)\n\n\nThe  router   is  trained  using  the  InfoNCE\nloss (van den Oord et al., 2018), which encourages\nhigher effective scores for positive context‚Äìfeature\npairs and lower effective scores for negative ones:\n\nL(Y<t, f+s , f‚àís,1, . . . , f‚àís,M)\n                        escore(Y<t,f s+ )\n= ‚àílog                                                                   ,                +                ‚àí\n           escore(Y<t,f s ) + PMk=1 escore(Y<t,f s,k)\n                                                 (9)\n\n  where (Y<t, f+s ) is labeled as a positive pair if\nsteering with feature f+s  leads to a correct final an-\nswer. All other pairings for that context are treated\nas negative pairs. At inference time, for a given\ncontext Y<t, we compute score(Y<t, fs) for all can-\ndidate features fs and select the feature with the\nhighest effective score as the selected feature to\nsteer the LRM.\n\n    Problem:\n     In triangle ùê¥ùêµùê∂, ùê¥ùêµ= 10, ùêµùê∂= 14, and ùê∂ùê¥= 16. Let ùê∑be a point in the interior of ùêµùê∂.‚Äæ  Let points ùêº! and ùêº\" denote the\n     incenters of triangles ùê¥ùêµùê∑and ùê¥ùê∂ùê∑, respectively. The circumcircles of triangles ùêµùêº!ùê∑and ùê∂ùêº\"ùê∑meet at distinct\n     points ùëÉand ùê∑. The maximum possible area of ‚ñ≥ùêµùëÉùê∂can be expressed in the form ùëé‚àíùëèùëê, where ùëé, ùëè, and ùëêare\n     positive integers and ùëêis not divisible by the square of any prime. Find ùëé+ ùëè+ ùëê.\n\n   No steering extended:\n     Let me recall that the area of triangle ABC is 40‚àö3. If the maximum area of BPC is 40‚àö3 - 14‚àö3 = 26‚àö3, then a =\n     40, b = 14, c = 3, giving 40 + 14 + 3 = 57. But why would the area of BPC be 26‚àö3?\n  ‚Ä¶\n\n   + Problem Understanding:\n     Let me think again. The problem says \"the maximum possible area of triangle BPC can be expressed in the\n    form a - b‚àöc\". Let me consider that the area might be related to the area of ABC minus some value.\n  ‚Ä¶\n\n   + Procedural Planning:\n    Let me check one more thing. The area of triangle BPC when P is at (7, 7‚àö3) is 49‚àö3, but the area of triangle\n   ABC is 40‚àö3. Since P is outside, it's possible ‚Ä¶\n    Let me try to calculate the area of BPC when P is the incenter of ABC, which is at (4, 2‚àö3). The area is 14‚àö3, as\n     calculated earlier. This is 14‚àö3, which is 14‚àö3 - 0, but again b must be positive ‚Ä¶\n    Let me recall that in triangle BPC, with ‚à†BPC = œÄ/3, and BC = 14 ‚Ä¶\n\n   + Backtracking:\n     Actually, let me recall that in the initial calculation, the area of triangle ABC is 40‚àö3. If the maximum area of\n   BPC is 40‚àö3, it would be equal to the area of ABC, but this is not possible since P is a point outside or inside.\n  ‚Ä¶\n\n   + Multi-perspective Verification:\n    Let me think about another approach.\n\n     Let me use the fact that the area of BPC is (1/2)*BP*CP*sin(‚à†BPC), and from the earlier relation, if ‚à†BPC is\n     2œÄ/3, then the maximum BP*CP is 392/5, so the area is (392/5)*(‚àö3/2) = 196‚àö3/5. But 196/5 is 39.2, so 39.2‚àö3 is\n     not in the form a - b‚àöc.\n\n     Alternatively, if the maximum area is when P is the incenter of ABC, the area is 14‚àö3, but this is also not in the\n     required form.\n\n    Another idea: maybe the maximum area is the area of ABC, which is 40‚àö3, but the problem says \"maximum\n     possible\", so it might be larger.\n  ‚Ä¶\n\n   + Hypothesis Reasoning:\n     Let me think again. If the area is 49‚àö3, and if the problem says it can be expressed as a - b‚àöc, perhaps there is\n    a different expression. Let me recall that 49‚àö3 is equal to 49‚àö3 + 0, but if there is a different expression, like if\n     the area is 49‚àö3 - something, but I need to think.\n\n\nFigure 6: A case study of SAE-based steering. By steering with different strategy-specific features as control vectors,\nwe steer the subsequent reasoning trajectory to follow different reasoning strategies.\n\nYou are a top-tier AI recognition reasoning behavior analysis expert. Your task is to precisely evaluate the success of a\nthought intervention experiment. An intervention is successful if the intervened thinking more explicitly and significantly\ndemonstrates the \"target cognitive reasoning strategy\". If the baseline thinking lacks this strategy, it must be added; if the\nbaseline already exhibits it, it must be strengthened.\n\nYou can first count the number of times the \"target cognitive reasoning strategy\" appears in both the baseline thinking\nand the intervened thinking, and then determine whether the frequency increases after intervention. Your output MUST\nbe a single valid JSON object. Provide the following:\n- \"before\": integer, the count of occurrences in the Before Intervention text.\n- \"after\": integer, the count of occurrences in the After Intervention text.\n- \"more_frequent\": boolean, true if the count after > before, else false.\n\n### Target Cognitive Reasoning Strategy ###\n{reasoning_strategy_description}\n\nExamples:\n\n{few_shot}\n\n### FINAL TASK ###\n\nReasoning Texts to Analyze:\n\nBefore Intervention:\n{before_text}\n\nAfter Intervention:\n{after_text}\n\nYour Answer:\n\n\n                    Figure 7: The prompt used to evaluate the control effectiveness.\n\n## Claim to Classify\n\"SAE-Steering identifies strategy-specific features from the vast pool of SAE features\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "SAE-Steering",
    "method_rung": 2,
    "claim_rung": 2,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The claim states that SAE-Steering identifies strategy-specific features. SAE-Steering is a two-stage pipeline: Stage 1 uses logit estimation (observational, Rung 1) to recall features, but Stage 2 uses interventional testing (steering vectors) to evaluate control effectiveness, which is Rung 2. Since the claim focuses on the identification capability validated through intervention, the method rung is 2. The claim verb 'identifies' implies causal selection validated by intervention, aligning with Rung 2 linguistic markers like 'mediates' or 'is sufficient for'. No hedging terms are present. Confidence is 4 due to minor ambiguity in the verb 'identifies' not being explicitly listed in codebook examples, but contextual evidence supports causal interpretation."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 19860,
      "completion_tokens": 1888,
      "total_tokens": 21748
    },
    "finish_reason": "stop"
  }
}