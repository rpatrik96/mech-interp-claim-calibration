{
  "timestamp": "2026-02-19T10:28:28.271265+00:00",
  "paper_id": "2512.05865",
  "claim_id": "2512.05865-04",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2512.05865\nTitle: Sparse Attention Post-Training for Mechanistic Interpretability\nFull text:\n              Sparse Attention Post-Training for Mechanistic Interpretability\n\n\n\n                     Florent Draye * 1 Anson Lei * 1 2 Hsiao-Ru Pan 1 Ingmar Posner 2 Bernhard Sch¨olkopf 1 3\n\n\n                         Abstract\n                                                                     Base Model          We introduce a simple post-training method that\n             makes transformer attention sparse without sacri-\n                 ficing performance. Applying a flexible sparsity\n                regularisation under a constrained-loss objective,\n           we show on models up to 7B parameters that it\n                   is possible to retain the original pretraining loss2026\n              while reducing attention connectivity to ≈0.4%\n               of its edges. Unlike sparse-attention methods de-\n               signed for computational efficiency, our approachFeb\n               leverages sparsity as a structural prior:  it pre-\n4\n                serves capability while exposing a more organized\n             and interpretable connectivity pattern. We find                                                                                                                                                                                                                                                                                                 Finetuning        Sparse Model\n                  that this local sparsity cascades into global circuit                simplification: task-specific circuits involve far                                                                                                       Sparsity-Regularised\n              fewer components (attention heads and MLPs)\n              with up to 100× fewer edges connecting them.[cs.LG]\n                Additionally, using cross-layer transcoders, we\n            show that sparse attention substantially simpli-\n                  fies attention attribution, enabling a unified view\n               of feature-based and circuit-based perspectives.\n             These results demonstrate that transformer atten-\n                 tion can be made orders of magnitude sparser, sug-\n                gesting that much of its computation is redundant\n              and that sparsity may serve as a guiding principle         Figure 1. Visualised attention patterns for a 4-layer toy model\n                 for more structured and interpretable models.              trained on a simple 2-digit addition task. The main idea of this\n                                                               work is to induce sparse attention between tokens via a post-\n                                                                                       training procedure that optimizes for attention sparsity while main-\n                                                                                  taining model performance. In this example, while both models\n          1. Introduction                                                                                are able to correctly predict the sum, the sparse model solves the\n                                                                    problem with a naturally interpretable circuit. Details of this toy\n          Scaling has driven major advances in artificial intelligence,\n                                                                              setup and more examples are provided in Appendix A\n          with ever-larger models trained on internet-scale datasetsarXiv:2512.05865v2    achieving remarkable capabilities across domains. Large\n          language models (LLMs) now underpin applications from\n            text generation to question answering, yet their increas-   components implement specific computations and behav-\n          ing complexity renders their internal mechanisms largely     iors. Recent advances in this area have successfully identi-\n         opaque (Bommasani, 2021). Methods of mechanistic in-    fied interpretable circuits, features, and algorithms within\n            terpretability have been developed to address this gap by   LLMs (Nanda et al., 2023; Olsson et al., 2022), showing\n           reverse-engineering neural networks to uncover how internal     that large complex models can, in part, be understood mech-\n                                                                                 anistically, opening avenues for improving transparency,\n             *Equal contribution  1Max Planck Institute for Intelligent\n                                                                                        reliability, and alignment (Bereska & Gavves, 2024).          Systems (MPI-IS), T¨ubingen, Germany 2Applied Artificial In-\n             telligence Lab,  University  of Oxford,  Oxford, UK 3ETH                                                             However, interpretability is bottlenecked by the model it-\n             Z¨urich, Z¨urich, Switzerland. Correspondence to: Florent Draye\n                                                                                        self: even with sophisticated reverse-engineering techniques          <fdraye@tuebingen.mpg.de>.\n                                                                              that can faithfully reveal internal algorithms, the underly-\n            Preprint. February 6, 2026.                                   ing computations implemented by large models can still\n\n                                                         1\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nremain highly complex and uninterpretable. Circuits for    be understood through explicit, tractable circuits. Taken\nseemingly simple tasks may span hundreds of interacting     together, these results position attention sparsity as an effec-\nattention heads and MLPs with densely intertwined contri-    tive and practical inductive tool for surfacing the minimal\nbutions across layers (Conmy et al., 2023), and features can    functional backbone underlying model behaviour.\ninfluence each other along combinatorially many attention-\nmediated paths, complicating attention attribution (Kamath                                                        2. Related Work\net al., 2025). To exemplify this, Figure 1 (top) illustrates\nthe attention patterns of a small, single-head transformer     2.1. Sparse Attention\ntrained on a simple two-digit addition task. Here, the model\n                                              As self-attention is a key component of the ubiquitous Trans-\nhas learned to solve the task in a highly diffused manner,\n                                                        former architecture, a large number of variants of attention\nwhere information about each token is dispersed across all\n                                                  mechanisms have been explored in the literature. Related\ntoken locations, rendering the interpretation of the underly-\n                                                                  to our approach are sparse attention methods, which are pri-\ning algorithm extremely difficult even in this simple case.\n                                                           marily designed to alleviate the quadratic scaling of vanilla\nThe crux of the problem is that models are not incentivised to     self-attention. These methods typically rely on masks based\nemploy simple algorithms during training. In this work, we    on fixed local and strided patterns (Child et al., 2019) or\nadvocate for directly embedding interpretability constraints    sliding-window and global attention patterns (Beltagy et al.,\ninto model design in a way that induces simple circuits while    2020; Zaheer et al., 2020) to constrain the receptive field\npreserving performance. We focus our analysis on atten-    of each token. While these approaches are successful in\ntion mechanisms and investigate sparsity regularisation on    reducing the computational complexity of self-attention,\nattention patterns, originally proposed in (Lei et al., 2025),    they require hand-defined heuristics that do not reflect the\nas an inductive bias. To demonstrate how sparse attention     internal computations learned by the model.\npatterns can give rise to interpretable circuits, we return to\n                                                  Beyond these fixed-pattern sparse attention methods, Top-k\nthe two-digit addition example: Figure 1 (bottom) shows\n                                                                    attention, which enforces sparsity by dynamically selecting\nthe attention patterns induced by penalising attention edges\n                                                             the k most relevant keys per query based on their atten-\nduring training. Here, the sparsity inductive bias forces the\n                                                                tion scores, has also been explored (Gupta et al., 2021;\nmodel to solve the problem with much smaller, intrinsically\n                                                     DeepSeek-AI, 2025). While Top-k attention enables learn-\ninterpretable computation circuits.\n                                                          able sparse attention, the necessity to specify k limits its\nIn this work, we investigate using this sparsity regularisation    scope for interpretability for two reasons. First, selecting\nscheme as a post-training strategy for pre-trained LLMs. We    the optimal k is difficult, and setting k too low can degrade\npropose a practical method for fine-tuning existing models    model performance. Second, and more fundamentally, Top-\nwithout re-running pretraining, offering a flexible way to    k attention does not allow the model to choose different\ninduce sparse attention patterns and enhance interpretabil-   k for different attention heads based on the context. We\nity. We show, on models of up to 7B parameters, that our    argue that this flexibility is crucial for maintaining model\nproposed procedure preserves the performance of the base    performance.\nmodels on pretraining data while reducing the effective at-\n                                                More recently, gated attention mechanisms (Qiu et al., 2025)\ntention map to less than 0.5% of its edges. To evaluate\n                                                          provide a scalable and performant framework for inducing\nour central hypothesis that sparse attention facilitates inter-\n                                                            sparse attention. In particular, Lei et al. (2025) introduce\npretability, we consider two complementary settings. First,\n                                                        a sparsity regularisation scheme for world modelling that\nwe study circuit discovery, where the objective is to identify\n                                                              reveals sparse token dependencies. We adopt this method\nthe minimal set of components responsible for task perfor-\n                                                     and examine its role as an inductive bias for interpretability.\nmance (Conmy et al., 2023). We find that sparsified models\nyield substantially simpler computational graphs: the re-\n                                                                    2.2. Circuit Discoverysulting circuits explain model behaviour using up to four\ntimes fewer attention heads and up to two orders of mag-    Mechanistic interpretability seeks to uncover how internal\nnitude fewer edges. Second, using cross-layer transcoders    components of LLMs implement specific computations. Ab-\n(Ameisen et al., 2025), we analyse attribution graphs, which     lation studies assess performance drops from removing com-\ncapture feature-level interactions across layers. In this set-    ponents (Nanda et al., 2023), activation patching measures\nting, sparse attention mitigates the attention attribution prob-    the effect of substituting activations (Zhang & Nanda, 2023),\nlem by making it possible to identify which attention heads    and attribution patching scales this approach via local lin-\ngive rise to a given edge, owing to the reduced number of     earisation (Syed et al., 2024). Together, these approaches\ncomponents mediating each connection. We argue that this    allow researchers to isolate sub-circuits, minimal sets of\nclarity enables a tighter integration of feature-based and     attention heads and MLPs that are causally responsible for a\ncircuit-based perspectives, allowing feature interactions to    given behavior or task (Conmy et al., 2023). Attention itself\n\n\n                                                2\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nplays a dual role: it both routes information and exposes          post-trained models do not lose prediction performance\ninterpretable relational structure, making it a key substrate        compared to their fully-connected counterparts.\nfor mechanistic study. Our work builds on this foundation\nby leveraging sparsity to simplify these circuits, amplifying                                                  To this end, we leverage the Sparse Transformer architecture\nthe interpretability of attention-mediated computation while                                                                 in the SPARTAN framework proposed in (Lei et al., 2025),\npreserving model performance.                                                     which uses sparsity-regularised hard attention instead of the\n                                                           standard softmax attention. In the following subsections,\n2.3. Attribution Graph                          we describe the Sparse Transformer architecture and the\n                                                             optimisation setup, highlighting how this approach satisfiesMechanistic interpretability has gradually shifted from an\n                                                              the above desiderata.emphasis on explicit circuit discovery towards the analysis\nof internal representations and features. Recent work on\n                                                                    3.1. Sparse Attention Layerattribution graphs and circuit tracing seeks to reunify these\nperspectives by approximating MLP outputs as sparse linear                                                    Given a set of token embeddings, the Sparse Transformer\ncombinations of features and computing causal effects along                                                              layer computes the key, query, and value embeddings,\nlinear paths between them (Dunefsky et al., 2024; Ameisen                                                              {ki, qi, vi}, via linear projections, analogous to the standard\net al., 2025; Lindsey et al., 2025b). This framework enables                                                            Transformer. Based on the embeddings, we sample a binary\nthe construction of feature-level circuits spanning the com-                                                            gating matrix from a learnable distribution parameterised\nputation from input embeddings to final token predictions.                                                   by the keys and queries,\nWithin attribution graphs, edges correspond to direct lin-\near causal relationships between features. However, these                                                                       Aij ∼Bern(σ(qTi kj)),               (1)\nrelationships are mediated by attention heads that transmit\ninformation across token positions. Identifying which atten-   where Bern(·) is the Bernoulli distribution and σ(·) is the\ntion heads give rise to a particular edge, and understanding                                                                    logistic sigmoid function. This sampling step can be made\nwhy they do so, is essential, as this mechanism forms a fun-    differentiable via the Gumbel Softmax trick (Jang et al.,\ndamental component of the computational graph (Kamath                                                          2017). This binary matrix acts as a mask that controls the\net al., 2025). A key limitation of current attribution-based                                                           information flow across tokens. Next, the message passing\napproaches is that individual causal edges are modulated by    step is carried out in the same way as standard softmax\ndozens of attention components. We show that this leads to                                                                  attention, with the exception that we mask out the value\nfeature-to-feature influences that are overly complex, render-                                                    embeddings using the sampled binary mask,\ning explanations in terms of other features in the graph both\ncomputationally expensive and conceptually challenging.\n                                                     SparseAttn(Q, K, V ) = A ⊙softmax(QKT ) V,  (2)                                                                         √dk\n3. Method\n                                                   where dk is the dimension of the key embeddings and ⊙\nOur main hypothesis is that post-training existing LLMs to    denotes element-wise multiplication. During training, we\nencourage sparse attention patterns leads to the emergence     regularise the expected number of edges between tokens\nof more interpretable circuits. In order to instantiate this    based on the distribution over the gating matrix. Concretely,\nidea, we require a post-training pipeline that satisfies three    the expected number of edges for each layer can be calcu-\nmain desiderata:                                                lated as\n                                                 E |A| = X σ(qTi kj).               (3)\n  1. To induce sparse message passing between tokens,                                              i,j\n   we need an attention mechanism that can ‘zero-out’\n                                                   Note that during the forward pass, each entry of A is a     attention edges, which in turn enables effective L0-\n                                                      hard binary sample that zeros out attention edges, which     regularisation on the attention weights.  This is in\n                                                            serves as an effective L0 regularisation. Moreover, since     contrast to the standard softmax attention mechanism,\n                                                             the functional form of the sparse attention layer after the    where naive regularisation would result in small but\n                                                        hard sampling step is the same as standard softmax atten-     non-zero attention weights that still allow information\n                                                                        tion, pre-trained model weights can be directly used without     flow between tokens.\n                                                                     alterations.1\n  2. The model architecture needs to be compatible with the                                                                           1Technically, the sampled A affects the computation. This can\n     original LLM such that the pre-trained LLM weights    be mitigated by adding a positive bias term inside the sigmoid func-\n    can be directly loaded at initialisation.                      tion to ensure all gates are open at initialisation. Experimentally,\n                                               we found this to be unnecessary as the models quickly recover\n  3. The post-training procedure needs to ensure that the      their original performance within a small number of gradient steps.\n\n                                                3\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n3.2. Constrained Optimisation                                                              1.0                          Benchmark Comparison\n                                                                                                                                                                          OLMo-7B\nIn order to ensure that the models do not lose prediction per-                                                                                          Sparse OLMo-7B\n                                                                                                                          0.8\nformance during the post-training procedure, as per desider-\natum 3, we follow the approach proposed in (Lei et al.,            0.6\n2025), which employs the GECO algorithm (Rezende &                       Accuracy\nViola, 2018). Originally developed in the context of regular-            0.4\nising VAEs, the GECO algorithm places a constraint on the\nperformance of the model and uses a Lagrangian multiplier             0.2\nto automatically find the right strength of regularisation dur-\n                                                                                                                          0.0\ning    training.             Concretely,                 we formulate\nas the      following                optimisation                           problem,                                      the learning process                       TruthfulQA              PIQA            OpenBookQA              ARC-Easy\n        min X E |Al|       s.t. CE ≤τ,         (4)             θ                                                 Figure 2. Comparison of model performance between the base\n                           l                             OLMo model and the sparsified model evaluated on the various\nwhere Al denotes the gating matrix at layer l, CE is the stan-    benchmarks. Across all tasks, the performance of the sparse model\n                                                             remains comparable with the base model despite using substan-dard next token prediction cross-entropy loss, and τ is the\n                                                                                  tially fewer attention edges.\nrequired target loss, and θ is the model parameters. In prac-\ntice, we set this target as the loss of the pre-trained baseline\nmodels. We solve this optimisation problem via Lagrangian                                            we verify on a 7B parameter model that LoRA finetuning is\nrelaxation, yielding the following max-min objective,                                                                     sufficiently expressive for inducing sparse attention patterns.\n\n      max min X E |Al| + λ(CE −τ)  .       (5)\n        λ>0   θ                                        FlashAttention (Dao, 2023)  FlashAttention has become                                   l\n                                                          a standard method for reducing the memory footprint of dot-\nThis can be solved by taking gradient steps on θ and λ alter-                                                          product attention mechanisms. In Appendix B, we discuss\nnately. During training, updating λ automatically balances                                             how the sampled sparse attention can be implemented in an\nthe strength of the sparsity regularisation: when CE is lower                                                        analogous manner.\nthan the threshold, λ decreases, and hence more weight is\ngiven to the sparsity regularisation term. This effectively\n                                                               Distillation (Gu et al., 2024).  Empirically, we find thatacts as an adaptive schedule which continues to increase the\n                                                       adding an auxiliary distillation loss based on the KL di-strength of the regularisation until the model performance\n                                                       vergence between the base model and the sparse modeldegrades. Here, the value of τ is selected as a hyperparame-\n                                                        improves training stability and ensures that the behaviour ofter to ensure that the sparse model’s performance remains\n                                                              the model remains unchanged during post-training.within a certain tolerance of the original base model. In prac-\ntice, the choice of τ controls a trade off between sparsity\nand performance: picking a tight τ can lead to a slower train-    4. Experiments\ning process, whereas a higher tolerance can substantially\n                                                To evaluate the effectiveness of our post-training pipeline,speed up training at the cost of potentially harming model\n                                           we finetune pre-trained LLMs and compare their predic-performance. In Appendix C, we provide further discussion\n                                                                tion performance and interpretability before and after ap-on this optimisation process and its training dynamics.\n                                                          plying sparsity regularisation. We perform full finetuning\n                                                  on a GPT-2 base model (Radford et al., 2019)(124M pa-\n3.3. Practical Considerations\n                                                           rameters) on the OpenWebText dataset (Gokaslan & Co-\nOne of the main strengths of our proposed method is that,    hen, 2019). To investigate the generality and scalability\narchitecturally, the only difference between a sparse Trans-    of our method, we perform LoRA finetuning on the larger\nformer and a normal one lies in how the dot-product atten-   OLMo-7B model (Groeneveld et al., 2024) on the Dolma\ntion is computed. As such, most practical training tech-    dataset (Soldaini et al., 2024), which is the dataset on which\nniques for optimising Transformers can be readily adapted    the base model was trained. The GPT-2 model and the\nto our setting. In our experiments, we find the following   OLMo model are trained on sequences of length 64 and\ntechniques helpful for improving computational efficiency    512, respectively.  In the following subsections, we first\nand training stability.                                         present a quantitative evaluation of model performance and\n                                                                sparsity after sparse post-training. We then conduct two\nLoRA finetuning (Hu et al., 2022).  Low rank finetuning     interpretability studies, using activation patching and attri-\ntechniques can significantly reduce the computational re-    bution graphs, to demonstrate that our method enables the\nquirements for training large models. In our experiments,    discovery of substantially smaller circuits.\n\n                                                4\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n  Model   Base CE    τ     Final CE   Active Attn\n\n  GPT-2     3.48     3.50    3.501      0.22%\n  OLMo     2.24     2.29    2.287      0.44%\n\nTable 1. Performance and sparsity of post-trained models. Final\ncross-entropy losses closely match the specified targets, while\nattention sparsity is substantially increased.\n\n\n\n4.1. Model Performance and Sparsity\n\nWe begin by evaluating both performance retention and the\ndegree of sparsity achieved by post-training. We set cross-\nentropy targets of 3.50 for GPT-2 (base model: 3.48) and\n2.29 for OLMo (base model: 2.24). After training, the mean\ncross-entropy loss for both models remains within ±0.01 of\nthe target, indicating that the dual optimisation scheme effec-\ntively enforces a tight performance constraint. To quantify\nthe sparsity achieved by the models, we evaluate them on\nthe validation split of their respective datasets and compute\nthe mean number of non-zero attention edges per attention\n                                                               Figure 3. Attention patterns of the heads required to explain 90%\nhead. We find that the sparsified GPT-2 model activates, on     of model behaviour on a copy task. The sparse model requires\naverage, only 0.22% of its attention edges, while the sparsi-     substantially fewer attention heads. Moreover, the selected heads\nfied OLMo model activates 0.44%, indicating substantial     exhibit the characteristic ‘induction head’ pattern: each token\nsparsification in both cases. Table 1 provides a summary of     attends to a previous token at a fixed relative offset, effectively\n                                                              copying information forward through the sequence, a pattern well\nthe results. To further verify that this drastic reduction in\n                                                    known to implement the copy mechanism in transformer models.\nmessage passing between tokens does not substantially alter     Equivalent plots for OLMo can be found in Appendix D.\nmodel behaviour, we evaluate the sparsified OLMo model\non a subset of the benchmarks used to assess the original\nmodel. As shown in Figure 2, the sparse model largely re-    Since searching over every possible subset of model compo-\ntains the performance of the base model across a diverse set    nents is infeasible due to the exponential number of potential\nof tasks. In sum, our results demonstrate that sparse post-    subsets, we adopt a common heuristic to rank each model\ntraining is effective in consolidating information flow into a    component. Specifically, for each individual component, we\nsmall number of edges while maintaining a commensurate    compute an importance score by replacing the activations of\nlevel of performance.                                          the component with the corrupted activations and measuring\n                                                                                  its effect on the logit difference. In our experiments, we use\n4.2. Circuit Discovery with Activation Patching              this ranking to select the top-k components and intervene\n                                                  on the model by freezing all remaining components, with\nWe begin by outlining the experimental procedure used for\n                                                             the goal of identifying the minimal set that accounts for at\ncircuit discovery. Activation patching (Nanda et al., 2023)\n                                                                    least 90% of the model’s preference for the correct predic-\nis a widely used technique for identifying task-specific cir-\n                                                                     tion. Note that these importance scores can be computed\ncuits in transformer models. In a typical setup, the model\n                                                                       at two levels: (i) a single-sentence level, using a single pair\nis evaluated on pairs of prompts: a clean prompt, for which\n                                                            of correct and corrupted inputs, and (ii) a global level, ob-\nthe model predicts a correct target token, and a corrupted\n                                                             tained by averaging scores across many task variants. In\nprompt that shares the overall structure of the clean prompt\n                                                          our experiments, we report the results using single-sentence\nbut is modified to induce an incorrect prediction. Here, the\n                                                               scores. In Appendix D, we also provide results using the\ngoal is to find the set of model components that is responsi-\n                                                          global scores, which are largely consistent with our main\nble for the model’s preference for the correct answer over\n                                                                        results. There are also two standard approaches for freezing\nthe wrong one, as measured by the logit difference between\n                                                  component activations: setting the activation to zero or re-\nthe corresponding tokens. In activation patching, individual\n                                                             placing it with a mean activation value (Conmy et al., 2023).\nmodel components, such as attention heads and individual\n                                       We evaluate both variants for each model and report results\nedges, can be ’switched-off’ by patching activation at the\n                                                                 for the patching strategy that yields the smallest circuits.\nspecific positions. Circuit discovery amounts to finding a\nset of components whose replacement causes the model’s   We first focus on the copy task with the following prompt:\nprediction to shift from the correct to the corrupted answer.  \"AJEFCKLMOPQRSTVWZS, AJEFCKLMOPQRSTVWZ\",\n\n                                                5\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n     Greater Than                      IOI                             Docstring                          IOI Long\n    1.0        4.5x                             1.0          2.2x                          1.0        2.2x                              1.0            1.4x\n\n Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect\n  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5\n\n\n                                  GPT-2                                                                                 OLMo-7B\n    0.0                            Sparse GPT-2        0.0                                                   0.0                         Sparse OLMo-7B        0.0\n                   50          100                             50          100                         250      500      750     1000               250      500      750     1000\n               Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept\n\nFigure 4. Logit attribution keeping only the top-k attention heads. Dotted line annotates the number of attention heads needed to explain\n90% of the logit difference. Sparse models yields 1.4× to 4.5× smaller circuits. Shaded areas show standard error across 20 prompts.\n\n     Greater Than                      IOI                             Docstring                          IOI Long\n    1.0             97.0x                   1.0            42.8x                    1.0                  8.6x             1.0                  5.4x\n\n Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect\n  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5\n\n\n                                    GPT-2                                                                                 OLMo-7B\n    0.0                              Sparse GPT-2      0.0                                                   0.0                           Sparse OLMo-7B      0.0\n      100       101       102       103       104        100       101       102       103                100    101    102    103    104    105          100    101    102    103    104    105\n                Number of Edges Kept                          Number of Edges Kept                          Number of Edges Kept                          Number of Edges Kept\n\nFigure 5. Logit attribution per sentence keeping only the top-k attention edges. Sparse models yields 5.4× to 97× smaller circuits.\nShaded area shows standard error across 20 prompts.\n\n\nwhere the model has to copy the letter S to the next token     string task where the model needs to predict an argument\nposition. This task is well studied and is widely believed to   name in a Docstring based on an implemented function.\nbe implemented by emergent induction heads (Elhage et al.,    Details of each task can be found in Appendix E. Figure 4\n2021), which propagate token information forward in the    and 5 show the fraction of model behaviour explained as\nsequence. Figure 3 illustrates the attention patterns of the    a function of the number of retained model components\nset of attention heads that explains this prompt for the sparse     (attention heads and attention edges, respectively). Across\nand base GPT-2 models. See Appendix D for analogous      all tasks and models, the sparse models consistently produce\nresults for the OLMo models. The sparse model admits     significantly smaller circuits, as measured by the number of\na substantially smaller set of attention heads (9 heads)    model components needed to explain 90% of model predic-\nthan its fully connected counterpart (61 heads). Moreover,     tion. This further corroborates our claim that sparse models\nthe identified heads in the sparse model exhibit cleaner    lead to simpler and more interpretable internal circuits.\ninduction head patterns, with each token attending to a\nsingle prior position at a fixed relative offset. These results     4.3. Attribution-graph\nillustrate how sparsification facilitates interpretability under\n                                                          Next, we present a more fine-grained, feature-level investi-simple ranking-based methods and support our hypothesis\n                                                              gation of whether sparsity in attention leads to interpretablethat sparse post-training yields models that are more\n                                                                    circuits in practice using cross-layer transcoders (CLTs).amenable to mechanistic interpretability techniques.\n                                                        Since training CLTs on OLMo-7B is computationally pro-\nTo further verify our hypothesis, we repeat the experiment     hibitive2, we focus our analysis on the GPT-2 models. For\non classical circuit discovery tasks. For GPT-2, we evaluate     the rest of the section, we perform analysis on CLTs trained\nvariants of the Indirect Object Identification (IOI) task, in    on the sparse and base GPT-2 models, trained with an ex-\nwhich the model copies a person’s name from the start of    pansion factor of 32 and achieve above 80% replacement\na sentence, and the Greater Than task, in which the model    score measured with Circuit Tracer (Hanna et al., 2025). See\npredicts a number that is larger than a previously mentioned    Appendix F and G for details on training and visualisation.\nnumber. To further assess the scalability of our approach,\n                                                              2The largest open-source CLT is on Gemma-2B at the time ofwe investigate more challenging and longer horizon tasks\n                                                                        writing.\nfor OLMo, including a longer context IOI task and a Doc-\n\n\n                                                6\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nWe study the problem of attention attribution, which seeks          Edges                      Heads\nto understand how edges between features are mediated.            1.00                                               1.00\n                                                                               16.1x                 3.4x\nThe key challenge here is that any given edge can be af-\n                                                                                                                                                    Mass                                                               Mass\nfected by a large number of model components, making            0.75                                               0.75\nmediation circuits difficult to analyse both computationally                                                                                                                                                                                                                                                                                                                                                                                  Cumulative                                                                                                                                                             Cumulative\nand conceptually: computationally, exhaustive enumeration\n                                                                                                                                                    Mean 0.50                                                         Mean 0.50\nis costly; conceptually, the resulting circuits are often large\nand uninterpretable. In this experiment, we demonstrate                                                                                  NonSparseSparse\nthat sparse attention patterns induced via post-training sub-               100        101        102        103                   25    50    75    100   125\n                                                                                                                                     Sorted Index (log scale)                                   Sorted Index\nstantially alleviate these challenges, as the vast majority of\nattention components have zero effect on the computation.     Figure 6. Mean cumulative distribution of the component scores\n                                                                             that mediate an attribution graph edge. The components are on the\nAs in (Ameisen et al., 2025), we define the total attribution                                                                                   left key-query pairs within a head, and on the right full attention\nscore between feature n at layer ℓand position k, and feature     heads.\nn′ at layer ℓ′ and position k′ as\n                  aℓ′,k′,n′ℓ,k,n = fk,nℓ   Jℓ′,k′ℓ,k   gℓ′k′,n′.             (6)    edge in the attribution graph. We find that, to reach a cu-\n          ℓ                                                  mulative attribution threshold of 90%, the sparse model on\nHere, fk,n denotes the decoder vector corresponding to                                                         average requires 16.1× fewer key–query pairs and 3.4×\nfeature n at layer ℓand position k, and gℓ′k′,n′ is the cor-    fewer attention heads when compared to the dense GPT-\nresponding encoder vector for feature n′ at layer ℓ′ and    2 model, supporting our hypothesis that sparse attention\nposition k′. The term Jℓ′,k′ℓ,k   is the Jacobian from the MLP     patterns leads to simpler mediation circuits.\noutput at (ℓ, k) to the MLP input at (ℓ′, k′). This Jacobian\n                                                         Next, we present a qualitative case-study to showcase theis computed during a forward pass in which all nonlinear-\n                                                                 benefits of sparse attention patterns. For a given key–queryities are frozen using stop-gradient operations. Under this\n                                                                      pair, we compute the causal effect from all other featureslinearisation, the attribution score represents the sum over\n                                                                in the attribution graph to both the key and the query vec-all linear paths from the source feature to the target feature.\n                                                                       tors. Figure 7 illustrates this analysis for the prompt “The\nTo analyse how this total effect between two features is me-    opposite of ‘large’ is”. The resulting attribution graph de-\ndiated by each model component, we define the component-   composes into four coherent clusters of features: features\nspecific attribution by subtracting the contribution of all     related to opposite, features related to large, features acti-\npaths that do not pass through the component:                vating on bracketed tokens, and the final next-token logit\n                                                        corresponding to small (see Appendix H for example of\n  aℓ′,k′,n′ℓ,k,n  (h) = fk,nℓ   Jℓ′,k′ℓ,k   gℓ′k′,n′ −fk,nℓ    Jℓ′,k′ℓ,k  h gℓ′k′,n′.      features and visualization).\n\n                                                                                       large                                                           Here,                                                                     the                                                                           features                                                                                     in                                                                                    the                                                                                                    cluster are                                                                                                               directly                                                                                                      connected\n              denotes a modified Jacobian computed un-Here,  Jℓ′,k′ℓ,k  h                                                               small                                                                to the                                                                                      logit.                                                              The                                                                        key                                                                                     question                                                                                                                      is then                                                                                                             to understand\nder the same linearization as above, but with the specific\n                                             how this connection from the large to the small logit comes\nattention component h additionaly frozen via stop-gradient.\n                                                               about. To this end, we analyse their mediation structure. We\nAs such, these component-specific scores quantifies how\n                                                                  find that 80% of the cumulative attribution score of the edges\nmuch each model component impacts a particular edge be-\n                                                         connecting the large cluster to the small logit is mediated\ntween features.\n                                                   by the same five late layer attention key–query pairs. These\nEmpicially, we evaluate the method on ten pruned attribution     attention components map features from token position 5\ngraphs, computed on the IOI, greater-than, completion, and     directly into the final-layer residual stream at position 8, and\ncategory tasks. Similar to our previous circuit discovery    thus operate in parallel.\nexperiment, we compute attribution scores on the level of\n                                                        For these five key–query pairs, we then compute the causal\nattention heads as well as individual key–query pairs. In\n                                                              influence of all other features in the graph on their key and\npractice, attention sparsity yields substantial computational\n                                                       query vectors. The query vectors are primarily modulated\nsavings: because inactive key–query pairs are known a\n                                                  by features associated with bracketed tokens in the last to-\npriori to have exactly zero attribution score, attribution need\n                                                    ken position, while the key vectors are driven by strongly\nonly be computed for a small subset of components. This\n                                                               active features in both the opposite and large clusters, as\nreduces the computation time per attribution graph from\n                                                shown in Figure 8.These results are in agreement with the\nseveral hours to several minutes.\n                                                             recent work on attention attribution and the ”opposite of”\nIn terms of circuit size, Figure 6 shows the mean cumula-    attribution graph (Kamath et al., 2025). In stark contrast,\ntive distribution of component attribution scores for each    Figure 7 (left) shows that a similar (and more computa-\n\n                                                7\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n                                                                                                           All heads map key pos 5 to query pos 8             Modulated at 80% by                                                            K\n\n                                                                                                    small\n                         Q                                                                           layer 12\n                                                         L11-H7     L10-H1      L9-H7      L9-H1      L8-H6\n                                                                     Sparse GPT-2\n\n                                                                                                     brackets                                                     opposite                    large                                                                                                                                      layer 0-10                                                                         layer 0-1                        layer 0-3\n\n                                      The     opposite     of   “      large     ”    is       ”\n                     GPT-2                        1          2          3    4        5        6    7        8\n\nFigure 7. Sketch of the attribution graph for the sentence “The opposite of ‘large’ is”. The cluster of features associated with large at\ntoken position 5 maps directly to the final next-token prediction logit small. We show the attention patterns of all key–query pairs required\nto account for 80% of the cumulative attribution score. In the sparse-attention setting, this corresponds to five attention heads, compared\nto more than forty heads in the dense-attention case. In the sparse model, these heads read from token position 5 and write directly to the\nlast token residual stream at token position 8. These heads thus compute in parallel and provide a clear picture of the internal computation.\n\n\n       →Query             →Key                                                       Across a range of tasks and analyses, we show that sparsity\n1. large (pos 5)         1. bracket (pos 8)    improves interpretability at the circuit level by reducing\n2. large (pos 5)         2. bracket (pos 8)     the number of components involved in specific behaviours.\n3. quantities (pos 5)    3. bracket (pos 8)                                                            In circuit discovery experiments, most of the model’s be-\n4. comparison (pos 3)    4. bracket (pos 8)\n                                                           haviour can be explained by circuits that are orders of mag-5. opposite (pos 3)      5. bracket (pos 8)\n                                                            nitude smaller than in dense models; in attribution graph\n                                                                analyses, the reduced number of mediating components ren-\nFigure 8.  Minimal description of the top5 features activating\n                                                            ders attention attribution tractable. Together, these resultsthe query and the key vectors for the attention head L8-H6 from\nFigure 7.                                                      position sparse post-training of attention as a practical and\n                                                                   effective tool for enhancing the mechanistic interpretability\n                                                             of pre-trained models.\n\ntionally expensive) analysis on the dense model produces a\n                                                       Limitations and Future Work.  One limitation of the\nmuch more complicated circuit. This case study illustrates\n                                                              present investigation is that, while we deliberately focus on\nthe potential of sparse attention in the context of attribution\n                                                                sparsity as a post-training intervention, it remains an open\ngraphs, as it enables a unified view of features and circuits.\n                                                           question whether injecting a sparsity bias directly during\nBy jointly analyzing feature activations, attention compo-\n                                                                training would yield qualitatively different or simpler cir-\nnents, and their mediating roles, we obtain a more faithful\n                                                                  cuit structures. Also, a comprehensive exploration of the\npicture of the computational graph underlying the model’s\n                                                       performance trade-offs for larger models and for tasks that\ninput–output behavior.\n                                                             require very dense or long-range attention patterns would\n                                                     be beneficial, even if beyond the computational means cur-\n5. Conclusion                                              rently at our disposal. Moreover, our study is primarily\n                                                                   restricted to sparsifying attention patterns, the underlying\nAchieving interpretability requires innovations in both inter-\n                                                               principle of leveraging sparsity to promote interpretability\npretation techniques and model design. We investigate how\n                                                               naturally extends to other components of the transformer\nlarge models can be trained to be intrinsically interpretable.\n                                                                    architecture. As such, combining the proposed method with\nWe present a flexible post-training procedure that sparsifies\n                                                    complementary approaches for training intrinsically inter-\ntransformer attention while preserving the original pretrain-\n                                                               pretable models, such as Sparse Mixture-of-Experts (Yang\ning loss. By minimally adapting the architecture, we apply a\n                                                                     et al., 2025), sparsifying model weights (Gao et al., 2024),\nsparsity penalty under a constrained-loss objective, allowing\n                                                             or limiting superposition () offers a promising direction for\npre-trained model to reorganise its connectivity into a much\n                                                              future work. Another exciting avenue for future work is\nmore selective and structured pattern.\n                                                                to apply the sparsity regularisation framework developed\nMechanistically, this induced sparsity gives rise to substan-    here within alternative post-training paradigms, such as rein-\ntially simpler circuits: task-relevant computation concen-    forcement learning (Ouyang et al., 2022; Zhou et al., 2024)\ntrates into a small number of attention heads and edges.    or supervised fine-tuning (Pareja et al., 2025).\n\n                                                8\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nImpact Statement                                            ral Information Processing Systems, 36:16318–16352,\n                                                           2023.\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal    Dao,  T.   Flashattention-2:  Faster attention with bet-\nconsequences of our work, none which we feel must be        ter parallelism and work partitioning.  arXiv preprint\nspecifically highlighted here.                                 arXiv:2307.08691, 2023.\n\n                                                      DeepSeek-AI. Deepseek-v3.2: Pushing the frontier of open\nAcknowledgment                                           large language models, 2025. URL https://arxiv.\n                                             org/abs/2512.02556.F. D. acknowledges support through a fellowship from the\nHector Fellow Academy. A. L. is supported by an EPSRC                                                        Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find\nProgramme Grant (EP/V000748/1). I. P. holds concurrent                                                                   interpretable LLM feature circuits. Advances in Neural\nappointments as a Professor of Applied AI at the University                                                             Information Processing Systems, 37:24375–24410, 2024.\nof Oxford and as an Amazon Scholar. This paper describes\nwork performed at the University of Oxford and is not asso-    Elhage,  N.,  Nanda,  N.,   et   al.   A  mathemati-\nciated with Amazon.                                             cal  framework  for  transformer  circuits.     Trans-\n                                                         former Circuits Thread, 2021.    https://transformer-\n                                                               circuits.pub/2021/framework/index.html.\nReferences\n                                                    Gao, L., Rajaram, A., Coxon, J., Govande, S. V., Baker,Ameisen, E., Lindsey, J., Pearce, A., Gurnee, W., Turner,\n                                                                  B., and Mossing, D.   Weight-sparse transformers  N. L., Chen, B., Citro, C., Abrahams, D., Carter, S., Hos-\n                                                        have interpretable circuits.  Technical report, OpenAI,  mer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T.,\n                                                         2024.  URL https://cdn.openai.com/pdf/  McDougall, C., Cunningham, H., Henighan, T., Jermyn,\n                                           41df8f28-d4ef-43e9-aed2-823f9393e470/  A., Jones, A., Persic, A., Qi, Z., Ben Thompson, T., Zim-\n                                             circuit-sparsity-paper.pdf.  merman, S., Rivoire, K., Conerly, T., Olah, C., and Bat-\n  son, J. Circuit tracing: Revealing computational graphs                                                        Gokaslan,  A.  and  Cohen,  V.    Openwebtext  cor-\n   in language models. Transformer Circuits Thread, 2025.                                                               pus.       http://Skylion007.github.io/\n  URL https://transformer-circuits.pub/                                             OpenWebTextCorpus, 2019.\n  2025/attribution-graphs/methods.html.\n                                                        Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nBeltagy,   I.,  Peters, M.  E., and Cohan, A.   Long-       R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\n  former: The long-document transformer. arXiv preprint     Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,\n  arXiv:2004.05150, 2020.                                     K., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel,\n                                                                                       J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N.,\nBereska, L. and Gavves, E. Mechanistic interpretability                                                           Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichan-\n   for ai safety–a review. arXiv preprint arXiv:2404.14082,                                                                     der, A., Schwenk, D., Shah, S., Smith, W., Subramani,\n  2024.                                                               N., Wortsman, M., Dasigi, P., Lambert, N., Richardson,\n                                                                 K., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Ha-\nBommasani, R. e. a. On the opportunities and risks of\n                                                                             jishirzi, H. Olmo: Accelerating the science of language\n  foundation models. ArXiv, 2021. URL https://crfm.\n                                                           models. Preprint, 2024.\n  stanford.edu/assets/report.pdf.\n                                                     Gu, Y., Dong, L., Wei, F., and Huang, M. MiniLLM: Knowl-\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-                                                         edge distillation of large language models. In The Twelfth\n   erating long sequences with sparse transformers. arXiv                                                                International Conference on Learning Representations,\n   preprint arXiv:1904.10509, 2019.                                                         2024. URL https://openreview.net/forum?\n                                              id=5h0qf7IBZZ.\nConerly,    T.,   Cunningham,   H.,   Templeton,   A.,\n  Lindsey,   J.,  Hosmer,  B.,  and Jermyn,  A.    Cir-    Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J.\n   cuits  updates  –  january  2025,  2025.    URL      Memory-efficient transformers via top-k attention. arXiv\n  https://transformer-circuits.pub/             preprint arXiv:2106.06899, 2021.\n  2025/january-update/index.html.    Trans-\n  former Circuits Thread.                              Hanna, M., Piotrowski, M., Lindsey,  J., and Ameisen,\n                                                            E.       circuit-tracer.    https://github.com/\nConmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S.,     safety-research/circuit-tracer,    2025.\n  and Garriga-Alonso, A. Towards automated circuit dis-     The first two authors contributed equally and are listed\n  covery for mechanistic interpretability. Advances in Neu-       alphabetically.\n\n                                                9\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nHeimersheim, S. and Janiak, J. A circuit for python doc-      with human feedback. Advances in neural information\n   strings in a 4-layer attention-only transformer. In Align-      processing systems, 35:27730–27744, 2022.\n  ment Forum, 2023.\n                                                               Pareja, A., Nayak, N. S., Wang, H., Killamsetty, K., Su-\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,       dalairaj, S., Zhao, W., Han, S., Bhandwaldar, A., Xu, G.,\n   S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation      Xu, K., Han, L., Inglis, L., and Srivastava, A. Unveil-\n  of large language models. ICLR, 1(2):3, 2022.                ing the secret recipe: A guide for supervised fine-tuning\n                                                           small LLMs.  In The Thirteenth International Confer-\nJang, E., Gu, S., and Poole, B. Categorical reparameter-                                                         ence on Learning Representations, 2025. URL https:\n   ization with gumbel-softmax. In International Confer-                                            //openreview.net/forum?id=eENHKMTOfW.\n  ence on Learning Representations, 2017. URL https:\n  //openreview.net/forum?id=rkE3y85ee.       Qiu, Z., Wang, Z., Zheng, B., Huang, Z., Wen, K., Yang,\n                                                                         S., Men, R., Yu, L., Huang, F., Huang, S., Liu, D.,\nKamath, H., Ameisen, E., Kauvar, I., Luger, R., Gurnee, W.,                                                       Zhou,  J., and Lin,  J.  Gated attention for large lan-\n  Pearce, A., Zimmerman, S., Batson, J., Conerly, T., Olah,                                                        guage models: Non-linearity, sparsity, and attention-sink-\n   C., and Lindsey, J. Tracing attention computation through                                                                       free. In The Thirty-ninth Annual Conference on Neural\n  feature  interactions.   Transformer Circuits Thread,                                                           Information Processing Systems, 2025. URL https:\n  2025. URL https://transformer-circuits.                                            //openreview.net/forum?id=1b7whO4SfY.\n  pub/2025/attention-qk/index.html.\n                                                        Radford, A., Wu,  J., Child, R., Luan, D., Amodei, D.,\nLei, A., Sch¨olkopf, B., and Posner,  I.  SPARTAN: A                                                               Sutskever, I., et al. Language models are unsupervised\n  sparse transformer world model attending to what mat-                                                                multitask learners. OpenAI blog, 1(8):9, 2019.\n   ters. In The Thirty-ninth Annual Conference on Neural\n  Information Processing Systems, 2025. URL https:   Rezende, D. J. and Viola, F. Taming vaes, 2018. URL\n  //openreview.net/forum?id=uS5ch7GjZ4.      https://arxiv.org/abs/1810.00597.\n\nLindsey,  J., Ameisen, E., Nanda, N., Shabalin, S., Pi-    Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson,\n  otrowski, M., McGrath, T., Hanna, M., Lewis, O., Tigges,       D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar,\n   C., Merullo, J., Watts, C., Paulo, G., Batson, J., Gorton,       Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X.,\n   L., Simon, E., Loeffler, M., McDougall, C., and Lin, J.      Lambert, N., Magnusson, I., Morrison, J., Muennighoff,\n  The circuits research landscape: Results and perspectives.      N., Naik, A., Nam, C., Peters, M. E., Ravichander, A.,\n  Neuronpedia, 2025a. URL https://neuronpedia.     Richardson, K., Shen, Z., Strubell, E., Subramani, N.,\n  org/graph/info.                                         Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N. A.,\n                                                                    Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J.,\nLindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce,                                                      and Lo, K. Dolma: an Open Corpus of Three Trillion\n  A., Turner, N. L., Citro, C., Abrahams, D., Carter,                                                        Tokens for Language Model Pretraining Research. arXiv\n   S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A.,                                                                    preprint, 2024.\n  Bricken, T., McDougall, C., Cunningham, H., Henighan,\n   T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thomp-    Syed, A., Rager, C., and Conmy, A. Attribution patching\n  son, T. B., Zimmerman, S., Rivoire, K., Conerly, T.,      outperforms automated circuit discovery. In Proceedings\n  Olah, C., and Batson, J. On the biology of a large       of the 7th BlackboxNLP Workshop: Analyzing and Inter-\n  language model. Transformer Circuits Thread, 2025b.      preting Neural Networks for NLP, pp. 407–416, 2024.\n URL https://transformer-circuits.pub/\n  2025/attribution-graphs/biology.html.     Yang, X., Venhoff, C., Khakzar, A., de Witt, C. S., Dokania,\n                                                                             P. K., Bibi, A., and Torr, P. Mixture of experts made intrin-\nNanda, N., Chan, L., Lieberum, T., Smith, J., and Stein-       sically interpretable. arXiv preprint arXiv:2503.07639,\n   hardt, J. Progress measures for grokking via mechanistic      2025.\n   interpretability. arXiv preprint arXiv:2301.05217, 2023.\n                                                          Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,        berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\n  N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,      Yang, L., et al. Big bird: Transformers for longer se-\n  A., et al. In-context learning and induction heads. arXiv      quences.  Advances in neural information processing\n  preprint arXiv:2209.11895, 2022.                            systems, 33:17283–17297, 2020.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,    Zhang, F. and Nanda, N. Towards best practices of activation\n  Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,      patching in language models: Metrics and methods. arXiv\n   et al. Training language models to follow instructions       preprint arXiv:2309.16042, 2023.\n\n                                                10\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nZhou, Y., Zanette, A., Pan,  J., Levine, S., and Kumar,\n  A.  Archer: Training language model agents via hier-\n  archical multi-turn rl.  In ICML, 2024. URL https:\n  //openreview.net/forum?id=b6rA0kAHT1.\n\n\n\n\n\n                                                11\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nA. Two-Digit Addition Study\n\n\n\n\n\n   Non-Sparse\n\n\n\n\n\n     Sparse\n\n\n\n\n\nFigure 9. Simple example showing the attention patterns (shown in blue) of sparse and non-sparse transformers trained on a two digit\naddition task. Both models are able to correctly predict the sum, but the attention patterns are very different: the non-sparse model solves\nthe task with highly dispersed information flow, while the sparse model uses a highly interpretable attention pattern: in Layer 0, the model\nfirst attends to the corresponding digits to be added, then in Layer 1, it attends to the carry bit only if it is needed (see middle and right\ncolumns, where the model has to carry once and twice respectively).\n\nIn the introduction, we used a two-digit addition task to demonstrate how sparse attention patterns can lead to intrinsically\ninterpretable circuits. The result presented is gathered in a small scale toy experiment described below. We train 4-layer\nsingle-head Transformer models on a two-digit addition task, where the input is a sequence of digits and the model is trained\nto predict the sum. In this task, there are 13 total tokens: ten digits and three symbols ”+”, ”=” and ”?”.\n\nWithin this setting, we train two models: a standard transformer model and a sparse transformer with a fixed sparsity\nregularisation strength. Figure 9 shows several examples of the learned attention patterns. In these examples, we can clearly\nsee that the pressure of sparsity leads to the emergence of human-recognisable algorithmic patterns: in the first layer, each\ndigit in the answer attends to the corresponding digits in the input, while the second layer computes the carry bit when\nnecessary. By enforcing selective information flow through sparse message-passing, the sparse model is able to learn crisp\nand localised mechanisms that are immediately amenable to interpretation.\n\n\n\n\n\n                                                12\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nB. Sparse Attention Implementation\n\nFor the experiments, we implemented efficient GPU kernels for the sparse attention layers using the helion domain-specific\nlanguage3. We refer to this implementation as Splash Attention (Sparse flash Attention). Our implementation follows the\nsame core algorithmic structure as FlashAttention-2 (Dao, 2023), including the use of online softmax computation and tiling.\nNote that the sparse attention variant (Eq. 2) only differs from the standard attention by a pointwise multiplication of the\nadjacency matrix, which can be easily integrated into FlashAttention by computing Aij on-the-fly. We additionally fuse the\nGumbel-softmax computation, the straight-through gradient, and the computation of the expected number of edges (required\nfor the penalty) into a single optimized kernel, the implementation of which will be released together with the experiment\ncode. Figure 10 compares our Splash Attention implementation against a naive baseline based on PyTorch-native operations.\n\n\n\n\n\n              Figure 10. Performance comparison between our implementation (Splash) and a naive PyTorch baseline.\n\n\n\n\n\n  3https://helionlang.com/\n\n\n                                                13\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nC. Training Details\n\nC.1. Hyperparameters and Compute Resources\n\n\n\n                Hyperparameter       OLMo                  GPT-2\n\n                 Base Model             allenai/OLMo-7B-hf   gpt2\n                   Context window            512                      64\n                    Dataset                dolma-v1             OpenWebText\n                  Batch size                 16                       256\n                   Gradient accumulation steps  4                        4\n                     Total steps                  400,000                     1,200,000\n                   Learning rate              1 × 10−5                  1 × 10−5\n               Minimum learning rate       1 × 10−6                  1 × 10−6\n                   Optimizer              Adam                 Adam\n                  Weight decay                 0.1                            0.1\n                   Scheduler                   Cosine (1 cycle)             Cosine (1 cycle)\n               Warmup steps               1,000                       1,000\n\n                    Finetuning strategy        LoRA                         Full\n              LoRA rank (r)             400                                  -\n              LoRA scaling (α)           800                                  -\n              LoRA dropout             0                                     -\n              LoRA target modules       q,k,v,o,fc in,fc out    -\n\n                 Dual Optimisation LR        0.01                          0.1\n                    Target cross-entropy          2.29                          3.5\n\n                      Table 2. Key hyperparameters used for sparse post-training experiments on OLMo-7B.\n\n\n\nWe provide the key hyperparameters for our experiments in table 2. All training are performed on NVIDIA H100 GPUs.\nThe GPT-2 model is trained on a single GPU while the OLMo model is trained on a node of 8 GPUs. The total training time\nfor both models is roughly 14 days. The main sparse attention code will be made available as a Transformer library wrapper.\nThe implementation code as well as model weights will also be released.\n\n\nC.2. Training Dynamics\n\n\n\n                                                                                                                                                                        3.0\n\n                                                                       3000\n                                                                                                                                                                        2.8\n    10 1                                                                2500\n                                                                                                                                                                                                           Strength 2000                                                                                                                                                                   Entropy 2.6\n  Sparsity                                                                   1500                                                                                                                    Cross\n                                                                                                                                                                        2.4                                                                                                                                                                                                                                                                                                                                                                    Regularisation 1000                                                                                                                                                                                                                                         Validation\n    10 2\n                                                                        500                                                                              2.2\n\n                                                                          0\n                                                                                                                                                                        2.0\n        0          100000        200000        300000        400000        0          100000        200000        300000        400000        0          100000        200000        300000        400000\n                                       Training Steps                                                                         Training Steps                                                                         Training Steps\n\nFigure 11. The training curves for post-training OLMo-7B tacking the model sparsity (left), regularisation strength (middle), and the\ncross-entropy loss (right). The black dotted line on the cross-entropy plot indicates the pre-defined threshold, τ.\n\n\nA key feature of our post-training framework is that the strength of the sparsity regularisation is automatically controlled\nvia a constrained optimisation scheme. By pre-specifying an accepted level for the cross-entropy target, τ, the training\n\n                                                14\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nprocedure can be written as the max-min objective:\n\n                          max min X E |Al| + λ(CE −τ)  ,                                       (7)\n                                 λ>0   θ\n                                                                                     l\n\nwhich can be optimised by taking alternating gradient steps in the model weight space and in the λ space. The resulting\ntraining dynamics means that the sparsity regularisation strength increases when the model cross-entropy is lower than\nthe target, and decreases when the model is above the threshold. Figure 11 shows the training curves for the OLMo-7B\nmodel. Here, we observe that the strength of sparsity regularisation keeps increasing slowly while the model cross-entropy is\nclipped at the desired level. Note that during a model spike (at around 100K steps), the sparsity regularisation automatically\ndecreases to let the model recover.\n\n\n\n\n\n                                                15\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nD. Extra Experiments for Circuit Discovery\n\nIn this section, we provide additional results for the activation patching circuit discovery experiment presented in the main\ntext.\n\nFigure 12 shows the attention patterns of the heads required to explain 90% of model behaviour on a copy task. To fully test\nthe longer context window afforded by OLMo, we use a longer prompt than the one used for GPT2 in the main text. The\nresult is consistent with the GPT-2 experiment: sparsified model facilitates the discovery of smaller circuits of induction\nheads that implement the copy task.\n\n\n\n\n\nFigure 12. Attention patterns of the heads required to explain 90% of model behaviour on a longer copy task. Similar to the GPT-2 results\nin Figure 3, the sparse model requires substantially fewer attention heads.\n\nFigure 13 and 14 show the fraction of explained model preference as a function of the number of model components kept\nun-ablated. The difference between these plots and Figure 4 and 5 lies in the way individual model components are ranked.\nHere, the ranking is performed on a task level, meaning that the importance score for each component is pooled across\ndifferent instances of the same task. Overall, the results are commensurate with results presented in the main paper, where\nthe ranking strategy consistently discover smaller circuits in sparse models. The only exception is the Greater Than task\nfor GPT-2 where the number of attention heads required for the sparse model is larger than that of the base model. We\nhypothesise that this is due to the sparse model choosing different circuits to implement different instances of the same task,\nrendering the task-level importance score less suitable for circuit discovery in this case. Finally, in Figure 15, we provide a\nqualitative visualisation of the edges required to completed the IOI task.\n\n\n\n\n\n                                                16\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n\n\n     Greater Than                      IOI                             Docstring                          IOI Long\n    1.0           0.6x                         1.0               2.0x                  1.0        1.3x                               1.0       1.6x\n\n Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect\n  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5\n\n\n                                  GPT-2                                                                                 OLMo-7B\n    0.0                            Sparse GPT-2        0.0                                                   0.0                         Sparse OLMo-7B        0.0\n                   50          100                             50          100                         250      500      750     1000               250      500      750     1000\n               Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept\n\nFigure 13. Logit attribution per sentence keeping only the top-k attention heads based on a global ranking score. Dotted line annotates the\nnumber of attention heads needed to explain 90% of the logit difference. With the exception of the Greater Than task for GPT-2, the\nsparse models admits smaller circuits.\n\n\n\n\n\n     Greater Than                      IOI                             Docstring                          IOI Long\n    1.0              41.9x                1.0              14.9x                1.0                   5.5x           1.0                   3.1x\n\n Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect\n  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5\n\n\n                                    GPT-2                                                                                 OLMo-7B\n    0.0                              Sparse GPT-2      0.0                                                   0.0                           Sparse OLMo-7B      0.0\n      100       101       102       103       104        100       101       102       103                100    101    102    103    104    105          100    101    102    103    104    105\n            Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)\n\nFigure 14. Logit attribution per sentence keeping only the top-k attention edges based on a global ranking score. Dotted line annotates the\nnumber of attention heads needed to explain 90% of the logit difference.\n\n\n\n\n\n                            Sparse GPT-2                             GPT-2 (baseline)\n\n\n                         Layers\n\n\n\n\n\nFigure 15. An example of the attention-head edges required to reach 0.9 cumulative score based on the averaged scores for the IOI task.\n\n\n\n\n                                                17\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nE. Circuit Discovery Tasks\n\nIn the following, we provide the details and the prompts for the various tasks used in section 4.2.\n\n\nE.1. Greater-Than Task\n\nEach example contains a clean prompt, a corrupt prompt, and two disjoint sets of candidate continuations, answers and\nwrong answers. A typical entry is:\n\n{\n  \"clean\":   \"The demonstrations lasted from the year 1363 to 13\",\n  \"corrupt\": \"The demonstrations lasted from the year 1301 to 13\",\n  \"answers\": [\"64\", \"65\", ..., \"99\"],\n  \"wrong_answers\": [\"00\", \"01\", ..., \"63\"]\n}\n\nFor the clean prompt, any token in answers yields an end year strictly greater than the start year (e.g. \"1364\"–\"1399\"),\nwhereas tokens in wrong answers correspond to years that are less than or equal to the start year. The corrupt prompt\nchanges only the starting year, shifting which continuations correspond to valid end years. We use the logit difference\nbetween the aggregated probability mass on answers vs. wrong answers in clean vs. corrupt contexts as our signal, in\nthe spirit of prior mechanistic studies on simple algorithmic tasks (Elhage et al., 2021; Nanda et al., 2023).\n\n\nE.2. Indirect Object Identification (IOI) Task\n\nOur IOI setup follows the standard indirect object identification paradigm for mechanistic interpretability (Elhage et al.,\n2021; Conmy et al., 2023). Each example is generated by combining:\n\n   • a pair of names (A, B), e.g. (\" Mary\", \" John\");\n\n   • a natural-language template with placeholders [A], [B], and [S].\n\n\nWe instantiate templates such as:\n\n\"Then, [B] and [A] went to the park. [S] gave a ball to\"\n\n\"When [B] and [A] got a snack at the cafe, [S] decided to give it to\"\n\n\"After the lunch, [B] and [A] went to the mall. [S] gave a gift to\"\n\nby sampling a name pair and substituting [A] and [B], then choosing the subject [S] (either one of the pair). The correct\ncontinuation is the indirect object, i.e. the other member of the pair.\n\nFor example, with (A, B) = (\" John\", \" Mary\") and S = B, one instance is:\n\n\n   Then, Mary and John went to the park.  Mary gave a ball to\n\nThe correct continuation is \" John\", while \" Mary\" and any distractor names are treated as incorrect candidates.\n\nIn the OLMo experiments, in order to further test the capability of our approach, we use a different set of IOI task with\nincreased complexity and prompt length. Example templates include:\n\n\"After several months without any contact due to conflicting schedules and\nunexpected personal obligations, [B] and [A] finally met again at the park,\nwhere they spent a long afternoon catching up on past events, sharing stories,\nand reflecting on how much had changed. As the day came to an end, [S] gave\na ball to\"\n\n\n\n                                                18\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n\"Although [B] and [A] had previously been involved in a long and emotionally\ncharged argument that left several issues unresolved, they agreed to meet in\norder to clarify their misunderstandings. After a tense but honest conversation,\n[S] said to\"\n\n\nE.3. Docstring Task\n\nWe also test the OLMo models on a more complex Docstring task (Heimersheim & Janiak, 2023; Conmy et al., 2023), where\nthe model needs to attend to a specific argument for a specified function in order to complete a Docstring. Similarly to the\nGreater Than task, each example contains a clean prompt, a corrupt prompt, and two disjoint sets of candidate continuations.\nA typical entry is:\n\n{\n  \"clean\": \"def model(self, results, old, option):\n                \"\"\"\n                stage agency security vision spot tone joy session river unit\n                :param results: bone paper selection sky\n                :param old: host action hell miss\n                :param\",\n  \"corrupt\": \"def model(self, command, output, state):\n                \"\"\"\n                stage agency security vision spot tone joy session river unit\n                :param old: bone paper selection sky\n                :param results: host action hell miss\n                param\",\n  \"answers\": [\" option\"],\n  \"wrong_answers\": [\" results\",\" old\"]\n}\n\n\n\n\n\n                                                19\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\n                        Category                 Setting\n                       Model                 GPT-2 (HookedTransformer)\n                            Input dimension (din)      768\n                            Latent dimension (dlatent)   24 576\n                         Expansion factor         32\n                          Context size             64\n                         Batch size (tokens)       1 024\n                             Precision               Mixed (FP32 / AMP)\n                         Device            CUDA\n                             Distributed training     DDP\n\n                          Optimizer            Adam\n                          Learning rate            2 × 10−4\n                   Adam β1 / β2              0.9 / 0.999\n                          Learning rate warm-up     Cosine (1 000 steps)\n                          Learning rate decay steps  1 874\n                             Final LR scale             0.1\n\n                      L0 coefficient            2\n                         Optimal L0              3\n                      L0 warm-up               Linear (18 749 steps)\n                      Dead feature penalty      10−5\n                      Dead feature window      250\n\n                              Table 3. Training configuration for the GPT-2 cross-layer-transcoders.\n\n\n\nF. Cross-Layer-Transcoder\n\nTo implement a cross-layer transcoder, let hℓ∈Rdmodel denote the input to the MLP at layer ℓfor a single token position.\nThis representation is projected into a sparse feature space via an encoder,\n\n\n                         zℓ= ReLU Wℓenchℓ+ bℓenc  ∈Rdfeatures,                                       (8)\n\nwhere Wℓenc ∈Rdfeatures×dmodel and bℓenc ∈Rdfeatures are layer-specific encoder parameters.\nThe CLT reconstructs the MLP output at a target layer ℓ′ by linearly aggregating feature activations originating from all\npreceding layers,\n\n                                                 ˆmℓ′ = X Wℓ→ℓ′dec zℓ+ bℓ′dec,                                             (9)\n                                                         ℓ≤ℓ′\n\n\nwhere Wℓ→ℓ′dec   ∈Rdmodel×dfeatures denotes the decoder mapping from layer ℓto layer ℓ′.\n\nThe summation over layers reflects the fact that a given semantic feature may manifest in different representations across\nmultiple MLP layers. For example, a feature that emerges in the MLP at layer ℓmay reappear, potentially in a transformed\nform, in the outputs of subsequent MLPs. Without accounting for these layer-dependent variations, such duplicated\nrepresentations would lead to redundant nodes in the attribution graph. By allowing features to be represented differently\nacross layers while being linked through a shared latent space, the cross-layer transcoder avoids this duplication and yields\na more compact and interpretable attribution structure. For a detailed comparison between cross-layer transcoders and\nstandard transcoders, we refer the reader to Lindsey et al. (2025a).\n\nFollowing the training procedure proposed by Anthropic (Ameisen et al., 2025), the final objective combines reconstruction\n\n\n                                                20\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\naccuracy with sparsity and dead-feature regularization:\n\n                     L = X ∥ˆmℓ′ −mℓ′∥22\n                                                             ℓ′\n                                     | MSE reconstruction{z      }\n                     + λ0 X tanh C (zℓ⊙∥Wℓdec∥)\n                                                          ℓ\n                                          |         L0 sparsity{z           }\n                     + λdf X ReLU exp(τ) −hpreℓ  ∥Wℓdec∥ ,                               (10)\n                                                           ℓ\n                                           |              dead-feature{z penalty         }\n\nwhere Wℓdec denotes the concatenated decoder weights associated with layer ℓ, hpreℓ   are the corresponding pre-activation\nvalues, τ is a threshold parameter, and C is a scaling constant. The hyperparameters λ0 and λdf control the strength of the\nsparsity and dead-feature regularization terms. We initialize the weights with following circuits updates (Conerly et al.,\n2025). The encoder biais is initialize to have a fixed proportion of the features active at initialization. We provide in\nFigure 16 the training curves of the sparsity value, the sparsity coefficient, the explained variance, and the amount of dead\nfeatures. We hope this can help the community in training their own cross-layer transcoders.\n\n\n\n\n\n                      (a) L0 vs steps                                                          (b) L0 coefficient vs steps\n\n\n\n\n\n                  (c) Dead features vs steps                                                  (d) Explained variance vs steps\n\nFigure 16. Training dynamics of the cross-layer transcoder, showing sparsity, regularization strength, dead features, and reconstruction\nquality over training.\n\n\n\n                                                21\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nG. Attribution-Graph\n\nFollowing Ameisen et al. (2025), we define the attribution score between feature n at layer ℓand position k, and feature n′\nat layer ℓ′ and position k′, as\n                                                   aℓ′,k′,n′ℓ,k,n = X fk,nℓ→s Jℓ′,k′s,k   gℓ′k′,n′,                                     (11)\n                                                   ℓ≤s≤ℓ′\nwhere fk,nℓ→s denotes the decoder vector associated with feature n projecting from layer ℓto layer s, Jℓ′,k′s,k   is the Jacobian\nmapping the MLP output at (ℓ, k) to the MLP input at (ℓ′, k′), and gℓ′k′,n′ is the corresponding encoder feature at layer ℓ′ and\nposition k′. The sum in this equation reflects the cross-layer mapping of the cross-layer transcoder.\n\nThe Jacobian is computed during a modified forward pass in which all nonlinear operations, including normalization layers,\nattention mechanisms, and MLPs, are frozen using stop-gradient operations. The resulting attribution graph is pruned by\nretaining only those features that cumulatively explain 80% of the contribution to the final logit, and only those edges that\naccount for 95% of the total edge-level effect. All attribution computations are performed using the circuit-tracer\nlibrary (Hanna et al., 2025). For a complete description of the attribution graph computation and pruning, we refer the user\nto reading (Ameisen et al., 2025).\n\nFor the visualization and the autointerp, we write our own pipeline. In Figure 17, we show a screenshot of the interface for\nthe ’The opposite of ”large” is ”’ attribution graph. The features are colored with respect to their corresponding clusters.\n\n\n\n\n                                             Clusters: opposite – large – brackets – say small\n\n\n\n\n\n                Figure 17. Circuit-tracing interface example for the ’The opposite of ”large” is ”’ with GPT2-sparse.\n\n\n\n\n\n                                                22\n\n                            Sparse Attention Post-Training for Mechanistic Interpretability\n\nH. Graph: The opposite of ”large” is ”\n\nWe obtain a replacement score of 0.82, with 459 features identified before pruning and 82 features remaining after pruning.\nThe majority of features in the resulting attribution graph fall into four dominant clusters:\n\n\n\n   • Opposition cluster: features associated with opposition and comparison, primarily localized at the token position\n     corresponding to opposite.\n\n\n\n   • Magnitude cluster: features related to notions of size (e.g., large, big, full, medium), predominantly located in the\n     residual stream at the large token position.\n\n\n\n   • Bracket cluster: features that activate on tokens enclosed in brackets.\n\n\n\n   • Final-logit cluster: mainly the final logit itself and a couple of features that activate before the token ”small” or related\n     terms.\n\n\n\nIn the boxes below, we present the top activations of representative feature sets for each cluster.\n\n\n   Feature 1117                                                                                    ”Opposite” cluster\n    in Washington has now adopted the wider measure of student debt outstanding. This new\n   the situation in Syria, Iran and the wider region. ”The\n   recharged by the wider dense forests of Sanjay Van and its overflow drained\n    public, with interesting accounts of Oswald’s demeanor at this significant moment\n   has a slightly wider range. Specifically, the Atom-powered NANO\n   56 becoming part of the wider Seven Years’ War in which Britain and France\n\n\n\n\n   Feature 1337                                                                                    ”Opposite” cluster\n    opposite, piece of Mexico’s cultural identity. I made the hour\n\n    opposite shows, or something bigger, “where there’s villains\n\n    opposite sides of Mars in 2004 and used their instruments to discover geologic evidence\n\n    opposite, but not anymore. Now everything he says to me is some kind\n\n    opposite direction, and had little trouble finding space at the campsites.\n\n   always seem to be just the opposite.\n\n   show a growing trend to cast “no” votes , opposing how much salary\n\n   and the occupation of the opposing forces was generally limited to mutual observation.\n\n   work hand in hand for the purpose of opposing  all movements of the thinking part\n\n   the defense’s inability to stop opposing run games. The Bills have\n\n    ing opposing quarterbacks. The Seahawks not only had depth, they were versatile.\n\n    to win more hand battles particularly when the opposing tackle neutralizes his initial\n\n\n\n                                                23\n\n                         Sparse Attention Post-Training for Mechanistic Interpretability\n\n\nFeature 901                                                                                      ”Large” cluster\nLet’s be honest: When someone advocates for large-scale Muslim\n\nrobot provides a tragicomic reminder of why RWD needs to consider large as\n\nwhat kind of social safety nets should be in place to protect people from large\n\nadvocates to limit the power of large, established corporations, analysts say.\n\nof large law firms is that they are so great that the only reason anyone\n\nthat by scaling up tests, the method would be conducive for use on larger\n\n\n\n\nFeature 933                                                                                      ”Large” cluster\npeople healthy and anticipating health issues before they become a problem . Big Data is\n\n Big brown bucks with funny accents.” Judy flinched at\n\n BIG UP UBUNTU: Ubuntu releases are named after industry.¡—endoftext—¿\n BIG LEAGUE: Barron’s Says The\nthey need to submit their content in the same way  . Big enough\n\napps and offering alternatives routes  . Big data and optical fiber\n\n\n\n\nFeature 1004                                                                                     ”Large” cluster\nguide said was ?  full of drinking saloons, dime museums, small\nwould have 2 mana sources next turn (unless his hand was  full of fast\n’s house, it  ’s full of adventure itself.?\nstatement that all German Catholics had a right to  full transparency”\nglimpsing a lobby  full of construction debris. The front hallway was  full of\nJokubas had recently been reading a newspaper article which was  full of\n\n\n\n\nFeature 412                                                                                       “Brackets” cluster\ngroup answered either “very” or “somewhat” attached – except\nsome work colleagues. Wilcox said she found it ‘highly unlikely\n\nvery rare , “very likely ,” “high  risk,” she says.\nulent. Pentagon spokesman Peter Cook said the sample was “\non of PopMatters called the album “brilliant” and said\narlene Lowe, described him as being “one of my biggest supporters”.\n\n\n\n\nFeature 518                                                                                       “Brackets” cluster\nKerry said Washington and Hanoi will “continue to have differences in opinions\nthe United States will “take care of it.” He told reporters after the\nlegislation would “provide new enforcement tools for protecting our citizens and will help\n\nGary Ross, said in a statement that the Air Force is currently “short\nsaid, and Syrian President Bashar al-Assad would “have to go”.\nintroduces politics into consumer policies,” said Palmor, adding that it would ”\n\n\n\n                                             24\n\n## Claim to Classify\n\"Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Ablation",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The method involves ablation (Rung 2 interventional). The claim uses 'circuit' with quantitative reduction claims (100x fewer edges), implying uniqueness and mechanistic structure (Rung 3). While not explicitly testing uniqueness, the magnitude claim and circuit framing suggest a R3 mechanistic claim about computational structure rather than just causal effects."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 23393,
      "completion_tokens": 510,
      "total_tokens": 23903
    },
    "finish_reason": "stop"
  }
}