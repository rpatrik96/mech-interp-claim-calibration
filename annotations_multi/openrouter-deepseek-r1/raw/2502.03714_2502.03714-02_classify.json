{
  "timestamp": "2026-02-19T11:10:29.058273+00:00",
  "paper_id": "2502.03714",
  "claim_id": "2502.03714-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2502.03714\nTitle: Universal SAEs: Interpretable Cross-Model Concept Alignment\nFull text:\n       Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n              Harrish Thasarathan 1 2  Julian Forsyth 1 Thomas Fel 3 Matthew Kowal 1 2 4 Konstantinos Derpanis 1 2\n\n\n2025\nFeb\n6\n\n\n\n\n\n           Figure 1. Overview of Universal Sparse Autoencoders. (A) We introduce Universal Sparse Autoencoders (USAEs), a method for[cs.CV]     discovering common concepts across multiple different deep neural networks. USAEs are simultaneously trained on the activations of\n            multiple models and are constrained to share an aligned and interpretable dictionary of discovered concepts. (B) We also demonstrate\n          one immediate application of USAEs, Coordinated Activation Maximization, where optimizing the inputs of multiple models to activate\n            the same concepts reveals how different models encode the same concept. Visualization reveals interesting concepts at various levels of\n             abstraction, such as ‘curves’ (top), ‘animal haunch’ (middle) and ‘the faces of crowds’ (bottom). Better viewed with zoom.\n\n                         Abstract                               portant universal concepts across vision models;\n                                                                        ranging from low-level features (e.g., colors and\n          We present Universal Sparse Autoencoders (US-\n                                                                                    textures) to higher-level structures (e.g., parts and\n              AEs), a framework for uncovering and aligning in-\n                                                                                     objects). Overall, USAEs provide a powerful new\n                 terpretable concepts spanning multiple pretrained\n                                                                 method for interpretable cross-model analysis and\n             deep neural networks. Unlike existing concept-\n                                                                                 offers novel applications—such as coordinated\n              based interpretability methods, which focus on\n                                                                                activation maximization—that open avenues for\n              a single model, USAEs jointly learn a universal\n                                                                        deeper insights in multi-model AI systems.\n              concept space that can reconstruct and interpret\n                the internal activations of multiple models at once.arXiv:2502.03714v1            Our core insight is to train a single, overcom-        1. Introduction\n                plete sparse autoencoder (SAE) that ingests ac-\n                                                                         In this work, we focus on discovering interpretable concepts                 tivations from any model and decodes them to\n                                                                   shared among multiple pretrained deep neural networks              approximate the activations of any other model\n                                                           (DNNs). The goal is to learn a universal concept space              under consideration. By optimizing a shared ob-\n                                                          – a joint space of concepts – that provides a unified lens                  jective, the learned dictionary captures common\n                                                                           into the hidden representations of diverse models. We de-                factors of variation—concepts—across different\n                                                                                fine concepts as the abstractions each network captures that                 tasks, architectures, and datasets. We show that\n                                                                     transcend individual data points—spanning low-level fea-           USAEs discover semantically coherent and im-\n                                                                             tures (e.g., colors and textures) to high-level attributes (e.g.,\n           1EECS York University, Toronto, Canada 2Vector Institute,    emotions like horror and ideas like holidays).\n            Toronto, Canada 3Kempner Institute, Harvard University, Boston,\n       USA 4FAR AI. Correspondence to: Harrish Thasarathan <har-    Grasping the underlying representations within DNNs is\n           ryt@yorku.ca>.                                                   crucial for mitigating risks during deployment (Buolamwini\n                                       & Gebru, 2018; Hansson et al., 2021), fostering the develop-\n            Preprint. Under review.\n\n                                                         1\n\n                       Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nment of innovative model architectures (Darcet et al., 2023),    els—trained on diverse tasks and datasets—compare and\nand abiding by regulatory frameworks (Commision, 2021;    diverge in their internal representations. Finally, we demon-\nHouse, 2023). Prior interpretability efforts often center on     strate a novel USAE application, coordinated activation\ndissecting a single model for a specific task, leaving risk    maximization, showcasing simultaneous visualization of\nmanagement unmanageable when each network is analyzed    universal concepts across models.\nin isolation. With a growing number of capable DNNs, find-\ning a canonical basis for understanding model internals may    2. Related work\nyield more tractable strategies for managing potential risks.\n                                                  Our work introduces a novel concept-based interpretability\nRecent work supports this possibility. The core idea behind\n                                                    method that adapts SAEs to discover universal concepts. We\n‘foundation models’ (Henderson et al., 2023) presupposes\n                                             now review the most relevant works in each of these fields.\nthat any DNN trained on a large enough dataset should en-\ncode concepts that generalize to an array of downstream    Concept-based interpretability (Kim et al., 2018) emerged\ntasks for that modality. Moreover, recent work has shown    as a response to the limitations of attribution methods (Si-\nthat there is a substantial amount of shared information be-   monyan et al., 2013; Zeiler & Fergus, 2014; Bach et al.,\ntween DNNs trained independently for different tasks or    2015; Springenberg et al., 2014; Smilkov et al., 2017; Sun-\nmodalities (Huh et al., 2024), and recent studies (Dravid    dararajan et al., 2017; Selvaraju et al., 2017; Fong et al.,\net al., 2023; Kowal et al., 2024a) have found shared con-    2019; Fel et al., 2021; Muzellec et al., 2024), which, de-\ncepts across vision models, implying that universality may     spite being widely used for explaining model predictions,\nbe more widespread than previously assumed. However, cur-    often fail to provide a structured or human-interpretable\nrent techniques for identifying universal features (Dravid    understanding of internal model computations (Hase &\net al., 2023; Huh et al., 2024; Kowal et al., 2024a) typically    Bansal, 2020; Hsieh et al., 2021; Nguyen et al., 2021; Colin\noperate post-hoc, extracting concepts from individual mod-    et al., 2021; Kim et al., 2022; Sixt et al., 2020).  Attri-\nels and then matching them through labor-intensive filtering    bution methods highlight input regions responsible for a\nor optimization. This approach is limited in scalability, lacks    given prediction, the where, but do not explain what the\nthe efficiencies of gradient-based training, and precludes    model has learned at a higher level. In contrast, concept-\ntranslation between models within a unified concept space.    based approaches aim to decompose internal representa-\nConsequently, tasks that require simultaneous interaction     tions into human-understandable concepts (Genone & Lom-\nacross multiple models, e.g., coordinated activation maxi-    brozo, 2012). The main components of concept-based inter-\nmization shown later, become more cumbersome.              pretability approaches can generally be broken down into\n                                                    two parts (Fel et al., 2023b): (i) concept discovery, which ex-\nTo overcome these challenges, we introduce a universal\n                                                                      tracts and visualizes the interpretable units of computation\nsparse autoencoder (USAE), Fig. 1, designed to jointly\n                                                    and (ii) concept importance estimation, which quantifies\nencode and reconstruct activations from multiple DNNs.\n                                                             the importance of these units to the model output. Early\nThrough qualitative and quantitative evaluations, we show\n                                                   work explored ‘closed-world’ concept settings in which they\nthat the resulting concept space captures interpretable fea-\n                                                           evaluated the existence of pre-defined concepts in model\ntures shared across all models. Crucially, a USAE imposes\n                                                       neurons (Bau et al., 2017) or layer activations (Kim et al.,\nconcept alignment during its end-to-end training, differing\n                                                            2018). Similar to our work, ‘open-world’ concept discovery\nfrom conventional post-hoc methods. We apply USAEs to\n                                                      methods do not assume the set of concepts is known a priori.\nthree diverse vision models and make several interesting\n                                                     These methods pass data through the model and cluster the\nfindings about shared concepts:  (i) We discover a broad\n                                                                activations to discover concepts and then apply a concept\nrange of universal concepts, at low and high levels of ab-\n                                                       importance method on these discoveries (Ghorbani et al.,\nstraction. (ii) We observe a strong correlation between con-\n                                                       2019; Zhang et al., 2021; Fel et al., 2023c; Graziani et al.,\ncept universality and importance. (iii) We provide quanti-\n                                                        2023; Vielhaben et al., 2023; Kowal et al., 2024a;b).\ntative and qualitative evidence that DinoV2 (Oquab et al.,\n2023) admits unique features compared to other considered    Sparse Autoencoders (SAEs) (Cunningham et al., 2023;\nvision models. (iv) Universal training admits shared repre-    Bricken et al., 2023; Rajamanoharan et al., 2024; Gao et al.,\nsentations not uncovered in model-specific SAE training.     2024; Menon et al., 2024) are a specific instance of dictio-\n                                                         nary learning (Rubinstein et al., 2010; Elad, 2010; Toˇsi´c &\nContributions. Our main contributions are as follows. First,\n                                                             Frossard, 2011; Mairal et al., 2014; Dumitrescu & Irofti,\nwe introduce USAEs: a framework that learns a shared, in-\n                                                        2018) that has regained attention (Chen et al., 2021; Tasissa\nterpretable concept space spanning multiple models, with\n                                                                     et al., 2023; Baccouche et al., 2012; Tariyal et al., 2016;\nfocus on visual tasks. Second, we present a detailed anal-\n                                                   Papyan et al., 2017; Mahdizadehaghdam et al., 2019; Yu\nysis contrasting universal concepts against model-specific\n                                                                       et al., 2023) for its ability to uncover interpretable concepts\nconcepts, offering new insights into how large vision mod-\n                                                                in DNN activations. This resurgence stems from evidence\n\n\n                                                 2\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n                                                         converging toward a shared, Platonic representation of the\n                                                      world (Huh et al., 2024). Another line of research focuses\n                                                  on identifying universal features across models trained on\n                                                                  different tasks. Rosetta Neurons (Dravid et al., 2023) iden-\n                                                                          tify image regions with correlated activations across models,\n                                                        while Rosetta Concepts (Kowal et al., 2024a) extract con-\n                                                            cept vectors from video transformers by analyzing shared\n                                                           exemplars. These methods perform post-hoc mining of uni-\n                                                               versal concepts rather than learning a shared conceptual\n                                                            space. This reliance on retrospective discovery is compu-\n                                                                  tationally prohibitive for many models and prevents direct\n                                                        concept translation between architectures. A concurrent\n                                                           study (Lindsey et al., 2024) explores training SAEs (termed\n                                                           crosscoders) between different states of the same model\nFigure 2. USAE training process. In each forward pass during\n                                                             before and after fine-tuning. In contrast, our work discovers\ntraining, an encoder of model i is randomly selected to encode a\n                                                              universal concepts shared across distinct model architec-\nbatch of that model’s activations, Z = Ψ(i)θ (A(i)). The concept                                                                 tures for vision tasks.\nspace, Z, is then decoded to reconstruct every model’s activations,\nA(j), using their respective decoders, D(j). b                                                      3. Method\nthat individual neurons are often polysemantic—i.e., they    Notations.  Let ∥·∥2 and ∥·∥F denote the ℓ2 and Frobenius\nactivate for multiple, seemingly unrelated concepts (Nguyen    norms, respectively, and set [n] = {1, . . . , n}. We focus on\net al., 2019; Elhage et al., 2022)—suggesting that deep net-    a broad representation learning paradigm, where a DNN,\nworks encode information in superposition (Elhage et al.,   f  : X →A, maps data from X into a feature space, A ⊆\n2022). SAEs tackle this by learning a sparse (Hurley &    Rd. Given a dataset, X ⊆X of size n, these activations are\nRickard, 2009; Eamaz et al., 2022) and overcomplete rep-    collated into a matrix A ∈Rn×d. Each row Ai (for i ∈[n])\nresentation, where the number of concepts exceeds the la-    corresponds to the feature vector of the i-th sample.\ntent dimensions of the activation space, encouraging dis-\nentanglement and interpretability. While SAEs and clus-   Background.  The main goal of a Sparse Autoencoder\ntering bear mathematical resemblance, SAEs benefit from   (SAE) is to find a sparse re-interpretation of the feature\ngradient-based optimization, enabling greater scalability     representations. Concretely, given a set of n inputs, X (e.g.,\nand efficiency in learning structured concepts. Though    images or text) and their encoding, A = f(X) ∈Rn×d, an\nwidely applied in natural language processing (NLP) (Wat-   SAE learns an encoder Ψθ(·) that maps A to codes Z =\ntenberg & Vi´egas, 2024; Clarke et al., 2024; Chanin et al.,   Ψθ(A) ∈Rn×m, forming a sparse representation. This\n2024; Tamkin et al., 2023), SAEs have also been used in    sparse representation must still allow faithful reconstruction\nvision (Fel et al., 2023b; Surkov et al., 2024; Bhalla et al.,    of A through a learned dictionary (decoder) D ∈Rm×d,\n2024a). Early work compared SAEs to clustering and ana-     i.e., ZD must be close to A.  If m > d, we say D is\nlyzed early layers of Inception v1 (Mordvintsev et al., 2015;    overcomplete.  In this work, we specifically consider an\nGorton, 2024), revealing hypothesized but hidden features.    (overcomplete) TopK SAE (Gao et al., 2024), defined as\nMore recently, SAEs have been leveraged to construct text-\nbased concept bottleneck models (Koh et al., 2020) from   Z = Ψθ(A) = TopK Wenc (A −bpre)  , Aˆ = ZD,  (1)\nCLIP representations (Radford et al., 2021; Rao et al., 2024;\nParekh et al., 2024; Bhalla et al., 2024b), showcasing their    where Wenc ∈Rm×d and bpre ∈Rd are learnable weights.\nversatility across modalities. Unlike prior work that apply   The TopK(·) operator enforces ∥Zi∥0 ≤K for all i ∈[m].\nSAEs independently to models, here we consider a joint ap-   The final training loss is given by the Frobenius norm of the\nplication of SAEs fit simultaneously across diverse models.    reconstruction error:\n\nFeature Universality studies the shared information across     LSAE = ∥f(X) −Ψθ f(X) D∥F = ∥A −ZD∥F , (2)\ndifferent DNNs. One approach, Representational Align-\nment, quantifies the mutual information between different    with the K-sparsity constraint applied to the rows of Z.\nsets of representations—whether across models or between\n                                                                    3.1. Universal Sparse Autoencoders (USAEs)biological and artificial systems (Kriegeskorte et al., 2008;\nSucholutsky et al., 2023).  Typically, these methods rely    Contrasting standard SAEs, which reinterpret the internal\non paired data (e.g., text-image pairs) to compare encod-    representations of a single model, universal sparse autoen-\nings across modalities. Recent work suggests that founda-    coders (USAEs) extend this notion across M different mod-\ntion models, regardless of their training modality, may be     els, each with its own feature dimension, di (see Fig. 2).\n\n                                                3\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nConcretely, for model i ∈[M], let A(i) ∈Rn×di denote\nthe matrix of activations for n samples. The key insight of       def train_usae(Ψθ, D, A, T, Optimizers):\n                                                               M = len(Ψθ)USAEs is to learn a shared sparse code, Z ∈Rn×m, which\n                                                               for t in range(T):\nallows every model to be reconstructed from the same sparse                                                                   i = random(M)\nembedding. Specifically, each activation from model i in                                          Z = Ψ(i)θ (A(i))\nA(i) is encoded via a model-specific encoder Ψ(i)θ  , as             L = 0.0\n                                                                   for j in range(M):\n                                                                           = Z @ D(j)   Z = Ψ(i)θ (A(i)) = TopK W enc(A(i)(i)     −b(i)pre)  .   (3)                   A(j)b\n                                                  L += (A(j) - A(j)).norm(p=’fro’)                                                                          b\nCrucially, once encoded into Z, each row of any model              L.backward()\nj ∈[M] can be reconstructed by a model-specific dictionary,              Optimizers[i].step()\nD(j) ∈Rdj×m, as                                              return Ψθ, D\n\n                  A(j) = ZD(j).                   (4)\n                b                                        Figure 3. Training Universal Sparse Autoencoder. During each\nBy   jointly   learning    all   encoder-decoder   pairs,     training iteration, LUniversal is the aggregated error computed from\n                                                              decoding each activation A(j). We then take an optimizer step for\n{(Ψ(i)θ  , D(i))}Mi=1,  the USAE enforces a unified con-                     b                                                           randomly selected encoder Ψ(i)θ  and associated dictionary D(i).cept space, Z, that aligns the internal representations of all\nM models. This shared code not only promotes consistency\nand interpretability across model architectures, but also\nensures each model’s features can be faithfully recovered\nfrom a common set of sparse ‘concepts’.                        3.3. Application: Coordinated Activation Maximization\n\n3.2. Training USAEs                      A common technique for interpreting individual neurons\n                                                            or latent dimensions in deep networks is Activation Maxi-\nRecall that X ⊆X is our dataset of size n, mapped into\n                                                           mization (AM) (Olah et al., 2017; Tsipras et al., 2019; San-their respective feature space using DNNs f (1), . . . , f (M).\n                                                                 turkar et al., 2019; Engstrom et al., 2019; Ghiasi et al., 2021;\nA naive approach to train our respective encoder and de-\n                                                        2022; Fel et al., 2023a; Hamblin et al., 2024). AM involves\ncoder would simultaneously encode and decode the features\n                                                            synthesizing an input that maximally activates a specific\nof all M models, which quickly grows expensive in memory\n                                                   component of a model—such as a neuron, channel, or con-\nand computation. Conversely, randomly sampling a pair of\n                                                            cept vector (Williams, 1986; Mahendran & Vedaldi, 2015;\nmodels to encode and decode results in slow convergence.\n                                          Kim et al., 2018; Fel et al., 2023c). However, in the case\nTo balance these concerns, we adopt an intermediate strat-\n                                                             of a USAE, the learned latent space is explicitly structured\negy (pseudocode detailed in Figure 3) that updates a single\n                                                                to capture shared concepts across multiple models. This\nencoder and decoder at each iteration with a reconstruction\n                                                           shared space enables a novel extension of AM: Coordinated\nloss computed through all decoders. Concretely, at each\n                                                             Activation Maximization, where a common concept index,\nmini-batch iteration, a single model i ∈[M] is selected at\n                                                               k, is simultaneously maximized across all aligned models.random, and a batch of features, A(i) ∈Rn×di, is sampled\nand encoded into the shared code space, Z = Ψ(i)θ (A(i)).   Given M models, our objective is to optimize one input per\nThis code space, Z, is then used to reconstruct the feature    model, x(1)⋆, . . . , x(M)⋆    , ensuring that all inputs maximally\nrepresentation A(j) of every model j ∈[M] via its decoder:    activate the same concept dimension k. This approach en-\nA(j) = ZD(j), where D(j) is the model-j decoder. All    ables the visualization of how a single concept manifests b\nreconstructions are aggregated to form the total loss:          across different models. By comparing these optimized\n           M                                           inputs, we can identify both consistent and divergent repre-\n         LUniversal = X ∥A(j)   A(j)∥F                 (5)    sentations of the same underlying concept. Let x(i) denote             −b                 j=1                                       the input to model i, and let f (i)(x(i)) ∈Rdi represent its\n           M                                            internal activations. Each model is associated with a USAE\n                                                                Ψ(i)θ  , which maps activations to the shared concept         = X ∥A(j) −Ψθ(A(i))D(j)∥F .     (6)    encoder\n                 j=1                                       space. The activation of concept k for model i given input\n                                                                 x(i) is defined as\nUsing this universal loss, backpropagation updates the cho-                Z(i)k (x) = h Ψ(i)θ  f (i)(x) i  ,           (7)\n                                                                                                    ksen encoder Ψ(i)θ  and decoder D(i). This method promotes\nconcept alignment, ensures an equal number of updates be-                                                   where k indexes the universal concept dimension in the\ntween encoders and decoders, and strikes a practical balance                                           USAE. The goal is to independently optimize each x(i)\nbetween training speed and memory usage.                                                        such that it maximizes the activation of the same concept k\n\n\n                                                4\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n         Concept 3152 - Yellow          Concept 4226 - Bird Background          Concept 5611 - Dog Face            Concept 972 - Animal Jaw\n\n  Input\n\n  Dino\n\n  SigLIP\n\n\n ViT\n\n\n          Concept 4235 - Blue             Concept 3859 - Thin Objects            Concept 2824 - Bolts          Concept 1935 - Animal Group Faces\n\n  Input\n\n  Dino\n\n  SigLIP\n\n\n ViT\n\n\nFigure 4. Qualitative results of universal concepts. We discover and visualize heatmaps of universal concepts across a broad range of\nvisual abstractions, where bright green denotes a stronger activation of a given concept. We observe colors, basic shapes, foreground-\nbackground, parts, objects and their groupings across all considered models.\n\nacross all M models:                                        cation of USAEs to coordinated activation maximization\n                                                              (Sec. 4.5).\n          x(i)⋆ = arg max Z(i)k  (x(i)) −λR(x(i)),       (8)\n              x∈X\n\nwhere R(x) is a regularizer that promotes natural and in-   Implementation Details. We train a USAE on the fi-\nterpretable inputs (e.g., total variation, ℓ2 penalty, or data    nal layer activations of three popular vision models: Di-\npriors), and λ controls its strength.  In all experiments,   noV2 (Oquab et al., 2023; Darcet et al., 2023), SigLIP (Zhai\nwe follow the optimization and regularization strategy of     et al., 2023), and ViT (Dosovitskiy et al., 2020) (trained\nMaco (Fel et al., 2023a), which optimizes the input phase    on ImageNet (Deng et al., 2009)). These models, sourced\nwhile preserving its magnitude. Once the optimized inputs    from the timm library (Wightman, 2019), were selected due\nx(i)⋆  are obtained for each model, they reveal the specific     to their diverse training paradigms—image and patch-level\nstructures or features (e.g., model- or task-specific biases)    discriminative learning (DinoV2), image-text contrastive\nthat model i associates with this universal concept.            learning (SigLIP), and supervised classification (ViT). For\n                                                                            all experiments, we train the USAE on the ImageNet train-\n4. Experimental Results                                                          ing set, while the validation set is reserved for qualitative\nThis section is split into six parts. We first provide ex-    visualizations and quantitative evaluations. Our USAE is\nperimental implementation details. Then, we qualitatively    trained on the final layer representations of each vision\nanalyze universal concepts discovered by USAEs (Sec. 4.1).   model, as previous work showed final-layer features facil-\nNext, we provide a quantitative analysis of USAEs through     itate improved concept extraction and yield accurate esti-\nthe validation of activation reconstruction (Sec. 4.2), measur-   mates of feature importance (Fel et al., 2023b). We base\ning the universality and importance of concepts (Secs. 4.3),    our SAE off of the TopK Sparse Autoencoder (SAE) (Gao\nand investigating the consistency between concepts in US-     et al., 2024) and for all experiments, use a dictionary of size\nAEs and individually trained SAE counterparts (Sec. 4.4).    6144. We train all USAEs on a single Nvidia RTX 6000\nFinally, we provide a finer-grained analysis via the appli-   GPU, with training completing in approximately three days\n                                                               (see Appendix A.1 for more implementation details).\n\n                                                5\n\n                       Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n                 Decoder and Activations j used for reconstruction                                  Distribution of Firing Entropy                    Co-firing Proportions for\n                                                                                                                                                       Top Energy-Ranked Concepts                               SigLIP    DinoV2      ViT                                                                                        1500  (a)      Maximum Entropy                                                                                                                                                                                                                                             SigLIP                                                                                                                                                                                     0.8   (b)                                                                                         1.0      Z\n          to                                                                                                                                                                              DinoV2\n                                                                                                                                                                                     0.6                                         ViT                                                                                                                                                                                                                                                                 Concepts                                                                                        1000\n                                                                                                                                                                              Mean                                                                of                      SigLIP    0.61      0.36      0.25            0.8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Proportion\n                                                                                                                                                                                     0.4\n                                                                                         500                                    Encoded                                                                                                                                                                                     0.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Co-firing                                                                                         0.6                                                    Number                                                                                                                                                                                                                                                                                                                                                                          SigLIP:DinoV2:ViT:Overall:0.3260.3440.3120.266                                                                                                                                                                                     0.0                                                                                           0      i DinoV2    0.31      0.77      0.31                                                                                                                0.0    0.2    0.4    0.6    0.8    1.0         0     200    400    600    800    1000\n                                                                                         0.4                                      Normalized Firing Entropy (Hk)              Top 1000 Concepts by Energy (Proportion Sorted)                                                        Activations\n                        ViT    0.30      0.45      0.59            0.2\n                         Model                                                                                         0.0\nFigure 5. Cross model activation reconstruction. Each entry\n(i, j) represents the average R2 score when activations from model\nA(i) are encoded into the shared code space, Z, then decoded via\nD(j) to reconstruct A(j). Positive off-diagonal R2 scores indicate               b\nthe presence of shared features across models captured by USAEs.\n\n                                                                 Figure 6. Quantitative analysis of universality and importance\n4.1. Universal Concept Visualizations\n                                                                     of USAE concepts via co-firing rates. (a) Histogram of firing en-\nWe qualitatively validate the most important universal con-    tropy across all k concepts. We observe a bimodal distribution over\ncepts found by USAEs. We determine concept impor-     firing entropy with peaks at Hk = 1 and Hk = 0.6, demonstrating\ntance by measuring its relative energy towards reconstruc-    a group of concepts that fire uniformly across models and a group\ntion (Gillis, 2020), where the energy of a concept k is de-     that preferentially activates for some models. (b) Proportion of con-\nfined as                                                          cept co-fires for the top 1000 energetic concepts per model. The\n                                                                                          first 200 concepts co-fire between 60 −80% of the time suggest-           Energy(k) = ∥Ex[Zk(x)]Dk∥22.          (9)\n                                                                  ing high universality. (c) Relationship between concept co-firing\nThis measures how much each concept contributes to re-    frequency and concept energy. We show all concepts (left) and\nconstructing the original features – formally, the squared ℓ2     only frequently co-firing concepts (≥1000 co-fires) (right). The\nnorm of the average activation of a concept multiplied by its     correlation strengthens (r = 0.63 vs r = 0.89) when focusing on\ndictionary element. Higher energy concepts have a greater     high-frequency concepts, suggesting a strong correlation between\n                                                  how energetic a concept is and its universality.impact on the reconstruction.\n                                                                    4.2. Validation of Cross-Model ReconstructionFigure 4 presents eight representative concepts selected\nfrom the 100 most important USAE concepts. These con-  A viable universal space of concepts should enable the re-\ncepts span a diverse range of ImageNet categories, demon-    construction of activations from any model. To quantify\nstrating the ability of USAEs to capture meaningful features    the reconstruction performance, we use the coefficient of\nacross multiple levels of abstraction and complexity (Olah    determination, or R2 score (Wright, 1921), which measures\net al., 2017; Fel et al., 2024). At lower levels, the USAE    the proportion of variance in the original activations that\nextracts fundamental color concepts, such as ‘yellow’ and     is captured by the reconstructed activations, relative to the\n‘blue’, activating over broad spatial regions across multiple   mean activation baseline, A.¯ The R2 score is defined as\nclasses. Notably, the blue bottle caps example highlights\na precisely captured checkerboard pattern, demonstrating                                                    R2 = 1 −∥A −bA∥2F /∥A −¯A∥2F ,       (10)\nspatial precision. At intermediate levels, the USAE uncov-\ners structural relationships consistent across models, such                                                   where ||A −bA||2F represents the residual sum of squares\n                                                                 (the reconstruction error), and ||A −¯A||2F is the total sumas foreground-background contrasts (e.g., birds against the\n                                                            of squares (the variance of the original activations relativesky) and thin, wiry objects, independent of model archi-\n                                                                to their mean). A higher R2 indicates better reconstruc-tecture. At higher levels, it identifies object-part concepts,\n                                                                 tion quality, with a score of one corresponding to a perfectlike ‘dog face’, excluding eye regions, and ‘bolts’, which\n                                                                reconstruction.activate across materials like metal and rubber. Finally, the\nUSAE reveals fine-grained, compositional concepts such as    Figure 5 shows the R2 scores as a confusion matrix across\n‘mouth-open animal jaws’ and ‘faces of animals in a group’,     all three models. As expected, self-reconstruction along the\nwhich generalize across ImageNet classes and persist even    diagonal achieves the highest explained variance, confirm-\nin ViT, despite its lack of explicit structured supervision.      ing the USAE’s effectiveness when encoding and decoding\n                                                            within the same model. More notably, positive off-diagonalOverall, these findings show that USAEs discover robust,\n                                            R2 scores indicate successful cross-model reconstruction,generalizable concepts that persist across different archi-\n                                                            suggesting the USAE captures shared, likely universal, fea-tectures, training tasks, and datasets. This highlights their\n                                                                     tures. DinoV2 exhibits the highest self-reconstruction per-ability to reveal invariant, semantically meaningful repre-\n                                                        formance, aligning with individual SAE results where itssentations that transcend the specifics of any single model.\n\n                                                 6\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nR2 score averages 0.8, compared to 0.7 for SigLIP and ViT.             Concept Matches Across                                                                                    Cosine Similarity Thresholds\nThis suggests DinoV2 features are sparser and more decom-          1.0                       SigLIP\n                                                                                                                DinoV2posable, a trend further supported in Secs. 4.3 and 4.5.                                   Threshold0.8                                                                                                                                   ViT         Model   AUC % Z > 0.5                                                        CS\n                            >                       Rand. Baseline\n4.3. Measuring Concept Universality and Importance             0.6                         SigLIP    0.30      0.23\n                                                                              DinoV2    0.36      0.26                                                                                                       0.4Having established the efficacy of cross-model reconstruc-                                                                                 ViT       0.41      0.38                                                                                                                                                                                                                                Concepts\ntion, we now assess concept universality using firing en-    of0.2                            Baseline   0.13      0.00\ntropy     and co-firing                   metrics.                 We                                 further examine                                                                                                       0.0                                               the relation-              Fraction                                                                                                           0.0    0.2    0.4    0.6    0.8    1.0              universality                           importanceship between                      and                                            in reconstructing\n                                                                                               Cosine Similarity Threshold\nground truth activations.\n                                                                 Figure 7. Concept consistency between independent SAEs and\nLet τ be a threshold value and V be the ImageNet validation    Universal SAEs. (left) Our universal training objective discov-\nset of patches. Given data points x ∈V, let Z(i)(x) =     ers concepts that have overlap (i.e., cosine similarity) with those\nΨ(i)θ (f (i)(x)) denote the sparse code from model i ∈[M].    discoveredably more overlap,with independentsuggestingtraining.its simplerSpecifically,architectureViTandhastrainingnotice-\nWe define a concept firing for dimension k when Z(i)k (x) >     objective may yield activations that naturally encode fundamental\nτ. A co-fire occurs when a concept fires simultaneously    and universal visual concepts. (right) We consider a cosine simi-\nacross all models for the same input. Formally, for concept      larity > 0.5 as a match between concepts in the SAE and USAE\ndimension k, the set of co-fires is defined as                    learned dictionaries. Across each vision model used in training,\n                                                       23 −37% of the highly universal concepts discovered by our ap-\n          Ck = {x ∈V : min Z(i)k (x) > τ}.       (11)    proach exist in independently trained SAEs.\n                          i∈[M]\nSimilarly, let F(i)k = {x ∈V  : Z(i)k (x) > τ} denote the   many concepts fire uniformly across models, they do not\nset of “fires” for model i and concept k. We are now ready    reveal how frequently they co-fire on the same tokens. For\nto introduce our two metrics (i) Firing Entropy (FE) and (ii)    each model i and concept k, we compute the proportion of\nCo-Fire Proportion (CFP).                                         total fires that are also co-fires:\nFiring Entropy (FE) measures, for each concept k, the\n                                                                     CFP(i)k = |Ck|/|F(i)k  |.              (14)normalized entropy across models, as\n\n                M\n                    1                             High co-fire proportions indicate concepts that are more\n         FEk = −        X p(i)k  log p(i)k  ,        (12)    universal, i.e., when one model detects the concept, others                      log M\n                            i=1\n                                                           tend to as well.\nwhere              M\n                                                         Figure 6 (b) shows the CFP for the top 1000 concepts\n                   p(i)k = |F(i)k  |/X |F(j)k   |.            (13)                                                          per model. The first ∼100 concepts exhibit high co-firing\n                           j=1\n                                           (> 0.5), activating together 50–80% of the time, indicating\nThe normalization ensures FEk ∈[0, 1], with FE = 1 indi-                                                        a core set of consistently recognized concepts across net-\ncating a shared concept with uniform firing across models                                                       works. The gradual decline in CFP suggests a spectrum\nand low entropy indicating that a concept has a model bias                                                            of universality, from widely shared to model-specific. For\nand fires for a single architecture or subset.                                                        our chosen models, we again notice a pattern distinguish-\nFigure 6 (a) shows a histogram of firing entropies across    ing DinoV2, which has a lower co-firing proportion (0.266)\nall concept dimensions K. Fully universal concepts should    compared to SigLIP (0.344) and ViT (0.326), suggesting\nhave a maximum entropy of one, indicating uniform firing    the latter two share more concepts. This may stem from\nacross models. Our results exhibit a bimodal distribution,    DinoV2’s architecture and distillation-based training, which\nwith over 1000 concepts at peak entropy, confirming the    enhance its adaptability to diverse vision tasks (Amir et al.,\nUSAE learns a strongly universal concept space. A second    2022). These findings also hint at a correlation between\ngroup shows moderate entropy, indicating concepts that fa-    co-firing and concept importance, raising the question: How\nvor two models but not all three. Few concepts fall in the    important are these highly co-firing features?\nlow-entropy range (0.0–0.2), suggesting most are shared                                                  To answer this, we plot the co-fire frequency of all concepts\nrather than model-specific. Appendix A.2.1 further exam-                                                            as well as their energy-based importance in Fig. 6 (c). We\nines these low-entropy concepts, revealing DinoV2’s unique                                                            see a moderate positive correlation r = 0.63, slope = 0.23;\nencoding of geometric features as well as SigLIP’s encoding                                                       however, zooming into concepts with > 1000 co-fires,\nof textual features.                                                  shows a much stronger correlation. Indeed, past a certain\nCo-Fire Proportion (CFP) quantifies how often concepts    threshold, co-firing frequency becomes highly predictable\nfire together for the same input. While previous results show    of concept importance. This suggests that the most impor-\n                                                           tant concept are also highly universal, firing consistently\n\n\n                                                7\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 8. Coordinated Activation Maximization. We show results for the three model USAE along with dataset exemplars, where bright\ngreen denotes stronger activation of the concept. We visualize the maximally activating input for a broad range of concepts, including\nbasic shape compositions, textures, and various objects.\n\nacross models.                                   SAEs have no such selection pressure, learning to represent\n                                                      any concept that helps reconstruction, including architecture\n4.4. Concept Consistency Between USAEs and SAEs                                                             or objective specific concepts that are not universal.\nHow many concepts discovered under our universal training\n                                                                    4.5. Coordinated Activation Maximization\nregime are present in an independently trained SAE for a\nsingle model? Further, what percentage of highly universal    Figure 8 shows a visual comparison of several universal\nconcepts appear in these same independently trained SAEs?    concepts and their corresponding coordinated activation\nTo assess the alignment between independently-trained and    maximization inputs. Our method produces interpretable vi-\nuniversal SAEs, we analyze the similarity of their learned     sualizations for a given USAE dimension across all models\nconceptual spaces. We quantify concept overlap by com-    for a broad range of visual concepts. We show examples\nputing pairwise cosine similarities between decoder vectors    of all models encoding low-level visual primitives, e.g.,\nand use the Hungarian algorithm (Kuhn, 1955) to optimally    ‘curves’ and ‘crosses’. Other basic entities are also shown,\nalign concepts, measuring consistency across models.          like ‘brown grass’ texture and ‘round objects’. Finally, we\n                                                               visualize higher-level concepts corresponding to ‘objects\nFigure 7 presents concept consistency distributions across                                                   from above’ and ‘keypads’. In all cases, our coordinated\nmodels. For a baseline to compare against, we sample con-                                                                activation maximization method produces plausible visual\ncept vectors from normal distributions, where the mean and                                                  phenomenon that can be used to identify differences between\nvariance are those of each independent model’s dictionary.   how each model encodes the same concept.\nWe observe that ViT has the strongest concept overlap with\n38% of its concepts having a cosine similarity > 0.5 with    For example, we note an interesting difference between\nits independent counterpart. This suggests ViT’s conceptual   DinoV2 and the other models: low-mid level concepts (i.e.,\nrepresentation under the independent SAE objective is most     left two columns) appear at a much larger scale than the\nwell preserved under universal training. USAEs achieve far    other models. Further, as shown in Fig. 1, DinoV2 exhibits\nbetter performance than the baseline (Area Under the Curve    stronger activation for the ‘curves’ concept, particularly for\n(AUC)=0.13) across models, suggesting that universal train-    larger curves, compared to the other models. Additionally,\ning preserves meaningful concept alignments rather than    while ‘brown grass’ activates on grass in our heatmaps, some\nlearn entirely new representations. On the other hand the rel-    models’ activation maximizations include birds, suggesting\natively low proportion of overlap (23% and 26% for SigLIP    animals also influence the concept’s activation.\nand DinoV2, respectively) for concepts indicates that uni-    5. Conclusion\nversal training discovers concepts that may not emerge\n                                                              In this work, we introduced Universal Sparse Autoencodersin independent training.  Importantly, this distribution\n                                                    (USAEs), a framework for learning a unified concept spaceremains when looking at the top 1,000 co-firing concepts\n                                                                  that faithfully reconstructs and interprets activations from(see Sec. A.3.1). Universal training naturally selects for\n                                                            multiple deep vision models at once. Our experiments re-concepts that are well-represented across all models, since\n                                                          vealed several important findings: (i) qualitatively, we dis-these will better minimize the total reconstruction loss, bias-\n                                                         cover diverse concepts, from low-level primitives like col-ing towards discovering fundamental visual concepts that\n                                                                    ors, shapes and textures, to compositional, semantic, andall models have learned to represent. Independently trained\n\n\n                                                8\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nabstract concepts like groupings, object parts, and faces, (ii)      Henighan, T., and Olah, C. Towards monosemanticity:\nmany concepts turn out to be both universal (firing consis-     Decomposing language models with dictionary learning.\ntently across different architectures and training objectives)      Transformer Circuits Thread, 2023. https://transformer-\nand highly important (responsible for a large proportion of       circuits.pub/2023/monosemantic-features/index.html.\neach model’s reconstruction), (iii) certain models, such as\nDinoV2, encode unique features even as they share much    Buolamwini, J. and Gebru, T. Gender shades: Intersectional\nof their conceptual basis with others, and (iv) while univer-      accuracy disparities in commercial gender classification.\nsal training recovers a significant fraction of the concepts       In Conference on Fairness, Accountability and Trans-\nlearned by independent single-model SAEs, it also uncovers       parency, 2018.\nnew shared representations that do not appear to emerge in\n                                                         Chanin, D., Wilken-Smith, J., Dulka, T., Bhatnagar, H., andmodel-specific training. Finally, we demonstrated a novel\n                                                     Bloom, J. A is for absorption: Studying feature splittingapplication of USAEs—coordinated activation maximiza-\n                                                      and absorption in sparse autoencoders. arXiv preprinttion—that enables simultaneous visualization of a universal\n                                                           arXiv:2409.14507, 2024.concept across multiple networks. Altogether, our USAE\nframework offers a practical and powerful tool for multi-\n                                                     Chen, J., Mao, H., Wang, Z., and Zhang, X. Low-rank\nmodel interpretability, shedding light on the commonalities\n                                                                 representation with adaptive dictionary learning for sub-\nand distinctions that arise when different architectures, tasks,\n                                                            space clustering. Knowledge-Based Systems, 223:107053,\nand datasets converge on shared high-level abstractions.\n                                                          2021.\n\nReferences                                               Clarke,   M.,   Bhatnagar,   H.,   and  Bloom,    J.\n                                                           Compositionality  and   ambiguity:     Latent   co-\nAmir, S., Gandelsman, Y., Bagon, S., and Dekel, T. Deep\n                                                           occurrence  and   interpretable   subspaces,   2024.\n  ViT Features as Dense Visual Descriptors. Proceedings of\n                                                          https://www.lesswrong.com/posts/WNoqEivcCSg8gJe5h/\n   the European Conference on Computer Vision Workshops\n                                                              compositionality-and-ambiguity-latent-co-occurrence-\n    , 2022.\n                                                              and.\n\nBaccouche, M., Mamalet, F., Wolf, C., Garcia, C., and\n                                                            Colin, J., Fel, T., Cad`ene, R., and Serre, T. What I cannot\n  Baskurt, A. Spatio-temporal convolutional sparse auto-\n                                                                    predict, I do not understand: A human-centered evalua-\n  encoder for sequence classification. In Proceedings of\n                                                                    tion framework for explainability methods. In Advances\n  the British Machine Vision Conference, 2012.\n                                                                    in Neural Information Processing Systems, 2021.\nBach, S., Binder, A., Montavon, G., Klauschen, F., M¨uller,\n                                                    Commision, E. Laying down harmonised rules on artificial  K.-R., and Samek, W. On pixel-wise explanations for\n                                                                    intelligence (artificial intelligence act) and amending cer-  non-linear classifier decisions by layer-wise relevance\n                                                                      tain union legislative acts. European Commision, 2021.  propagation. PloS one, 10(7), 2015.\n\n                                                  Cunningham, H., Ewart, A., Riggs, L., Huben, R., andBau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A.\n                                                            Sharkey, L.   Sparse autoencoders find highly inter-  Network dissection: Quantifying interpretability of deep\n                                                                pretable features in language models.  arXiv preprint  visual representations. In IEEE Conference on Computer\n                                                           arXiv:2309.08600, 2023.  Vision and Pattern Recognition, 2017.\n\nBhalla, U., Oesterling, A., Srinivas, S., Calmon, F. P., and    Darcet, T., Oquab, M., Mairal,  J., and Bojanowski, P.\n  Lakkaraju, H. Interpreting clip with sparse linear concept      Vision transformers need registers.   arXiv preprint\n  embeddings (splice). arXiv preprint arXiv:2402.10376,      arXiv:2309.16588, 2023.\n  2024a.\n                                                     Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nBhalla, U., Srinivas, S., Ghandeharioun, A., and Lakkaraju,      L. ImageNet: A large-scale hierarchical image database.\n  H. Towards unifying interpretability and control: Evalua-      In Proceedings of the IEEE Conference on Computer\n   tion via intervention. arXiv preprint arXiv:2411.04430,      Vision and Pattern Recognition, 2009.\n  2024b.\n                                                           Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nBricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A.,       D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\n  Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A.,      Heigold, G., Gelly, S., et al. An image is worth 16x16\n  Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell,      words: Transformers for image recognition at scale. In\n   T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen,      International Conference on Learning Representations,\n  K., McLean, B., Burke, J. E., Hume, T., Carter, S.,      2020.\n\n                                                9\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nDravid, A., Gandelsman, Y., Efros, A. A., and Shocher, A.      In Proceedings of the IEEE International Conference on\n  Rosetta neurons: Mining the common units in a model      Computer Vision, 2019.\n  zoo. In Proceedings of the IEEE/CVF International Con-\n  ference on Computer Vision, 2023.                     Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R.,\n                                                          Radford, A., Sutskever, I., Leike, J., and Wu, J. Scal-\nDumitrescu, B. and Irofti, P. Dictionary learning algorithms      ing and evaluating sparse autoencoders. arXiv preprint\n  and applications. Springer, 2018.                           arXiv:2406.04093, 2024.\n\nEamaz, A., Yeganegi, F., and Soltanalian, M. On the build-   Genone, J. and Lombrozo, T. Concept possession, experi-\n  ing blocks of sparsity measures. IEEE Signal Processing      mental semantics, and hybrid theories of reference. Philo-\n   Letters, 29:2667–2671, 2022.                                sophical Psychology, 25(5):717–742, 2012.\n\nElad, M.  Sparse and redundant representations: From                                                             Ghiasi, A., Kazemi, H., Reich, S., Zhu, C., Goldblum, M.,\n  theory to applications in signal and image processing.                                                      and Goldstein, T. Plug-in inversion: Model-agnostic in-\n  Springer Science & Business Media, 2010.                                                               version for vision with data augmentations. Proceedings\n                                                                  of the International Conference on Machine Learning,Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan,\n                                                          2021.   T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain,\n  D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J.,\n                                                             Ghiasi, A., Kazemi, H., Borgnia, E., Reich, S., Shu, M.,\n  Amodei, D., Wattenberg, M., and Olah, C. Toy models of\n                                                        Goldblum, M., Wilson, A. G., and Goldstein, T. What do\n  superposition. arXiv preprint arXiv:2209.10652, 2022.\n                                                               vision transformers learn? A visual exploration. arXiv\nEngstrom,  L.,  Ilyas,  A.,  Santurkar,  S.,  Tsipras,  D.,       preprint arXiv:2212.06727, 2022.\n  Tran, B., and Madry, A.  Adversarial robustness as\n                                                        Ghorbani, A., Wexler, J., Zou, J. Y., and Kim, B. Towards\n  a prior for learned representations.   arXiv preprint\n                                                           automatic concept-based explanations. In Advances in\n  arXiv:1906.00945, 2019.\n                                                         Neural Information Processing Systems, 2019.\nFel, T., Cadene, R., Chalvidal, M., Cord, M., Vigouroux,\n                                                                        Gillis, N. Nonnegative matrix factorization. SIAM, 2020.  D., and Serre, T. Look at the variance! Efficient black-\n  box explanations with sobol-based sensitivity analysis.\n                                                        Gorton, L. The missing curve detectors of inceptionv1:\n  In Advances in Neural Information Processing Systems,\n                                                         Applying sparse autoencoders to inceptionv1 early vision.\n  2021.\n                                                           arXiv preprint arXiv:2406.03662, 2024.\nFel, T., Boissin, T., Boutin, V., Picard, A., Novello, P., Colin,\n                                                               Graziani, M., Nguyen, A.-p., O’Mahony, L., M¨uller, H., and\n   J., Linsley, D., Rousseau, T., Cad`ene, R., Goetschalckx,\n                                                         Andrearczyk, V. Concept discovery and dataset explo-\n   L., et al. Unlocking feature visualization for deeper net-\n                                                                   ration with singular value decomposition. In Workshop-\n  works with magnitude constrained optimization. In Ad-\n                                                            Proceedings of the International Conference on Learning\n  vances in Neural Information Processing Systems, 2023a.\n                                                               Representations, 2023.\nFel, T., Boutin, V., Moayeri, M., Cad`ene, R., Bethune, L.,\n                                                     Hamblin, C., Fel, T., Saha, S., Konkle, T., and Alvarez, G.  Chalvidal, M., and Serre, T. A holistic approach to unify-\n                                                            Feature accentuation: Revealing’what’features respond  ing automatic concept extraction and concept importance\n                                                                  to in natural images. arXiv preprint arXiv:2402.10039,  estimation. Advances in Neural Information Processing\n                                                          2024.  Systems, 2023b.\nFel, T., Picard, A., Bethune, L., Boissin, T., Vigouroux, D.,    Hansson, S. O., Belin, M.-˚A., and Lundgren, B. Self-driving\n  Colin, J., Cad`ene, R., and Serre, T. CRAFT: Concept      vehicles-An ethical overview. Philosophy & Technology,\n  recursive activation factorization for explainability. In       pp. 1–26, 2021.\n  Proceedings of the IEEE Conference on Computer Vision\n                                                        Hase, P. and Bansal, M. Evaluating explainable AI: Which\n  and Pattern Recognition, 2023c.\n                                                               algorithmic explanations help users predict model behav-\nFel, T., Bethune, L., Lampinen, A. K., Serre, T., and Her-       ior? Proceedings of the Annual Meeting of the Associa-\n  mann, K. Understanding visual feature reliance through       tion for Computational Linguistics, 2020.\n  the lens of complexity. Advances in Neural Information\n                                                       Henderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley,  Processing Systems, 2024.\n                                             M. A., and Liang, P. Foundation models and fair use.\nFong, R., Patrick, M., and Vedaldi, A. Understanding deep      Journal of Machine Learning Research, 24(400):1–79,\n  networks via extremal perturbations and smooth masks.      2023.\n\n                                                10\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nHouse, T. W. President biden issues executive order on safe,   Kuhn, H. W. The hungarian method for the assignment\n   secure, and trustworthy artificial intelligence. The White      problem. Naval Research Logistics Quarterly, 2:83–97,\n  House, 2023.                                            1955. doi: 10.1002/nav.3800020109.\n\nHsieh, C.-Y., Yeh, C.-K., Liu, X., Ravikumar, P., Kim, S.,    Lindsey, J., Templeton, A., Marcus, J., Conerly, T., Bat-\n  Kumar, S., and Hsieh, C.-J. Evaluations and methods for       son, J., and Olah, C. Sparse crosscoders for cross-layer\n  explanation through robustness analysis. In Proceedings       features and model diffing. 2024.  https://transformer-\n   of the International Conference on Learning Representa-      circuits.pub/2024/crosscoders/index.html.\n   tions, 2021.\n                                                  Mahdizadehaghdam, S., Panahi, A., Krim, H., and Dai,\nHuh, M., Cheung, B., Wang,  T., and Isola,  P.  The      L. Deep dictionary learning: A parametric network ap-\n  platonic representation hypothesis.   arXiv preprint       proach. IEEE Transactions on Image Processing, 28(10):\n  arXiv:2405.07987, 2024.                                4790–4802, 2019.\n\n                                                     Mahendran, A. and Vedaldi, A. Understanding deep imageHurley, N. and Rickard, S. Comparing measures of sparsity.\n                                                                representations by inverting them. In IEEE Conference  IEEE Transactions on Information Theory, 55(10):4723–\n                                                     on Computer Vision and Pattern Recognition, 2015.  4741, 2009.\n\n                                                            Mairal, J., Bach, F., and Ponce, J.  Sparse modeling forIoffe, S. and Szegedy, C. Batch normalization: accelerat-\n                                                     image and vision processing. Foundations and Trends®  ing deep network training by reducing internal covariate\n                                                                    in Computer Graphics and Vision, 8(2-3):85–283, 2014.   shift. In Proceedings of the International Conference on\n  Machine Learning, 2015.                                                  Menon, A., Shrivastava, M., Krueger, D., and Lubana, E. S.\n                                                        Analyzing (in) abilities of SAEs via formal languages.\nKim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J.,\n                                                           arXiv preprint arXiv:2410.11767, 2024.\n  and Viegas, F.  Interpretability beyond feature attribu-\n   tion: Quantitative testing with concept activation vectors    Mordvintsev,  A.,  Olah,  C.,  and Tyka, M.    Incep-\n  (TCAV). In International Conference on Machine Learn-      tionism:    Going  deeper   into  neural  networks.\n   ing, 2018.                                                   https://blog.research.google/2015/06/inceptionism-\n                                                            going-deeper-into-neural.html?m=1, 2015.\nKim, S. S. Y., Meister, N., Ramaswamy, V. V., Fong, R.,\n  and Russakovsky, O. HIVE: Evaluating the human inter-    Muzellec, S., Andeol, L., Fel, T., VanRullen, R., and Serre,\n   pretability of visual explanations. In Proceedings of the       T. Gradient strikes back: How filtering out high frequen-\n  IEEE European Conference on Computer Vision, 2022.        cies improves explanations. Proceedings of the Interna-\n                                                                    tional Conference on Learning Representations, 2024.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\n  optimization. arXiv preprint arXiv:1412.6980, 2014.      Nguyen, A., Yosinski, J., and Clune, J. Understanding neu-\n                                                                            ral networks via feature visualization: A survey. Explain-\nKoh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson,      able AI: interpreting, explaining and visualizing deep\n   E., Kim, B., and Liang, P. Concept bottleneck models. In                                                                 learning, 2019.\n  International Conference on Machine Learning, 2020.\n                                                    Nguyen, G., Kim, D., and Nguyen, A. The effectiveness of\nKowal, M., Dave, A., Ambrus, R., Gaidon, A., Derpanis,       feature attribution methods and its correlation with auto-\n  K. G., and Tokmakov, P. Understanding video transform-      matic evaluation scores. Advances in Neural Information\n   ers via universal concept discovery. In Proceedings of      Processing Systems, 2021.\n  the IEEE Conference on Computer Vision and Pattern\n  Recognition, 2024a.                                    Olah, C., Mordvintsev, A., and Schubert, L.  Feature vi-\n                                                                   sualization.  Distill, 2017. doi: 10.23915/distill.00007.\nKowal, M., Wildes, R. P., and Derpanis, K. G. Visual con-       https://distill.pub/2017/feature-visualization.\n  cept connectome (VCC): Open world concept discovery\n  and their interlayer connections in deep models. In Pro-   Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec,\n  ceedings of the IEEE Conference on Computer Vision      M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F.,\n  and Pattern Recognition, 2024b.                           El-Nouby, Alaaeldin Assran, M., Ballas, N., Galuba, W.,\n                                                     Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M.,\nKriegeskorte, N., Mur, M., and Bandettini, P. A. Repre-     Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J.,\n   sentational similarity analysis-connecting the branches of       Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learn-\n  systems neuroscience. Frontiers in systems neuroscience,      ing robust visual features without supervision. Transac-\n  2:249, 2008.                                                    tions on Machine Learning Research, 2023.\n\n                                                11\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nPapyan, V., Romano, Y., and Elad, M. Convolutional dictio-    Sucholutsky, I., Muttenthaler, L., Weller, A., Peng, A., Bobu,\n  nary learning via local processing. International Confer-       A., Kim, B., Love, B. C., Grant, E., Groen, I., Achterberg,\n  ence on Computer Vision, 2017.                                         J., et al. Getting aligned on representational alignment.\n                                                           arXiv preprint arXiv:2310.13018, 2023.\nParekh, J., Khayatan, P., Shukor, M., Newson, A., and Cord,\n  M. A concept-based explainability framework for large    Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribu-\n  multimodal models. arXiv preprint arXiv:2406.08074,       tion for deep networks. In Proceedings of the Interna-\n  2024.                                                           tional Conference on Machine Learning, 2017.\n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,    Surkov, V., Wendler, C., Terekhov, M., Deschenaux, J.,\n  Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,      West, R., and Gulcehre, C. Unpacking sdxl turbo: Inter-\n   J., Krueger, G., and Sutskever, I. Learning transferable       preting text-to-image models with sparse autoencoders.\n  visual models from natural language supervision.  In      arXiv preprint arXiv:2410.22366, 2024.\n  International Conference on Machine Learning, 2021.\n                                                       Tamkin, A., Taufeeque, M., and Goodman, N. D. Codebook\nRajamanoharan, S., Lieberum, T., Sonnerat, N., Conmy, A.,       features: Sparse and discrete interpretability for neural\n  Varma, V., Kram´ar, J., and Nanda, N. Jumping ahead:      networks. arXiv preprint arXiv:2310.17230, 2023.\n  Improving reconstruction fidelity with jumprelu sparse\n                                                                 Tariyal, S., Majumdar, A., Singh, R., and Vatsa, M. Deep  autoencoders. arXiv preprint arXiv:2407.14435, 2024.\n                                                                 dictionary learning. IEEE Access, 4:10096–10109, 2016.\nRao, S., Mahajan, S., B¨ohle, M., and Schiele, B. Discover-\n                                                                 Tasissa, A., Tankala, P., Murphy, J. M., and Ba, D. K-deep  then-name: Task-agnostic concept bottlenecks via auto-\n                                                             simplex: Manifold learning via local dictionaries. IEEE  mated concept discovery. In Proceedings of the IEEE\n                                                             Transactions on Signal Processing, 2023.  European Conference on Computer Vision, 2024.\n\n                                                                                     Toˇsi´c, I. and Frossard, P. Dictionary learning. IEEE SignalRubinstein, R., Bruckstein, A. M., and Elad, M. Dictionaries\n                                                            Processing Magazine, 28(2):27–38, 2011.   for sparse representation modeling. Proceedings of the\n  IEEE International Conference on Acoustics, Speech and                                                              Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and\n  Signal Processing, 2010.                                                       Madry, A. Robustness may be at odds with accuracy. In\n                                                            Proceedings of the International Conference on LearningSanturkar, S., Ilyas, A., Tsipras, D., Engstrom, L., Tran, B.,\n                                                               Representations, 2019.  and Madry, A. Image synthesis with a single (robust)\n   classifier. Advances in Neural Information Processing                                                          Vielhaben,  J., Bluecher, S., and Strodthoff, N.  Multi-\n  Systems, 32, 2019.                                                            dimensional concept discovery (MCD): A unifying frame-\n                                                  work with completeness guarantees.  Transactions onSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R.,\n                                                     Machine Learning Research, 2023.  Parikh, D., and Batra, D. Grad-CAM: Visual explana-\n   tions from deep networks via gradient-based localization.                                                         Wattenberg, M. and Vi´egas, F. B. Relational composition\n  In IEEE International Conference on Computer Vision,                                                                  in neural networks: A survey and call to action. arXiv\n  2017.                                                                 preprint arXiv:2407.14662, 2024.\n\nSimonyan, K., Vedaldi, A., and Zisserman, A. Deep in-                                                   Wightman, R. PyTorch image models, 2019.\n  side convolutional networks:  Visualising image clas-\n   sification models and saliency maps.  arXiv preprint    Williams, R. Inverting a connectionist network mapping by\n  arXiv:1312.6034, 2013.                                    back-propagation of error. In Proceedings of the Annual\n                                                        Meeting of the Cognitive Science Society, 1986.\nSixt, L., Granz, M., and Landgraf, T. When explanations lie:\n Why many modified BP attributions fail. In Proceedings    Wright, S. Correlation and causation. Journal of Agricul-\n   of the International Conference on Machine Learning,       tural Research, 20(7):557–585, 1921.\n  2020.\n                                                       Yu, Y., Buchanan, S., Pai, D., Chu, T., Wu, Z., Tong, S.,\nSmilkov, D., Thorat, N., Kim, B., Vi´egas, F., and Watten-      Haeffele, B., and Ma, Y. White-box transformers via\n  berg, M. Smoothgrad: Removing noise by adding noise.      sparse rate reduction. Advances in Neural Information\n  arXiv preprint arXiv:1706.03825, 2017.                     Processing Systems, 2023.\n\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-    Zeiler, M. D. and Fergus, R. Visualizing and understanding\n   miller, M. Striving for simplicity: The all convolutional       convolutional networks.  In Proceedings of the IEEE\n   net. arXiv preprint arXiv:1412.6806, 2014.                European Conference on Computer Vision, 2014.\n\n                                                12\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nZhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sig-\n  moid loss for language image pre-training. In IEEE Inter-\n  national Conference on Computer Vision, 2023.\n\nZhang, R., Madumal, P., Miller, T., Ehinger, K. A., and Ru-\n   binstein, B. I. Invertible concept-based explanations for\n CNN models with non-negative concept activation vec-\n   tors. In Proceedings of the AAAI Conference on Artificial\n   Intelligence, 2021.\n\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.,\n  and Kong, T. iBoT: Image bert pre-training with online\n   tokenizer. Proceedings of the International Conference\n  on Learning Representations, 2021.\n\n\n\n\n\n                                                13\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\nA. Appendix\n\nA.1. SAE Training Implementation details\n\n We modify the TopK Sparse Autoencoder (SAE) (Gao et al., 2024) by replacing the ℓ2 loss with an ℓ1 loss, as we find that\nthis adjustment improves both training dynamics and the interpretability of the learned concepts. The encoder consists of\na single linear layer followed by batch normalization (Ioffe & Szegedy, 2015) and a ReLU activation function, while the\ndecoder is a simple dictionary matrix.\n\nFor all experiments, we use a dictionary of size 8 × 768 = 6144 which is an expansion factor of 8 multiplied by the\nlargest feature dimension in any of the three models, 768.  All SAE encoder-decoder pairs have independent Adam\noptimizers (Kingma & Ba, 2014), each with an initial learning rate of 3e−4, which decays to 1e−6 following a cosine\nschedule with linear warmup. To account for variations in activation scales caused by architectural differences, we\nstandardize each model’s activations using 1000 random samples from the training set. Specifically, we compute the mean\nand standard deviation of activations for each model and apply standardization, thereby preserving the relative relationship\nbetween activation magnitudes and directions while mitigating scale differences.\n\nSince SigLIP does not incorporate a class token, we remove class tokens from DinoV2 and ViT to ensure consistency across\nmodels. Additionally, we interpolate the DinoV2 token count to match a patch size of 16 × 16 pixels, aligning it with SigLIP\nand ViT. We train all USAEs on a single NVIDIA RTX 6000 GPU, with training completing in approximately three days.\n\n\nA.2. Discovering Unique Concepts with USAEs\n\nWith our universal training objective, we are in a unique position to explore concepts that may arise independently in one\nmodel, but not in others. Using metrics for universality, Eqs. 13 and 12, we can search for concepts that fire with a low\nentropy, thereby isolating firing distributions whose probability mass is allocated to a single model. We explore this direction\nby isolating unique concepts for DinoV2 and SigLIP, both of which have been studied for their unique generalization\ncapabilities to different downstream tasks (Amir et al., 2022; Zhai et al., 2023).\n\nA.2.1. UNIQUE DINOV2 CONCEPTS\n\n DinoV2’s unique concepts are presented in Figures 9 and 11. Interestingly, we find concepts that solely fire for DinoV2\nrelated to depth and perspective cues. These features follow surfaces and edges to vanishing points as in concept 3715\nand 4189, demonstrating features for converging perspective lines. Further, we find features for object groupings placed\nin the scene at varying depths in concept 4756, and background depth cues related to uphill slanted surfaces in concept\n1710. We also find features that suggest a representation of view invariance, such as concepts related to the angle or tilt of an\nimage (Fig. 10) for both left (concept 3003) and right views (concept 2562). Lastly, we observe unique geometric features in\nFig. 12 that suggest some low-level 3D understanding, such as concept 4191 that fires for the top face of rectangular prisms,\nconcept 3448 for brim lines that belong to dome shaped objects, as well as concept 1530 for corners of objects resembling\nrectangular prisms.\n\nView invariance, depth cues, and low-level geometric concepts are all features we expect to observe unique to DinoV2’s\ntraining regime and architecture (Oquab et al., 2023). Specifically, self-distillation across different views and crops at the\nimage level emphasizes geometric consistency across viewpoints. This, in combination with the masked image modelling\niBOT objective (Zhou et al., 2021) that learns to predict masked tokens in a student-teacher distillation framework, would\nexplain the sensitivity of DinoV2 to perspective and geometric properties, as well as view-invariant features.\n\nA.2.2. UNIQUE SIGLIP CONCEPTS\n\n Similar to DinoV2, we isolate concepts with low firing-entropy where probability mass is concentrated for SigLIP. Example\nconcepts are presented in Fig. 13. We observe concepts that fire for both visual and textual elements of the same concept.\nConcept 5718 fires for both the shape of a star, as well as regions of images with the word or even just a subset of letters on\na bottlecap and sign at different scales. Additionally, concept 2898 fires broadly for various musical instruments, as well as\nmusic notes, while concept 923 fires for the letter ‘C’. For each of these concepts, the coordinated activation maximization\nvisualization has both the physical semantic representation of the concept, as well as printed text. The presence of image and\ntextual elements are expected given SigLIP is trained as a vision-language model with a contrastive learning objective, where\nthe aim is to align image and text latent representations from separate image and language encoders. While we do not train\n\n\n\n                                                14\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 9. Qualitative results of DinoV2 low-entropy concepts. These concepts fire frequently for DinoV2, depicting converging\nperspective lines to the right (concept 3715, above) and to the left (concept 4189, below).\n\n\n\n\n\non any activations directly from the language model, we still observe textual concepts in our image-space visualizations.\n\n\nA.3. Additional Results\n\nA.3.1. ADDITIONAL QUANTITATIVE RESULTS\n\n Figure 14 presents concept consistency distributions across models for the top 1,000 co-firing concepts. We observe\nconsistent findings with Sec. 4.4, mainly that ViT has the strongest concept overlap with 35% of its concepts having a\ncosine similarity > 0.5 with its independent counterpart. USAEs again achieve far better performance than the baseline\nfor all models, suggesting that universal training preserves meaningful concept alignments rather than learn entirely new\nrepresentations. The lower proportion of overlap for SigLIP and DinoV2 indicates that universal training discovers\nuniversal concepts that may not emerge in independent training. Universal training favors concepts that are consistently\nrepresented across all models, as these concepts more effectively reduce overall reconstruction loss. This may lead to a bias\ntoward fundamental visual concepts that are commonly learned by all models. In contrast, independently trained SAEs lack\nthis selection pressure, allowing them to learn any concept that aids reconstruction, including those specific to a particular\narchitecture or objective, rather than universally shared ones.\n\nA.3.2. ADDITIONAL QUALITATIVE RESULTS\n\nWe provide additional universal concept visualizations for the top activating images for that concept across each model.\nSpecifically, we showcase low-level concepts in Fig. 15 related to texture like shell and wood for concepts 1716 and 2533,\nrespectively, as well as tiling for concept 5563. We also showcase high-level concepts in Fig. 16 related to environments like\nauditoriums in concept 4691, object interactions like ground contact in concept 5346, as well as facial features like snouts in\nconcept 3479.\n\n                                                15\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 10. Qualitative results of low-entropy concepts that fire for DinoV2. We discover concepts related to view-invariance, such as\nangled scenes both right (above) and left (below) in concept 2562 and 3003, respectively.\n\n\nA.4. Limitations\n\nOur universal concept discovery objective successfully discovers fundamental visual concepts encoded between vision\nmodels trained under distinct objectives and architectures, and allows us to explore features that fire distinctly for a particular\nmodel of interest under our regime. However, we note some limitations that we aim to address in future work. We\nnotice some sensitivity to hyperparameters when increasing the number of models involved in universal training, and use\nhyperparameter sweeps to find an optimal configuration. We also constrain our problem to discovering features at the\nlast layer of each vision model. We choose to do so as a tractable first step in this novel paradigm of learning to discover\nuniversal features. We leave an exploration of universal features across different layer depths for future work. Lastly, we do\nfind qualitatively that a small percentage of concepts are uninterpretable. They may be still stored in superposition (Elhage\net al., 2022) or they could be useful for the model but simply difficult for humans to make sense of. This is a phenomena\nthat independently trained SAEs suffer from as well. Many of the limitations of our approach are tightly coupled to the\nlimitations of training independent SAEs, an active area of research.\n\n\n\n\n\n                                                16\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 11. Qualitative results of low-entropy concepts that fire for DinoV2. We discover features related to depth cues for foreground\nobjects as well as background in concept 4756 (above) and 1710 (below).\n\n\n\n\n\nFigure 12. Qualitative results for low-entropy concepts that fire for DinoV2. We discover DinoV2 independent features that are not\nuniversal suggesting 3D understanding like corners (concepts 1530), top face of rectangular prism (concept 4191), and brim of dome\n(concept 3448).\n\n\n                                                17\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 13. Qualitative results of low-entropy SigLIP concepts. We consistently find concepts that fire for abstract concepts in image\nspace such as images or text of ‘star’ (concept 923), letters (concept 5718), and music notes (concept 2958).\n\n\n\n\n                                                                Single to Universal:\n                                         Concept Matches Across Cosine Similarity Thresholds\n                                                       Top 1000 cofiring Concepts\n                                                         1.0                                                     SigLIP (AUC=0.30)\n                                                                                           DinoV2 (AUC=0.36)\n                                                                                                          ViT (AUC=0.40)\n                                                                                Random Baseline (AUC=0.13)\n                                                         0.8                                                                                                                       Threshold\n                                        MCS 0.6\n             >\n\n\n                                                         0.4                                                                                                         Concepts\n                          of\n\n                                                         0.2                                                                                                         Fraction\n\n                                                         0.0\n\n                                                                0.0             0.2             0.4             0.6             0.8             1.0\n                                                                    Cosine Similarity Threshold\n\n\nFigure 14. Top 1000 co-firing concept consistency between independent SAEs and Universal SAEs. Our universal training objective\ndiscovers universal concepts that have overlap (i.e., cosine similarity) with those discovered with independent training. ViT again has\nnoticeably more overlap, suggesting its simpler architecture and training objective may yield activations that naturally encode fundamental\nand universal visual concepts.\n\n\n\n                                                18\n\n                      Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment\n\n\n\n\n\nFigure 15. Qualitative results of universal concepts. We depict low-level visual features related to textures, such as shells (concept\n1716), wood (concept 2533), and tiling (concept 5563).\n\n\n\n\n\nFigure 16. Qualitative results of universal concepts. We depict high-level visual features related to environments, such as auditoriums\n(concept 4691), ground contact (concept 5346), and animal snouts (concept 3479).\n\n\n\n\n\n                                                19\n\n## Claim to Classify\n\"the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "SAE feature attribution",
    "method_rung": 1,
    "claim_rung": 1,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "The method used is SAE feature attribution (explicitly listed as Rung 1 in the codebook), which is observational and establishes correlational evidence without intervention. The claim uses 'captures' to describe identifying common factors of variation (concepts), which aligns with Rung 1 linguistic markers like 'information is present' or 'can be extracted'. The decision tree for polysemous terms like 'represents' confirms R1 when no interventional evidence exists and context implies decodability (here, reconstruction via autoencoders). No hedging language is present. Confidence is 4 due to minor ambiguity in 'captures', but context strongly supports associational interpretation."
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "usage": {
      "prompt_tokens": 23189,
      "completion_tokens": 1077,
      "total_tokens": 24266
    },
    "finish_reason": "stop"
  }
}