paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,Activation patching and causal interventions,2,3,1,4,0,"The claim 'presenting an explanation for how GPT-2 small performs [the IOI task]' uses functional/mechanistic language ('how... performs') consistent with Rung 3 claims. The paper employs interventional methods like activation patching (path patching) and knockouts (Rung 2) but makes a mechanistic claim about the model's internal algorithm. This matches the overclaim pattern 'Patching → THE circuit' (+1 gap). Though the paper later qualifies completeness (Section 4), the abstract claim is unhedged and definitive. Confidence is 4 due to minor ambiguity in scope (e.g., backup heads indicate partial redundancy), but the core claim is clearly Rung 3."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,Activation patching and ablation,2,3,1,4,0,"The claim describes the circuit (a mechanistic explanation) found using interventional methods (activation patching and ablation, Rung 2). The claim about the circuit's structure is part of a mechanistic explanation (Rung 3) as it encompasses the grouping of heads that implement the task. There is no hedging language."
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,"Activation patching, causal tracing, ablation",2,3,1,5,0,"The claim uses 'reverse-engineering', a Rung 3 linguistic marker indicating a mechanistic/uniqueness claim about the model's internal algorithm. The paper's methods (activation patching, causal tracing, ablation) are Rung 2 interventions that establish causal effects but not counterfactual necessity or uniqueness. This matches the calibration pattern (IOI paper) where 'reverse-engineering' claims Rung 3 despite Rung 2 evidence. No hedging language is present. Confidence is high due to explicit codebook alignment and calibration precedent."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,Activation patching and ablation,2,3,1,5,0,"The claim 'move the name from the subject to the end position' uses the functional verb 'move', which is a Rung 3 linguistic marker (mechanistic/functional attribution) per the codebook. The supporting methods—activation patching (Rung 2) and ablation (Rung 2)—establish causal sufficiency but not uniqueness or full mechanistic implementation. This matches the calibration example (IOI paper) where 'moves' claims were Rung 3 despite Rung 2 evidence. No hedging language is present."
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,Activation patching,2,2,0,4,0,"The claim 'S-Inhibition Heads inhibit attention from the Name Mover Heads to the S token' is supported by activation patching interventions (Section 3.2, Figure 4c), a Rung 2 method. The term 'inhibit' implies a causal effect (reducing attention to S tokens), aligning with Rung 2 linguistic markers like 'intervening on X changes Y'. No hedging language is present. Confidence is 4 due to clear experimental validation but minor ambiguity in the scope of 'inhibit'."
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,Activation patching and ablation,2,2,0,4,0,"The claim 'the circuit is faithful complete and minimal' is about properties evaluated using causal interventions (Rung 2 methods). Faithfulness, completeness, and minimality are tested via activation patching and ablation, which are Rung 2 methods. The claim does not assert a unique mechanism (which would be Rung 3) but rather the circuit's causal sufficiency and necessity under interventions. The use of 'the circuit' is a naming convention for the circuit they discovered, and the paper acknowledges gaps, so it is not a uniqueness claim."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,Causal Tracing and ROME,2,3,1,4,1,"The claim is supported by interventional methods (Causal Tracing and ROME, Rung 2). The claim describes a mechanistic property of factual associations being localized and directly-editable, which is a Rung 3 claim. The phrase 'finding evidence that' in the abstract introduces hedging. Confidence is 4 due to minor ambiguity in the phrasing 'correspond to', but the context clearly indicates a mechanistic claim."
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,Causal mediation analysis (causal tracing),2,2,0,5,0,"The method used is causal tracing, which is an interventional method (Rung 2). The claim uses the term 'mediate', which is a linguistic marker for Rung 2 (Causal Claims). There is no hedging in the claim. This is a clear case with high confidence."
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal Tracing and ROME editing,2,3,1,4,0,"The claim 'storing factual associations' uses the term 'storing', which is a Rung 3 mechanistic marker per the codebook's polysemous terms decision tree. The paper supports this with Rung 2 methods: causal tracing (interventional patching) and ROME editing (weight intervention). No hedging language is present. Confidence is 4 due to strong empirical support but inherent ambiguity in inferring storage mechanisms from interventions."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,ROME editing,2,2,0,5,0,"ROME editing is explicitly classified as a Rung 2 (Interventional) method in the codebook (under 'Steering vectors' and 'ROME/MEMIT edits'). The claim 'ROME is effective' describes a causal outcome of this intervention (model performance improvement), matching Rung 2 linguistic markers ('is effective' implies causal sufficiency). No hedging language is present, and the classification aligns perfectly with the codebook definitions and examples."
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,Ablation and weight/activation analysis,2,3,1,4,0,"The claim 'fully reverse engineer the algorithm' uses the linguistic marker 'reverse engineer' which typically implies a complete mechanistic understanding (Rung 3). The methods include interventional techniques like ablation (Rung 2) and observational analysis (Rung 1), with ablation being the highest-rung method directly supporting causal claims. However, the paper does not test uniqueness (e.g., via counterfactual scrubbing), so the claim overreaches the method by one rung. The claim is stated definitively without hedging. Confidence is 4 due to minor ambiguity from the lack of explicit uniqueness proof, though the field convention supports Rung 3 for such phrasing."
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,Ablation in Fourier space,2,3,1,4,0,"The claim 'uses discrete Fourier transforms and trigonometric identities to convert addition to rotation' describes a functional mechanism (Rung 3 linguistic marker: 'uses X to do Y'). The paper confirms this via ablation in Fourier space (Rung 2 method), which intervenes by removing frequencies and measures performance changes. This establishes causal necessity but not counterfactual uniqueness (Rung 3). The gap (+1) aligns with common overclaim patterns (e.g., ablation → functional claims). No hedging language is present. Confidence is high (4) due to clear mechanistic language and direct method support, though minor ambiguity arises from the paper's use of supplementary observational methods (Rung 1)."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,Ablation,2,3,1,4,0,"The claim 'grokking arises from the gradual amplification of structured mechanisms encoded in the weights' is supported by interventional methods (ablation of frequencies to define progress measures). Ablation is a Rung 2 method. The claim uses 'encoded' and explains the underlying mechanism of grokking, which, following the decision tree for polysemous terms, is classified as Rung 3 due to interventional evidence and the mechanistic nature of the claim. The claim is stated without hedging. Confidence is 4 due to minor ambiguity in claim strength interpretation, but the decision tree and evidence support Rung 3."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,interchange intervention,2,2,0,5,0,"The claim 'SAEs struggle to reach the neuron baseline' is supported by interchange interventions (Rung 2 method) measuring causal effects on model outputs. The claim describes performance degradation relative to a baseline without asserting mechanistic uniqueness or representation (e.g., no 'encodes' or 'the circuit' markers). It aligns with Rung 2 linguistic patterns like 'degrades performance' and reflects empirical outcomes from interventions. No hedging language is present."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,Interchange intervention,2,2,0,5,0,"The claim uses 'mediate' – a Rung 2 linguistic marker for causal claims. The method (interchange interventions) is Rung 2 interventional, matching the causal claim level. No hedging language is present. High confidence due to precise term alignment and clear causal framing without overclaiming."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,Activation probing,1,1,0,4,1,"The method 'activation probing' is classified as Rung 1 (Observational/Associational) because it involves analyzing model activations without intervention. The claim that activation probes 'may be a promising misuse mitigation technique' is an associational claim about the utility of the method, similar to examples in Rung 1 of the claim classification (e.g., 'Probe accuracy predicts model behavior'). The word 'may' indicates hedging, so hedge_flag is set to 1. Confidence is 4 due to minor ambiguity in claim classification, but overall alignment with Rung 1 examples."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,Observational testing on distribution shifts,1,1,0,5,0,"The claim 'probes fail to generalize under important production distribution shifts' is supported by observational testing where probes were evaluated across datasets representing distribution shifts (e.g., multi-turn conversations, long-context prompts). This method establishes correlational evidence without intervention, fitting Rung 1 (Observational/Associational). The claim uses associational language ('fail to generalize') without causal or mechanistic markers, aligning with Rung 1 linguistic markers. No hedging terms are present. Confidence is high (5) due to clear alignment with Rung 1 methods and claims in the codebook."
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Activation patching,2,3,1,4,0,"The claim 'reverse-engineered nontrivial behaviors' implies a mechanistic understanding (Rung 3). The primary method used is activation patching, which is an interventional technique (Rung 2). This creates a +1 gap, consistent with the codebook's overclaiming pattern 'Patching → THE circuit'. The claim is stated definitively without hedging. Confidence is high but marked as 4 due to minor ambiguity in whether 'reverse-engineered' strictly requires Rung 3 evidence, though contextual analysis supports this classification."
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,Activation patching,2,3,1,4,0,"The ACDC algorithm uses activation patching, an interventional method (Rung 2), to identify important edges in the computational graph. The claim 'rediscovered 5/5 of the component types in a circuit' implies recovery of the mechanistic circuit, which is a unique structure (Rung 3) as per the decision tree for 'the circuit' in the codebook. The claim is stated as a fact without hedging."
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,Activation patching,2,3,1,4,0,"The claim 'researchers can understand the functionality of each component' is supported by activation patching (a Rung 2 interventional method). Understanding functionality implies mechanistic/functional attribution (Rung 3), creating a +1 overclaim. The claim lacks hedging words. Confidence is 4 due to minor ambiguity in whether the understanding is fully mechanistic or based on interventional evidence."
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,Activation patching,2,2,0,4,0,"The claim describes 'finding connections between abstract neural network units that form a circuit,' which directly references activation patching (an interventional Rung 2 method) as the core technique. The claim focuses on identifying causal connections (e.g., 'connections that form a circuit'), aligning with Rung 2 linguistic markers like 'mediates' or 'influences' without asserting uniqueness or mechanistic computation (Rung 3). No hedging language is present. Confidence is 4 due to clear alignment with Rung 2, though minor ambiguity arises from the term 'circuit' potentially implying mechanism, but context confirms a causal-not-unique interpretation."
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,ACDC and Edge Attribution Patching (EAP),2,3,1,4,0,"The phrase 'reverse-engineer the circuit responsible' uses definite article 'the' and functional attribution ('responsible for'), indicating R3 mechanistic claim. Methods used (ACDC/EAP) are R2 interventional techniques. 'Partially' doesn't hedge the mechanistic claim itself. Classic R2→R3 overclaim pattern matching codebook's 'THE circuit' example."
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,Resample ablation and layer removal,2,2,0,5,0,The claim 'Layer 39 is a key bottleneck' is supported by Rung 2 interventional methods: resample ablation (Section 4.1.1) and layer removal (Section 4.1.2). These show causal effects on task performance when intervening on Layer 39. The claim uses causal language ('bottleneck') consistent with Rung 2 markers like 'ablating X degrades Y'. No uniqueness claims (Rung 3) or hedging terms are present. Confidence is high due to multiple converging experimental lines of evidence.
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,Causal tracing,2,3,1,4,0,"The claim 'Convolutions in layer 39 shift names one position forward' uses the functional verb 'shift', indicating a mechanistic role (Rung 3). The primary supporting method is causal tracing (resample ablation), an interventional technique (Rung 2). There is no hedging in the claim. The gap of +1 is a common overclaim pattern (e.g., Patching → functional attribution). Confidence is 4 due to the clear method and claim, but the abstract prominence and the gap introduce minor ambiguity."
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,Activation patching and representation averaging,2,3,1,4,0,"The claim 'stored linearly' uses 'stored', a polysemous term triggering the decision tree. The paper provides interventional evidence (activation patching and representation averaging in Section 4.3) showing output changes when modifying SSM representations. Since the claim describes an internal storage mechanism ('stored linearly') rather than an intervention result, it qualifies as Rung 3. The method (activation patching) is Rung 2. No hedging is present. Confidence is 4 due to strong alignment with the decision tree, though minor ambiguity exists in whether 'stored' solely implies mechanism."
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,Benchmark evaluation with LLM judge and AUROC,1,1,0,5,0,"The claim compares method performance using AXBENCH benchmark metrics (LLM judge scores for steering, AUROC for concept detection), which establishes correlational evidence without interventions. The claim uses performance language ('outperforms') consistent with Rung 1 associational claims. No hedging is present. Confidence is high as the claim directly reports empirical results from the benchmark."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,Benchmark evaluation (observational),1,1,0,5,0,"The claim 'SAEs are not competitive' is supported by observational benchmark comparisons in AXBENCH, where SAEs are evaluated against other methods using correlational metrics (AUROC for concept detection, LLM-judged scores for steering). This fits Rung 1 methods (observational/associational) as per the codebook. The claim itself is a direct performance comparison ('not competitive'), using Rung 1 linguistic markers for empirical associations. No hedging is present. Confidence is high (5) due to explicit quantitative results in the paper showing SAEs underperforming baselines."
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,difference-in-means,2,1,0,4,0,"The claim states that difference-in-means (a Rung 2 method) performs the best in concept detection. This is an empirical, associational claim about method performance, fitting Rung 1. The claim does not use causal or mechanistic language, and there are no hedging words."
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,Causal tracing/interchange interventions,2,2,0,5,0,The method used (causal tracing) is a Rung 2 interventional method. The claim uses explicit Rung 2 language ('causal effects') without making uniqueness claims. The claim is stated definitively without hedging.
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,ROME editing,2,2,0,4,1,"The method 'rank-one model editing' refers to ROME, which is a Rung 2 interventional method (editing weights and observing change). The claim states that this method 'can successfully insert facts', which is a causal outcome (Rung 2 claim) about the intervention's effect. The word 'can' acts as a hedge, indicating possibility. Confidence is high (4) due to clear method classification and claim type, with minor ambiguity from hedging."
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,Linearity of Relation Embedding (LRE),1,1,0,4,0,"The method LRE analyzes linear approximations of model computations without interventions, falling under Rung 1 (observational/associational). The claim 'linearity of representations' uses associational language ('linearity') consistent with Rung 1 markers like 'is decodable from'. No hedging terms are present. Confidence is 4 due to clear method classification but slight ambiguity in claim phrasing."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,Activation patching and interchange interventions,2,3,1,4,0,"The claim 'LM binds each character-object-state triple together by co-locating their reference information' describes a mechanistic process (binding triples via co-located references). The paper uses Rung 2 interventional methods (activation patching and interchange interventions) to support this. Applying the decision tree for 'encodes/represents/stores': (1) Interventional evidence is provided (Fig. 3-6 interventions), (2) The claim focuses on the underlying mechanism (how binding occurs), not just intervention results, warranting Rung 3. No hedging language is present. Confidence is 4 due to strong alignment with mechanistic claims in the paper, though minor ambiguity exists in whether 'binds' strictly implies unique representation."
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,Activation patching,2,3,1,4,0,"The claim describes a 'lookback mechanism' that 'enables the LM to recall important information,' using functional language ('enables') and mechanistic framing ('mechanism') characteristic of Rung 3 claims. The paper supports this with Rung 2 methods (activation patching/interchange interventions), establishing causal effects but not full counterfactual uniqueness. While 'mechanism' implies functional attribution, the lack of explicit uniqueness testing introduces minor ambiguity. No hedging language is present."
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,Activation patching and causal tracing,2,3,1,4,0,"The paper uses activation patching and causal tracing (Rung 2 methods) to support the claim that a specific mechanism (binding lookback) retrieves the state OI. The claim describes a functional mechanism, which is characteristic of Rung 3 claims. There is no hedging in the claim."
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,Activation patching,2,3,1,4,0,"The paper uses activation patching (interchange interventions), which is a Rung 2 method. The claim 'reverse-engineering ToM reasoning' implies uncovering the internal mechanism, which is a Rung 3 claim. The claim is stated without hedging. Confidence is 4 due to strong evidence but minor ambiguity in the strength of the mechanistic claim."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,Activation patching,2,3,1,5,0,"The claim 'LMs implement such retrieval via a positional mechanism' is a mechanistic claim about model internals, using functional attribution language ('implement via a mechanism') characteristic of Rung 3. The paper employs interchange interventions (a form of activation patching) to test this claim, which is a Rung 2 method. No hedging is present. Confidence is high due to clear alignment with Rung 3 linguistic markers and explicit methodological context."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,Interchange interventions,2,3,1,5,0,"The claim describes three mechanisms (positional, lexical, reflexive) that LMs use for entity retrieval, asserting a mechanistic explanation of model internals. The method used (interchange interventions) is a Rung 2 interventional technique. The claim uses definitive language about model mechanisms ('supplement', 'mechanisms') without hedging, matching Rung 3 linguistic markers for mechanistic claims. The confidence is high due to extensive experimental validation across models and tasks documented in the paper."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,Interchange interventions and causal abstraction,2,3,1,5,0,"The claim describes a causal model combining positional, lexical, and reflexive mechanisms to estimate next token distributions with 95% agreement. The method involves interchange interventions (a form of activation patching) and causal abstraction, which are Rung 2 interventional techniques per the codebook. The claim asserts a mechanistic explanation ('how LMs mix these mechanisms') and functional attribution ('the model uses these components'), with definitive language ('THE mechanism') implying uniqueness and underlying computation. This aligns with Rung 3 (counterfactual/mechanistic claims). No hedging is present, and confidence is high due to explicit experimental validation across models and tasks."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,Activation patching,2,3,1,4,0,"The paper uses activation patching (a Rung 2 interventional method) to investigate entity binding and retrieval mechanisms. The claim 'how LMs bind and retrieve entities in-context' describes the underlying mechanisms, which is a Rung 3 claim. The paper provides interventional evidence for causal effects but does not use Rung 3 methods (e.g., counterfactual patching or uniqueness tests) to fully establish mechanistic uniqueness. The claim is stated without hedging. Confidence is 4 due to strong interventional support but minor ambiguity in whether the evidence fully elevates to Rung 3."
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,Activation patching and path patching,2,3,1,4,1,"The claim 'circuits within LLMs may be more flexible and general than previously recognized' uses Rung 3 linguistic markers ('flexible and general') implying mechanistic properties of circuits. The paper's primary methods (activation patching, path patching) are Rung 2 interventional techniques. The hedge 'may be' triggers hedge_flag=1. Confidence is 4 due to strong experimental support but minor ambiguity in generalizing beyond the IOI circuit studied."
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,Activation patching,2,3,1,5,0,"The claim 'the circuit generalizes surprisingly well reusing all of its components and mechanisms' is evaluated using activation patching (a Rung 2 interventional method) to test circuit behavior on prompt variants. The claim asserts mechanistic reuse ('the circuit', 'mechanisms'), implying uniqueness and functional persistence (Rung 3). No hedging language is present. Confidence is high (5) due to direct experimental evidence of 100% node overlap and quantitative path analysis supporting the claim."
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,Activation patching,2,3,1,4,0,"The claim 'discover a mechanism' (S2 Hacking) is a Rung 3 mechanistic claim. The method used is activation patching (a Rung 2 interventional method), creating a +1 gap. No hedging language is present. Confidence is 4 due to the clear Rung 3 linguistic markers but minor ambiguity from the method-claim gap."
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,Activation patching,2,3,1,4,0,"The claim uses the Rung 3 marker 'implement' to attribute a functional mechanism to circuits. The paper's primary method (activation patching) is Rung 2, establishing causal effects but not counterfactual uniqueness. The gap represents a +1 overclaim. No hedging language is present."
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Multi-task Distributed Alignment Search (MDAS),2,1,0,4,0,"The method used is MDAS, a multi-task extension of Distributed Alignment Search (DAS), which involves interchange interventions to establish causal effects under specific interventions (Rung 2). The claim states MDAS 'achieves state-of-the-art results' on the RAVEL benchmark, which is an associational claim about empirical performance (Rung 1) as it correlates method effectiveness with benchmark outcomes without asserting causal mechanisms. No hedging language (e.g., 'may' or 'suggests') is present. Confidence is high (4) due to clear method classification, but minor ambiguity exists in whether performance claims inherently imply causality."
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,Activation patching,2,3,1,4,1,"The method used is activation patching (interchange intervention), which is classified as Rung 2 (Interventional) per the codebook. The claim uses the term 'encodes', which is a Rung 3 marker. Following the decision tree for polysemous terms: interventional evidence is provided (Rung 2 method), but the claim is about the underlying mechanism (how the feature represents the attribute) rather than just the intervention result, leading to Rung 3. The phrase 'we have evidence that' acts as a hedge. Confidence is 4 due to the clear method classification and claim marker, with minor ambiguity from the hedge."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,Interchange interventions (RAVEL evaluation),2,1,0,4,0,"The claim summarizes experimental results from the RAVEL benchmark, which uses interchange interventions (Rung 2 interventional method) to evaluate interpretability methods. The claim itself is an empirical performance comparison ('achieve strong results' vs 'struggle') without causal or mechanistic language, fitting Rung 1 associational claims. No hedging terms are present. Confidence is high (4) due to clear experimental context, but minor ambiguity exists as the claim focuses on method performance rather than direct model internals."
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,Multi-task Distributed Alignment Search (MDAS),2,2,0,4,0,"The claim 'The representations of different attributes gradually disentangle as we move towards later layers' is supported by MDAS, an interventional method (Rung 2) involving interchange interventions to measure causal effects (Cause and Iso scores). The claim describes a causal trend in model behavior observed through these interventions, using language like 'disentangle' that aligns with Rung 2 causal effects rather than Rung 3 mechanistic uniqueness. No hedging terms are present. Confidence is 4 due to minor ambiguity in 'disentangle,' but context confirms it refers to intervention-based isolation of causal effects."
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,"Interchange interventions (DAS, MDAS)",2,3,1,4,1,"The methods used (DAS/MDAS) involve causal interventions (Rung 2). The claim makes a mechanistic statement about attribute entanglement and ripple effects ('unavoidable' interactions between attributes), which requires counterfactual reasoning about model mechanisms (Rung 3). The hedge 'seemingly' triggers the flag. Confidence is 4 due to clear intervention methods but some ambiguity in proving full uniqueness of mechanisms."
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,"Linear probing, activation logging, SAE feature attribution, attention visualization, PCA/SVD, correlation analysis",1,1,0,4,0,"The claim references prior interpretability work using observational methods (Rung 1) like linear probing and SAE attribution. The term 'encode' is used in the context of information being linearly decodable from representations, as confirmed by the decision tree for polysemous terms. The claim is stated as established fact without hedging language."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,Distributed interchange interventions (DII) on linear subspaces,2,3,1,5,0,The method involves interventional subspace editing (Rung 2). The claim uses 'encoded linearly' which implies a mechanistic claim about representation (Rung 3). The decision tree for 'encodes' shows this is R3 when supported by interventions but making a mechanism claim. No hedging present. High confidence due to explicit method-claim alignment per codebook examples.
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,DAS interchange,2,3,1,4,0,"The claim states DAS can 'effectively localize concepts within model representations.' DAS is an interventional method (Rung 2 per codebook). 'Localize concepts' implies identifying specific representations that encode concepts, which falls under Rung 3 markers ('encodes,' 'represents'). The decision tree for 'encodes/represents' confirms this is a mechanistic claim (Rung 3) as it asserts how concepts are stored/represented, not just causal effects. No hedging language is present. Confidence is high (4) due to clear Rung 3 markers, though minor ambiguity exists in 'localize' interpretation."
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,LoReFT,2,2,0,4,0,"The claim is based on the LoReFT method, which is an interventional technique (Rung 2). The claim states that the linear subspace can achieve control, which is a causal sufficiency claim (Rung 2) and does not assert uniqueness or a detailed mechanism. There is no hedging in the claim."
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,Low-rank linear subspace intervention (LoReFT),2,2,0,5,0,"The method used is LoReFT, which involves learned interventions on hidden representations through low-rank projections. This qualifies as Rung 2 (Interventional) per the codebook since it involves causal interventions that modify representations to steer model behavior. The claim states the method 'can induce a base LM to follow instructions,' using causal language ('induce') that matches Rung 2 linguistic markers like 'can produce' or 'enables.' There are no uniqueness claims (Rung 3) or hedging terms. Confidence is high (5) because the claim directly describes the method's purpose and aligns with experimental results in the paper."
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,Activation logging,1,1,0,4,0,"The claim 'stored in the parameters' is based on prior observational studies (e.g., Petroni et al., 2019) that use methods like prompting, which falls under Rung 1 (Activation logging). The decision tree for 'stored' indicates that without interventional evidence and given the context of being decodable/present, it should be classified as Rung 1. The claim is stated as a fact without hedging."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,Hyper-network training for parameter update prediction,2,2,0,4,1,"The method involves training a hyper-network to predict weight updates (Δθ) through constrained optimization, which constitutes an intervention on model parameters (Rung 2: Interventional). The claim uses causal language ('need to be changed to manipulate') but does not assert uniqueness or mechanistic computation (Rung 3 markers). The hedge 'can be regarded as' reduces certainty. Confidence is high (4) due to clear interventional framing, though 'probe' introduces minor ambiguity as probes are typically Rung 1, but here it refers to identifying causal components via intervention."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,Weight editing,2,1,0,4,1,"The method used is weight editing (similar to ROME/MEMIT edits), which is an interventional technique (Rung 2). The claim 'the updates tend to be concentrated on a small subset of components' describes an observed pattern from the analysis of updates, making it an associational claim (Rung 1). The phrase 'tend to be' acts as a hedge. Confidence is 4 due to minor ambiguity in whether the claim might imply mechanistic insights, but the explicit statement is observational."
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,Hyper-network for weight update,2,2,0,4,1,"The method involves predicting weight updates to intervene on the model, which is interventional (Rung 2). The claim uses 'causal mediation', a Rung 2 marker, and does not assert uniqueness. The phrase 'can be regarded as' is a hedge."
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Hyper-network based parameter editing with constrained optimization,2,3,1,4,1,"The method used (hyper-network predicting parameter updates with constrained optimization) is interventional (Rung 2) as it involves modifying model weights to change outputs. The claim uses functional attribution language ('achieved by primarily modifying parameters') and describes a mechanistic relationship between attention parameters and knowledge manipulation, which is Rung 3. The word 'seems' triggers the hedge flag. Confidence is 4 due to minor ambiguity in whether this qualifies as a full uniqueness claim, though the functional attribution is clear."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,Conceptual arguments and analysis of existing works and case studies,1,3,2,3,0,"The claim states that distinguishing strategic deception requires claims about internal beliefs and goals, which is a mechanistic (Rung 3) assertion about model internals. The method used to support this claim is conceptual analysis and observational review of existing empirical works and case studies, classified as Rung 1 (Observational/Associational) since it does not involve direct intervention or counterfactual testing. There are no hedging terms in the claim. Confidence is moderate (3) due to the conceptual nature of the claim and lack of a standard empirical method from the rungs."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,Activation probe,1,3,2,3,1,"The claim discusses conditions for activation probes (a Rung 1 method) to detect deception effectively, but the paper does not empirically apply this method; it is a conceptual analysis. The claim's focus on necessary internal mechanisms for classification performance constitutes a Rung 3 mechanistic claim. The phrasing as a question ('What must be true...') introduces hedging. Confidence is moderate due to ambiguity in method rung assignment (conceptual rather than empirical application)."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,Conceptual analysis and observational case studies,1,3,2,4,0,"The claim 'Model beliefs are not stable and are far more context dependent than animal or human beliefs' is supported through conceptual arguments and observational case studies (e.g., roleplaying experiments in Section 2.2 and identity-switching in Section 2.3), which are Rung 1 methods as they involve no intervention. The claim itself makes a mechanistic assertion about how models internally represent and process beliefs ('model beliefs'), using implicit Rung 3 language about representational properties. No hedging terms are present. Confidence is 4 due to minor ambiguity in whether the comparison to animal/human beliefs elevates the claim, but the core assertion about model internals aligns with Rung 3."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,Correlation analysis,1,1,0,5,0,"The claim reports an observed correlation (low agreement) between two labeling methods (autorater vs MASK labels) without any intervention on the model. This fits Rung 1 methods (observational/associational) like correlation analysis. The claim uses associational language ('agreement') and makes no causal or mechanistic assertions, placing it at Claim Rung 1. The statement is presented as an empirical finding without hedging terms."
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,Correlation analysis,1,3,2,4,1,"The claim asserts that strategically deceptive models have a 'consistent mechanism' (Rung 3 marker for mechanistic claims). The paper supports this through observational analysis of existing works (e.g., Greenblatt et al., 2024; Scheurer et al., 2024), which is a Rung 1 correlational method. No interventional evidence is provided to establish causality or uniqueness of the mechanism. The hedge 'mostly true' triggers hedge_flag=1. Confidence is 4 due to minor ambiguity in whether 'mechanism' refers strictly to observable verbalization patterns (Rung 1) versus internal computation (Rung 3), but the mechanistic framing dominates."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,Interchange intervention,2,2,0,5,0,"The method HyperDAS performs interchange interventions (a Rung 2 method) by patching subspaces of hidden states based on counterfactual inputs. The claim states that HyperDAS achieves state-of-the-art performance on the RAVEL benchmark, which evaluates causal disentanglement of concepts through interventions. This is an empirical claim about method performance in causal disentanglement (Rung 2), without any hedging language."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,interchange interventions,2,2,0,5,0,The paper uses interchange interventions (a Rung 2 method) to identify features that mediate concepts. The claim 'features that mediate concepts and enable predictable manipulation' uses 'mediate' - a Rung 2 linguistic marker for causal claims - and describes predictable manipulation consistent with causal effects. This matches the method's capabilities without overclaiming to mechanistic (Rung 3) levels. No hedging language is present.
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,Interchange interventions via hypernetwork (HyperDAS),2,3,1,4,0,"The method uses interchange interventions (patching) to measure causal effects, which is Rung 2. The claim 'locates the token-positions ... that a concept is realized in' uses 'realized' as a mechanistic term (similar to 'represented' or 'encoded'), and the decision tree for such terms assigns Rung 3 when interventional evidence is provided and the claim is about the mechanism. The claim is stated without hedging. Confidence is 4 due to minor ambiguity in the term 'realized', but overall fits Rung 3 based on context and decision rules."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,Interchange intervention,2,2,0,5,0,"The claim explicitly mentions 'interchange interventions' as the method, which is classified as Rung 2 in the codebook. The claim states that these interventions identify 'causal mediators', which is a Rung 2 linguistic marker. There are no hedging terms, and the classification is clear and directly supported by the codebook."
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,Activation Patching,2,3,1,4,0,"The claim describes HyperDAS learning to intervene on positions that 'store attributes'. 'Stores' is a polysemous term triggering the decision tree: 1) Interventional evidence is provided (activation patching), 2) The claim focuses on the storage mechanism ('previously unknown to store attributes') rather than intervention results, indicating a Rung 3 mechanistic claim. No hedging language is present. Confidence is 4 due to potential ambiguity in whether 'store' implies unique representation versus correlational storage, but the mechanistic framing and context favor Rung 3."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,Steering vectors,2,2,0,5,0,"The method used is steering vectors, which is classified as Rung 2 (Interventional) in the codebook. The claim states that behaviors are 'mediated' by linear directions and can be controlled, which uses Rung 2 linguistic markers (causal claims). There is no hedging in the claim. Confidence is high (5) because the method and claim align clearly with Rung 2 without overclaiming or ambiguity."
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,Steering vectors,2,2,0,5,0,"The method used is steering vectors, which is an interventional technique (Rung 2). The claim states that steering interventions increase or suppress specific behaviors and confirms causal influence, which is a causal claim (Rung 2). The claim is stated as a fact without hedging. Confidence is high due to clear alignment between method and claim."
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,Steering vectors,2,3,1,4,1,"The claim states that reasoning mechanisms are 'encoded as linear directions,' using the polysemous term 'encode.' The paper provides interventional evidence (steering vectors, a Rung 2 method) to support this. Following the decision tree for 'encodes/represents/stores': (1) interventional evidence is present, and (2) the claim is about the underlying mechanism (how reasoning is represented) rather than just the intervention's result. This qualifies as a Rung 3 mechanistic claim. The phrase 'reinforcing the hypothesis' acts as an explicit hedge. Confidence is 4 due to clear mechanistic language and interventional support, with minor ambiguity from the hedge."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,Steering vectors,2,3,1,4,0,"The paper uses steering vectors (Rung 2 interventional method) to extract vectors and control behaviors. The claim states that reasoning behaviors are isolated to specific directions (a mechanistic representation claim, Rung 3) and that this enables control. The claim is stated as a fact without hedging. Confidence is 4 due to minor ambiguity in the strength of the representation claim versus the interventional evidence."
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,Steering vectors,2,3,1,4,1,"The method used is steering vectors, which is an interventional technique (Rung 2). The claim about 'distinct mechanisms' implies a mechanistic explanation of the model's internal processes, which is Rung 3. The word 'indicate' acts as a hedge. Confidence is 4 due to minor ambiguity in the claim's generality, but the use of 'mechanisms' strongly suggests Rung 3."
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,HyperSteer (hypernetwork-based steering vector generation),2,1,0,4,0,"Method involves generating and applying steering vectors via hypernetworks (interventional, Rung 2). Claim compares performance against other methods without causal/mechanistic language, making it an empirical associational claim (Rung 1). No hedging present. Confidence 4 due to minor ambiguity in claim focus (method performance vs model internals)."
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,HyperSteer (hypernetwork-based steering vector generation and intervention),2,1,0,5,0,"The method involves generating steering vectors via a hypernetwork and adding them to the residual stream of the base LM, which is an interventional technique (Rung 2). The claim 'performs on par with steering-via-prompting' is an empirical performance comparison without internal mechanistic attribution, making it associational (Rung 1). The verb 'performs' refers to method-level efficacy rather than component functionality. No hedging is present."
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,HyperSteer (cross attention variant),2,1,0,5,0,"The method involves generating steering vectors via a hypernetwork and applying them as interventions to the base model's residual stream, which is an interventional method (Rung 2). The claim compares performance on unseen steering prompts against baselines, making it an associational performance claim (Rung 1) without hedging. High confidence due to clear method classification and straightforward claim interpretation."
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,Empirical efficiency comparison via FLOPs analysis,1,1,0,4,0,"The claim compares computational efficiency (FLOPs) between HYPERSTEER and supervised activation steering as training data scales. This is an observational correlation (Rung 1 method) evidenced by Figure 3 showing FLOPs vs. dataset size. The claim uses associational language ('becomes more economical') without causal/mechanistic terms, fitting Rung 1. No hedging is present. Confidence is high (4) due to clear empirical data, though minor ambiguity exists in whether efficiency implies mechanistic superiority."
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,"Correlation analysis and dimensionality reduction (PCA, t-SNE)",1,1,0,4,0,"The claim is supported by observational analysis of steering vector similarities using cosine similarity and dimensionality reduction techniques (PCA, t-SNE). The claim describes an associational pattern (weakened inter-concept similarity without performance cost) without causal or mechanistic language, fitting Rung 1 for both method and claim. No hedging is present. Confidence is 4 due to clear qualitative support but inherent subjectivity in such analyses."
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,Eigendecomposition,1,1,0,4,0,"The method 'eigendecomposition' is an observational/associational technique (Rung 1) as it analyzes weight structure without intervention, similar to PCA/SVD in the codebook. The claim 'reveals interpretable low-rank structure' uses Rung 1 linguistic markers ('reveals', 'structure') describing correlational patterns, not causal or mechanistic claims. No hedging language is present. Confidence is 4 due to clear alignment with Rung 1 methods/claims, though 'interpretable' introduces minor ambiguity about feature semantics."
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,Eigendecomposition of bilinear tensor (weight analysis),1,3,2,4,0,"The method involves decomposing model weights via eigendecomposition without interventions, fitting Rung 1 (observational). The claim uses 'represent' and 'function as'—polysemous terms defaulting to Rung 3 (mechanistic) per codebook decision rules, as no interventional evidence supports the claim and context doesn't imply linear decodability. No hedging language is present. Confidence is 4 due to minor ambiguity in visual interpretation of eigenvectors, but linguistic markers strongly align with Rung 3."
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,Steering vectors,2,2,0,4,0,"The method uses adversarial masks (input-space steering vectors) to intervene and measure effects on model outputs, aligning with Rung 2 interventional methods. The claim about causal importance uses appropriate Rung 2 terminology ('demonstrating causal importance') without making uniqueness claims. While steering can imply sufficiency rather than necessity, the language stays within Rung 2 bounds. Confidence is 4 due to potential ambiguity around steering vector interpretation."
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,Weight decomposition and interaction analysis,1,3,2,4,0,"The method involves decomposing the bilinear tensor from model weights to analyze interactions, which is observational and falls under Rung 1. The claim uses the verb 'computes' and describes a specific circuit mechanism ('AND-gate-like interactions'), which are Rung 3 linguistic markers. The claim is stated definitively without hedging."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,Weight-based analysis via eigendecomposition and correlation with SAE features,1,1,0,5,0,"The claim uses the term 'correlated', which is a Rung 1 linguistic marker for associational claims. The method involves decomposing the bilinear tensor (from weights) and comparing with SAE features, which is observational (Rung 1). There is no hedging language in the claim."
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,Relevance Patching (RelP),2,1,0,5,0,"The claim compares RelP's approximation accuracy to attribution patching using empirical correlation metrics (Pearson correlation coefficients), which is an associational claim (Rung 1). RelP is an interventional method (Rung 2) as it modifies activations via propagation rules to estimate causal effects. No Rung 3 markers (e.g., 'encodes,' 'the circuit') or hedging terms are present. Confidence is high due to clear quantitative validation across multiple models/tasks."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,Activation patching,2,1,0,4,0,"The claim reports a Pearson correlation coefficient between RelP and activation patching, which is an associational claim (Rung 1). The method used to generate the evidence is activation patching (Rung 2) as the ground truth. No hedging is present."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,Relevance Patching (RelP),2,3,1,4,0,"RelP is an interventional method (Rung 2) that approximates activation patching for causal localization. The claim about identifying sparse feature circuits is a mechanistic claim (Rung 3) because circuits represent the underlying mechanisms. The claim has no hedging words. Confidence is 4 due to minor ambiguity in the claim being about method performance rather than a direct mechanistic claim, but it is based on empirical evidence about model internals."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,Relevance Patching (RelP),2,3,1,4,0,"The claim 'small feature circuits explain most of the model's behavior' uses Rung 3 markers: 'explain' implies a mechanistic narrative about how the model works. The method RelP is Rung 2 (interventional) as it approximates activation patching through causal interventions. The gap (+1) matches the 'Patching → THE circuit' overclaim pattern. No hedging language is present. Confidence is 4 due to minor ambiguity in whether 'explain' strictly requires uniqueness, but the circuit context strongly suggests Rung 3."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,Relevance Patching (RelP),1,1,0,4,0,"RelP is an attribution method using propagation coefficients from LRP, which analyzes activations without intervention (observational/Rung 1). The claim 'faithful localization of influential components' uses associational language ('localization') without causal/mechanistic markers like 'causes' or 'encodes', fitting Rung 1. No hedging terms are present."
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,Sparse attention regularization under constrained-loss objective,2,2,0,5,0,The method uses interventional sparsity regularization during post-training (Rung 2). The claim makes a causal assertion about intervention effects ('reduced to 0.3% edges' while 'retaining loss') matching Rung 2 linguistic markers for causal effects. No hedging language present. High confidence due to clear method-claim alignment in intervention description and performance retention claim.
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,Activation patching,2,2,0,5,1,The claim reports empirical results from activation patching experiments (Rung 2 method per codebook) comparing circuit sizes between sparse and standard models. The claim uses Rung 2 language ('recover...effect') describing causal sufficiency of fewer components without uniqueness/mechanistic claims. 'Roughly' constitutes hedging. High confidence due to explicit method description in paper and clear alignment with Rung 2 linguistic markers.
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,Activation patching,2,2,0,4,0,"The claim is empirically supported by activation patching experiments (Rung 2 method) measuring causal effects on circuit complexity. It reports quantitative outcomes ('50-100x fewer edges') from interventions without making uniqueness or mechanistic claims. The language ('require...to reach') describes causal sufficiency under intervention, aligning with Rung 2 linguistic markers like 'reduces' or 'changes Y'. No hedging terms are present."
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,Ablation,2,3,1,4,0,"The method involves ablation (Rung 2 interventional). The claim uses 'circuit' with quantitative reduction claims (100x fewer edges), implying uniqueness and mechanistic structure (Rung 3). While not explicitly testing uniqueness, the magnitude claim and circuit framing suggest a R3 mechanistic claim about computational structure rather than just causal effects."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,Sparse post-training with constrained optimization,2,2,0,4,0,"The method involves an intervention (sparsity regularization during fine-tuning) to alter attention patterns, which is classified as Rung 2 (Interventional). The claim states a causal effect of this intervention on internal information flow (consolidating flow into fewer edges), making it a Rung 2 (Causal) claim. There are no hedging words present."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,SAE feature attribution,1,1,0,5,0,"The claim explicitly references TopK SAEs, classified as Rung 1 (Observational) per the codebook's SAE feature attribution category. The claim describes revealing 'biologically meaningful latent features' and notes that 'high feature-concept correlation does not guarantee causal control,' using Rung 1 linguistic markers ('reveal,' 'correlation'). It avoids causal/mechanistic language (Rung 2/3) and directly acknowledges correlational limitations. No hedging terms are present. Confidence is high due to clear alignment with Rung 1 definitions and explicit disavowal of causal claims."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Steering vectors,2,2,0,5,0,"The claim 'Ordered SAEs impose an hierarchical structure that reliably identifies steerable features' describes an interventional method (steering vectors) where features are used to causally influence model outputs. This aligns with Rung 2 methods (interventional) in the codebook. The claim rung is 2 because 'steerable features' implies causal sufficiency ('can produce' output changes), matching Rung 2 linguistic markers. No hedging language is present. Confidence is high (5) due to clear method-claim alignment and explicit steering interventions described in the paper."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,SAE feature attribution and linear probing,1,1,0,4,0,"The claim 'SAE latents collectively represent antibody information following sparsification' is supported by observational methods (SAE feature attribution and linear probing), which are Rung 1. The word 'represent' is used in the context of information being decodable (as evidenced by high probe accuracy), aligning with Rung 1 linguistic markers. There is no hedging. Confidence is 4 due to minor ambiguity in 'represent' but resolved by context."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE feature attribution,1,3,2,4,0,"The claim 'top latents encoded contextual information' uses the polysemous term 'encoded'. Following the decision tree: (1) No interventional evidence is provided for this specific claim (only observational SAE analysis). (2) The context does not clarify 'linearly decodable' meaning, as the paper discusses mechanistic interpretations of residue-level patterns. Thus, the default Rung 3 (mechanistic claim) applies. The method is SAE feature attribution (Rung 1 per codebook). No hedging language is present. Confidence is high due to clear linguistic markers but minor ambiguity in contextual interpretation."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering vectors,2,2,0,5,0,"The method used is steering, which involves adding a scaled decoder vector to the hidden state to intervene on the model and observe changes in output. This is an interventional method (Rung 2). The claim states that steering on latent 12 caused an increase in IGHJ4 proportion, which is a causal effect and matches Rung 2 linguistic markers (e.g., 'intervening on X changes Y'). There is no hedging in the claim. Confidence is high because the method and claim are clearly aligned with Rung 2."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,Sparse autoencoders and feature steering,2,2,0,4,0,"The claim 'We successfully reproduce basic feature extraction and steering capabilities' refers to two methods: SAE feature extraction (Rung 1 observational) and feature steering interventions (Rung 2 interventional). Since steering is the dominant novel capability and aligns with Rung 2 methods like activation clamping, method_rung is 2. The claim describes successful causal manipulation ('steering capabilities'), matching Rung 2 linguistic markers like 'can produce' or 'intervening changes Y'. No hedging language is present. Confidence is 4 due to minor ambiguity in combining methods, but steering is clearly Rung 2."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,feature steering,2,1,0,4,0,"The claim describes feature steering (an interventional method classified as Rung 2) and observes its fragility/sensitivity to parameters like layer selection and magnitude. This is an associational claim (Rung 1) as it reports correlations between steering parameters and outcomes without asserting causal mechanisms. No hedging language is present. Confidence is high (4) due to clear method alignment, but minor ambiguity exists in whether sensitivity implies pure correlation."
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,SAE feature attribution,1,1,0,5,0,"The claim 'We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another' is based on analyzing feature activations from sparse autoencoders (SAEs), which is an observational method (Rung 1). The claim uses associational language ('observe', 'demonstrate') and reports on activation patterns and feature distinguishability without making causal or mechanistic claims, so it is a Rung 1 claim. There are no hedging words, so hedge_flag is 0. Confidence is high (5) because the method and claim rung are clear from the context and codebook."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,Feature steering,2,1,0,2,1,The claim is a meta-claim about the reliability of mechanistic interpretability methods (SAE and steering) for safety-critical applications. The paper uses both SAE feature attribution (Rung 1) and feature steering (Rung 2) as evidence. We assign the method as feature steering (Rung 2) because safety-critical applications emphasize control. The claim is observational about the methods' performance (thus Rung 1) and is hedged by 'often'.
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,subspace activation patching,2,3,1,4,1,"The claim discusses subspace interventions (an interventional method, Rung 2) but makes a mechanistic assertion about how effects can be achieved via dormant pathways (Rung 3). The presence of 'may' triggers the hedge flag. Confidence is high due to clear alignment with codebook definitions, but minor ambiguity exists in whether this qualifies as a full uniqueness claim."
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,Activation patching,2,3,1,4,1,"The claim references 'patching of subspaces', which directly corresponds to activation patching interventions classified as Method Rung 2 (Interventional) in the codebook. The claim asserts this method can create an 'illusory sense of interpretability', which implies a mechanistic property of models (i.e., that subspaces may appear meaningful but not faithfully represent computations). This aligns with Rung 3 Claim markers like 'illusion' in the context of mechanistic fidelity. The word 'can' introduces hedging. Confidence is 4 due to contextual ambiguity in mapping meta-claims about methods to claim rungs, though the mechanistic implication is clear."
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,Activation patching,2,2,0,5,0,"The claim describes demonstrating an empirical phenomenon (interpretability illusion) using activation patching interventions. Activation patching is explicitly defined as a Rung 2 method in the codebook under 'Interventional' methods. The claim uses the verb 'demonstrate', which aligns with Rung 2 linguistic markers ('intervening on X changes Y') rather than Rung 3 mechanistic language. There are no hedging terms, and the claim is stated as an established finding. Confidence is high (5) as the paper directly shows interventional results in both synthetic and real-world settings."
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,Correlation analysis,1,1,0,4,0,"The claim states an observed inconsistency between fact editing performance and fact localization, which is an associational statement. The method to establish this is inferred as correlation analysis from prior work, placing it at Rung 1 for both method and claim."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,Activation patching,2,2,0,4,0,"The claim describes activation patching as a mechanistic interpretability technique with subtleties. Activation patching is explicitly classified as a Rung 2 (Interventional) method in the codebook. The claim does not assert mechanistic properties (Rung 3) like 'encodes' or 'the circuit'; it neutrally describes the technique's characteristics. No hedging language is present. Confidence is high (4) due to clear method classification, though slightly reduced because the claim is meta-level (about the technique itself)."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching,2,2,0,4,1,"The claim discusses hyperparameters in activation patching (a Rung 2 interventional method). The phrase 'could lead to' implies a causal relationship between hyperparameter variation and interpretability outcomes, aligning with Rung 2 linguistic markers like 'influences' or 'can produce'. The hedge 'could' triggers hedge_flag=1. Confidence is high (4) due to clear causal framing but minor ambiguity from the generalized context."
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,Activation patching,2,1,0,5,0,"The claim describes an empirical examination of activation patching methodologies, which is an interventional method (Rung 2). The claim itself uses observational language ('examine the impact') without causal or mechanistic markers, fitting Rung 1 (associational). No hedging is present, and confidence is high due to explicit methodological focus and alignment with codebook examples like SAE evaluation papers."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching,2,2,0,5,1,The claim 'varying these hyperparameters could lead to disparate interpretability results' describes a causal relationship between methodological choices (hyperparameters) and outcomes (interpretability results). Activation patching is explicitly classified as a Rung 2 (Interventional) method in the codebook. The claim uses causal language ('lead to') and matches Rung 2 linguistic markers ('intervening on X changes Y'). The hedge 'could' triggers hedge_flag=1. Confidence is high (5) as the method and claim align directly with codebook definitions without ambiguity.
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,Activation patching,2,3,1,4,0,"The claim uses Rung 3 markers: 'act as' (functional attribution) and 'encoding' (mechanistic). The method is activation patching (Rung 2). No hedging is present. Confidence is 4 due to minor ambiguity in the term 'encoding', but the context supports a mechanistic reading."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,Activation patching,2,3,1,5,0,"The paper uses activation patching (Rung 2 interventional method) to support a claim about where contextual integration occurs. The claim uses mechanistic language ('integrated primarily') and makes a functional attribution about layer-specific processing, which constitutes a Rung 3 claim. This matches the common overclaim pattern of using intervention results (R2) to support mechanism claims (R3). The abstract presents this as a definitive finding without hedging."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,activation patching,2,3,1,4,0,"The claim 'GPT-2's sentiment computation differs from the predicted hierarchical pattern' uses the term 'computation', which is a Rung 3 linguistic marker indicating a mechanistic claim about how the model processes information. The paper's primary method is activation patching (Rung 2 interventional), which establishes causal effects but not full mechanistic uniqueness. The claim lacks hedging language (e.g., 'may' or 'suggests') and asserts the computational process as fact. Confidence is 4 due to minor ambiguity: while 'computation' strongly implies Rung 3, the claim is a negative assertion about architecture rather than a positive mechanistic description like 'encodes' or 'performs'."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,Causal Tracing,2,3,1,4,0,"The claim 'play a critical role in aggregating cross-modal information' uses functional attribution language ('play a critical role') consistent with Rung 3 markers (functional mechanisms). The supporting method is causal tracing (activation patching), a Rung 2 interventional technique. The gap (+1) matches common overclaim patterns (e.g., patching → functional claims). No hedging language is present. Confidence is high due to clear functional attribution, though minor ambiguity exists in whether 'critical role' implies uniqueness."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,Causal tracing (activation patching),2,3,1,4,0,"The claim uses mechanistic terms 'storage' and 'transfer' to describe FFNs' hierarchical progression, which implies a functional mechanism (Rung 3). The supporting method is causal tracing (Rung 2 interventional). The decision tree for 'storage/transfer' terms confirms R3 when interventional evidence exists and the claim describes underlying mechanisms. No hedging language is present. Confidence is high (4) due to clear mechanistic framing, though the hierarchical staging adds minor complexity."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,Intermediate Representation Injection (IRI),2,2,0,5,0,"The method IRI involves inference-time interventions on model activations (MHSA and MLP outputs) by injecting stored representations, which aligns with Rung 2 interventional methods (e.g., steering vectors or activation patching). The claim 'reinforces visual object information flow' describes a causal effect of the intervention without asserting mechanistic uniqueness or representation (Rung 3 markers absent), making it a Rung 2 causal claim. No hedging language is present. Confidence is high due to clear method description and claim alignment."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,Steering vectors,2,3,1,4,0,"The method is explicitly 'steering vectors', classified as Rung 2 (Interventional) per codebook examples. The claim uses functional language 'acts like' to describe the vector's mechanism (token-substitution bias), which is a Rung 3 marker for mechanistic claims. No hedging is present. Confidence is 4 due to minor ambiguity in whether 'acts like' implies full mechanistic attribution, but the functional description aligns with Rung 3 patterns like 'performs' or 'controls'."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,Steering vectors,2,3,1,4,0,"The claim describes the mechanism of the steering vector (operating through MLP and unembedding) and the effect (up-weighting process words), which is a mechanistic narrative (Rung 3). The method is steering vectors, an interventional technique (Rung 2). There is no hedging in the claim."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,Steering vectors,2,2,0,4,0,"The method used is steering vectors (explicitly listed as Rung 2 in the codebook under 'Interventional'). The claim 'steering vectors transfer to other models' describes a causal effect (intervening by applying the vector to new models changes behavior/performance), aligning with Rung 2 linguistic markers like 'intervening on X changes Y'. No uniqueness or mechanistic language (Rung 3) or hedging terms are present. Confidence is high (4) due to clear method classification and causal framing, with minor ambiguity from implicit intervention context."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,Steering vectors,2,2,0,5,0,"The method used is steering vectors (specifically Contrastive Activation Addition), which is an interventional method (Rung 2). The claim describes the net effect of the intervention (net positive steering effect) and its variance, which is a causal outcome of the intervention, fitting Rung 2. There are no Rung 3 markers or hedging terms."
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,steering vectors,2,1,0,5,0,"The claim uses the word 'predicts', which is a Rung 1 marker for associational claims. The method, steering vectors, is a Rung 2 interventional method. There is no hedging in the claim."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Steering vectors,2,2,0,5,0,"The paper uses steering vectors (a Rung 2 interventional method) to demonstrate that steering reliability depends on directional coherence of the target behavior. The claim is about the causal effectiveness of the intervention under specific conditions, fitting Rung 2 linguistic markers for causal claims (e.g., 'unreliable' implies causal effect variability). No hedging terms are present."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,Library development and case studies (mixed R1-R2),2,1,0,3,0,"The claim describes pyvene's capability to support interventions, which is demonstrated through case studies using Rung 1 (e.g., activation collection) and Rung 2 (e.g., causal tracing, DAS) methods. The claim itself uses neutral language ('supports customizable interventions') without causal/mechanistic markers, fitting Rung 1 (observational/capability claim). No hedging is present. Confidence is moderate due to the claim's focus on tool functionality rather than model internals."
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,Case studies (causal tracing and DAS),2,2,0,4,0,"The claim 'pyvene provides a unified and extensible framework for performing interventions on neural models' is supported by empirical case studies in the paper, including reproducing Meng et al.'s causal tracing (Rung 2) and implementing Distributed Alignment Search (DAS, Rung 2). The claim uses neutral language ('performing interventions') without Rung 3 markers like 'encodes' or 'the mechanism', and aligns with Rung 2 causal intervention methods demonstrated. No hedging is present. Confidence is high (4) due to clear alignment with demonstrated Rung 2 methods, though the abstract-level prominence adds minor ambiguity."
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,causal abstraction and knowledge localization,2,2,0,3,0,"The claim states the library was used for interpretability analyses using causal abstraction (a Rung 2 method) and knowledge localization (which in this paper uses causal tracing, a Rung 2 method). The claim is about the application of Rung 2 methods, so we assign claim rung 2. However, the claim is a meta-claim about the library, which introduces ambiguity. No hedging is present."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,SAE feature attribution,1,3,2,4,0,"The claim states SAEs 'decompose strategy-entangled hidden states into a disentangled feature space,' which implies a mechanistic representation (Rung 3) using terms like 'decompose' and 'disentangled.' The method is SAE feature attribution (Rung 1), creating a +2 gap. No hedging language is present. Confidence is high due to clear Rung 3 markers but minor ambiguity in whether 'decompose' strictly implies mechanistic encoding."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE-Steering,2,2,0,4,0,"The claim states that SAE-Steering identifies strategy-specific features. SAE-Steering is a two-stage pipeline: Stage 1 uses logit estimation (observational, Rung 1) to recall features, but Stage 2 uses interventional testing (steering vectors) to evaluate control effectiveness, which is Rung 2. Since the claim focuses on the identification capability validated through intervention, the method rung is 2. The claim verb 'identifies' implies causal selection validated by intervention, aligning with Rung 2 linguistic markers like 'mediates' or 'is sufficient for'. No hedging terms are present. Confidence is 4 due to minor ambiguity in the verb 'identifies' not being explicitly listed in codebook examples, but contextual evidence supports causal interpretation."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,SAE-Steering (interventional steering using SAE features),2,2,0,4,0,"The method SAE-Steering involves adding identified SAE features to the residual stream to steer the model's reasoning, which is an interventional method (Rung 2). The claim is about the method's performance (over 15% improvement in control effectiveness) compared to other methods, which is an outcome-based claim similar to Rung 2 examples in the codebook (e.g., 'ablating X degrades Y'). There is no hedging in the claim. Confidence is 4 due to minor ambiguity in classifying performance claims, but the intervention context supports Rung 2."
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,Steering vectors,2,2,0,5,0,The claim 'controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones' describes a causal effect achieved through intervention (steering vectors applied to hidden states). This matches Rung 2 linguistic markers ('redirect' implies causal change) and aligns with the paper's SAE-Steering method (Rung 2 interventional technique). No hedging language is present. Confidence is high due to explicit experimental validation in the paper showing 7% accuracy improvement from error correction via strategy steering.
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,Linear probing,1,3,2,3,0,"The claim 'neural networks represent meaningful concepts as directions in their representation spaces' uses the term 'represent', which is a Rung 3 marker (mechanistic claim). The paper does not provide interventional evidence for this claim, and the context does not indicate it is meant in a linearly decodable sense (which would be R1). The method used in prior work to establish such linear directions is typically linear probing (Rung 1), an observational method. Thus, the claim rung (3) is higher than the method rung (1), indicating a +2 gap. Confidence is moderate (3) because the paper does not specify the method, but linear probing is the most common basis for the Linear Representation Hypothesis."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,Theoretical framework development,1,1,0,5,0,"The claim presents a theoretical unification of existing methods through mathematical analysis rather than empirical intervention. This constitutes Rung 1 (Observational) methodology. The claim describes an organizational framework without making causal or mechanistic assertions, remaining at Rung 1 (Associational) for claims. No hedging language is present. High confidence from clear alignment with theoretical/mathematical analysis characteristics in the codebook."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,Theoretical analysis and proofs,1,3,2,5,0,"The method used is theoretical analysis of optimization landscapes (Rung 1 observational). The claim provides mechanistic explanations for empirical phenomena (Rung 3) by proving these emerge from fundamental properties of the optimization problem. No hedging present. High confidence due to clear alignment with codebook definitions: theoretical proofs are R1 methods, while explaining 'why' failure modes occur constitutes R3 mechanistic claims."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,SAE feature attribution,1,3,2,4,0,"The claim is a mechanistic statement about neural networks using superposition (R3). The paper uses SAE feature attribution (R1) to measure superposition, but the claim itself is not directly supported by intervention. No hedging present."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,SAE feature attribution,1,3,2,4,0,"The claim presents a framework based on sparse autoencoders (SAE) to measure effective degrees of freedom, which is an observational method (Rung 1). The claim about measuring a neural representation's properties implies a mechanistic interpretation (Rung 3) as it addresses how many features are represented. No hedging is present. Confidence is high but with minor ambiguity due to the claim being about a framework rather than a direct empirical result."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,Correlation analysis,1,1,0,5,0,"The claim explicitly uses the term 'correlates' (a Rung 1 linguistic marker) to describe the relationship between their metric and ground truth in toy models. The method involves statistical correlation analysis of observational data (SAE activations vs. known toy model features), which falls under Rung 1 methods (correlational evidence without intervention). No hedging language is present, and the alignment between the claim wording and method is clear."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,SAE feature attribution,1,2,1,4,1,"The claim is supported by SAE feature attribution (Rung 1 observational method) measuring effective features. The claim describes a causal relationship where adversarial training (intervention) increases effective features and robustness, using linguistic markers like 'can increase' and 'improving' that align with Rung 2 causal claims. The hedge 'can' triggers hedge_flag=1. Confidence is high (4) due to clear method-claim alignment, though the abstract location adds minor ambiguity."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,Sparse Autoencoder (SAE) training with group symmetries,1,1,0,4,0,"The method involves training an SAE on activations, which is observational (Rung 1) as it analyzes correlations without intervention. The claim about features being 'more useful in downstream tasks' is associational (Rung 1) as it is based on probing results showing correlation with task performance, not causal or mechanistic claims. No hedging words are present. Confidence is 4 due to minor ambiguity in the word 'yields' potentially implying causation, but context confirms correlational evidence."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,Correlation analysis,1,1,0,5,0,"The claim describes fitting a linear transformation matrix (M) to model how activations change under input rotations, which is a correlational analysis between activations and transformations. This matches Rung 1 methods like 'Correlation analysis' in the codebook. The claim uses 'explain' in an associational context without causal/mechanistic markers, aligning with Rung 1 linguistic markers ('correlates with', 'explains'). No hedging language is present. Confidence is high (5) due to clear method-claim alignment and absence of ambiguity."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,Probing,1,1,0,5,0,"The claim is supported by probing tasks (Rung 1 method) that evaluate the features discovered by the SAEs. The claim states that the adaptive SAEs' features lead to superior probing performance, which is an associational claim (Rung 1) about correlation between the features and probing accuracy. There are no hedge words, and the assignment is clear."
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Steering vectors,2,2,0,5,0,"The claim 'steering effectively controls the targeted summary properties' refers to causal effects established through interventional methods (steering vectors added to activations). This aligns with Rung 2 linguistic markers ('controls') and the method classification (steering vectors are explicitly listed under Rung 2 interventional methods in the codebook). The claim describes causal influence without asserting uniqueness or mechanistic explanations (Rung 3), and lacks hedging language. Confidence is high due to clear alignment with codebook definitions and empirical results in the paper."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,steering vectors,2,2,0,5,0,"The method used is steering vectors (specifically Contrastive Activation Addition), which is explicitly categorized as a Rung 2 (Interventional) method in the codebook. The claim describes a causal effect ('degrade') resulting from varying the intervention strength (steering strength λ), matching Rung 2 linguistic markers for causal claims like 'intervening on X changes Y'. There is no overclaim (claim_rung ≤ method_rung), and the statement contains no hedging language ('consistently' indicates certainty). Confidence is high (5) due to clear alignment with codebook definitions and direct experimental evidence in the paper."
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,Steering vectors and prompting,2,2,0,4,0,"The claim asserts that combining steering vectors (Rung 2 interventional method) and prompting (input-based control) yields the 'strongest control' over text properties. This is a causal sufficiency claim ('yields'), fitting Rung 2 linguistic markers. The evidence involves direct interventions (steering vector addition) and input modifications (prompts), aligning with Rung 2 methods. No uniqueness or mechanistic language (Rung 3) is present. Confidence is 4 due to minor ambiguity in classifying the hybrid method combination, but the causal framing and interventional core are clear. No hedging terms are used."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,Theoretical analysis,3,3,0,3,0,"The claim is theoretical and does not report empirical results from specific methods like probing or interventions. It asserts that causal abstraction provides a 'fruitful lens' for understanding computational implementation, implying a mechanistic explanation (Rung 3). The term 'lens' suggests a framework for uncovering unique mechanisms, aligning with Rung 3 markers. No hedging language is present. Confidence is moderate (3) due to the abstract nature of the claim and lack of direct empirical support in the quoted text."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,theoretical framework,1,3,2,2,0,"The claim 'we offer an account of computational implementation grounded in causal abstraction' is a theoretical contribution, not an empirical claim about model internals. Since the codebook focuses on empirical methods, this falls outside standard classification. As a fallback: (1) Method used is 'theoretical framework' (not in codebook), assigned Rung 1 arbitrarily as base level. (2) Claim uses 'account' and 'implementation' implying a mechanistic explanation (Rung 3 per linguistic markers like 'computational explanation'). Confidence is low (2) due to non-empirical nature and edge-case status. No hedging language present."
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,Activation patching,2,3,1,4,0,"The claim 'the output language is encoded in the latent at an earlier layer than the concept to be translated' is supported by activation patching (Rung 2 interventional method). The term 'encoded' implies a mechanistic representation (Rung 3), as it describes the internal structure and timing of information processing. The decision tree for 'encodes/represents' confirms Rung 3 when interventional evidence exists and the claim is about the underlying mechanism (layer-specific encoding order). No hedging language is present. Confidence is 4 due to strong experimental support but minor ambiguity in mechanistic attribution."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,Activation patching,2,2,0,5,0,The method used (activation patching) is a Rung 2 interventional technique. The claim about changing concept/language independently through patching makes a causal sufficiency claim (Rung 2) without asserting uniqueness or full mechanistic understanding (Rung 3). The phrasing 'we can change' is direct with no hedging. High confidence due to clear alignment between method and claim rungs.
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,Activation patching,2,2,0,5,0,"The method is activation patching (interventional technique replacing activations to measure causal effects), classified as Rung 2. The claim 'improves translation' asserts a causal effect (patching enhances performance), matching Rung 2 linguistic markers ('improves' implies causal sufficiency). No hedging terms are present. Confidence is high due to explicit experimental evidence in the paper (Section 5) showing translation accuracy increases with mean-concept patching."
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,Activation patching,2,3,1,4,1,"The claim asserts the existence of language-agnostic concept representations, using 'representations'—a Rung 3 marker per the codebook's polysemous term decision tree. The paper uses activation patching (Rung 2 interventional method) to support this mechanistic claim. The hedge 'provide evidence for' reduces definitiveness, but the core claim remains Rung 3. Confidence is high (4) due to clear method-claim alignment, with minor ambiguity from hedging."
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,Activation patching,2,3,1,4,0,"The claim 'early MLP layers attend not only to the syntactic structure but also process its semantic content' uses functional language ('attend', 'process') that describes mechanistic behavior, consistent with Rung 3 claims about computational roles. The supporting method (activation patching) is interventional (Rung 2), creating a +1 gap. The claim lacks hedging terms and asserts mechanistic processing beyond causal effects. Confidence is high (4) due to clear Rung 3 markers, though 'process' isn't explicitly covered in codebook examples."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,Activation patching,2,3,1,4,0,"The claim describes a mechanistic process (early layers transform persona tokens into richer representations, which are then used by middle MHA layers), falling under Rung 3 linguistic markers for functional attribution and mechanistic narrative. The method used is activation patching, an interventional technique (Rung 2). This creates a +1 gap, consistent with common overclaim patterns. No hedging language is present."
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,Attention visualization,1,1,0,5,0,"The claim is based on observing attention patterns, an observational method (Rung 1). The claim describes an association between attention heads and identity tokens without causal or mechanistic language, making it an associational claim (Rung 1). No hedging words are present."
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,Activation patching,2,3,1,5,0,"The method used (activation patching) is Rung 2 interventional. The claim that associative knowledge is 'distributed' makes a mechanistic assertion about knowledge representation (R3). This constitutes a +1 overclaim gap (R2→R3), matching the common pattern where activation patching evidence is used to support distributed mechanism claims. No hedging language is present."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation patching,2,3,1,5,0,"The method used is activation patching (Rung 2 interventional). The claim uses mechanistic language 'localised' to assert knowledge representation in a specific layer, implying a unique storage mechanism (Rung 3). No hedging terms are present. The abstract context and linguistic markers align with Rung 3 claims about localized representations."
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Activation Patching,2,3,1,4,0,"The method used is activation patching (Rung 2). The claim states that factual knowledge is localized and associative knowledge is distributed, which is a mechanistic claim about internal representations (Rung 3). The claim is unhedged. Confidence is 4 due to minor ambiguity in linguistic markers, but the meaning aligns with Rung 3."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,Sparse Autoencoders (SAEs),1,1,0,5,0,"The method is SAE feature attribution, which is classified as Rung 1 (Observational/Associational) in the codebook. The claim uses associational language ('reconstruct and interpret') without causal or mechanistic markers, aligning with Rung 1. There are no hedging terms, and the claim is stated as fact. Confidence is high due to clear alignment between method and claim rungs."
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,SAE feature attribution,1,1,0,4,0,"The method used is SAE feature attribution (explicitly listed as Rung 1 in the codebook), which is observational and establishes correlational evidence without intervention. The claim uses 'captures' to describe identifying common factors of variation (concepts), which aligns with Rung 1 linguistic markers like 'information is present' or 'can be extracted'. The decision tree for polysemous terms like 'represents' confirms R1 when no interventional evidence exists and context implies decodability (here, reconstruction via autoencoders). No hedging language is present. Confidence is 4 due to minor ambiguity in 'captures', but context strongly supports associational interpretation."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,SAE feature attribution,1,1,0,4,0,"The method involves training sparse autoencoders (SAEs) on activations to identify concepts, which falls under Rung 1 (observational/associational) per the codebook. The claim uses 'discover' to describe identifying universal concepts through feature attribution without causal language, aligning with Rung 1 markers like 'can be extracted' or 'information is present'. No hedging terms are present. Confidence is high due to clear observational methodology and associational claim framing, with minor ambiguity from 'important' resolved as reconstruction-based importance (non-causal)."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,SAE feature attribution,1,1,0,4,0,"The method used is SAE feature attribution, which is classified as Rung 1 (Observational/Associational) in the codebook. The claim states that SAEs facilitate interpretability research by explaining atomic features to clarify model behavior, which uses associational language (e.g., 'explain' in the context of providing insights without asserting causal or mechanistic roles). The claim does not contain hedging terms and is stated as fact. Confidence is 4 due to minor ambiguity in the word 'explain', but overall context supports Rung 1 classification."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,"SAE evaluation via Concept Contrastive Query Pairs and metrics (L0,t and ICDF)",1,1,0,4,0,"The claim describes evaluating SAEs using observational metrics (Concept Contrastive Query Pairs, L0,t, ICDF) to identify those with optimal concept-specific interpretability. This involves analyzing activation patterns without model intervention, fitting Rung 1 (Observational/Associational) methods. The claim uses associational language ('identifies') without causal/mechanistic markers, aligning with Rung 1 claims. No hedging terms are present. Confidence is high (4) due to clear method alignment, though minor ambiguity exists in whether 'systematically identifies' implies causal inference (ruled out by method description)."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,SAE feature attribution,1,1,0,4,0,"The claim uses 'extract' and 'capture' to describe identifying safety-relevant features via Sparse Autoencoders (SAEs), which is explicitly listed as a Rung 1 (Observational) method in the codebook. The language aligns with Rung 1 markers (e.g., 'extract' mirrors 'can be extracted'). No causal or mechanistic terms (e.g., 'causally affects', 'encodes') are present, and the claim lacks hedging. Confidence is 4 due to slight ambiguity in 'capture', but context confirms an associational claim."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,Activation patching and causal mediation analysis,2,3,1,4,0,"The claim uses mechanistic language ('computed and stored') which typically indicates Rung 3. The paper employs interventional methods (Rung 2) like activation patching and causal mediation to support this claim. Following the decision tree for 'stores': (1) Interventional evidence is provided (YES), (2) The claim describes the underlying mechanism (how counts are represented) rather than intervention results, warranting Rung 3. No hedging language is present. Confidence is 4 due to minor ambiguity in whether the evidence fully proves storage versus correlation, but the mechanistic framing and attention to final tokens strongly support Rung 3."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,"Activation patching, ablation, attention knockout",2,3,1,4,0,"The claim 'counts are transferred to intermediate steps via dedicated attention heads' attributes a functional mechanism to specific components (attention heads), which is a mechanistic claim (Rung 3). The evidence comes from causal mediation analyses including activation patching, ablation, and attention knockout, which are interventional methods (Rung 2). This creates a +1 gap. There is no hedging in the claim. Confidence is 4 due to strong interventional evidence but some ambiguity from the use of observational methods in conjunction."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,Causal mediation analysis,2,2,0,4,0,"The claim 'this strategy enables LLMs to surpass architectural limitations and achieve high accuracy' uses causal language ('enables') consistent with Rung 2 claims about causal effects. The supporting method is causal mediation analysis (Rung 2), which involves interventions like activation patching and attention knockout to establish causal relationships. The claim matches Rung 2 linguistic markers ('enables') without hedging. Confidence is 4 due to minor ambiguity: while the claim is causal, the paper also uses observational methods, but causal mediation directly supports the causal claim."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,"Theoretical analysis and simulation studies with synthetic and real data, measuring similarity metrics (e.g., accuracy, AUROC) without intervention",1,1,0,4,0,"The claim 'neuron identification can be viewed as the inverse process of machine learning' is an associational analogy. The paper supports it with observational methods (Rung 1), including simulations and real-data experiments that compute correlation-based similarity metrics without interventions. The claim uses associational language ('viewed as') and is stated without hedging."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,Similarity metrics,1,1,0,4,0,"The claim is about deriving generalization bounds for similarity metrics to guarantee faithfulness in neuron identification. Similarity metrics are used in Rung 1 methods (e.g., linear probing, activation logging). The claim is associational (Rung 1) as it pertains to the reliability of the association between concepts and neurons. The claim is stated as a fact without hedging."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,Bootstrap ensemble,1,1,0,5,0,"The bootstrap ensemble procedure involves resampling probing datasets without intervening on the model, making it an observational method (Rung 1). The claim focuses on quantifying stability and providing statistical guarantees, which are associational claims about method reliability (Rung 1). The claim is stated definitively without hedging terms."
