paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,circuit analysis with path patching and knockouts,2,3,1,5,0,"The method used is primarily 'path patching' and 'knockouts,' which are interventional techniques (Rung 2). The claim states the paper is 'presenting an explanation' for how GPT-2 small performs IOI, which implies a mechanistic or functional understanding of the model's behavior. This aligns with Rung 3 (mechanistic/counterfactual claims) due to the use of terms like 'explanation' and the implication of reverse-engineering the algorithm. The paper explicitly frames this as reverse-engineering, which is a Rung 3 claim. No hedging is present in the claim."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,circuit analysis with path patching and knockouts,2,3,1,5,0,"1. **Method Rung (2 - Interventional):** The paper uses path patching (an activation patching variant) and knockouts (ablation) to identify and validate the circuit. These are interventional methods that establish causal effects of specific components on model behavior, placing them firmly in Rung 2.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim uses the term ""explanation"" and groups heads into ""classes,"" implying a mechanistic narrative about how the model implements the IOI task. The phrase ""encompasses 26 attention heads"" suggests a completeness or uniqueness claim, which is characteristic of Rung 3. The paper does not merely state causal effects (Rung 2) but frames the circuit as the *mechanism* underlying the behavior.

3. **Overclaim Gap:** The method (Rung 2) does not establish uniqueness or necessity of the circuit, only causal sufficiency. The claim (Rung 3) implies a stronger mechanistic interpretation, resulting in a +1 gap.

4. **Confidence:** High confidence (5) due to clear linguistic markers (""explanation,"" ""encompasses"") and the paper's explicit framing of the circuit as a mechanistic account.

5. **Hedge Flag:** No hedging language (e.g., ""may,"" ""suggests"") is present in the claim."
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,"path patching, activation patching, knockouts",2,3,1,4,0,"The method used (path patching, activation patching, and knockouts) is classified as Rung 2 (Interventional) because it involves causal interventions on model components to measure their effects on behavior. The claim, however, uses the term 'reverse-engineering,' which implies a mechanistic understanding of the model's behavior (Rung 3). This term suggests a uniqueness or completeness claim about the identified circuit, which goes beyond the causal evidence provided by the methods. The paper does not explicitly test for uniqueness or necessity (e.g., via counterfactual patching or causal scrubbing), so the claim overreaches the method's capabilities by one rung."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,activation_patching,2,3,1,5,0,"The claim uses the functional verb 'move,' which is a Rung 3 linguistic marker indicating a mechanistic or functional attribution. The paper primarily uses activation patching (path patching), an interventional method (Rung 2), to establish causal effects of attention heads. However, the claim goes beyond the interventional evidence by asserting a specific mechanistic role ('move the name'), which implies a unique and direct computation. This creates a method-claim gap of +1 (R2 → R3). The confidence is high because the claim explicitly attributes a mechanistic function, and the paper does not hedge this assertion."
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,path_patching,2,3,1,4,0,"The method used is path patching, which is classified as Rung 2 (Interventional) because it involves causal interventions by replacing activations and measuring effects on model behavior. The claim states that 'S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,' which is a mechanistic and functional attribution (e.g., 'inhibit' implies a specific role in the circuit). This aligns with Rung 3 (Mechanistic/Counterfactual) linguistic markers, as it describes a unique and specific computational role. However, the paper does not explicitly test for uniqueness or necessity (e.g., whether these heads are the *only* ones performing this role), which slightly reduces confidence. The claim is stated as a factual assertion without hedging."
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,path_patching,2,3,1,4,0,"The claim ""the circuit is faithful complete and minimal"" implies a mechanistic understanding of the model's behavior, specifically uniqueness and necessity of the identified circuit. This is a Rung 3 claim (mechanistic/counterfactual) as it suggests the circuit is THE mechanism responsible for the task, not just causally sufficient or correlational.

The primary method used to support this claim is path patching (activation patching variant), which is a Rung 2 method (interventional). Path patching establishes causal effects of specific components but does not inherently prove uniqueness or necessity, which are required for Rung 3 claims.

The paper does attempt to validate these criteria (faithfulness, completeness, minimality) through quantitative experiments, but the results show gaps (e.g., the circuit fails the most challenging tests of completeness). The claim thus overreaches the method's capacity, resulting in a +1 gap (R2 method → R3 claim).

Confidence is 4 due to clear linguistic markers (""the circuit,"" implying uniqueness) and well-documented method (path patching), though minor ambiguity exists about whether the authors explicitly claim uniqueness or just sufficiency."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,causal mediation analysis (causal tracing),2,3,1,5,0,"The method used here is causal mediation analysis (referred to as causal tracing in the paper), which involves interventions on model activations (e.g., corrupting and restoring activations) to measure causal effects. This places the method firmly in Rung 2 (Interventional).

The claim states that 'factual associations correspond to localized directly-editable computations.' The use of 'correspond to' in this context, combined with the mechanistic framing ('localized directly-editable computations'), implies a Rung 3 (Mechanistic/Counterfactual) claim. The paper goes beyond establishing causal effects (Rung 2) and suggests a specific computational mechanism underlying factual recall, which aligns with Rung 3 markers like 'encodes,' 'represents,' or 'performs.' The term 'localized' further implies uniqueness, a hallmark of Rung 3 claims.

No hedging language is present in the claim, so hedge_flag is 0. Confidence is high (5) due to the clear alignment of the method and claim with the codebook definitions."
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,causal mediation analysis (activation patching),2,2,0,5,0,"The claim states that the paper identifies 'a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions.' The method used to support this claim is causal mediation analysis (activation patching), which is explicitly described in the paper (e.g., Figure 1 and Section 2.1). This method is classified as Rung 2 (Interventional) because it involves intervening on activations to measure causal effects. The claim itself uses the term 'mediate,' which is a Rung 2 linguistic marker, indicating a causal relationship without asserting uniqueness or mechanistic necessity. Thus, the claim rung aligns with the method rung."
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal tracing (activation patching),2,3,1,5,0,"The claim states that 'mid-layer feed-forward modules... [are] storing factual associations.' The linguistic marker 'storing' is a Rung 3 term, implying a mechanistic or representational claim about how factual associations are encoded in the model. According to the decision tree for polysemous terms like 'stores' or 'encodes,' the paper provides interventional evidence (causal tracing via activation patching, a Rung 2 method) but does not establish uniqueness or counterfactual necessity. However, the claim goes beyond the interventional result (causal mediation) to assert a mechanistic narrative ('storing'), which defaults to Rung 3. The method used (causal tracing) is classified as Rung 2 because it establishes causal effects but not unique mechanisms."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,Rank-One Model Editing (ROME),2,2,0,5,0,"The claim states that 'ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task.' The method used, ROME, is classified as Rung 2 (Interventional) because it involves direct weight modifications to alter factual associations and observes the causal effects of these interventions on model predictions. The claim itself is about the effectiveness of ROME, which is an empirical observation of the outcomes of these interventions, aligning with Rung 2. There is no hedge language present, and the claim is stated as a fact, so hedge_flag is 0. The confidence is high (5) because the method and claim are clearly aligned with the definitions provided in the codebook."
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,Ablation in Fourier space,2,3,1,5,0,"The claim ""We fully reverse engineer the algorithm learned by these networks"" is a strong mechanistic claim implying a complete and unique understanding of the model's internal workings (Rung 3). The primary method used to support this claim is ablation in Fourier space, which is an interventional method (Rung 2). While the paper provides multiple lines of evidence (e.g., weight analysis, activation analysis, and polynomial approximations), the core interventional method (ablation) does not establish counterfactual necessity or uniqueness, which are required for Rung 3 claims. Thus, the claim overreaches the method's capabilities by one rung. The confidence is high (5) because the claim and method are explicitly stated, and the overclaim pattern is clear."
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,mechanistic_reverse_engineering,2,3,1,5,0,"1. **Method Rung Classification**: The paper employs multiple methods to support its claims, including activation analysis, weight analysis, and ablations in Fourier space. The core method used for this specific claim is mechanistic reverse engineering, which involves analyzing weights and activations to infer the algorithm (Fourier transforms and trigonometric identities). This method is classified as **Rung 2 (Interventional)** because it involves causal interventions (e.g., ablations in Fourier space) to validate the proposed mechanism, even though the initial discovery is observational.

2. **Claim Rung Classification**: The claim ""uses discrete Fourier transforms and trigonometric identities to convert addition to rotation"" is a **Rung 3 (Mechanistic/Counterfactual)** claim. It attributes a specific, unique algorithm (Fourier multiplication) to the model's computation, implying that this is *the* mechanism by which the model solves the task. The use of ""uses"" and the explicit description of the algorithm (discrete Fourier transforms and trigonometric identities) are strong linguistic markers of a mechanistic claim.

3. **Polysemous Term Handling**: The term ""uses"" in this context is interpreted as a mechanistic attribution (R3) because the paper provides interventional evidence (ablations, weight analysis) to support the claim that the model implements this specific algorithm. The claim goes beyond causal sufficiency (R2) to assert that this is the unique or primary mechanism.

4. **Hedge Flag**: The claim is stated as a factual assertion (""uses"") without hedging language, so `hedge_flag` is 0.

5. **Confidence**: High confidence (5) due to the explicit description of the algorithm, the supporting interventional evidence (e.g., ablations in Fourier space), and the clear mechanistic narrative in the paper."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,"mechanistic interpretability (reverse engineering, ablation in Fourier space, progress measures)",2,3,1,5,0,"1. **Method Rung Classification**: The paper uses multiple methods to support its claims:
   - **Reverse engineering the weights** (Section 4): This involves analyzing the weights and activations to identify the Fourier multiplication algorithm, which is observational (Rung 1).
   - **Ablation in Fourier space** (Section 4.4): This is an interventional method where specific frequencies are ablated to measure their causal effect on model performance, placing it in Rung 2.
   - **Progress measures** (Section 5): These are derived from the mechanistic understanding and involve tracking metrics like restricted and excluded loss, which are interventional (Rung 2).
   - The **highest-rung method** directly supporting the claim is ablation in Fourier space and the use of progress measures, both of which are Rung 2.

2. **Claim Rung Classification**: The claim ""grokking arises from the gradual amplification of structured mechanisms encoded in the weights"" is a mechanistic narrative. It attributes the phenomenon of grokking to a specific underlying mechanism (structured mechanisms encoded in the weights) and implies a functional role for these mechanisms. This aligns with Rung 3 linguistic markers such as ""arises from,"" ""structured mechanisms,"" and ""encoded in the weights.""

3. **Overclaim Analysis**: The claim is Rung 3, while the highest-rung method used is Rung 2. This results in a +1 gap (R2 → R3), which is consistent with the paper's broader pattern of using interventional evidence to support mechanistic claims.

4. **Confidence**: High confidence (5) due to the clear mechanistic language in the claim and the explicit use of interventional methods (ablation, progress measures) to support it.

5. **Hedge Flag**: No hedging language is present in the claim (e.g., ""may,"" ""suggests""), so hedge_flag is 0."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,interchange intervention,2,2,0,5,0,"The claim ""SAEs struggle to reach the neuron baseline"" is supported by the paper's use of interchange interventions (e.g., patching features to change model outputs while preserving other attributes). This method is classified as Rung 2 (Interventional) because it establishes causal effects of interventions on model behavior. The claim itself is also Rung 2, as it describes the outcome of these interventions (e.g., ""struggle to reach baseline"") without asserting mechanistic uniqueness or counterfactual necessity. The paper explicitly evaluates SAEs using interchange intervention accuracies (Figures 1b and 1c), which directly measure causal mediation of knowledge, aligning the method and claim rungs. No hedging is present, and the evidence is clear and direct."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,interchange interventions,2,2,0,5,0,"The claim states that SAE features ""separately mediate knowledge"" of country and continent. The paper uses interchange interventions (activation patching) to evaluate whether features mediate knowledge, which is a Rung 2 method (interventional). The term ""mediate"" is explicitly listed in the codebook as a Rung 2 linguistic marker for causal claims. The claim does not assert uniqueness or mechanistic narratives (e.g., ""the circuit"" or ""encodes""), so it does not qualify as Rung 3. The method and claim rungs align perfectly."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,Activation probing,1,1,0,5,1,"The claim 'activation probes may be a promising misuse mitigation technique' is associational (Rung 1) because it suggests potential utility without asserting causal or mechanistic relationships. The method used, activation probing, is classified as Rung 1 (Observational/Associational) as it involves training classifiers on frozen activations to detect correlations, not interventions or counterfactuals. The hedge 'may' explicitly flags uncertainty, warranting a hedge_flag of 1. The claim aligns with Rung 1 linguistic markers like 'promising' (suggesting potential correlation) and does not imply causal or mechanistic conclusions."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,activation_probing,1,1,0,5,0,"The claim ""probes fail to generalize under important production distribution shifts"" is based on the method of activation probing, which is classified as Rung 1 (Observational/Associational) according to the codebook. Activation probing involves training a classifier on frozen activations to detect correlations, without any intervention on the model. The claim itself is observational, describing a failure in generalization, which aligns with Rung 1 (Associational Claims). There is no hedge language present, and the confidence is high as the claim is directly supported by the method described."
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Activation patching,2,3,1,4,0,"The method used is explicitly stated as 'activation patching' in the abstract and throughout the paper, which falls under Rung 2 (Interventional) methods according to the codebook. The claim uses the term 'reverse-engineered,' which implies a mechanistic understanding of the model's behavior, aligning with Rung 3 (Mechanistic/Counterfactual) claims. While the term 'reverse-engineered' is not explicitly listed in the Rung 3 linguistic markers, its mechanistic implications (understanding the underlying algorithm) justify the Rung 3 classification. No hedging language is present in the claim."
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,ACDC (Automatic Circuit DisCovery) with activation patching,2,3,1,5,0,"The method used is ACDC, which employs activation patching (an interventional method) to identify causal effects of model components on behavior. This places it firmly in Rung 2 according to the codebook (e.g., 'Activation patching: Replace activation, measure effect'). The claim states that ACDC 'rediscovered 5/5 of the component types in a circuit,' which implies a mechanistic understanding of the circuit's functionality and uniqueness (e.g., 'THE circuit' or 'component types'). This language aligns with Rung 3 claims, as it suggests the algorithm identifies the specific, unique components responsible for the behavior, not just correlational or causal effects. The paper provides interventional evidence (activation patching) but does not explicitly test for uniqueness or counterfactual necessity, which would be required for a strict Rung 3 classification. However, the claim's phrasing ('rediscovered... component types') leans toward a mechanistic interpretation, justifying Rung 3. No hedging is present."
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,activation_patching,2,3,1,4,0,"The method used here is activation patching, which is explicitly mentioned in the abstract ('apply activation patching to find which abstract neural network units are involved in the behavior'). Activation patching is classified as a Rung 2 (Interventional) method according to the codebook. 

The claim states that 'researchers can understand the functionality of each component.' The term 'understand the functionality' implies a mechanistic or functional attribution, which aligns with Rung 3 (Mechanistic/Counterfactual) claims. The codebook specifies that terms like 'performs,' 'computes,' or 'underlies' are indicative of Rung 3 claims. Although the claim does not explicitly use these terms, 'understand the functionality' suggests a mechanistic narrative about how components work, which goes beyond mere causal effects (Rung 2) or correlational evidence (Rung 1). 

There is no explicit hedge in the claim, so the hedge_flag is 0. Confidence is rated 4 due to minor ambiguity in interpreting 'understand the functionality' as a Rung 3 claim, but the context strongly supports this classification."
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,activation_patching,2,3,1,5,0,"The method used here is activation patching (specifically, the ACDC algorithm automates this process), which is classified as Rung 2 (Interventional) according to the codebook. The claim states that the paper is ""finding the connections between abstract neural network units that form a circuit,"" which implies identifying a unique subgraph responsible for a behavior. This aligns with Rung 3 (Mechanistic/Counterfactual) claims, as it suggests uncovering ""the circuit"" (uniqueness) and framing it as the mechanism underlying the behavior. The claim does not contain hedging language."
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,Edge Attribution Patching (EAP) and Automated Circuit Discovery (ACDC),2,3,1,4,0,"1. **Method Rung Classification**: The paper uses Edge Attribution Patching (EAP) and Automated Circuit Discovery (ACDC), which are interventional methods (Rung 2). These techniques involve patching activations and measuring causal effects on model behavior, but they do not establish counterfactual necessity or uniqueness of mechanisms.

2. **Claim Rung Classification**: The claim ""partially reverse-engineer the circuit responsible for the Indirect Object Identification task"" uses mechanistic language (""reverse-engineer the circuit"") and implies a functional understanding of the circuit's role. This aligns with Rung 3 claims, which involve mechanistic or counterfactual assertions about model internals.

3. **Overclaim Analysis**: The method (Rung 2) does not fully support the claim (Rung 3) because the techniques used (EAP/ACDC) establish causal sufficiency but not the uniqueness or necessity of the identified circuit. The claim suggests a stronger mechanistic interpretation than the methods justify.

4. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""), so the hedge flag is 0.

5. **Confidence**: Confidence is 4 due to the clear linguistic markers (""reverse-engineer,"" ""circuit"") indicating a Rung 3 claim, but minor ambiguity exists about whether the authors intend to imply uniqueness or necessity."
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,"Resample ablation, Layer removal, Removing token cross-talk",2,3,1,4,0,"1. **Method Rung Classification**: The paper uses resample ablation (Section 3.1.4), layer removal (zero-ablating layer outputs), and experiments removing token cross-talk to identify critical layers. These methods involve interventions (e.g., patching activations, ablating layers) to measure causal effects, placing them firmly in **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim ""Layer 39 is a key bottleneck"" implies a mechanistic role (e.g., uniqueness, necessity) for Layer 39 in the IOI task. The term ""bottleneck"" suggests a functional or structural necessity, which aligns with **Rung 3 (Mechanistic/Counterfactual)** linguistic markers. The paper does not explicitly test uniqueness (e.g., via causal scrubbing or necessity tests), but the phrasing and narrative framing (e.g., ""key bottleneck"") push the claim toward R3.

3. **Overclaim Gap**: The method (R2) does not fully support the mechanistic claim (R3), resulting in a +1 gap. The paper provides causal evidence (e.g., ablation degrades performance) but does not establish Layer 39 as the *unique* or *necessary* bottleneck.

4. **Confidence**: High (4/5) due to clear interventional methods and unambiguous mechanistic language in the claim. Minor ambiguity arises from the lack of explicit uniqueness testing.

5. **Hedge Flag**: 0 (no explicit hedging in the claim)."
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,Resample ablation on convolution slices and activation analysis,2,3,1,4,0,"1. **Method Rung Classification**: The paper uses resample ablation (an interventional method) on individual convolution slices in layer 39 to test their causal effects on model behavior (Section 4.2, Figure 8). This establishes causal sufficiency, placing the method in **Rung 2 (Interventional)**. Additionally, the paper analyzes cosine similarity of activations (Figure 7), but the primary evidence for the claim comes from the ablation experiments.

2. **Claim Rung Classification**: The claim ""Convolutions in layer 39 shift names one position forward"" uses mechanistic language (""shift names"") and implies a functional role for the convolution component. This is a **Rung 3 (Mechanistic/Counterfactual)** claim because it attributes a specific computational mechanism to the model's internals, not just a causal effect.

3. **Overclaim Gap**: The method (Rung 2) does not fully support the claim (Rung 3). While the ablation experiments show that the convolution slices are causally sufficient for the behavior, they do not establish that this is the *unique* or *necessary* mechanism (e.g., no counterfactual testing or uniqueness proofs). The claim thus overreaches by implying a definitive mechanistic explanation.

4. **Hedge Flag**: The claim is stated as a factual assertion (""Convolutions... shift names..."") with no hedging language, so **hedge_flag = 0**.

5. **Confidence**: The method and claim are clearly described, and the overclaim pattern (R2 → R3) is well-documented in the codebook. However, minor ambiguity exists about whether the claim could be interpreted as a Rung 2 causal sufficiency statement, so confidence is **4**."
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,Activation patching and representation modification,2,3,1,4,0,"1. **Method Used**: The paper employs activation patching (resample ablation) and representation modification (e.g., substituting SSM inputs with averaged values) to test the causal role of Layer 39 in the IOI task. These are interventional methods (Rung 2).

2. **Method Rung**: The primary methods (activation patching, resample ablation, and representation modification) establish causal effects but do not test counterfactual necessity or uniqueness. Thus, the method rung is **2 (Interventional)**.

3. **Claim Rung**: The claim ""name entities are stored linearly in Layer 39's SSM"" uses mechanistic language (""stored"") and implies a functional representation within the SSM. This is a **Rung 3 (Mechanistic/Counterfactual)** claim, as it suggests a specific internal mechanism (linear storage) rather than just a causal effect.

4. **Overclaim Gap**: The claim (Rung 3) exceeds the method's capabilities (Rung 2), resulting in a +1 gap. The paper does not provide evidence for uniqueness or counterfactual necessity, which would be required for a Rung 3 claim.

5. **Hedge Flag**: The claim is stated as a factual assertion (""are stored""), with no explicit hedging (e.g., ""may"", ""suggests""). Thus, **hedge_flag = 0**.

6. **Confidence**: The classification is confident (4/5) due to clear linguistic markers (""stored"") and well-defined methods, though minor ambiguity exists about whether ""linear storage"" is fully justified by the evidence."
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,benchmarking and empirical evaluation,1,2,1,5,0,"The claim ""prompting outperforms all existing methods followed by finetuning"" is based on empirical evaluation using AXBENCH, a benchmarking framework. Benchmarking and empirical evaluation fall under **Rung 1 (Observational/Associational)** methods, as they establish correlational evidence without direct intervention on the model's internals.

The claim itself is **Rung 2 (Causal)**, as it implies a causal relationship (""outperforms"") between the method (prompting/finetuning) and the outcome (performance on steering and concept detection tasks). While the paper provides empirical evidence, it does not establish counterfactual or mechanistic explanations for *why* prompting outperforms other methods, which would be required for a **Rung 3** claim.

The claim is stated as a factual observation without hedging, so `hedge_flag` is 0. Confidence is high (5) due to the clear empirical support and the straightforward classification of the method and claim rungs."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,Benchmarking and Evaluation,1,1,0,5,0,"The claim 'SAEs are not competitive' is an empirical observation based on the benchmarking results presented in the paper. The method used to support this claim is the evaluation of SAEs on AXBENCH, which involves measuring performance metrics (AUROC for concept detection and overall steering scores) and comparing them to other methods. This is purely observational/associational (Rung 1), as it does not involve interventions or counterfactual analysis. The claim itself is also associational (Rung 1), as it states a comparative performance outcome without implying causality or mechanistic insight. There is no hedge in the claim, and the confidence is high given the clear empirical evidence provided in the paper."
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,difference-in-means,2,2,0,5,0,"The method used is 'difference-in-means,' which is explicitly listed under Rung 2 (Interventional) methods in the codebook. The claim states that 'representation-based methods such as difference-in-means perform the best,' which is a comparative performance claim based on empirical results from interventions (e.g., activation patching or steering). This aligns with Rung 2 linguistic markers such as 'perform,' which is appropriate for causal or interventional claims. There is no hedge language, and the claim is stated as an established fact, so the hedge_flag is 0. Confidence is high (5) because the method and claim are clearly aligned with Rung 2 definitions."
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,activation_patching,2,2,0,5,0,"The claim states that 'specific components within middle layers show strong causal effects at the last token of the subject.' The method used is activation patching (also referred to as causal tracing or interchange interventions in the paper), which is classified as a Rung 2 method according to the codebook, as it involves interventions to establish causal effects. The claim itself uses causal language ('show strong causal effects'), which aligns with Rung 2 claims. There is no hedge present, and the claim is directly supported by the method used."
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,ROME (Rank One Model Editing),2,2,0,5,0,"The method used here is ROME (Rank One Model Editing), which involves intervening on model weights to insert or modify factual associations. This is classified as a Rung 2 method because it establishes causal effects through direct intervention (editing weights and observing changes in output).

The claim states that ""rank-one model editing methods can successfully insert facts at specific locations."" This is a causal claim about the sufficiency of the intervention (editing) to produce a specific outcome (inserting facts), which aligns with Rung 2. There is no uniqueness or counterfactual necessity implied, nor does the claim assert a mechanistic narrative (e.g., ""the model uses X to store facts""). Thus, the claim rung is appropriately Rung 2.

No hedging language is present in the claim, so hedge_flag is 0."
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,Jacobian analysis (LRE - Linearity of Relation Embedding),1,1,0,5,0,"The claim ""linearity of Mamba's representations of factual relations"" is supported by the method of Jacobian analysis (LRE), as described in Section 5 of the paper. This method involves computing the first-order Taylor approximation of the model's computation to assess whether factual relations can be linearly decoded from embeddings. This is an observational/associational method (Rung 1) because it measures correlations between subject and object embeddings without intervening on the model's internal states.

The claim itself uses the term ""linearity,"" which is a correlational descriptor, aligning with Rung 1 linguistic markers (e.g., ""can be approximated,"" ""faithfulness of LRE""). There is no mechanistic or causal language (e.g., ""encodes,"" ""controls,"" ""the mechanism"") that would elevate the claim to Rung 2 or Rung 3. Thus, the claim rung matches the method rung."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,interchange interventions (activation patching) with causal abstraction and desiderata-based component masking,2,3,1,5,0,"1. **Method Rung Classification (Rung 2 - Interventional):**
   - The paper primarily uses **interchange interventions (activation patching)** to establish causal effects of specific internal representations (e.g., ordering IDs, binding lookbacks) on model outputs. This is a hallmark of Rung 2 methods, as it involves direct interventions on model activations to observe causal outcomes.
   - Additional techniques like **causal abstraction** and **desiderata-based component masking (DCM)** are used to localize and verify hypothesized causal variables (e.g., ordering IDs, pointers, payloads) within the model's residual stream. While these methods refine the analysis, they still operate within the interventional framework (Rung 2).

2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**
   - The claim ""LM binds each character-object-state triple together by co-locating their reference information"" is a **mechanistic** assertion. It describes *how* the model internally represents and processes information (binding via co-location of ordering IDs), not just *that* an intervention affects output (Rung 2) or *that* information is correlated with outputs (Rung 1).
   - The phrase ""binds"" implies a functional, algorithmic mechanism (Rung 3), as it attributes a specific computational role to the model's internal representations. The paper further supports this with evidence of *unique* binding mechanisms (e.g., binding lookback, answer lookback) that are necessary for belief tracking.
   - The claim goes beyond causal sufficiency (Rung 2) by proposing a **unified mechanism** (the lookback) that explains *how* the model achieves belief tracking, aligning with Rung 3's focus on counterfactual or mechanistic explanations.

3. **Overclaim Gap:**
   - The method (Rung 2) establishes causal effects of interventions (e.g., patching ordering IDs changes outputs), but the claim (Rung 3) asserts a *specific mechanistic implementation* (co-location of reference information as the binding mechanism). This is a +1 gap, as the method does not *uniquely* prove the proposed mechanism (though the paper provides strong evidence via causal abstraction and subspace localization).

4. **Confidence:**
   - High confidence (5/5) due to:
     - Clear interventional evidence (e.g., Figs. 4, 5, 6) showing that manipulating ordering IDs or binding lookbacks causally affects outputs.
     - Explicit alignment of hypothesized causal variables (e.g., ordering IDs, pointers) with model activations via DCM and interchange interventions.
     - The claim is directly supported by the paper's core findings and methodological rigor.

5. **Hedge Flag:**
   - No hedging language (e.g., ""may,"" ""suggests"") is present in the claim. It is stated as a factual assertion (hedge_flag = 0)."
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,causal_mediation_analysis_and_interchange_interventions,2,3,1,4,0,"The claim states that the ""lookback mechanism... enables the LM to recall important information."" This is a mechanistic claim (Rung 3) because it attributes a specific computational function (recall via lookback) to the model's internal workings. 

The method used to support this claim includes **causal mediation analysis** and **interchange interventions** (e.g., activation patching), which are classified as Rung 2 methods. These methods establish causal effects of interventions but do not directly prove the *unique* or *necessary* mechanism underlying the behavior. The paper hypothesizes the lookback mechanism and tests it via interventions, but the claim itself goes beyond the interventional evidence by asserting a mechanistic narrative (e.g., ""enables the LM to recall"").

The gap between method (R2) and claim (R3) is +1, as the claim implies a mechanistic understanding not fully justified by the interventional evidence alone. Confidence is 4 due to the paper's detailed causal abstraction framework, but some ambiguity remains about whether the lookback mechanism is the *only* possible explanation."
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,interchange interventions (activation patching) with causal abstraction and desiderata-based component masking,2,3,1,5,0,"The method used here is primarily **interchange interventions (activation patching)**, which is classified as **Rung 2 (Interventional)** according to the codebook. The paper also employs **causal abstraction** and **desiderata-based component masking** to localize and verify the hypothesized causal model, but these are supportive techniques that do not elevate the method beyond Rung 2.

The claim ""the binding lookback retrieves the correct state OI"" is a **mechanistic claim** (Rung 3). The phrase ""retrieves the correct state OI"" implies a functional, algorithmic mechanism by which the model operates, which goes beyond merely establishing causal effects (Rung 2). The paper provides interventional evidence (e.g., activation patching experiments in Figures 4, 5, and 6) to support this claim, but the claim itself is framed as a unique, mechanistic explanation of how the model tracks beliefs, aligning with Rung 3 linguistic markers such as ""retrieves"" and the implication of a specific computational process (the lookback mechanism).

The claim does not contain explicit hedging, so the **hedge_flag is 0**. The confidence is **5** due to the clear alignment of the claim with Rung 3 linguistic markers and the robust interventional evidence provided to support the mechanistic narrative."
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,causal abstraction with interchange interventions,2,3,1,5,0,"The method used is primarily **causal abstraction with interchange interventions**, which involves activation patching and testing alignment between a hypothesized high-level causal model and the model's internal activations. This method is classified as **Rung 2 (Interventional)** because it establishes causal effects of specific interventions on model behavior, but it does not test counterfactual necessity or uniqueness of mechanisms.

The claim ""reverse-engineering ToM reasoning in LMs"" uses the term ""reverse-engineering,"" which implies a **mechanistic or algorithmic understanding** of the model's computations. This term is a **Rung 3 linguistic marker** because it suggests the identification of a unique or complete mechanism underlying the model's behavior. The paper does not explicitly test for uniqueness or necessity of the proposed lookback mechanism, but the framing of the claim strongly implies a mechanistic narrative.

Thus, the **claim rung (3) exceeds the method rung (2)**, resulting in an overclaim gap of +1."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,activation_patching,2,3,1,4,0,"The claim states that 'LMs implement such retrieval via a positional mechanism,' which implies a mechanistic and functional attribution (Rung 3). The paper uses activation patching (interchange interventions) as its primary method, which is classified as Rung 2 (interventional). The claim overreaches the method's capacity by asserting a mechanistic implementation rather than a causal effect, resulting in a method-claim gap of +1. The confidence is rated 4 due to the clear linguistic marker ('implement') and the well-defined method, though there is minor ambiguity in whether 'implement' is used strictly mechanistically."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,interchange_interventions,2,3,1,5,0,"1. **Method Used and Method Rung**: The paper employs interchange interventions (e.g., activation patching, counterfactual inputs) to test causal effects of specific mechanisms. This is classified as Rung 2 (Interventional) because it involves interventions on model activations to measure their effects on behavior, but it does not establish counterfactual necessity or uniqueness.

2. **Claim Rung**: The claim states that ""LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism."" This implies a mechanistic narrative about how the model *uses* these mechanisms to perform entity binding, which goes beyond causal sufficiency (Rung 2) and into functional/mechanistic attribution (Rung 3). The use of terms like ""supplement"" and the description of how these mechanisms interact (e.g., reflexive pointers) suggests a mechanistic explanation of model behavior.

3. **Polysemous Term Handling**: The term ""mechanism"" is used in a functional/mechanistic sense (e.g., ""lexical mechanism,"" ""reflexive mechanism""), which aligns with Rung 3 claims. The paper provides interventional evidence (Rung 2) but frames the claim as a mechanistic explanation (Rung 3).

4. **Overclaim Gap**: The claim is Rung 3, while the method is Rung 2, resulting in a +1 overclaim gap. This is consistent with the paper's broader narrative, which often uses mechanistic language (e.g., ""how LMs retrieve bound entities"") despite relying primarily on interventional methods.

5. **Confidence**: High confidence (5) due to clear linguistic markers (""mechanism,"" ""supplement"") and the paper's explicit framing of these as functional components of the model's behavior."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,interchange_interventions,2,2,0,5,0,"The method used here is interchange interventions (activation patching), which is classified as Rung 2 (Interventional) according to the codebook. The claim states that the authors developed a ""causal model combining all three mechanisms that estimates next token distributions with 95% agreement."" This is a claim about the causal effects of the mechanisms (positional, lexical, reflexive) on model behavior, which aligns with Rung 2 linguistic markers such as ""causally affects"" or ""intervening on X changes Y."" There is no overclaiming, as the claim does not assert mechanistic uniqueness or counterfactual necessity (Rung 3). The confidence is high (5) because the method and claim are clearly aligned, and the paper explicitly describes the use of interchange interventions to support this claim."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,interchange interventions,2,3,1,5,0,"The paper uses interchange interventions (e.g., activation patching, counterfactual inputs) as its primary method, which is classified as Rung 2 (Interventional) in the codebook. The claim ""how LMs bind and retrieve entities in-context"" implies a mechanistic understanding of the process, including uniqueness and functional attribution (e.g., ""the model uses X to do Y""), which aligns with Rung 3 (Mechanistic/Counterfactual) linguistic markers. The paper explicitly proposes and tests three distinct mechanisms (positional, lexical, reflexive) and demonstrates their interplay, further supporting a Rung 3 claim."
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,activation_patching,2,3,1,4,1,"1. **Method Used**: The paper primarily uses activation patching (path patching), a variant of causal intervention, to analyze the IOI circuit in GPT-2 small. This is classified as Rung 2 (Interventional) per the codebook.

2. **Claim Rung**: The claim ""circuits within LLMs may be more flexible and general than previously recognized"" implies a mechanistic understanding of how circuits adapt and generalize. The use of ""may"" is a hedge, but the underlying claim suggests a Rung 3 (Mechanistic/Counterfactual) interpretation because it posits a broader, more generalizable property of circuits (flexibility and generality) beyond the specific interventions studied.

3. **Hedge Flag**: The claim includes the hedge ""may,"" so hedge_flag is set to 1.

4. **Confidence**: The confidence is 4 because the claim is framed as a general mechanistic insight (R3) but is supported by R2 methods (activation patching). The hedge reduces the strength of the claim but does not eliminate its R3 nature. The paper provides interventional evidence (R2) but does not fully establish counterfactual necessity or uniqueness (R3), leading to minor ambiguity."
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,activation_patching,2,3,1,4,0,"The method used is activation patching (path patching variant), which is classified as Rung 2 (Interventional) per the codebook. The claim ""the circuit generalizes surprisingly well reusing all of its components and mechanisms"" implies a mechanistic understanding of the circuit's behavior (""reusing mechanisms""), which aligns with Rung 3 (Mechanistic/Counterfactual) linguistic markers. The paper does not provide uniqueness tests or counterfactual necessity evidence, but the claim uses mechanistic language (""mechanisms"") and suggests a generalizable, interpretable algorithm. Confidence is 4 due to clear Rung 2 method and Rung 3 claim, but minor ambiguity exists around the strength of the mechanistic interpretation."
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,activation_patching,2,3,1,5,0,"The claim ""we discover a mechanism that explains this which we term S2 Hacking"" is a mechanistic claim (Rung 3) because it asserts the identification of a specific, unique mechanism (""S2 Hacking"") that explains model behavior. The paper uses activation patching (Rung 2) as the primary method to uncover this mechanism, as seen in Section 4 where they analyze deviations in attention patterns and causal effects of patching activations. The claim goes beyond correlational or causal sufficiency (Rung 1/2) by proposing a functional narrative for how the circuit operates, which aligns with Rung 3 markers like ""mechanism"" and ""explains."" No hedging is present, and the confidence is high due to the detailed interventional evidence supporting the mechanism."
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,circuit discovery via path patching,2,3,1,5,0,"The method used is circuit discovery via path patching (activation patching), which is classified as Rung 2 (Interventional) according to the codebook. The claim ""implement algorithms responsible for performing specific tasks"" uses the term ""implement algorithms,"" which is a mechanistic claim (Rung 3) as it implies a functional and unique explanation of how the model solves the task. This creates a method-claim gap of +1, consistent with the overclaim pattern for circuit discovery papers (e.g., IOI paper calibration). The claim is stated as an established fact (no hedge). Confidence is high (5) due to clear alignment with the codebook's decision rules for mechanistic language."
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Multi-task Distributed Alignment Search (MDAS) with interchange interventions,2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The paper uses MDAS, which is an extension of Distributed Alignment Search (DAS). DAS and MDAS rely on interchange interventions (activation patching) to identify causal effects of features on model behavior. This places the method squarely in Rung 2, as it establishes causal effects under specific interventions but does not test counterfactual necessity or uniqueness.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that MDAS ""achieves state-of-the-art results"" and ""demonstrates the importance of going beyond neuron-level analyses to identify features distributed across activations."" The phrase ""demonstrates the importance"" implies a mechanistic narrative about how the model works, which is characteristic of Rung 3. Additionally, the claim suggests that MDAS identifies features that are *the* distributed representations used by the model, which leans toward a uniqueness or mechanistic interpretation.

3. **Overclaim Gap:** The method (Rung 2) does not provide evidence for the uniqueness or necessity of the identified features, which would be required to fully support the Rung 3 claim. The claim thus overreaches by implying a mechanistic understanding that the method does not fully establish.

4. **Hedge Flag (0):** The claim is stated as an established fact (e.g., ""MDAS achieves state-of-the-art results"") with no explicit hedging.

5. **Confidence (4):** The classification is confident due to clear linguistic markers (e.g., ""demonstrates the importance,"" ""identify features distributed across activations"") that align with Rung 3 claims. However, there is minor ambiguity because the claim does not explicitly use terms like ""the mechanism"" or ""the circuit,"" which would make it a clearer Rung 3 case."
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,interchange intervention,2,3,1,5,0,"The method used is an interchange intervention (activation patching), which is classified as Rung 2 (Interventional) according to the codebook. The claim states that the feature F ""encodes"" the attribute continent, which is a mechanistic/counterfactual claim (Rung 3) as it implies a functional or representational role beyond mere causal effect. The codebook specifies that ""encodes"" without interventional evidence supporting uniqueness defaults to Rung 3. Here, the intervention establishes a causal effect (Rung 2), but the claim overreaches by implying a mechanistic interpretation (Rung 3)."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,interchange interventions,2,2,0,5,0,"The method used here is interchange interventions (also known as activation patching), which is explicitly classified as a Rung 2 (Interventional) method in the codebook. The claim states that methods with counterfactual supervision (e.g., interchange interventions) achieve strong results, which is a causal claim about the effectiveness of interventions. This aligns with Rung 2 linguistic markers such as ""achieves,"" which is appropriate for claims about causal effects. There is no overclaiming, as the claim does not imply mechanistic or counterfactual uniqueness (Rung 3). The claim is stated as an established fact, so hedge_flag is 0."
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,interchange intervention,2,3,1,4,0,"1. **Method Used**: The paper employs interchange interventions (activation patching) to evaluate the causal effects of features on model behavior. This is explicitly described in Section 2.2 and used throughout the results (e.g., Figure 3b). Interchange interventions are classified as Rung 2 (Interventional) per the codebook.

2. **Claim Rung**: The claim states that ""representations of different attributes gradually disentangle as we move towards later layers,"" which implies a mechanistic understanding of how the model organizes and separates information across layers. This goes beyond causal effects (Rung 2) and suggests a functional or algorithmic mechanism (Rung 3). The use of ""disentangle"" and the layer-wise analysis imply a deeper, mechanistic interpretation of model internals.

3. **Hedge Flag**: The claim is stated as an established fact (no hedging language like ""may"" or ""suggests""), so hedge_flag is 0.

4. **Confidence**: The claim is supported by empirical results (Figure 3b) and aligns with the paper's narrative about disentanglement. However, the leap from interventional evidence (Rung 2) to mechanistic claims (Rung 3) introduces minor ambiguity, warranting a confidence score of 4."
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,Interchange intervention (activation patching),2,3,1,4,0,"1. **Method Identification**: The paper uses interchange interventions (activation patching) as the primary method to evaluate interpretability methods like MDAS, DAS, and others. This is explicitly described in Section 2.2 and used throughout the results (e.g., Figure 3a). Interchange interventions are classified as Rung 2 (Interventional) per the codebook.

2. **Claim Analysis**: The claim states that ""some groups of attributes are more difficult to disentangle than others"" and that ""changing one of these entangled attributes has seemingly unavoidable ripple effects."" This implies a mechanistic understanding of how attributes interact within the model, suggesting uniqueness or necessity of certain representations. The language ""seemingly unavoidable ripple effects"" leans toward a Rung 3 (Mechanistic/Counterfactual) claim, as it describes a unique or necessary relationship between attributes.

3. **Polysemous Term Handling**: The term ""disentangle"" is not explicitly addressed in the codebook's decision trees, but the context here suggests a mechanistic interpretation (e.g., ""unavoidable ripple effects""). This aligns with Rung 3 claims, which often involve uniqueness or necessity.

4. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as an observation, so hedge_flag is 0.

5. **Confidence**: The claim is supported by empirical results (e.g., Figure 3a), but the mechanistic interpretation (""unavoidable ripple effects"") is somewhat ambiguous. Confidence is rated 4 due to minor ambiguity in whether the claim is purely causal (Rung 2) or mechanistic (Rung 3)."
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,"Observational/Associational (e.g., probing, activation analysis)",1,3,2,4,0,"The claim uses the term 'encode,' which is a polysemous term requiring decision tree analysis. The paper cites prior interpretability work but does not provide interventional evidence in this specific claim. The context suggests the authors are referring to the decodability sense of 'encode' (e.g., linear probing), but the term is used in a mechanistic narrative, implying a Rung 3 claim. The default interpretation of 'encode' in mechanistic interpretability is Rung 3 unless explicitly clarified otherwise. The claim is stated as an established fact without hedging."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,distributed interchange intervention (DII) / distributed alignment search (DAS),2,3,1,4,0,"1. **Method Rung Classification**: The claim references ""interventions on linear subspaces of representations,"" which aligns with methods like distributed interchange intervention (DII) or distributed alignment search (DAS). These methods are classified as **Rung 2 (Interventional)** because they involve interventions on model representations to establish causal effects, but they do not test counterfactual necessity or uniqueness.

2. **Claim Rung Classification**: The claim uses the term ""encoded,"" which is a polysemous term. Following the decision tree for ""encodes"":
   - The paper provides interventional evidence (DII/DAS) for this claim.
   - The claim is about the *mechanism* (how concepts are represented) rather than just the *result* of an intervention. This pushes the claim into **Rung 3 (Mechanistic/Counterfactual)**.

3. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as an established fact.

4. **Confidence**: The classification is confident (4/5) due to the clear alignment with the method and claim rung definitions, though there is minor ambiguity in interpreting ""encoded"" as a mechanistic claim."
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,Distributed Alignment Search (DAS),2,3,1,4,0,"1. **Method Identification**: The claim references 'DAS' (Distributed Alignment Search), which is explicitly mentioned in the paper as a method for finding subspaces that maximize the probability of counterfactual outputs after intervention (Section 3.1). This aligns with the definition of **interventional methods** (Rung 2) in the codebook, as DAS involves interventions on model representations to test causal effects.

2. **Method Rung Classification**: DAS is described as performing distributed interchange interventions (DII) to localize concepts within representations. This is an interventional method (Rung 2) because it involves modifying activations and measuring the effect on model outputs, but it does not establish counterfactual necessity or uniqueness (Rung 3).

3. **Claim Rung Classification**: The claim uses the terms 'highly expressive' and 'effectively localize concepts,' which imply mechanistic or functional attribution. The phrase 'localize concepts' suggests a stronger claim than mere causal mediation, leaning toward a **mechanistic claim** (Rung 3). The codebook specifies that terms like 'localize' or 'encode' often imply Rung 3 claims, especially when combined with functional or mechanistic language.

4. **Polysemous Term Handling**: The term 'localize' is analyzed using the decision tree for polysemous terms in the codebook. The paper provides interventional evidence (DAS) for the claim, but the claim itself is about the *mechanism* (how concepts are localized) rather than the *result* of the intervention. Thus, it defaults to Rung 3.

5. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., 'may,' 'suggests,' 'potentially'). It is stated as a factual assertion, so the hedge flag is set to 0.

6. **Confidence**: The confidence is rated 4 due to minor ambiguity in interpreting 'localize concepts.' While the claim leans toward Rung 3, the method (DAS) is clearly Rung 2, and the claim does not explicitly assert uniqueness or necessity, which would be required for a definitive Rung 3 classification."
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,Representation Finetuning (ReFT) with Low-rank Linear Subspace ReFT (LoReFT),2,3,1,5,0,"1. **Method Rung Classification**: The method used here is LoReFT, which involves learning task-specific interventions on hidden representations. This method is interventional because it manipulates specific representations to steer model behavior, aligning with Rung 2 (Interventional) methods as per the codebook. Specifically, LoReFT builds on distributed interchange intervention (DII) and distributed alignment search (DAS), which are interventional techniques.

2. **Claim Rung Classification**: The claim states that ""a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks."" This implies a mechanistic or counterfactual understanding of how the model works, suggesting a unique and generalizable mechanism. Such claims are classified as Rung 3 (Mechanistic/Counterfactual) in the codebook, as they involve assertions about the model's internal mechanisms and their broader applicability.

3. **Confidence**: The confidence is high (5) because the claim explicitly uses mechanistic language (""generalised control over a vast number of tasks"") and the method is clearly described as interventional, making the rung assignments straightforward.

4. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""), so the hedge flag is set to 0."
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,Representation Finetuning (ReFT) with Low-rank Linear Subspace ReFT (LoReFT),2,3,1,4,0,"1. **Method Rung Classification**: The method used here is LoReFT, which involves interventions on hidden representations of a frozen base model. Specifically, LoReFT applies low-rank linear subspace interventions (eq. 2 in the paper) to steer model behavior. This method is interventional because it manipulates specific representations to observe causal effects on model outputs, placing it in **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim states that LoReFT ""can induce a base LM to follow instructions."" This phrasing implies a mechanistic or functional attribution, suggesting that the interventions are responsible for enabling the model to perform a specific behavior (instruction-following). Such claims typically fall under **Rung 3 (Mechanistic/Counterfactual)** due to the use of terms like ""induce,"" which imply a direct and unique causal mechanism.

3. **Polysemous Term Handling**: The term ""induce"" is analyzed here. The paper provides interventional evidence (LoReFT's ability to steer model behavior), but the claim goes beyond causal effects to imply a mechanistic narrative about how the model ""follows instructions."" This aligns with the Rung 3 interpretation, as it suggests a functional role for the interventions.

4. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as a factual outcome of the method.

5. **Confidence**: The confidence is rated 4 due to the clear interventional nature of the method and the mechanistic framing of the claim. However, there is minor ambiguity in whether ""induce"" is strictly mechanistic or could be interpreted as a strong causal effect (Rung 2). The context of the discussion section leans toward Rung 3."
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,KnowledgeEditor (hyper-network for parameter updates),2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The paper introduces KNOWLEDGEEDITOR, a hyper-network that predicts weight updates to modify factual knowledge in language models. This method involves direct interventions on model parameters (e.g., editing weights to change predictions for specific inputs while preserving others), which aligns with Rung 2 (Interventional) methods as per the codebook. The use of constrained optimization to enforce specific behavioral changes further supports this classification.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that factual knowledge is ""stored in the parameters"" of language models. The term ""stored"" implies a mechanistic interpretation of how knowledge is represented or encoded within the model, which is a Rung 3 claim. The codebook specifies that terms like ""stores,"" ""encodes,"" or ""represents"" typically indicate Rung 3 claims, especially when they suggest a mechanistic narrative about how the model functions internally.

3. **Overclaim Gap:** The claim (Rung 3) exceeds the method's capabilities (Rung 2). While the method demonstrates causal effects of parameter edits (e.g., changing predictions for specific inputs), it does not establish that the model ""stores"" knowledge in a mechanistic or unique way. The codebook notes that storage/encoding language often implies Rung 3 claims, which are not fully supported by interventional methods alone.

4. **Hedge Flag (0):** The claim is stated as an established fact (""stored in the parameters"") without explicit hedging (e.g., ""may,"" ""suggests"").

5. **Confidence (4):** The classification is confident but not definitive due to minor ambiguity in interpreting ""stored."" The codebook's decision tree for polysemous terms (e.g., ""encodes""/""stores"") suggests defaulting to Rung 3 for mechanistic claims unless the context explicitly limits the interpretation to decodability (Rung 1). Here, the context leans toward a mechanistic reading, but the lack of counterfactual or uniqueness testing (e.g., causal scrubbing) introduces slight uncertainty."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,hyper-network parameter update,2,3,1,4,0,"1. **Method Rung Classification**: The paper uses a hyper-network to predict parameter updates for a language model (KNOWLEDGEEDITOR). This method involves *interventions* on the model's parameters (e.g., constrained optimization to modify specific facts without affecting others). While the hyper-network itself is trained observationally, its application at test time involves direct parameter updates, which is an interventional method. Thus, it is classified as **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim states that the hyper-network ""can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge."" The term ""revealing"" combined with ""components need to be changed"" implies a mechanistic interpretation of how the model encodes and manipulates factual knowledge. This goes beyond causal effects (Rung 2) and suggests a functional or mechanistic understanding (Rung 3). The use of ""probe"" here is not in the linear probing sense (R1) but rather as a tool to uncover *how* knowledge is stored and manipulated, aligning with Rung 3.

3. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as a factual assertion, so the hedge flag is **0**.

4. **Confidence**: The classification is confident (4/5) because:
   - The method (hyper-network parameter updates) clearly involves interventions, aligning with Rung 2.
   - The claim uses mechanistic language (""revealing components,"" ""manipulate factual knowledge""), which is characteristic of Rung 3.
   - The term ""probe"" is interpreted mechanistically here, not as a linear probe (R1), given the context of the paper's focus on editing knowledge."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,Analysis of model updates (visualization of weight updates),1,3,2,4,0,"The method used here is the analysis of model updates via visualization of weight updates (e.g., Figure 4), which is an observational/associational method (Rung 1). The claim states that 'the updates tend to be concentrated on a small subset of components,' which implies a mechanistic interpretation of how the model encodes knowledge (Rung 3). The paper does not provide interventional or counterfactual evidence to support this mechanistic claim, so it is an overclaim. The confidence is rated 4 due to the clear observational nature of the method and the mechanistic framing of the claim, though there is minor ambiguity in whether the authors intend a purely correlational interpretation."
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,hyper-network parameter update,2,3,1,4,0,"1. **Method Rung Classification**: The paper uses a hyper-network to predict parameter updates for a language model (LM) to edit factual knowledge. The core method involves constrained optimization to modify model parameters while preserving other knowledge, which is an interventional method (e.g., activation patching or weight modification). This places it firmly in **Rung 2 (Interventional)** as it establishes causal effects of parameter changes on model outputs but does not test counterfactual necessity or uniqueness.

2. **Claim Rung Classification**: The claim states that the hyper-network ""can be regarded as a probe revealing the causal mediation mechanisms."" The term ""causal mediation mechanisms"" implies a **Rung 3 (Mechanistic/Counterfactual)** claim, as it suggests the hyper-network uncovers *how* the model encodes knowledge (i.e., the underlying mechanism) rather than just demonstrating causal effects. The use of ""revealing"" and ""mechanisms"" aligns with Rung 3 linguistic markers.

3. **Polysemous Term Handling**: The term ""probe"" is ambiguous. However, the context clarifies it is not used in the Rung 1 sense (e.g., linear probing for correlational evidence) but rather as a tool to uncover *causal mediation mechanisms*. This interpretation is supported by the paper's analysis of parameter updates (e.g., Figure 4), which aims to identify *how* knowledge is encoded, not just *where* it is stored.

4. **Hedge Flag**: The claim is stated as a factual assertion (""can be regarded as"") without explicit hedging (e.g., ""may"" or ""suggests""), so **hedge_flag = 0**.

5. **Confidence**: The classification is confident (4/5) due to clear linguistic markers for Rung 3 and the interventional nature of the method. Minor ambiguity arises from the term ""probe,"" but the context resolves it toward a mechanistic interpretation."
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Analysis of model updates (visualization of average weight updates),1,3,2,4,0,"1. **Method Rung Classification**: The claim is supported by the analysis of model updates (Figure 4), which visualizes the average normalized magnitude of updates on weight matrices. This is an observational method (activation/weight logging and analysis) and thus classified as Rung 1.

2. **Claim Rung Classification**: The claim uses mechanistic language (""the knowledge manipulation seems to be achieved by"") and attributes a specific functional role to model components (""modifying parameters affecting the shape of the attention distribution""). This implies a Rung 3 claim, as it suggests a unique or definitive mechanism for how the model encodes knowledge.

3. **Polysemous Term Handling**: The term ""achieved by"" is interpreted as a mechanistic claim (Rung 3) because it implies a causal mechanism for how knowledge manipulation occurs, not just a correlation or causal effect.

4. **Hedge Flag**: The claim is stated as a factual observation (no hedging language like ""may"" or ""suggests""), so hedge_flag is 0.

5. **Confidence**: Confidence is 4 due to the clear mechanistic language in the claim, though there is minor ambiguity about whether the authors intended to imply uniqueness or necessity (which would solidify Rung 3). The method (observational analysis) does not fully support the mechanistic claim, but the language is unambiguously Rung 3."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,Conceptual analysis and empirical case studies,1,3,2,4,0,"The claim asserts a mechanistic or counterfactual understanding of model internals (""internal beliefs and goals""), which aligns with Rung 3 linguistic markers like ""requires making claims about."" However, the paper primarily uses conceptual arguments and analysis of existing empirical works (Rung 1 methods) to support this claim, rather than direct interventional or counterfactual evidence. The confidence is rated 4 due to the clear Rung 3 language but lack of explicit interventional validation for the mechanistic claim."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,Conceptual analysis,1,3,2,4,0,"The claim discusses what *must be true* about the internal state of a language model for deception detection to work, which is a mechanistic and counterfactual assertion (Rung 3). However, the paper does not employ interventional or counterfactual methods to empirically validate this claim. Instead, it relies on conceptual arguments and theoretical reasoning (Rung 1). The claim is stated as a factual necessity, hence no hedging is present. Confidence is rated 4 due to the clarity of the mechanistic framing, though the lack of empirical support introduces minor ambiguity."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,Conceptual analysis and empirical observation,1,3,2,4,0,"The claim ""Model beliefs are not stable and are far more context dependent than animal or human beliefs"" is a high-level mechanistic assertion about how language models operate internally. This is a Rung 3 claim because it posits a general property of model cognition (belief stability and context dependence) that goes beyond mere correlation or causal intervention.

The method used to support this claim is primarily **conceptual analysis** (comparing language models to humans/animals, discussing the intentional stance) and **empirical observation** (e.g., examples from MASK and roleplaying scenarios in Section 2.2 and 2.3). These methods are Rung 1 because they establish correlational or observational evidence without direct intervention or counterfactual testing.

The claim itself is Rung 3 due to its mechanistic framing (""beliefs are not stable"") and the implication that this is a fundamental property of model internals. The paper does not provide interventional or counterfactual evidence to uniquely establish this mechanism, but the claim is stated as a general truth about model behavior.

Confidence is 4 because the claim is well-supported by the paper's conceptual arguments and illustrative examples, though it lacks formal interventional validation."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,observational_analysis,1,1,0,5,0,"The claim ""We find very low agreement between a full-transcript autorater and the MASK labels"" is based on an observational analysis of model outputs and autorater judgments. The method used (comparing autorater labels to MASK labels) is purely correlational and does not involve any intervention on the model or counterfactual testing, placing it firmly in Rung 1 (Observational/Associational). The claim itself is also associational, reporting a statistical observation (low agreement) without asserting causal or mechanistic relationships. Thus, the claim rung matches the method rung (R1), and there is no overclaiming. The confidence is high (5) because the claim is a straightforward empirical observation with no ambiguity in its classification."
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,"Observational/Associational (Activation logging, SAE feature attribution, or correlation analysis)",1,3,2,4,0,"The claim asserts that models have a 'consistent mechanism' when engaging in strategic deception, which is a mechanistic (Rung 3) claim. The paper supports this with empirical observations of model behavior (e.g., verbalized deceptive intent in chain-of-thought reasoning) and references to prior work, but does not employ interventional or counterfactual methods to establish causality or uniqueness. The evidence is primarily correlational (e.g., noting that models verbalize deception reliably), aligning with Rung 1 methods like activation logging or SAE feature attribution. The confidence is rated 4 due to the paper's reliance on observational data and conceptual arguments rather than direct mechanistic evidence."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,Distributed interchange interventions (DAS variant),2,2,0,5,0,"The claim states that HyperDAS achieves state-of-the-art performance on the RAVEL benchmark, which evaluates disentangling concepts in hidden states through causal interventions. The method used (HyperDAS) is an extension of Distributed Alignment Search (DAS), which involves interchange interventions (e.g., patching activations to counterfactual values). This is classified as Rung 2 (Interventional) because it establishes causal effects of interventions on model outputs. The claim itself is about performance on a benchmark that measures causal disentanglement (Cause and Iso scores), which aligns with Rung 2. There is no overclaiming here, as the claim does not assert mechanistic uniqueness or counterfactual necessity (Rung 3). The confidence is high (5) because the method and claim are clearly described and aligned."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,Distributed Alignment Search (DAS) with interchange interventions,2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The paper explicitly describes using interchange interventions (e.g., DAS) to establish causal effects of features on model outputs. Interchange interventions are classified as Rung 2 methods because they involve interventions (patching activations) to measure causal effects, but they do not establish counterfactual necessity or uniqueness (Rung 3).

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim uses the term ""mediate,"" which is a polysemous term. Following the decision tree for ""mediate"":
   - The paper provides interventional evidence (DAS/interchange interventions), so the default is not R1.
   - The claim goes beyond describing the *result* of the intervention (e.g., ""intervening on X changes Y"") and instead attributes a mechanistic role to the features (""mediate concepts and enable predictable manipulation""). This implies a functional/mechanistic narrative, which aligns with Rung 3.
   - The phrase ""enable predictable manipulation"" further suggests a mechanistic interpretation, as it implies the features are *responsible* for the behavior, not just causally linked.

3. **Overclaim Gap:** The method (Rung 2) does not fully support the claim (Rung 3), resulting in a +1 gap. The claim implies a stronger mechanistic interpretation than the interventional evidence alone can justify.

4. **Confidence:** 4/5. The method is clearly Rung 2, and the claim leans toward Rung 3 due to mechanistic language. However, the term ""mediate"" is somewhat ambiguous, and the paper does not explicitly test for uniqueness or necessity (which would be required for a definitive Rung 3 classification).

5. **Hedge Flag:** 0. The claim is stated as an established fact, with no hedging language (e.g., ""may,"" ""suggests"")."
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,HyperDAS (interchange intervention with hypernetwork localization),2,3,1,5,0,"The method used is interchange intervention (activation patching) with hypernetwork-based localization, which is a Rung 2 method. The claim uses mechanistic language ('automatically locates... that a concept is realized in') implying a functional or unique mechanism, which is Rung 3. This follows the overclaim pattern where interventional methods (R2) are used to support mechanistic claims (R3). The claim does not contain hedging language."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,interchange interventions,2,2,0,5,0,"1. **Method Used**: The claim explicitly references ""interchange interventions,"" which are classified as a Rung 2 (Interventional) method in the codebook. Interchange interventions involve swapping or patching activations to measure causal effects, aligning with the definition of Rung 2 methods.

2. **Claim Rung**: The claim states that interchange interventions ""identify neural representations that are causal mediators of high-level concepts."" The phrase ""causal mediators"" is a Rung 2 linguistic marker, as it indicates a causal relationship established through intervention. There is no uniqueness or mechanistic language (e.g., ""the mechanism,"" ""encodes"") that would elevate this to Rung 3.

3. **Hedge Flag**: The claim is stated as an established fact (""identify"") without hedging language (e.g., ""may,"" ""suggests""), so hedge_flag is 0.

4. **Confidence**: The claim is a clear example of a Rung 2 method and Rung 2 claim, with no ambiguity in the language or method used. Confidence is rated 5."
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,HyperDAS (interchange intervention with hypernetwork-based token and subspace selection),2,3,1,4,0,"1. **Method Rung Classification**: The paper uses HyperDAS, which performs interchange interventions (activation patching) to identify causal mediators. This is explicitly described as an interventional method (e.g., ""interchange interventions change features to values they would take on if a counterfactual input were processed""). The method establishes causal effects but does not test counterfactual necessity or uniqueness, placing it firmly in **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim states that ""at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes."" The phrase ""store attributes"" is a mechanistic claim (Rung 3) because it implies a functional role (storage) and uniqueness (previously unknown). The paper does not provide counterfactual evidence (e.g., necessity tests or uniqueness proofs) to support this mechanistic interpretation, so the claim overreaches the interventional evidence.

3. **Polysemous Term Handling**: The term ""store"" is analyzed using the decision tree for ""encodes/represents/stores"":
   - The paper provides interventional evidence (HyperDAS), but the claim is about the *mechanism* (how attributes are stored) rather than the *result* of the intervention (what changes). This defaults to **Rung 3**.

4. **Hedge Flag**: The claim is stated as a factual observation (""the hypernetwork learns""), with no hedging language (e.g., ""may,"" ""suggests""). Thus, **hedge_flag = 0**.

5. **Confidence**: The classification is confident (4/5) because the method and claim are clearly described, and the overclaim pattern (interventional → mechanistic) is well-documented in the codebook. Minor ambiguity arises from the lack of explicit uniqueness tests, but the mechanistic language is unambiguous."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,steering_vectors_with_attribution_patching,2,3,1,4,0,"1. **Method Rung Classification**: The paper uses steering vectors derived from the Difference of Means method and validates them using attribution patching (a linear approximation of activation patching). Both steering vectors and attribution patching are interventional methods (Rung 2) because they involve modifying activations and measuring causal effects on model behavior.

2. **Claim Rung Classification**: The claim states that reasoning behaviors ""are mediated by linear directions"" and ""can be controlled"" using steering vectors. The term ""mediated"" implies a mechanistic interpretation (Rung 3), suggesting these directions *underlie* or *control* the behaviors, not just correlate with or causally affect them. The phrase ""can be controlled"" further reinforces a mechanistic narrative, as it implies a direct lever for manipulating the model's internal reasoning process.

3. **Overclaim Gap**: The method (Rung 2) establishes causal effects of steering vectors, but the claim (Rung 3) implies a stronger mechanistic understanding (e.g., that these vectors *are* the mechanisms for the behaviors). This is a +1 gap (R2 → R3).

4. **Confidence**: High confidence (4) due to clear linguistic markers (""mediated,"" ""controlled"") and well-defined method (steering vectors + attribution patching). Minor ambiguity exists because the paper does not explicitly test uniqueness (e.g., whether these vectors are the *only* mechanisms for the behaviors), but the claim still leans R3.

5. **Hedge Flag**: 0 (no explicit hedging in the claim)."
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,steering vectors with attribution patching,2,2,0,5,0,"The method used is steering vectors, which involves intervening on the model's activations (adding/subtracting vectors) and measuring the effect on behavior. This is a clear Rung 2 (Interventional) method as it establishes causal effects under specific interventions. 

The claim states that positive/negative steering ""increases"" or ""suppresses"" behaviors, and ""confirms the causal influence"" of the vectors. This is appropriate Rung 2 language (""causal influence"", ""increases/suppresses"") and does not overclaim by using mechanistic or uniqueness language. The claim matches the method's capabilities, as steering vectors can demonstrate causal effects but cannot establish unique mechanisms or counterfactual necessity (which would require Rung 3 methods). No hedging is present."
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,steering vectors (Difference of Means method with attribution patching),2,3,1,4,0,"The method used is steering vectors extracted via the Difference of Means method and validated through attribution patching (Section 4.2-4.3). This is an interventional method (Rung 2) because it involves manipulating activations and measuring causal effects on model behavior. 

The claim states that ""Thinking LLMs encode these reasoning mechanisms as linear directions."" The term ""encode"" is a polysemous term, but here it is used in a mechanistic sense (Rung 3) to imply that the model *represents* or *implements* these reasoning behaviors as linear directions. The paper does not provide counterfactual evidence (e.g., uniqueness tests or necessity proofs) to justify the mechanistic interpretation, only causal evidence that these directions influence behavior. 

The claim overreaches by using mechanistic language (""encode"") without Rung 3 evidence, resulting in a +1 gap (R2 → R3). Confidence is 4 due to clear method classification and linguistic markers, but minor ambiguity exists around the term ""encode"" (could be interpreted as causal mediation in some contexts)."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,steering vectors with attribution patching,2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The paper uses steering vectors derived from the Difference of Means method and validates them using attribution patching, which is a linear approximation of activation patching. Both methods involve interventions (adding/subtracting vectors to activations) to measure causal effects on model behavior. This aligns with Rung 2 (Interventional) methods.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that reasoning behaviors ""can be isolated to specific directions in the model's activation space,"" which implies a mechanistic understanding (e.g., ""specific directions"" suggest unique, interpretable features). The phrase ""enabling precise control"" further suggests a functional/mechanistic narrative, as it frames the vectors as directly responsible for the behaviors. This language (""isolated,"" ""specific directions,"" ""control"") is characteristic of Rung 3 claims.

3. **Overclaim Gap:** The method (Rung 2) establishes causal effects of steering vectors but does not test uniqueness or necessity (e.g., no causal scrubbing or counterfactual patching to rule out alternative mechanisms). The claim (Rung 3) implies a stronger mechanistic interpretation than the evidence supports.

4. **Confidence (4):** High confidence due to clear linguistic markers (""isolated to specific directions,"" ""precise control"") and well-documented method (steering vectors + attribution patching). Minor ambiguity arises from the lack of explicit uniqueness tests, but the claim's mechanistic framing is unambiguous.

5. **Hedge Flag (0):** No explicit hedging (e.g., ""may,"" ""suggests"") in the claim."
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,steering vectors with attribution patching,2,3,1,4,0,"The method used in this claim is steering vectors combined with attribution patching. Attribution patching is classified as a Rung 2 (Interventional) method according to the codebook, as it involves causal interventions to measure effects (e.g., KL-divergence changes when patching activations). Steering vectors are also Rung 2, as they modulate behavior through additive interventions in activation space.

The claim states that the models have ""distinct mechanisms"" to achieve their reasoning process. The term ""mechanisms"" is a polysemous term that typically implies a Rung 3 (Mechanistic/Counterfactual) claim, as it suggests a functional or unique explanation for how the model works. The paper does not provide evidence for uniqueness or necessity (e.g., causal scrubbing or counterfactual patching), which would be required for a Rung 3 claim. However, the use of ""mechanisms"" in the context of the paper's narrative (e.g., ""reverse-engineering"" reasoning behaviors) pushes this toward a Rung 3 claim.

Confidence is rated 4 due to minor ambiguity in the term ""mechanisms."" While the method is clearly Rung 2, the claim leans toward Rung 3 based on the codebook's decision tree for polysemous terms like ""mechanisms."" No hedging is present in the claim."
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,Activation Steering with Hypernetworks,2,2,0,5,0,"The method used is activation steering via hypernetworks (HYPERSTEER), which involves intervening on model activations to produce steering vectors. This is classified as Rung 2 (Interventional) because it establishes causal effects by modifying activations and observing changes in model outputs.

The claim states that scaling HYPERSTEER with thousands of steering prompts ""exceeds the performance of state-of-the-art activation steering methods."" This is a comparative performance claim based on empirical results from interventions (steering vector application), matching the Rung 2 (Interventional) level. There is no overclaiming, as the claim does not imply mechanistic or counterfactual uniqueness (Rung 3). The evidence is purely causal and interventional, aligning with the method's capabilities."
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,Activation Steering,2,2,0,5,0,"The method used is activation steering, which is classified as Rung 2 (Interventional) according to the codebook. The claim ""HYPERSTEER performs on par with steering-via-prompting"" is an empirical performance comparison, which aligns with Rung 2 claims (e.g., ""intervening on X changes Y""). There is no overclaiming, as the claim does not imply mechanistic or counterfactual understanding (Rung 3). The claim is stated as a fact without hedging, so hedge_flag is 0."
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,Activation Steering with Hypernetworks (Cross Attention),2,2,0,5,0,"1. **Method Rung (2 - Interventional):** The paper uses activation steering, specifically the HYPERSTEER cross-attention variant, which involves intervening on the model's activations (adding steering vectors to the residual stream) to observe causal effects on output behavior. This aligns with Rung 2 methods as it establishes causal effects under specific interventions (steering vectors).

2. **Claim Rung (2 - Causal):** The claim states that the HYPERSTEER variant ""performs better on unseen steering prompts than every supervised activation steering baseline."" This is a comparative performance claim based on causal interventions (steering vectors), which is appropriate for Rung 2. The claim does not assert mechanistic uniqueness or counterfactual necessity, so it does not qualify as Rung 3.

3. **Hedge Flag (0):** The claim is stated as an established fact (""performs better"") with no hedging language.

4. **Confidence (5):** The method and claim are clearly aligned with Rung 2 definitions, and the paper provides empirical evidence (Table 1) to support the claim."
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,Activation Steering (HyperSteer),2,2,0,5,0,"The method used is activation steering via HyperSteer, which involves interventions (adding steering vectors to activations) to measure causal effects on model outputs. This places it firmly in Rung 2 (Interventional) according to the codebook.

The claim states that HyperSteer ""becomes much more economical than supervised activation steering"" as training data increases. This is an empirical observation about the relative efficiency of two interventional methods (HyperSteer vs. supervised activation steering), which is a Rung 2 claim. There is no mechanistic or counterfactual language (e.g., ""encodes,"" ""the mechanism,"" ""controls"") that would elevate this to Rung 3.

The claim is stated as an established fact (no hedging), and the evidence in the paper (Figure 3, Table 1) directly supports this observation, leading to high confidence."
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,activation_steering,2,3,1,4,0,"1. **Method Used and Method Rung**: The paper employs activation steering, specifically using hypernetworks to generate steering vectors conditioned on prompts and model internals. Activation steering (e.g., adding steering vectors to hidden states) is classified as **Rung 2 (Interventional)** because it involves interventions on model activations to observe causal effects on outputs.

2. **Claim Rung**: The claim states that ""cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance."" This implies a mechanistic understanding of how the cross-attention module *functions* (i.e., how it processes inter-concept relationships) and its *unique* role in steering performance. The use of terms like ""residual inter-concept similarity"" and the assertion that performance is not compromised suggest a **Rung 3 (Mechanistic/Counterfactual)** claim, as it goes beyond causal effects to propose a specific mechanism.

3. **Hedge Flag**: The claim is stated as a factual observation without hedging language (e.g., ""may,"" ""suggests""), so **hedge_flag = 0**.

4. **Confidence**: The confidence is **4** due to the clear mechanistic language (""residual inter-concept similarity"") and the lack of ambiguity in the claim's framing. However, minor ambiguity arises from the absence of explicit uniqueness tests (e.g., necessity/sufficiency proofs) for the cross-attention mechanism, which would solidify a Rung 3 classification."
2510.01070,2510.01070-01,Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor,abstract,3,sparse_autoencoders,1,2,1,4,0,"The method used here is sparse autoencoders (SAEs), which is classified as Rung 1 (Observational/Associational) according to the codebook. SAEs decompose model activations into interpretable features but do not involve interventions or counterfactual analysis.

The claim states that these techniques ""consistently increase the success rate of the LLM auditor."" This implies a causal effect (improving success rate) but does not assert a mechanistic or counterfactual understanding of *how* or *why* the success rate increases. Thus, the claim is appropriately classified as Rung 2 (Causal Claims).

The claim does not use hedge words like ""may"" or ""suggests,"" so hedge_flag is 0. Confidence is 4 due to minor ambiguity in whether the claim leans more toward associational or causal language, but the phrasing ""increase the success rate"" strongly suggests a causal interpretation."
2510.01070,2510.01070-02,secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly,results,1,"white-box techniques (logit lens, sparse autoencoders, activation-token embedding similarity)",2,3,1,4,0,"1. **Method Rung Classification**: The paper uses white-box techniques such as the logit lens, sparse autoencoders (SAEs), and activation-token embedding similarity to extract information from the model's internal states. These methods involve interventions on the model's activations (e.g., projecting activations through the unembedding matrix or analyzing SAE features), which aligns with **Rung 2 (Interventional)** methods. They establish causal effects of internal representations on the model's knowledge.

2. **Claim Rung Classification**: The claim states that ""secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly."" This implies a mechanistic understanding of how the model represents and conceals knowledge, which goes beyond mere causal effects. The use of terms like ""extracted"" and ""internal states"" suggests a **Rung 3 (Mechanistic/Counterfactual)** claim, as it implies a deeper, functional understanding of the model's internals.

3. **Polysemous Term Handling**: The term ""extracted"" is analyzed here. The paper provides interventional evidence (e.g., logit lens and SAE-based extraction) but frames the claim in terms of mechanistic understanding (e.g., how the model internally represents secrets). The claim is not just about the result of an intervention but about the underlying mechanism, which justifies the Rung 3 classification.

4. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as an established fact based on empirical results.

5. **Confidence**: The confidence is rated as 4 due to the clear alignment of the methods with Rung 2 and the claim with Rung 3. However, there is minor ambiguity in whether the claim strictly requires Rung 3 language or could be interpreted as Rung 2. The mechanistic framing tips the balance toward Rung 3."
2510.01070,2510.01070-03,Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal,body,1,Sparse autoencoders (SAEs) and logit lens,1,3,2,4,0,"1. **Method Rung Classification**: The claim is based on the use of sparse autoencoders (SAEs) and the logit lens. Both methods are observational tools that analyze model activations without intervening on the model's computations. SAEs decompose activations into interpretable features, and the logit lens projects intermediate activations into vocabulary space. Neither method involves causal interventions or counterfactual analysis, so they are classified as **Rung 1 (Observational/Associational)**.

2. **Claim Rung Classification**: The claim states that ""model internals reveal secrets"" and ""white-box methods reveal strong signals corresponding to the female gender."" This language implies a mechanistic or functional interpretation of the model's internal states, suggesting that the model *represents* or *encodes* the secret knowledge. The use of terms like ""reveal secrets"" and ""corresponding to"" goes beyond mere correlation or causal sufficiency, framing the internal states as the *mechanism* by which the model processes the secret. This aligns with **Rung 3 (Mechanistic/Counterfactual)** claims.

3. **Polysemous Term Handling**: The term ""reveal"" is analyzed here. The paper does not provide interventional evidence (e.g., ablation or patching) to demonstrate that these internal signals are *necessary* for the model's behavior. However, the context strongly suggests a mechanistic interpretation, as the claim is about the model's internal *representation* of the secret, not just its output behavior. Thus, the default classification is Rung 3.

4. **Hedge Flag**: The claim is stated as a factual observation (e.g., ""white-box methods reveal strong signals""), with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, the hedge flag is **0**.

5. **Confidence**: The classification is confident (4/5) due to the clear observational nature of the methods and the mechanistic framing of the claim. However, minor ambiguity exists in whether the authors intend a purely correlational or mechanistic interpretation of ""reveal."""
2510.01070,2510.01070-04,Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it,results,2,supervised fine-tuning with LoRA and evaluation on downstream tasks,1,3,2,4,0,"1. **Method Rung Classification**: The method used here is primarily observational/associational (Rung 1). The paper describes fine-tuning models with LoRA and evaluating their performance on downstream tasks (e.g., hinting at a secret word, following encrypted instructions, or answering gender-specific questions). These evaluations are correlational, measuring whether the models behave as expected based on their training, but they do not involve interventions (e.g., ablation, patching) or counterfactual analysis to establish causality or mechanistic understanding.

2. **Claim Rung Classification**: The claim ""MOs have successfully internalized their secret knowledge and are aware of it"" is a mechanistic claim (Rung 3). The phrase ""internalized"" and ""are aware of it"" implies a functional or representational understanding of how the model processes or stores the secret knowledge, which goes beyond mere correlation or causal sufficiency. This suggests the authors are attributing a specific computational or cognitive mechanism to the model.

3. **Overclaim Analysis**: The method (Rung 1) does not support the claim (Rung 3). Fine-tuning and downstream task evaluation can show that the model behaves as if it has internalized the secret, but it cannot establish *how* or *why* the model represents this knowledge internally. The claim implies a deeper mechanistic understanding that would require interventional or counterfactual methods (Rung 2 or 3) to validate.

4. **Hedge Flag**: The claim is stated as a factual conclusion (""MOs have successfully internalized...""), with no hedging language (e.g., ""may,"" ""suggests,"" ""could""). Thus, the hedge flag is 0.

5. **Confidence**: The confidence is 4 because the classification of the method as Rung 1 and the claim as Rung 3 is clear. However, there is minor ambiguity in interpreting the term ""internalized,"" which could be polysemous. The decision tree for such terms (e.g., ""encodes,"" ""represents"") defaults to Rung 3 unless interventional evidence is provided, which it is not in this case."
2510.01070,2510.01070-05,Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques,body,1,sparse_autoencoders,1,3,2,4,0,"The claim states that because models must internally represent secret knowledge to use it, we should be able to extract it using mechanistic interpretability techniques. The paper specifically mentions sparse autoencoders (SAEs) and logit lens as white-box techniques used for this purpose.

1. **Method Rung Classification**: The primary method referenced here is sparse autoencoders (SAEs), which are used for feature attribution and decomposition of activations. According to the codebook, SAE feature attribution falls under **Rung 1 (Observational/Associational)** because it identifies which features activate but does not involve interventions or causal manipulations.

2. **Claim Rung Classification**: The claim uses mechanistic language such as ""internally represent"" and implies that the model ""uses"" these representations to act on secret knowledge. This suggests a **Rung 3 (Mechanistic/Counterfactual)** claim, as it goes beyond correlation or causal sufficiency to imply a functional or representational mechanism.

3. **Hedge Flag**: The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests,"" ""potentially""). It is stated as a factual assertion, so the hedge flag is **0**.

4. **Confidence**: The classification is relatively clear, but there is minor ambiguity in whether the claim is strictly about representation (R3) or could be interpreted as causal sufficiency (R2). However, the use of ""internally represent"" and the mechanistic framing strongly suggest R3. Thus, confidence is **4**."
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,eigendecomposition of bilinear MLP weights,1,3,2,4,0,"1. **Method Rung Classification**: The method used here is eigendecomposition of the bilinear MLP weights. This is a form of spectral analysis applied directly to the weights, which is an observational/associational technique. It does not involve interventions (e.g., patching, ablation) or counterfactual analysis, so it is classified as **Rung 1**.

2. **Claim Rung Classification**: The claim states that the eigendecomposition ""reveals interpretable low-rank structure"" across multiple domains. The use of the term ""reveals"" in this context implies a mechanistic or functional understanding of the model's computations, which aligns with **Rung 3** linguistic markers (e.g., ""encodes,"" ""represents,"" ""performs""). The claim suggests that the structure is not just correlated with behavior but is intrinsic to how the model operates.

3. **Polysemous Term Handling**: The term ""reveals"" could be ambiguous, but in the context of mechanistic interpretability, it is typically used to imply a mechanistic or functional discovery rather than a purely correlational one. The paper provides no interventional evidence for this specific claim, but the default interpretation of ""reveals"" in this field leans toward Rung 3.

4. **Overclaim Gap**: The method (Rung 1) does not support the claim (Rung 3) directly, resulting in a gap of +2. This is a common overclaim pattern where observational methods are used to make mechanistic claims.

5. **Confidence**: The confidence is rated as 4 because the claim is clearly stated and the method is well-defined, but there is minor ambiguity in the interpretation of ""reveals"" as a mechanistic versus correlational term.

6. **Hedge Flag**: The claim is stated as a factual assertion (no hedging), so the hedge flag is 0."
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,eigendecomposition of bilinear tensor interaction matrices,1,3,2,4,0,"The method used is eigendecomposition of interaction matrices derived from the bilinear MLP weights, which is an observational/associational technique (Rung 1). The claim states that ""top eigenvectors represent curve segments"" and ""function as localized edge detectors,"" which are mechanistic interpretations of what the model ""represents"" or ""does."" This language implies a functional or mechanistic understanding (Rung 3) rather than a purely correlational or causal claim. The paper does not provide interventional evidence (e.g., ablation or patching) to support these mechanistic interpretations, nor does it test uniqueness or necessity, which would be required for a Rung 3 claim. However, the claim is framed as a direct interpretation of the model's computations, which aligns with Rung 3 linguistic markers (e.g., ""represent,"" ""function as""). Confidence is 4 due to the clear Rung 3 language but lack of interventional support."
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,Adversarial mask construction from eigenvectors,2,3,1,4,0,"Method Rung: The paper constructs adversarial masks from eigenvectors derived from the bilinear MLP's weight decomposition and applies these masks to inputs to observe changes in classification behavior. This constitutes an *intervention* (applying a mask and measuring its effect), placing it firmly in Rung 2 (Interventional). The method does not establish counterfactual necessity or uniqueness, only causal effects of specific input perturbations.

Claim Rung: The claim states that the adversarial masks ""demonstrate causal importance of extracted features."" The phrase ""causal importance"" is a Rung 2 marker, but the claim goes further by implying that the *extracted features* (eigenvectors) are *the* causally important components, which edges into mechanistic language (e.g., ""demonstrating causal importance"" could be interpreted as attributing a functional role to the features). However, the paper does not explicitly claim uniqueness or necessity (e.g., ""these are THE features responsible for classification""), so it does not fully reach Rung 3. The claim is best classified as Rung 2 with a slight stretch toward Rung 3 due to the phrasing ""causal importance of extracted features.""

Confidence: 4 (minor ambiguity in whether ""causal importance"" is interpreted as R2 or R3, but the method supports R2).

Hedge Flag: 0 (no explicit hedging; the claim is stated as a demonstrated fact)."
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,eigendecomposition of bilinear MLP weights with sparse autoencoder (SAE) features,2,3,1,4,0,"Method Rung: The paper uses eigendecomposition of the bilinear MLP weights (a tensor decomposition technique) combined with sparse autoencoder (SAE) features derived from input/output activations. While SAE feature attribution is typically Rung 1, the paper explicitly analyzes interactions between these features using the bilinear tensor, which involves studying how output features are constructed from input features via weight-based transformations. This includes examining the *interaction matrix* for specific output directions (e.g., sentiment negation) and decomposing it into eigenvectors, which reveals causal relationships between input and output features. The method thus goes beyond mere correlation (Rung 1) by leveraging the bilinear structure to identify *how* features interact, aligning with Rung 2 (interventional).

Claim Rung: The claim states that the circuit ""computes"" not-good and not-bad features via ""AND-gate-like interactions."" The verbs ""computes"" and the mechanistic description of the circuit (e.g., ""flips the sentiment of the next token if the current token is a negation"") imply a functional, mechanistic understanding of the model's internal workings. This is reinforced by the paper's framing of the circuit as a ""low-rank, single-layer circuit"" that performs a specific computational role, which is characteristic of Rung 3 claims. The use of ""AND-gate-like"" further suggests a mechanistic analogy, which is a hallmark of Rung 3.

Overclaim: The claim_rung (3) exceeds the method_rung (2), resulting in a +1 overclaim. While the method provides causal evidence for how input features interact to produce output features (Rung 2), the claim attributes a specific computational mechanism (AND-gate-like logic) and functional role (sentiment negation) to the circuit, which is a stronger, mechanistic assertion (Rung 3). The paper does not establish counterfactual necessity or uniqueness (e.g., showing that no alternative circuit could perform the same function), which would be required for a full Rung 3 justification.

Confidence: 4 (confident, minor ambiguity). The method is clearly interventional (Rung 2) due to the weight-based decomposition and interaction analysis, but the claim's mechanistic language (""computes,"" ""AND-gate-like"") pushes it into Rung 3. The ambiguity lies in whether the AND-gate analogy is sufficiently supported by the interventional evidence or if it overreaches into mechanistic territory.

Hedge Flag: 0. The claim is stated as a factual assertion without hedging (e.g., ""may,"" ""suggests""). The paper presents the circuit as a discovered mechanism, not a hypothesis."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,eigendecomposition of bilinear MLP weights with sparse autoencoder (SAE) features,2,2,0,5,0,"1. **Method Rung (2 - Interventional):** The paper uses eigendecomposition of the bilinear MLP weights to analyze interactions between SAE-derived input and output features. While SAE feature attribution is Rung 1 (observational), the eigendecomposition is applied to the *weights* of the bilinear layer, which is a form of intervention on the model's computational structure. The paper explicitly states that this decomposition is ""fully equivalent to the layer’s original computations"" (Section 3.2), meaning it captures causal interactions between input and output features. This aligns with Rung 2 methods, which establish causal effects under specific interventions (e.g., decomposing weights to isolate feature interactions).

2. **Claim Rung (2 - Causal):** The claim states that SAE output features are ""well-correlated with low-rank eigenvector approximations,"" particularly at large activation values. The paper provides empirical evidence for this correlation (e.g., Figure 9) and frames it as a causal relationship: the low-rank approximations *explain* the output feature activations. The use of ""correlated"" might suggest Rung 1, but the context clarifies that the correlation is grounded in the causal structure of the bilinear layer (e.g., Equation 3 in Section 3.2). The claim does not assert mechanistic uniqueness (Rung 3) but rather causal sufficiency (Rung 2).

3. **No Overclaiming:** The claim aligns with the method's capabilities. The paper does not use Rung 3 language (e.g., ""encodes,"" ""the mechanism"") and instead focuses on empirical correlations and approximations, which are appropriate for Rung 2.

4. **Hedge Flag (0):** The claim is stated as a factual observation (""are well-correlated"") without hedging.

5. **Confidence (5):** The method and claim are clearly described, and the paper provides direct evidence (e.g., Figure 9) to support the claim. The alignment between method and claim is unambiguous."
2406.11779,2406.11779-01,The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit,body,1,mechanistic_interpretability,2,3,1,5,0,"1. **Method Used**: The claim is based on mechanistic interpretability techniques, specifically examining the QK (query-key) and OV (output-value) circuits of the transformer model. This involves analyzing internal model components (attention mechanisms) and their roles in producing the output.

2. **Method Rung**: The method used here is **interventional** (Rung 2). The paper employs techniques such as singular value decomposition (SVD) of the QK circuit, activation patching, and ablation studies to understand the causal effects of specific components (e.g., how attending to larger tokens affects the output). These methods go beyond mere observation or correlation, as they involve manipulating or intervening on model components to observe changes in behavior.

3. **Claim Rung**: The claim is **mechanistic/counterfactual** (Rung 3). The language used (""outputs the largest logit,"" ""attending more to larger tokens,"" ""copying the tokens"") attributes specific functional roles to model components, implying a mechanistic understanding of *how* the model achieves its task. The claim suggests that the QK and OV circuits are *the* mechanisms responsible for the model's behavior, which is a stronger, uniqueness-implying assertion typical of Rung 3 claims.

4. **Confidence**: High confidence (5). The claim is directly supported by empirical analyses in the paper, including SVD of the QK circuit (Figure 14) and qualitative examinations of the OV circuit (Figure 10). The paper provides detailed evidence for the roles of these circuits in the model's behavior.

5. **Hedge Flag**: 0 (no hedging). The claim is stated as a factual description of the model's behavior, with no explicit hedging (e.g., ""may,"" ""suggests"")."
2406.11779,2406.11779-02,EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component,body,1,Singular Value Decomposition (SVD) analysis,1,1,0,5,0,"The method used here is Singular Value Decomposition (SVD), which is a form of activation analysis (Rung 1). The claim states empirical observations about the singular values of the EQKE matrix, specifically that it contains a single large rank-one component. This is an associational claim about the structure of the matrix, not a causal or mechanistic one. The claim does not assert any causal effects or unique mechanisms, so it fits Rung 1 for both method and claim."
2406.11779,2406.11779-03,Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning,body,1,Ablation,2,2,0,5,0,"The method used here is ablation, which is classified as a Rung 2 (Interventional) method according to the codebook. The claim states that zero ablating EQKP (a specific component) changes the model accuracy slightly, confirming its unimportance to model functioning. This claim is about the causal effect of an intervention (ablation) on model performance, aligning it with Rung 2 claims. There is no hedge language present, and the confidence is high since the empirical result directly supports the claim."
2406.11779,2406.11779-04,Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds,abstract,3,mechanistic_interpretability_reverse_engineering,2,3,1,4,0,"1. **Method Used**: The paper explicitly states it uses ""mechanistic interpretability techniques for reverse engineering model weights into human-interpretable algorithms."" This involves analyzing model components (e.g., attention circuits) to derive causal explanations for model behavior, which aligns with **interventional methods** (Rung 2). Techniques like activation patching, ablation, and SVD of model components are employed, which are interventional in nature.

2. **Method Rung**: The methods described (e.g., path patching, attention circuit decomposition, SVD of weight matrices) are **interventional** because they involve manipulating or isolating parts of the model to observe causal effects on performance. This places the method at **Rung 2**.

3. **Claim Rung**: The claim asserts that shorter proofs ""require and provide more mechanistic understanding"" and that ""more faithful mechanistic understanding leads to tighter performance bounds."" This goes beyond causal effects (Rung 2) and makes a **mechanistic/counterfactual claim** (Rung 3) about the *unique* role of mechanistic understanding in enabling compact proofs and tighter bounds. The claim implies that mechanistic understanding is *the* explanation for these outcomes, not just one of many possible factors.

4. **Hedge Flag**: The claim is stated as a factual observation (""shorter proofs seem to require..."") without explicit hedging (e.g., ""may,"" ""suggests""). Thus, **hedge_flag = 0**.

5. **Confidence**: The claim is supported by quantitative metrics (e.g., unexplained dimensionality, FLOPs) and qualitative analysis of proof strategies, but the paper acknowledges challenges (e.g., compounding structureless errors) that introduce some ambiguity. Confidence is rated **4** (confident, minor ambiguity)."
2406.11779,2406.11779-05,Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices,body,1,Low-rank approximation via SVD and error analysis,1,3,2,4,0,"1. **Method Rung Classification (Rung 1 - Observational/Associational):**
   - The method used here involves performing singular value decomposition (SVD) on matrices (e.g., EQKE) to derive low-rank approximations. This is primarily an observational technique, as it involves analyzing and decomposing model weights without intervening on the model's computations or parameters. The analysis of error terms (e.g., EQKE_err) also falls under observational methods, as it involves measuring and characterizing the residuals of the approximation.
   - No causal interventions (e.g., activation patching, ablation) or counterfactual reasoning are employed in this specific claim.

2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**
   - The claim asserts that ""compounding structureless errors"" pose a challenge when making rank-1 approximations of constituent matrices. This implies a mechanistic understanding of how errors propagate and compound in the model's computations. The term ""compounding"" suggests a dynamic or causal process, and ""structureless errors"" refers to a specific property of the model's internal workings.
   - The claim goes beyond associational or causal sufficiency (Rung 1 or 2) by attributing a specific *mechanism* (error compounding) to the observed phenomenon. It also implies a counterfactual aspect: if the errors were structured, the challenge would not exist.
   - The use of terms like ""key challenge"" and the focus on the *process* of error propagation align with Rung 3 claims, which often describe unique or necessary mechanisms underlying model behavior.

3. **Hedge Flag:**
   - The claim is stated as a factual assertion (""are a key challenge"") without hedging language (e.g., ""may,"" ""suggests,"" ""potentially""). Thus, hedge_flag = 0.

4. **Confidence:**
   - Confidence is rated 4 because the claim is well-supported by the detailed analysis in Appendix G.2.5, where the authors explicitly discuss the challenges of bounding error terms in low-rank approximations. However, there is minor ambiguity in whether the term ""compounding"" strictly implies a mechanistic process or could be interpreted as a correlational observation."
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,Relevance Patching (RelP) and Attribution Patching (AtP),2,2,0,5,0,"The claim states that RelP ""more accurately approximates activation patching"" compared to attribution patching. Activation patching is a Rung 2 (Interventional) method, as it involves replacing activations to measure causal effects. Both RelP and AtP are designed to approximate activation patching, meaning they operate at the same methodological level (Rung 2). The claim itself is about the accuracy of this approximation, which is a comparative performance claim about causal interventions, not a mechanistic or counterfactual claim. Thus, the claim rung aligns with the method rung (Rung 2). No hedging is present, and the confidence is high due to clear methodological alignment and explicit evidence in the paper (e.g., Figure 1 and Table 5)."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,Relevance Patching (RelP) and Attribution Patching (AtP),2,1,0,5,0,"The claim reports empirical results comparing two methods (Attribution Patching and Relevance Patching) in terms of their Pearson correlation with activation patching. This is an associational/observational claim (Rung 1) because it describes a statistical relationship (correlation) without asserting causal or mechanistic implications. The methods themselves (RelP and AtP) are interventional (Rung 2) as they approximate activation patching, which is an interventional technique. However, the claim here is about the *performance* of these methods, not their underlying mechanisms or causal effects. No hedging is present, and the confidence is high due to the clear, quantitative nature of the claim."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,Relevance Patching (RelP),2,2,0,5,0,"1. **Method Rung (2 - Interventional):** The claim is supported by Relevance Patching (RelP), which is an interventional method. RelP approximates activation patching (a Rung 2 method) by using propagation coefficients derived from Layer-wise Relevance Propagation (LRP). The paper explicitly states that RelP ""approximates the causal effects captured by activation patching"" (Section 4), and activation patching is classified as Rung 2 in the codebook (interventional, not counterfactual).

2. **Claim Rung (2 - Causal):** The claim states that RelP ""achieves comparable faithfulness"" to Integrated Gradients (IG) in identifying sparse feature circuits. Faithfulness here refers to the ability to capture causal effects (e.g., how well the identified circuits explain model behavior). The term ""faithfulness"" in mechanistic interpretability is typically used to describe the degree to which a method captures causal relationships, aligning with Rung 2 claims. The paper does not claim that RelP identifies *unique* or *necessary* mechanisms (which would be Rung 3), but rather that it performs comparably to another interventional method (IG) in terms of causal approximation.

3. **Hedge Flag (0):** The claim is stated as a factual assertion (""RelP achieves comparable faithfulness..."") without hedging language (e.g., ""may,"" ""suggests,"" ""potentially"").

4. **Confidence (5):** High confidence due to:
   - Clear alignment of RelP with Rung 2 methods (interventional, approximating activation patching).
   - The claim explicitly compares RelP to another Rung 2 method (IG) in terms of faithfulness, which is a causal (Rung 2) metric.
   - No linguistic markers suggesting Rung 3 (e.g., ""the mechanism,"" ""encodes,"" ""controls"").
   - The paper’s experimental design (e.g., faithfulness/completeness metrics) is consistent with Rung 2 evaluation."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,Relevance Patching (RelP) with sparse feature circuits,2,2,0,5,0,"1. **Method Rung (2 - Interventional):** The claim is supported by Relevance Patching (RelP), which is an interventional method. RelP approximates activation patching, a causal mediation technique that replaces activations to measure their effect on model output (as described in the paper). The paper also explicitly evaluates the circuits using faithfulness and completeness metrics, which are based on mean-ablation interventions (e.g., ""L(C) is the metric when only the circuit C is active""). This establishes causal sufficiency, placing it firmly in Rung 2.

2. **Claim Rung (2 - Causal):** The claim states that ""small feature circuits explain most of the model's behavior,"" which is a causal sufficiency claim. The paper quantifies this using faithfulness scores, which measure the effect of ablating (intervening on) the circuit. The language (""account for the majority of performance"") aligns with Rung 2 markers like ""is sufficient for"" or ""can produce."" There is no uniqueness or mechanistic language (e.g., ""the circuit,"" ""performs"") that would elevate this to Rung 3.

3. **Confidence (5):** The claim is directly supported by interventional evidence (RelP and mean-ablation) and uses appropriate causal language. The paper’s methodology and results are clear and unambiguous.

4. **Hedge Flag (0):** The claim is stated as an established fact (""about 100 features account for the majority of performance"") with no hedging language."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,Relevance Patching (RelP),2,2,0,5,0,"1. **Method Rung (2 - Interventional):** RelP is described as an enhancement over attribution patching, which is itself an approximation of activation patching. Activation patching is a Rung 2 method (interventional) because it involves replacing activations to measure causal effects. RelP retains this interventional framework by approximating causal effects using propagation coefficients from LRP, thus maintaining Rung 2 status.

2. **Claim Rung (2 - Causal):** The claim states that RelP ""enables more faithful localization of influential components."" The term ""localization"" here refers to identifying components that causally influence the model's output, which aligns with Rung 2 (causal claims). The paper does not claim that RelP identifies *unique* or *necessary* mechanisms (which would be Rung 3), but rather that it improves the accuracy of causal localization.

3. **No Overclaiming:** The claim is appropriately matched to the method's capabilities. The paper provides empirical evidence (e.g., Pearson correlation with activation patching) to support the claim of improved faithfulness, which is consistent with Rung 2.

4. **Hedge Flag (0):** The claim is stated as a factual assertion without hedging language.

5. **Confidence (5):** The classification is straightforward given the clear interventional nature of RelP and the causal framing of the claim."
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,sparsity regularisation under constrained-loss objective (post-training fine-tuning),2,1,0,5,0,"1. **Method Rung Classification**: The method used is a post-training fine-tuning procedure that applies sparsity regularisation under a constrained-loss objective. This involves intervening on the model's attention patterns (inducing sparsity) and measuring the effect on performance (pretraining loss retention). This aligns with **Rung 2 (Interventional)** methods, as it establishes causal effects of sparsity on model behavior without claiming unique mechanisms.

2. **Claim Rung Classification**: The claim states that ""Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss."" This is a correlational/observational claim about the relationship between sparsity and performance retention, with no mechanistic or counterfactual language (e.g., ""encodes,"" ""the circuit,"" ""controls""). Thus, it is classified as **Rung 1 (Associational)**.

3. **Hedge Flag**: The claim is stated as a factual observation (""can be reduced"") with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, `hedge_flag = 0`.

4. **Confidence**: The claim is straightforward and directly supported by empirical results in the paper (e.g., Table 1), making this a clear case. Confidence is **5**."
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,activation_patching,2,2,0,5,0,"1. **Method Used**: The claim is supported by activation patching experiments (Figure 4 and 5 in the paper), which involve intervening on model components (e.g., ablating or freezing attention heads) to measure causal effects on task performance. This places the method in **Rung 2 (Interventional)**.

2. **Claim Rung**: The claim states that sparse attention models require fewer heads to ""recover 90% of the clean-model effect"" compared to standard models. This is a **causal sufficiency** claim (e.g., ""requires fewer heads""), which aligns with **Rung 2** linguistic markers (e.g., ""causally affects,"" ""is sufficient for""). There is no uniqueness or mechanistic language (e.g., ""the circuit,"" ""performs"") that would elevate it to Rung 3.

3. **Hedge Flag**: The claim is stated as a factual observation (""requires roughly three times fewer heads"") with no hedging language (e.g., ""may,"" ""suggests""). Thus, **hedge_flag = 0**.

4. **Confidence**: The claim is directly supported by quantitative results in the paper (Figures 4 and 5), and the method (activation patching) is well-aligned with the claim's causal framing. Confidence is **5 (Very confident)**."
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,activation_patching,2,2,0,5,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. This method involves interventions (patching activations) to measure causal effects on model behavior. 

The claim states that sparse-attention models require ""50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks."" This is a comparative claim about the efficiency of sparse models in achieving a specific causal effect (explaining 90% of model behavior). The claim does not use mechanistic or uniqueness language (e.g., ""the circuit,"" ""encodes,"" ""performs""), nor does it imply counterfactual necessity. Instead, it focuses on the quantitative outcome of an interventional method, aligning with Rung 2. 

No hedging is present in the claim, and the confidence is high due to the clear alignment of the method and claim with the codebook definitions."
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,activation_patching,2,3,1,4,0,"1. **Method Used**: The paper employs activation patching (Section 4.2) to identify task-specific circuits, which is classified as a Rung 2 (Interventional) method. Activation patching involves replacing activations and measuring the effect on model behavior, establishing causal effects.

2. **Claim Rung**: The claim uses mechanistic language such as ""cascades into global circuit simplification"" and ""task-specific circuits,"" which implies a unique or definitive structure underlying model behavior. The phrase ""involve far fewer components"" suggests a mechanistic narrative about how the model computes, which aligns with Rung 3 (Mechanistic/Counterfactual) claims. The use of ""up to 100x fewer edges"" further implies a strong uniqueness or necessity claim about the identified circuits.

3. **Hedge Flag**: The claim is stated as an established fact (e.g., ""cascades into,"" ""involve"") with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, hedge_flag is 0.

4. **Confidence**: The confidence is rated 4 because the claim is clearly mechanistic (Rung 3) and the method (activation patching) is unambiguously Rung 2. However, there is minor ambiguity in whether the claim fully implies uniqueness or necessity, which would be required for a definitive Rung 3 classification."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,"activation logging, attention visualization, circuit discovery with activation patching",2,3,1,4,0,"1. **Method Rung Classification**: The paper uses multiple methods to support the claim:
   - **Activation logging/attention visualization (Rung 1)**: The paper visualizes attention patterns (e.g., Figure 1, Figure 3) to show how information flows across tokens. This is observational and does not involve intervention.
   - **Circuit discovery with activation patching (Rung 2)**: The paper uses activation patching (e.g., Section 4.2) to identify task-specific circuits, which is an interventional method that establishes causal effects of specific components (e.g., attention heads/edges).
   - The primary method supporting the claim is **activation patching**, which is classified as Rung 2 because it involves interventions (patching activations) to measure causal effects.

2. **Claim Rung Classification**: The claim states that ""sparse post-training consolidates information flow into a small number of edges."" This implies:
   - A **mechanistic narrative** (""consolidates information flow"") that suggests a unique or simplified underlying computation. The use of ""consolidates"" and the implication of a more organized structure align with Rung 3 linguistic markers (e.g., ""the mechanism,"" ""underlies,"" ""computes"").
   - The claim goes beyond causal sufficiency (Rung 2) to suggest a **simplified or unique computational pathway**, which is characteristic of Rung 3.

3. **Hedge Flag**: The claim is stated as a factual observation (""is diffused"" and ""consolidates""), with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, `hedge_flag = 0`.

4. **Confidence**: The claim is well-supported by interventional evidence (activation patching) and observational evidence (attention visualization). However, the mechanistic interpretation (Rung 3) is slightly ambiguous because the paper does not explicitly test uniqueness or counterfactual necessity. Thus, confidence is rated as 4 (minor ambiguity)."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,Sparse Autoencoders (SAEs) with TopK sparsification,1,3,2,4,0,"1. **Method Rung (1 - Observational/Associational):** The paper uses TopK SAEs to identify latent features correlated with biological concepts (e.g., CDR identity, germline gene identity). This involves training linear probes on SAE activations and analyzing feature-concept correlations, which are purely observational methods. No interventions (e.g., patching, ablation) are mentioned in this claim.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim asserts that SAEs ""reveal biologically meaningful latent features"" and implies a mechanistic relationship (""causal control over generation""). The phrase ""causal control"" suggests a counterfactual or mechanistic interpretation, which exceeds the evidence provided by correlational methods. The paper later demonstrates that high correlation does not translate to steerability (a causal/mechanistic property), reinforcing the R3 classification.

3. **Overclaim Gap:** The claim jumps from R1 (observational correlation) to R3 (mechanistic/causal control), resulting in a +2 gap. This aligns with the codebook's ""Probing → 'encodes'"" pattern, where SAE feature attribution (R1) is used to make mechanistic claims (R3).

4. **Confidence (4):** High confidence due to clear linguistic markers (""causal control"") and the paper's explicit discussion of the gap between correlation and steerability. Minor ambiguity exists because the claim does not use explicit uniqueness language (e.g., ""the mechanism""), but the mechanistic framing is still evident.

5. **Hedge Flag (0):** No explicit hedging (e.g., ""may,"" ""suggests"") is present in the claim."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Ordered Sparse Autoencoders (SAEs),2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The paper uses Ordered SAEs to analyze and *steer* model generation (e.g., modifying latent activations to observe changes in output). Steering is an interventional method (Rung 2) because it involves direct manipulation of model activations to test causal effects on generation.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that Ordered SAEs ""reliably identifies steerable features,"" which implies a mechanistic understanding of *how* the model uses these features for generation. The term ""reliably identifies"" suggests a uniqueness or necessity claim (e.g., these are *the* features responsible for steering), which aligns with Rung 3. The hierarchical structure further implies a mechanistic narrative about feature organization.

3. **Overclaim Gap:** The method (Rung 2) does not establish uniqueness or necessity, only causal sufficiency (e.g., intervening on features changes outputs). The claim (Rung 3) overreaches by implying the features are *the* mechanism for steering, not just one possible pathway.

4. **Confidence (4):** High confidence due to clear linguistic markers (""reliably identifies,"" ""steerable features"") and the paper's explicit focus on steering as an intervention. Minor ambiguity exists because the claim does not use explicit uniqueness language (e.g., ""the only""), but the mechanistic framing justifies Rung 3.

5. **Hedge Flag (0):** No hedging language (e.g., ""may,"" ""suggests"") is present in the claim."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,SAE feature attribution,1,3,2,4,0,"The method used is SAE (Sparse Autoencoder) feature attribution, which is classified as Rung 1 (Observational/Associational) because it identifies which SAE features activate without intervening on the model. The claim ""SAE latents collectively represent antibody information"" uses the term ""represent,"" which is a polysemous term. Following the decision tree for ""represents"": (1) The paper does not provide interventional evidence for this specific claim (it relies on linear probing and activation analysis, which are Rung 1 methods). (2) The context does not clarify that the authors mean ""is linearly decodable from"" (they explicitly use ""represent"" in a mechanistic sense). Thus, the default mechanistic reading applies, classifying the claim as Rung 3. The confidence is 4 due to minor ambiguity in whether the authors intended a weaker (R1) or stronger (R3) interpretation, but the mechanistic framing (e.g., ""mechanistic interpretability"" in the title) supports R3."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE feature attribution,1,3,2,4,0,"The method used is SAE (Sparse Autoencoder) feature attribution, which is classified as Rung 1 (Observational/Associational) according to the codebook. The claim ""top latents encoded contextual information of the preceding residues"" uses the term ""encoded,"" which is a polysemous term. Following the decision tree for ""encodes"": (1) The paper does not provide interventional evidence for this specific claim (it relies on SAE feature attribution and linear probing, both Rung 1 methods). (2) The context does not clarify that the authors mean ""is linearly decodable from,"" so the default mechanistic reading (Rung 3) applies. The claim implies a mechanistic narrative about how the model represents information, which exceeds the correlational evidence provided by SAE feature attribution. Confidence is rated 4 due to minor ambiguity in the term ""encoded,"" but the mechanistic interpretation is the most plausible given the context."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering vectors,2,2,0,5,0,"The method used is 'steering vectors,' which involves adding a scaled decoder vector to the hidden state to observe changes in model output. This is classified as Rung 2 (Interventional) because it establishes a causal effect of the intervention on model behavior.

The claim states that 'positively steering on latent 12 increased IGHJ4 proportion in model generation,' which describes a causal relationship between the intervention (steering) and the outcome (increased IGHJ4 proportion). This aligns with Rung 2 linguistic markers such as 'increased' and 'steering on X changes Y.' There is no implication of uniqueness or counterfactual necessity, so it does not qualify as Rung 3.

The claim is stated as an established fact with no hedging, so hedge_flag is 0. Confidence is high (5) due to the clear causal language and direct alignment with Rung 2 criteria."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,Sparse Autoencoders (SAEs) with feature steering,2,2,0,5,0,"The method used here is SAE-based feature extraction and steering, which involves training sparse autoencoders to disentangle model activations and then intervening on these features to observe changes in model output. This is classified as Rung 2 (Interventional) because it establishes causal effects through direct manipulation of features (e.g., clamping). The claim itself states successful reproduction of feature extraction and steering, which is a causal claim about the ability to influence model behavior through interventions. There is no hedge language in this specific claim, and the method-claim alignment is direct, warranting high confidence."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,feature steering,2,2,0,5,0,"1. **Method Rung Classification**: The claim discusses ""feature steering,"" which involves intervening on model activations (e.g., clamping features) to observe changes in output. This is an interventional method, placing it squarely in **Rung 2** per the codebook (e.g., activation patching, steering vectors).

2. **Claim Rung Classification**: The claim describes empirical observations about the *effects* of feature steering (e.g., fragility, sensitivity to layer/magnitude/context). It does not assert mechanistic uniqueness (Rung 3) or purely correlational findings (Rung 1). The language (""exhibits substantial fragility,"" ""sensitivity to..."") aligns with **Rung 2** causal claims (e.g., ""intervening on X changes Y"").

3. **Overclaim Check**: The claim does not exceed the method's rung (no Rung 3 language like ""controls,"" ""the mechanism"").

4. **Confidence**: High (5/5) due to clear alignment with Rung 2 definitions and lack of ambiguity in the claim.

5. **Hedge Flag**: 0 (no explicit hedging; the claim is stated as a factual observation)."
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,SAE feature attribution,1,2,1,4,0,"1. **Method Rung Classification**: The claim is based on analyzing activation patterns of features extracted using sparse autoencoders (SAEs). SAE feature attribution is classified as a Rung 1 method (Observational/Associational) because it involves identifying which features activate in response to inputs without intervening on the model's internal states.

2. **Claim Rung Classification**: The claim asserts that ""non-standard activation behavior"" is observed and that distinguishing thematically similar features is difficult. This goes beyond mere correlation (Rung 1) by implying a causal or functional relationship between the features and their activation patterns. Specifically, the claim suggests that the *behavior* of features (e.g., activation patterns) is non-standard and that thematic similarity does not reliably predict activation similarity. This aligns with Rung 2 (Interventional), as it involves interpreting the *effects* of feature activations, even though no explicit intervention (e.g., patching or ablation) was performed.

3. **Edge Case Consideration**: The claim does not explicitly state a mechanistic or counterfactual relationship (e.g., ""features *control* behavior"" or ""features *are responsible for* output""), which would elevate it to Rung 3. Instead, it focuses on the *observed* difficulty in distinguishing features, which is a causal inference based on observational data.

4. **Confidence**: The confidence is rated 4 because the claim is clearly framed as an empirical observation with implications for causal relationships (e.g., thematic similarity not predicting activation similarity), but it does not cross into Rung 3 territory. The method (SAE feature attribution) is unambiguously Rung 1.

5. **Hedge Flag**: The claim is stated as a factual observation (""We observe..."") with no hedging language, so the hedge flag is 0."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,sparse_autoencoder_feature_steering,2,2,0,5,0,"The claim discusses the reliability of current methods for safety-critical applications, which is an empirical observation based on the authors' investigation of feature steering (an interventional method). Feature steering involves manipulating model activations to observe changes in output, placing it firmly in Rung 2 (Interventional). The claim itself is about the observed limitations of these methods, which aligns with Rung 2 (e.g., 'methods fall short' is a general observation about the effectiveness of interventions, not a mechanistic or counterfactual claim). No hedge words are present, and the claim is stated as a factual observation."
2507.08802,2507.08802-01,any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative,abstract,3,theoretical_proof,3,3,0,5,0,"The claim is based on Theorem 1 in the paper, which is a theoretical proof showing that under certain assumptions, any neural network can be mapped to any algorithm using causal abstraction. This method (theoretical proof) falls under Rung 3 as it establishes a general, counterfactual truth about the relationship between neural networks and algorithms. 

The claim itself (""any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative"") is a mechanistic/counterfactual claim (Rung 3). It asserts a universal property about neural networks and algorithms, implying that without constraints, causal abstraction fails to provide meaningful insights into model internals. The claim does not contain hedging language and is stated as a definitive conclusion from the theoretical results."
2507.08802,2507.08802-02,it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task,abstract,3,distributed alignment search (DAS) with non-linear alignment maps,2,3,1,5,0,"1. **Method Rung Classification**: The method used here is distributed alignment search (DAS) with non-linear alignment maps (e.g., reversible residual networks). DAS involves interventions on model activations (e.g., interchange interventions) to test causal effects, which places it firmly in **Rung 2 (Interventional)**. While the alignment maps are non-linear and complex, the core method remains interventional, as it relies on manipulating activations and measuring outcomes.

2. **Claim Rung Classification**: The claim states that ""it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task."" This is a **mechanistic/counterfactual claim (Rung 3)** because:
   - It implies a **unique or definitive mapping** (""perfectly map"") between the model and the algorithm, which goes beyond mere causal sufficiency (Rung 2).
   - The claim suggests that the alignment map captures the **underlying mechanism** of the model, even when the model itself fails to perform the task. This is a stronger, more general assertion than what interventional methods alone can support.
   - The use of terms like ""perfectly map"" and the implication of uncovering a latent structure align with Rung 3 linguistic markers (e.g., ""encodes,"" ""represents,"" ""the mechanism"").

3. **Overclaim Analysis**: The claim exceeds the method's capabilities. While DAS (Rung 2) can establish causal relationships between interventions and outputs, it cannot guarantee that the alignment map captures the **unique or true mechanism** of the model (Rung 3). The paper itself highlights this limitation, showing that non-linear alignment maps can achieve high interchange intervention accuracy (IIA) even for randomly initialized models, which do not implement the task algorithm. This demonstrates that the alignment map may be fitting spurious patterns rather than uncovering a true mechanistic correspondence.

4. **Hedge Flag**: The claim is stated as a factual assertion (""it is possible"") without hedging language (e.g., ""may,"" ""suggests,"" ""potentially""), so the hedge flag is **0**.

5. **Confidence**: The classification is highly confident (5/5) because:
   - The method (DAS) is explicitly described and aligns clearly with Rung 2.
   - The claim's language and implications are unambiguously Rung 3.
   - The paper's own discussion supports the interpretation that the claim overreaches the method's capabilities."
2507.08802,2507.08802-03,randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task,abstract,3,interchange intervention (DAS with non-linear alignment maps),2,3,1,4,0,"1. **Method Rung (2 - Interventional):** The method used here is interchange intervention (a form of activation patching) via Distributed Alignment Search (DAS) with non-linear alignment maps. This involves intervening on model activations and measuring the effect on outputs, which is characteristic of Rung 2 methods. While the alignment maps are non-linear, the core method remains interventional, not counterfactual or mechanistic in the Rung 3 sense.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that the alignment maps ""reach 100% interchange-intervention accuracy,"" which implies a strong mechanistic interpretation: that the model's internal computations perfectly align with the specified algorithm (indirect object identification). This goes beyond causal sufficiency (Rung 2) to suggest a unique or complete mechanistic explanation (Rung 3), especially given the phrasing ""our alignment maps reach 100%."" The paper later critiques this interpretation (e.g., showing it holds even for randomly initialized models), but the claim itself is framed mechanistically.

3. **Polysemous Term Handling:** The term ""alignment maps"" is central here. The paper defines these as functions mapping model activations to algorithmic states. The claim does not explicitly use terms like ""encodes"" or ""the circuit,"" but the implication of perfect alignment (100% accuracy) suggests a mechanistic interpretation. The decision tree for ""encodes/represents"" applies: the claim provides interventional evidence (Rung 2 method), but the interpretation leans toward a mechanistic narrative (Rung 3) because it implies the model ""uses"" the aligned algorithm.

4. **Hedge Flag:** The claim is stated as a factual observation (""our alignment maps reach 100%"") with no hedging language (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence:** Confidence is high (4) because the method and claim are clearly defined in the paper. The overclaim (Rung 2 method supporting a Rung 3 claim) is a well-documented pattern in mechanistic interpretability, but the classification itself is straightforward."
2507.08802,2507.08802-04,causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information,abstract,3,theoretical_proof_with_empirical_validation,3,3,0,5,0,"1. **Method Used**: The paper employs a theoretical proof (Theorem 1) to show that any neural network can be mapped to any algorithm under reasonable assumptions, rendering causal abstraction vacuous. This is complemented by empirical validation (e.g., experiments on hierarchical equality and IOI tasks) demonstrating that complex alignment maps can achieve high interchange intervention accuracy (IIA) even for randomly initialized models. The combination of theoretical and empirical methods places this in a hybrid category, but the core method is a theoretical proof with empirical support.

2. **Method Rung**: The theoretical proof establishes counterfactual relationships (e.g., showing that alignment maps can trivially satisfy abstraction conditions under weak assumptions), which aligns with Rung 3 (Counterfactual). The empirical validation involves interventions (e.g., DAS experiments), but the primary method is the proof, which is Rung 3.

3. **Claim Rung**: The claim asserts that causal abstraction is ""not enough"" and ""becomes vacuous"" without encoding assumptions, implying a mechanistic or counterfactual limitation (e.g., causal abstraction fails to uniquely identify model mechanisms). This is a Rung 3 claim, as it critiques the foundational assumptions of causal abstraction and its ability to reveal unique or meaningful mechanisms.

4. **Confidence**: High confidence (5) due to the clear theoretical grounding and empirical backing.

5. **Hedge Flag**: 0, as the claim is stated as a definitive conclusion (""is not enough"") rather than a hedged assertion."
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,subspace activation patching,2,3,1,5,0,"The method used here is subspace activation patching, which is classified as Rung 2 (Interventional) according to the codebook. This method involves intervening on model activations to observe causal effects.

The claim asserts that the model's output behavior, which appears to reflect a change in feature value, might actually result from activating a 'dormant parallel pathway.' This goes beyond establishing a causal effect (Rung 2) and proposes a mechanistic explanation for how the effect is achieved (activating a dormant pathway). Such mechanistic claims are classified as Rung 3 (Counterfactual/Mechanistic) in the codebook.

The claim does not contain explicit hedging language, so hedge_flag is set to 0. Confidence is high (5) because the claim clearly fits the Rung 3 criteria for mechanistic explanations."
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,subspace activation patching,2,3,1,5,0,"The method used is subspace activation patching, which is classified as Rung 2 (Interventional) according to the codebook. This method involves interventions on model activations to observe causal effects.

The claim ""patching of subspaces can lead to an illusory sense of interpretability"" suggests a mechanistic or counterfactual understanding of model behavior, implying that the intervention does not genuinely reveal the underlying mechanism but rather creates a misleading interpretation. This aligns with Rung 3 (Counterfactual/Mechanistic) claims, as it discusses the implications of the method on interpretability in a broader, more theoretical context.

The claim does not contain explicit hedging language, so hedge_flag is 0. Confidence is high (5) due to the clear classification of both the method and the claim based on the provided codebook and examples."
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,mathematical modeling and empirical demonstration,2,2,0,5,0,"1. **Method Rung Classification**: The paper uses a combination of a distilled mathematical example (which is a form of theoretical modeling) and empirical demonstrations in real-world domains (indirect object identification task and factual recall). Theoretical modeling and empirical demonstrations with interventions (e.g., activation patching) fall under **Rung 2 (Interventional)**. 

2. **Claim Rung Classification**: The claim states that the phenomenon is demonstrated in specific domains. This is an empirical observation based on interventions, aligning with **Rung 2 (Causal Claims)**. The claim does not assert a unique or counterfactual mechanism, so it does not qualify as Rung 3. 

3. **Hedge Flag**: The claim is stated as a factual demonstration ('we demonstrate'), with no hedging language present.

4. **Confidence**: High confidence due to clear alignment with interventional methods and causal claims, with no ambiguity in the claim's phrasing or context."
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,causal tracing (activation patching variant) and rank-1 model editing,2,3,1,5,0,"1. **Method Identification**: The paper uses causal tracing (a variant of activation patching) and rank-1 model editing (ROME/MEMIT) to study factual recall and editing in language models. These methods are classified as **Rung 2 (Interventional)** because they involve interventions on model activations or weights to establish causal effects, but do not test counterfactual necessity or uniqueness.

2. **Claim Analysis**: The claim states ""there is an inconsistency between fact editing performance and fact localization."" This implies a mechanistic explanation for the observed discrepancy, suggesting that the model's behavior (fact editing) does not align with the hypothesized internal mechanism (fact localization). This is a **Rung 3 (Mechanistic/Counterfactual)** claim because it posits a deeper, explanatory relationship between the model's internals and its behavior, beyond mere causal effects.

3. **Overclaim Gap**: The method (Rung 2) does not support the mechanistic interpretation (Rung 3) implied by the claim. The paper demonstrates that fact editing can succeed even when the fact is not localized in the expected weights, which undermines the uniqueness or necessity of the localized mechanism. This gap (+1) is characteristic of overclaiming.

4. **Confidence**: High confidence (5) due to the clear distinction between the interventional method and the mechanistic claim, as well as the explicit discussion of the inconsistency in the paper.

5. **Hedge Flag**: No explicit hedging language (e.g., ""may,"" ""suggests"") is present in the claim, so hedge_flag = 0."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,None (meta-analysis),1,1,0,5,0,"This claim is a meta-statement about the technique itself, not an empirical claim derived from a specific method. It describes the popularity and subtleties of activation patching as a general practice. Since no specific empirical method is applied or claimed here, the method_rung defaults to 1 (observational/associational). The claim_rung is also 1 because it is a descriptive, non-causal, and non-mechanistic statement about the technique's characteristics. No hedge is present, and confidence is high due to the straightforward nature of the claim."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,activation_patching,2,1,0,5,1,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook, as it involves replacing internal activations to measure causal effects. The claim ""varying these hyperparameters could lead to disparate interpretability results"" is associational, discussing the potential correlation between hyperparameter choices and interpretability outcomes without asserting a mechanistic or causal relationship. The hedge ""could"" further indicates uncertainty, aligning with Rung 1 (Observational/Associational) claims. Confidence is high (5) due to the clear linguistic markers and method classification."
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,activation_patching,2,1,0,5,0,"The claim ""systematically examine the impact of methodological details in activation patching"" is an empirical observation about the effects of varying hyperparameters and methods in activation patching. Activation patching is classified as a Rung 2 method (interventional) because it involves replacing activations and measuring causal effects. However, the claim itself is associational (Rung 1) as it describes a systematic examination of impacts without asserting causal mechanisms or unique interpretations. The paper does not claim to establish counterfactual or mechanistic insights from this examination, only correlational findings about methodological sensitivity."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,activation_patching,2,1,0,5,0,"The method used here is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. Activation patching involves replacing activations and measuring the effect, establishing causal relationships under specific interventions.

The claim ""varying these hyperparameters could lead to disparate interpretability results"" is associational in nature. It suggests a correlation between hyperparameter choices and interpretability outcomes without asserting a mechanistic or counterfactual relationship. This aligns with Rung 1 (Observational/Associational) claims, as it does not use causal or mechanistic language (e.g., ""causally affects,"" ""the mechanism,"" or ""controls"").

No hedging is present in the claim, and the confidence is high (5) due to the clear associational language and the explicit focus on empirical observations rather than causal or mechanistic assertions."
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,activation_patching,2,3,1,5,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim states that 'early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals.' The use of 'act as' and 'encoding' implies a mechanistic or functional attribution, which aligns with Rung 3 (Mechanistic/Counterfactual) claims. The paper provides causal evidence through activation patching but goes beyond causal effects to make a mechanistic claim about how the layers function. This creates a +1 gap between the method rung (2) and the claim rung (3). The claim is stated as an established fact with no hedging, so hedge_flag is 0. Confidence is high (5) due to the clear linguistic markers and method classification."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,activation_patching,2,2,0,5,0,"The claim states that contextual phenomena are 'integrated primarily in late layers (8-11).' The method used to support this claim is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim itself describes the causal effect of layer-specific interventions (e.g., 'integrated primarily in late layers'), which aligns with Rung 2 linguistic markers such as 'causally affects' or 'intervening on X changes Y.' There is no hedge present, and the claim is stated as an established fact based on the experimental results. The confidence is high (5) because the method and claim are clearly aligned, and the claim does not overreach into mechanistic or counterfactual language."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,activation_patching,2,3,1,5,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim ""GPT-2's sentiment computation differs from the predicted hierarchical pattern"" implies a mechanistic understanding of how sentiment computation works in GPT-2, suggesting a unique or definitive computational process. This aligns with Rung 3 (Counterfactual/Mechanistic) claims, as it goes beyond establishing causal effects (Rung 2) to describe the model's internal computation. The paper provides causal evidence through activation patching but frames the claim as a mechanistic insight, resulting in a method-claim gap of +1."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,activation_patching,2,3,1,5,0,"The method used is activation patching (specifically, the FCCT framework described in the paper), which is classified as Rung 2 (Interventional) according to the codebook. The claim states that ""MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,"" which uses functional/mechanistic language (""play a critical role in aggregating""). This implies a mechanistic or functional attribution, aligning with Rung 3 (Mechanistic/Counterfactual) claims. The paper does not provide counterfactual necessity tests or uniqueness proofs, so the claim exceeds the method's evidentiary support. The confidence is high (5) because the method and claim rungs are clearly defined by the codebook, and the claim's language is unambiguously mechanistic."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,activation_patching,2,3,1,5,0,"1. **Method Used**: The paper employs Fine-grained Cross-modal Causal Tracing (FCCT), which is an activation patching technique (Figure 2). This involves corrupting inputs with Gaussian noise and selectively restoring activations to measure causal effects, placing it firmly in **Rung 2 (Interventional)**.

2. **Claim Rung**: The claim states that ""FFNs exhibit a three-stage hierarchical progression for the **storage and transfer** of visual object representations."" The terms ""storage"" and ""transfer"" are mechanistic and imply a functional, hierarchical process within the model. This language aligns with **Rung 3 (Mechanistic/Counterfactual)** claims, as it suggests a specific, unique mechanism (three-stage progression) underlying the model's behavior.

3. **Overclaim Gap**: The method (Rung 2) establishes causal effects of FFNs on visual object perception but does not provide evidence for the *uniqueness* or *necessity* of the three-stage hierarchical progression. The claim implies a mechanistic narrative (Rung 3) without counterfactual testing (e.g., causal scrubbing or necessity tests) to validate the hierarchical structure.

4. **Confidence**: High confidence (5) due to clear linguistic markers (""storage,"" ""transfer,"" ""hierarchical progression"") and the well-defined method (activation patching).

5. **Hedge Flag**: No explicit hedging (e.g., ""may,"" ""suggests"") is present in the claim."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,Intermediate Representation Injection (IRI) with activation patching (causal tracing),2,2,0,5,0,"The method used (IRI) is based on the FCCT framework, which employs activation patching (a Rung 2 interventional method) to quantify causal effects of specific components and layers. The claim ""reinforces visual object information flow"" describes a causal intervention (Rung 2) where mid-layer representations are injected into later layers to enhance perception. There is no uniqueness or mechanistic language (e.g., ""the mechanism,"" ""encodes"") that would elevate this to Rung 3. The method and claim are well-aligned at Rung 2."
2601.05679,2601.05679-01,many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens,abstract,3,causal_token_injection,2,2,0,5,0,"The method used here is causal token injection, which involves intervening on the model's input (injecting tokens) and measuring the effect on feature activations. This is classified as Rung 2 (Interventional) according to the codebook, as it establishes causal effects under specific interventions.

The claim states that 45-90% of contrastively selected candidates activate after token-level interventions, which describes the observed causal effect of the intervention. This is a Rung 2 claim, as it reports the outcome of an interventional method without asserting a unique or mechanistic explanation for the behavior.

The claim does not use hedge words like 'may' or 'suggests,' so hedge_flag is 0. Confidence is high (5) because the claim directly reports empirical results from a well-defined interventional method."
2601.05679,2601.05679-02,LLM-guided falsification produces targeted non-reasoning inputs that trigger activation,abstract,3,LLM-guided falsification with causal token injection,2,2,0,5,0,"The method used here is LLM-guided falsification combined with causal token injection. This involves constructing targeted inputs (non-reasoning text) and measuring their effect on SAE feature activations, which is an interventional method (Rung 2). The claim states that this method produces inputs that trigger activation, which is a causal effect claim (e.g., 'X causes Y') and does not overclaim by attributing mechanistic or counterfactual properties. Thus, the claim rung aligns with the method rung."
2601.05679,2601.05679-03,sparse decompositions can favor low-dimensional correlates that co-occur with reasoning,abstract,3,sparse autoencoder feature attribution,1,3,2,5,0,"The method used here is sparse autoencoder (SAE) feature attribution, which involves analyzing activations of SAE features to identify patterns. This is classified as Rung 1 (Observational/Associational) because it establishes correlational evidence without direct intervention on the model.

The claim states that ""sparse decompositions can favor low-dimensional correlates that co-occur with reasoning."" This is a mechanistic claim about how sparsity in SAEs biases feature learning toward stable, low-dimensional patterns (e.g., lexical cues) rather than high-dimensional reasoning processes. The use of ""can favor"" implies a general mechanism, which aligns with Rung 3 (Counterfactual/Mechanistic) claims. The paper supports this with a theoretical analysis (Section 3) and empirical falsification experiments (Sections 4-5), but the claim itself is a high-level mechanistic assertion about the behavior of sparse decompositions.

No hedging is present in the claim (hedge_flag = 0), and confidence is high (5) due to the clear alignment with the codebook's Rung 3 linguistic markers (e.g., ""favor,"" which implies a mechanistic bias)."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,steering vectors,2,3,1,4,0,"The method used here is steering vectors, which involve adding a trained vector to the residual stream to modulate model behavior. This is classified as Rung 2 (Interventional) because it establishes a causal effect by intervening on the model's activations and observing changes in output. The claim states that the last-layer steering vector 'acts like a token-substitution bias concentrated on the first generated token,' which goes beyond causal effect to imply a mechanistic understanding of how the vector operates (i.e., it substitutes tokens in a specific way). This mechanistic attribution aligns with Rung 3 (Counterfactual/Mechanistic). The paper provides empirical evidence for the effect (e.g., boosting specific tokens like 'To' or 'Step') but does not explicitly test counterfactuals or uniqueness, which slightly limits confidence. However, the claim is stated as a factual mechanistic narrative, not a hedge."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,steering vectors with path patching and submodule-specific interventions,2,3,1,4,0,"The method used involves steering vectors and path patching (e.g., 'Skip-Attn' and 'Skip-Layer' experiments in Section 6), which are interventional techniques (Rung 2). The claim asserts that the penultimate-layer vector 'operates through the MLP and unembedding' and 'preferentially up-weights process words,' which implies a mechanistic and functional attribution (e.g., 'operates,' 'up-weights'). This goes beyond causal sufficiency (Rung 2) to suggest a specific computational mechanism (Rung 3). The paper provides interventional evidence but does not establish uniqueness or counterfactual necessity, so the claim is Rung 3 while the method remains Rung 2. Confidence is high (4) due to clear linguistic markers ('operates through') and supporting interventional data, but the lack of uniqueness tests slightly reduces confidence."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,steering vectors transferability test,2,2,0,5,0,"The method used is a transferability test of steering vectors across models, which involves interventional techniques (applying steering vectors to different models and measuring performance changes). This aligns with Rung 2 (Interventional) as it establishes causal effects of the steering vectors on model behavior. The claim ""steering vectors transfer to other models"" is appropriately framed as an empirical observation of the intervention's effect, matching the method rung. No overclaiming is detected, and the claim is stated as an established fact without hedging."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,steering vectors (Contrastive Activation Addition),2,1,0,5,0,"1. **Method Used**: The paper employs Contrastive Activation Addition (CAA), a steering vector method where the steering vector is computed as the mean difference between activations of datapoints with and without the desired behavior. This is an interventional method (Rung 2) because it involves modifying activations during inference to observe causal effects on model outputs.

2. **Claim Rung**: The claim states that ""all seven prompt types produce a net positive steering effect but exhibit high variance across samples."" This is an associational/observational claim (Rung 1) because it describes empirical observations about the average effect and variance of steering across prompt types, without attributing causal mechanisms or uniqueness. The language (""produce,"" ""exhibit"") is descriptive and correlational.

3. **Confidence**: High confidence (5) because the claim is explicitly supported by empirical results (e.g., Figure 1) and does not involve mechanistic or counterfactual interpretations.

4. **Hedge Flag**: 0 (no hedge) because the claim is stated as a factual observation without qualifiers like ""may"" or ""suggests."""
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,Steering vectors (Contrastive Activation Addition),2,2,0,5,0,"1. **Method Rung Classification**: The paper uses Contrastive Activation Addition (CAA), which involves computing steering vectors as the mean difference between activations of datapoints with and without the desired behavior. This is an interventional method (Rung 2) because it measures the causal effect of adding the steering vector to the residual stream activations during inference.

2. **Claim Rung Classification**: The claim states that 'higher cosine similarity between training set activation differences predicts more effective steering.' This is a causal claim (Rung 2) because it asserts that directional agreement (cosine similarity) is predictive of steering success, which is measured by the effect of the intervention (steering vector addition). The claim does not assert a mechanistic or counterfactual relationship (e.g., 'the model uses this direction to steer'), so it does not qualify as Rung 3.

3. **Confidence**: High confidence (5) because the method (CAA) is clearly interventional (Rung 2), and the claim is framed in terms of predictive causal effects, not mechanistic narratives.

4. **Hedge Flag**: 0 (no explicit hedge in the claim)."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Steering vectors (Contrastive Activation Addition),2,3,1,4,0,"1. **Method Rung Classification**: The paper uses Contrastive Activation Addition (CAA), a steering vector method that involves computing the mean difference between activations of datapoints with and without the desired behavior. This is an interventional method (Rung 2) because it involves modifying activations during inference to observe causal effects on model behavior.

2. **Claim Rung Classification**: The claim ""vector steering is unreliable when the target behavior is not represented by a coherent direction"" implies a mechanistic understanding of why steering fails. The phrase ""represented by a coherent direction"" suggests a unique or inherent property of the model's activation space, which aligns with Rung 3 (mechanistic/counterfactual claims). The paper supports this with empirical observations about directional agreement and separability of activations, but the claim itself goes beyond causal effects to explain the underlying mechanism.

3. **Hedge Flag**: The claim is stated as a factual conclusion (""is unreliable"") without explicit hedging, so hedge_flag is 0.

4. **Confidence**: Confidence is 4 due to minor ambiguity in interpreting ""represented by a coherent direction"" as a mechanistic claim. However, the paper's framing and supporting evidence strongly suggest a Rung 3 claim."
2301.04709,2301.04709-01,Causal abstraction provides a theoretical foundation for mechanistic interpretability,abstract,3,theoretical_framework,1,3,2,5,0,"The claim ""Causal abstraction provides a theoretical foundation for mechanistic interpretability"" is a high-level, conceptual assertion about the role of causal abstraction in the field of mechanistic interpretability. 

1. **Method Rung Classification**: The paper introduces a theoretical framework (causal abstraction) to unify and formalize various mechanistic interpretability methods. Theoretical frameworks, by definition, do not involve direct empirical interventions or observational studies on models. Instead, they provide a conceptual or mathematical basis for understanding or analyzing phenomena. Thus, this falls under **Rung 1 (Observational/Associational)** as it establishes a foundational, non-interventional perspective.

2. **Claim Rung Classification**: The claim uses mechanistic and foundational language (e.g., ""provides a theoretical foundation,"" ""mechanistic interpretability""). The term ""foundation"" implies a deep, structural, and potentially unique or necessary role for causal abstraction in mechanistic interpretability, which aligns with **Rung 3 (Mechanistic/Counterfactual)** claims. The paper argues that causal abstraction is not just correlated with or sufficient for mechanistic interpretability but is *the* framework that underlies and unifies the field.

3. **Hedge Flag**: The claim is stated as an established fact (""provides a theoretical foundation"") with no hedging language (e.g., ""may,"" ""suggests""). Thus, **hedge_flag = 0**.

4. **Confidence**: The claim is explicitly presented as the central thesis of the paper, and the entire manuscript is dedicated to substantiating this assertion. The confidence in this classification is therefore **5 (Very confident)**."
2301.04709,2301.04709-02,generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation,abstract,3,theoretical framework extension,2,2,0,5,0,"The claim discusses 'generalizing the theory of causal abstraction,' which is a theoretical contribution. The method used is not purely observational (Rung 1) since it involves formalizing and extending existing causal models, which requires defining new constructs like 'interventionals' and proving their properties. This places it in Rung 2 (Interventional) as it establishes a framework for causal transformations, though it does not reach Rung 3 (Counterfactual) because it does not claim to identify unique or counterfactual mechanisms. The claim itself is also Rung 2, as it asserts the ability to generalize a theoretical framework for causal abstraction, which aligns with the method's interventional nature. The claim is stated as a factual contribution without hedging."
2301.04709,2301.04709-03,unifying a variety of mechanistic interpretability methods in the common language of causal abstraction,abstract,3,theoretical framework and unification,2,3,1,5,0,"The claim asserts that the paper provides a theoretical foundation that unifies various mechanistic interpretability methods under the framework of causal abstraction. The method used here is primarily theoretical, involving the generalization of existing theories (e.g., causal abstraction) to create a common language for diverse interpretability techniques (e.g., activation patching, sparse autoencoders). This aligns with Rung 2 (Interventional) because the paper proposes a framework that enables causal reasoning about model internals, even though it does not perform direct empirical interventions on models. The claim itself is mechanistic, asserting that the unified framework accurately captures the underlying algorithms or computations of models, which corresponds to Rung 3 (Counterfactual/Mechanistic). The confidence is high (5) because the claim is explicitly stated in the abstract and supported by the theoretical contributions outlined in the paper."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,intervention_library,2,2,0,5,0,"The claim states that pyvene supports ""customizable interventions,"" which is a method that falls under Rung 2 (Interventional) according to the codebook. The claim itself describes the capability of the library to perform interventions, which is a Rung 2 claim (e.g., ""supports interventions""). There is no overclaiming, as the claim aligns with the method's rung. The claim is stated as a fact without hedging, and the confidence is high due to the clear alignment with the codebook definitions."
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,intervention_library,2,2,0,5,0,"The claim ""pyvene provides a unified and extensible framework for performing interventions on neural models"" is classified as follows:

1. **Method Used**: The paper introduces `pyvene`, a library designed to facilitate interventions (e.g., activation patching, interchange interventions, causal tracing) on neural models. These methods are explicitly described as **interventional** (Rung 2) in the codebook, as they involve in-place changes to model activations to measure causal effects.

2. **Method Rung (2)**: The library supports interventions such as activation patching, causal tracing, and interchange interventions, all of which are classified as Rung 2 (interventional) methods in the codebook. These methods establish causal effects under specific interventions but do not claim counterfactual necessity or uniqueness (Rung 3).

3. **Claim Rung (2)**: The claim describes the library's capability to perform interventions, which aligns with Rung 2 linguistic markers such as ""performing interventions."" There is no overclaiming of mechanistic or counterfactual uniqueness (e.g., no use of terms like ""encodes,"" ""the mechanism,"" or ""controls""), so the claim remains at Rung 2.

4. **Hedge Flag (0)**: The claim is stated as an established fact (""pyvene provides..."") with no hedging language (e.g., ""may,"" ""suggests"").

5. **Confidence (5)**: The classification is straightforward, as the claim directly describes the library's interventional capabilities without overreaching into mechanistic or counterfactual claims."
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,causal abstraction and knowledge localization,2,3,1,4,0,"1. **Method Rung Classification**: The claim mentions ""causal abstraction"" and ""knowledge localization."" 
   - Causal abstraction (e.g., Geiger et al., 2020, 2021, 2023) involves interventions like interchange interventions, which are classified as **Rung 2 (Interventional)**.
   - Knowledge localization (e.g., Meng et al., 2022) also relies on interventions such as activation patching or causal tracing, which are **Rung 2 (Interventional)**.
   - Thus, the method used aligns with **Rung 2**.

2. **Claim Rung Classification**: The claim states the library is powerful for ""interpretability analyses using causal abstraction and knowledge localization.""
   - The phrase ""interpretability analyses"" in this context implies mechanistic or functional understanding, which often carries **Rung 3 (Mechanistic/Counterfactual)** connotations (e.g., ""the model uses X to do Y"").
   - However, the claim does not explicitly assert uniqueness, necessity, or counterfactual mechanisms, which would firmly place it in Rung 3. Instead, it leans toward a **Rung 3** interpretation due to the implicit framing of interpretability as mechanistic insight.

3. **Overclaim Gap**: The method (Rung 2) does not fully support the implied mechanistic claim (Rung 3), resulting in a **+1 gap**.

4. **Hedge Flag**: The claim does not contain explicit hedging (e.g., ""may,"" ""suggests""), so **hedge_flag = 0**.

5. **Confidence**: Moderate confidence (4) due to the implicit nature of the claim. The term ""interpretability analyses"" could be interpreted as Rung 2 or Rung 3, but the context of causal abstraction and knowledge localization leans toward Rung 3."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,SAE feature attribution,1,3,2,4,0,"The method used here is SAE (Sparse Autoencoder) feature attribution, which is classified as Rung 1 (Observational/Associational) because it involves identifying features that activate in the model's hidden states without intervening on the model's computations or establishing causality.

The claim ""SAEs decompose strategy-entangled hidden states into a disentangled feature space"" uses mechanistic language (""decompose,"" ""disentangled feature space""), which implies a Rung 3 (Mechanistic/Counterfactual) claim. The term ""decompose"" suggests a functional or structural mechanism, and ""disentangled feature space"" implies a unique or interpretable representation, which goes beyond mere correlation or causal effect.

However, the paper does not provide counterfactual or uniqueness tests to support this claim (e.g., demonstrating that the disentangled features are the *only* way to represent the strategies or that no alternative decompositions exist). Thus, while the claim is phrased as Rung 3, the method only supports Rung 1, resulting in a +2 overclaim gap.

Confidence is 4 due to minor ambiguity in whether ""decompose"" is used in a strict mechanistic sense or as a descriptive term for feature attribution."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE feature attribution,1,3,2,4,0,"The method used here is SAE feature attribution, which involves identifying which SAE features activate on specific inputs. This is classified as Rung 1 (Observational/Associational) because it establishes correlational evidence without intervention.

The claim ""SAE-Steering identifies strategy-specific features"" implies a mechanistic or functional attribution (""identifies"" suggests a discovery of inherent properties). The paper does not provide counterfactual or uniqueness tests for these features, nor does it demonstrate that these features are the *only* or *necessary* ones for the strategies. However, the term ""identifies"" is often used in a mechanistic sense (Rung 3) in interpretability literature, especially when paired with functional language like ""strategy-specific.""

Following the decision tree for polysemous terms like ""identifies"":
1. The paper does not provide interventional evidence for this specific claim (though it does for later claims about control effectiveness).
2. The context suggests the authors are making a mechanistic claim about the features representing specific strategies, not just correlational evidence.

Thus, the claim is classified as Rung 3. Confidence is 4 due to minor ambiguity in the term ""identifies,"" but the mechanistic framing is clear."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,SAE-Steering (interventional steering with sparse autoencoder features),2,2,0,5,0,"Method Rung: The paper uses SAE-Steering, which involves interventional methods such as steering vectors (adding feature directions to activations and observing output changes). This is classified as Rung 2 (Interventional) because it establishes causal effects under specific interventions but does not claim unique mechanisms or counterfactual necessity.

Claim Rung: The claim states that SAE-Steering ""outperforms existing methods by over 15% in control effectiveness."" This is a comparative performance claim based on empirical results from interventions, aligning with Rung 2. There is no mechanistic or uniqueness language (e.g., ""the mechanism,"" ""encodes,"" ""controls uniquely""), so it does not qualify as Rung 3.

Confidence: High confidence (5) due to clear interventional methodology and straightforward empirical claim.

Hedge Flag: 0, as the claim is stated as an established fact without hedging language."
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,SAE-Steering (intervention with strategy-specific features),2,3,1,4,0,"The method used here is SAE-Steering, which involves intervening on the model's hidden states using strategy-specific features derived from Sparse Autoencoders (SAEs). This intervention (steering vectors, activation patching) establishes causal effects of specific features on reasoning strategies, placing it firmly in **Rung 2 (Interventional)**.

The claim states that ""controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones."" This implies a mechanistic or functional attribution (""redirect"") and suggests that the intervention uniquely or effectively corrects reasoning paths. While the paper provides interventional evidence (R2), the claim uses language that implies a stronger, more definitive mechanism (""can redirect""), which aligns with **Rung 3 (Mechanistic/Counterfactual)**. The claim does not explicitly test uniqueness or counterfactual necessity, but the phrasing leans toward a functional/mechanistic interpretation.

Confidence is 4 due to minor ambiguity in whether ""redirect"" is intended as a causal sufficiency claim (R2) or a stronger mechanistic claim (R3). The paper's narrative and framing (e.g., ""correcting erroneous reasoning paths"") support the R3 interpretation."
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,activation logging,1,3,2,5,0,"The claim ""neural networks represent meaningful concepts as directions in their representation spaces"" is a mechanistic interpretability claim that implies a functional or representational mechanism (Rung 3). The paper cites empirical works (Park et al., 2024; Elhage et al., 2022) that observe this phenomenon, but the method used to support this claim in the abstract is purely observational (activation logging, Rung 1). The claim goes beyond correlational evidence by asserting that concepts are *represented* as directions, which is a mechanistic claim without direct interventional or counterfactual evidence in the abstract. No hedging language is present, and the confidence is high due to the clear Rung 1 method and Rung 3 claim."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,Theoretical analysis,1,3,2,5,0,"1. **Method Rung Classification**: The claim is based on developing a *unified theoretical framework*. Theoretical analysis without empirical validation (e.g., experiments or interventions) is classified as Rung 1 (Observational/Associational) under the codebook, as it establishes correlational or conceptual relationships rather than causal or mechanistic evidence.

2. **Claim Rung Classification**: The claim uses the term ""framework,"" which implies a mechanistic or foundational understanding. The phrase ""unified theoretical framework"" suggests a Rung 3 claim, as it goes beyond associational or causal relationships to propose a comprehensive, mechanistic explanation for SDL methods.

3. **Hedge Flag**: The claim is stated as an established fact (""we develop"") with no hedging language (e.g., ""may,"" ""suggests""), so hedge_flag is 0.

4. **Confidence**: The classification is straightforward, with clear linguistic markers for both method and claim rungs, warranting a confidence score of 5."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,theoretical_analysis,3,3,0,5,0,"The method used here is theoretical analysis, which involves developing a unified theoretical framework to explain observed phenomena. This is classified as Rung 3 because it goes beyond mere observation or intervention to establish a mechanistic understanding of why certain phenomena (feature absorption and dead neurons) occur.

The claim is that the paper provides 'novel theoretical explanations' for these phenomena. This is a Rung 3 claim because it asserts a mechanistic or counterfactual understanding of the underlying causes of feature absorption and dead neurons, rather than just describing correlations (Rung 1) or causal effects (Rung 2).

The confidence is high (5) because the claim is explicitly framed as a theoretical contribution, and the paper's abstract and body clearly support this classification. There is no hedging in the claim itself."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,SAE feature attribution,1,3,2,4,0,"The claim states that neural networks ""achieve remarkable performance through superposition: encoding multiple features as overlapping directions."" This is a mechanistic claim (Rung 3) about how neural networks *encode* and *achieve* performance, implying a functional or representational mechanism. However, the primary method used to support this claim in the paper is **SAE feature attribution** (Rung 1), which is an observational/associational method. The paper uses sparse autoencoders (SAEs) to extract and analyze features, but this method does not establish causality or counterfactual necessity, only correlation or presence of features. The claim overreaches by using mechanistic language (""encoding,"" ""achieve"") without interventional or counterfactual evidence to support the uniqueness or necessity of superposition as the mechanism."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,Sparse autoencoder feature attribution with entropy-based measurement,1,3,2,4,0,"1. **Method Rung (R1):** The paper uses sparse autoencoders (SAEs) to extract features from neural activations, which is a form of activation logging and feature attribution. This is purely observational/correlational (Rung 1) as it does not involve interventions or counterfactuals.

2. **Claim Rung (R3):** The claim introduces an ""information-theoretic framework"" that measures ""effective degrees of freedom,"" which implies a mechanistic interpretation of how neural networks organize information. The term ""measuring"" here is used in a way that suggests a deeper, functional understanding of neural representations, aligning with Rung 3 linguistic markers (e.g., ""effective degrees of freedom"" implies a mechanistic or representational claim).

3. **Hedge Flag:** The claim does not contain explicit hedging language (e.g., ""may,"" ""suggests""). It is stated as a factual presentation of the framework.

4. **Confidence:** The confidence is high (4) because the claim clearly fits the R3 pattern of mechanistic language, but there is minor ambiguity in whether ""measuring"" could be interpreted as purely observational. However, the context of the abstract and the paper's focus on superposition as a mechanistic phenomenon supports the R3 classification."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,correlation analysis with ground truth in controlled toy models,1,1,0,5,0,"The method used here is correlation analysis between the proposed metric and ground truth in toy models, which is an observational/associational method (Rung 1). The claim states a strong correlation, which is a direct empirical observation matching the method's capabilities. There is no hedge language, and the claim is appropriately aligned with the method rung."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,sparse autoencoder feature attribution with intervention analysis (adversarial training),2,3,1,4,0,"1. **Method Rung Classification (Rung 2 - Interventional):**
   - The paper uses sparse autoencoders (SAEs) to extract features from neural activations, which is primarily an observational method (Rung 1). However, the core evidence for the claim comes from *interventional* experiments involving adversarial training (e.g., PGD, FGSM). Adversarial training modifies the model's weights and activations to test causal effects on robustness and feature organization. This intervention establishes causal relationships between training regimes and model properties, placing the method in Rung 2.

2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**
   - The claim uses mechanistic language: ""adversarial training can increase effective features while improving robustness."" The phrase ""contradicting the hypothesis that superposition causes vulnerability"" implies a counterfactual or mechanistic narrative about how neural networks organize information under computational constraints. This goes beyond merely stating a causal effect (Rung 2) and suggests a deeper, explanatory mechanism (Rung 3).
   - The claim also invokes a *uniqueness* or *exclusivity* implication (e.g., contradicting an existing hypothesis), which is characteristic of Rung 3 claims.

3. **Hedge Flag:**
   - The claim is stated as a factual observation (""adversarial training can increase..."") with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, `hedge_flag` is 0.

4. **Confidence:**
   - Confidence is 4 (high but with minor ambiguity). The method (adversarial training) is clearly interventional (Rung 2), and the claim is mechanistic (Rung 3). However, there is slight ambiguity in whether the claim fully meets the Rung 3 criteria of establishing *unique* or *necessary* mechanisms, as it primarily refutes a hypothesis rather than proving an alternative mechanism."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,Sparse autoencoders (SAEs) with adaptive equivariance (invariant SAE + transformation matrix M),1,2,1,4,0,"1. **Method Rung (1 - Observational/Associational):** The primary method used to support this claim is the training and evaluation of sparse autoencoders (SAEs) with adaptive equivariance. While the paper introduces an invariant SAE and a transformation matrix M to approximate equivariance, the core evidence for the claim comes from **probing performance** (a downstream task). Probing is an observational method that establishes correlational evidence (e.g., ""features that lead to superior probing performance""). The paper does not use interventional methods (e.g., ablation or patching) to directly test the causal role of these features in downstream tasks. Thus, the method is classified as Rung 1.

2. **Claim Rung (2 - Causal):** The claim states that incorporating group symmetries **""yields features more useful in downstream tasks.""** The verb ""yields"" implies a causal relationship between the method (equivariant SAEs) and the outcome (improved downstream performance). While the evidence is correlational (probing performance), the linguistic framing suggests a causal interpretation (i.e., the equivariant SAE *causes* better performance). This aligns with Rung 2 claims, which assert causal effects based on observational or interventional evidence. However, since the evidence is observational (probing), the claim slightly overreaches the method's rung.

3. **Hedge Flag (0):** The claim is stated as a factual assertion (""yields"") without explicit hedging (e.g., ""may,"" ""suggests"").

4. **Confidence (4):** High confidence due to clear linguistic markers (""yields"") and the paper's explicit focus on probing performance as evidence. The ambiguity lies in whether probing performance fully supports a causal claim, but the claim's phrasing justifies Rung 2."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,activation_transformation_matrix_optimization,1,2,1,4,0,"1. **Method Rung Classification**: The method used here is the optimization of a matrix M to explain activation transformations under input rotations. This is an observational/associational method (Rung 1) because it establishes a correlational relationship (R² > 0.98) between the predicted and actual transformed activations without intervening on the model's internal mechanisms or establishing counterfactuals.

2. **Claim Rung Classification**: The claim states that ""a single matrix can explain how their activations transform as the images are rotated."" The term ""explain"" in this context is used to describe a causal-like relationship (how activations *transform*), which suggests a Rung 2 claim. However, the evidence provided is purely associational (R² metric), not interventional. The claim does not rise to Rung 3 because it does not assert a unique or mechanistic explanation (e.g., ""the matrix *is* the mechanism"").

3. **Confidence**: The confidence is 4 because the claim is framed in causal language (""explain how"") but is supported by correlational evidence. This is a common overclaim pattern, but the gap here is only +1 (R1 method → R2 claim), and the claim is not as strong as a full mechanistic assertion.

4. **Hedge Flag**: The claim is stated as a factual assertion (""can explain""), with no explicit hedging, so hedge_flag is 0."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,Sparse Autoencoders (SAEs) with adaptive equivariance (invariant SAE + transformation matrix M),1,1,0,5,0,"1. **Method Rung (1 - Observational/Associational):** The primary method used is sparse autoencoders (SAEs), which are trained to reconstruct activations and evaluate feature usefulness via probing tasks. While the paper introduces adaptive equivariance (invariant SAE + transformation matrix M), the core evidence for the claim comes from **probing performance**, a correlational/observational method (Rung 1). Probing measures associations between SAE latents and task labels, not causal or mechanistic relationships.

2. **Claim Rung (1 - Associational):** The claim states that adaptive SAEs ""discover features that lead to superior probing performance."" This is an associational claim, as it describes a correlation between the features discovered by adaptive SAEs and improved performance on downstream probing tasks. There are no linguistic markers suggesting causality (Rung 2) or mechanistic uniqueness (Rung 3). The term ""lead to"" here is used in a comparative sense (better performance) rather than implying a causal or mechanistic relationship.

3. **No Overclaiming:** The claim aligns with the method rung (R1 → R1), as both the method (probing) and the claim (performance comparison) are associational. The paper does not overclaim by using mechanistic or causal language (e.g., ""encode,"" ""control,"" ""the mechanism"").

4. **Confidence (5):** High confidence due to clear alignment between the method (probing) and the claim (performance comparison), with no polysemous terms or ambiguity in the claim text."
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,5,0,"The method used is Contrastive Activation Addition (CAA), which involves adding a learned bias (steering vector) to model activations at inference time. This is an interventional method (Rung 2) as it directly modifies activations to observe causal effects on output properties.

The claim states that ""steering effectively controls the targeted summary properties."" This is a causal claim about the effect of the intervention (steering vectors) on summary properties, aligning with Rung 2 linguistic markers such as ""controls"" (which, per the codebook, is Rung 2 when supported by interventional evidence).

No overclaiming is present here, as the claim rung matches the method rung. The paper provides empirical evidence (e.g., Figures 1-4) to support the causal effect of steering on properties like topic, sentiment, and readability."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,steering vectors (Contrastive Activation Addition),2,2,0,5,0,"The method used here is Contrastive Activation Addition (CAA), which involves adding a learned bias (steering vector) to language model activations at inference time. This is an interventional method because it directly modifies model activations to observe causal effects on output properties (e.g., sentiment, toxicity, readability). Thus, it is classified as Rung 2 (Interventional).

The claim states that ""high steering strengths consistently degrade both intrinsic and extrinsic text quality."" This is an empirical observation about the causal effect of steering strength on text quality metrics, which aligns with Rung 2 (Causal Claims). The paper does not claim that steering vectors *uniquely* or *mechanistically* degrade text quality (which would be Rung 3), but rather that intervening with high steering strengths *causally* leads to degradation. The evidence provided (e.g., Figures 5 and 6) supports this causal relationship.

No hedging language is present in the claim, and the confidence is high (5) due to clear empirical support and alignment with the method's interventional nature."
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors with prompting,2,2,0,5,0,"The method used is Contrastive Activation Addition (CAA), which involves adding a learned bias (steering vector) to model activations at inference time. This is an interventional method (Rung 2) because it directly modifies activations to observe causal effects on output properties (e.g., sentiment, toxicity, readability). The claim states that combining steering and prompting yields the ""strongest control"" over text properties, which is a comparative empirical observation about the efficacy of the intervention. This aligns with Rung 2 claims, as it describes the causal effect of the intervention (steering + prompting) on model behavior without asserting mechanistic uniqueness or counterfactual necessity. No hedge words are present, and the claim is stated as an established fact."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,theoretical analysis,1,3,2,4,0,"1. **Method Rung (1 - Observational/Associational):** The claim is made in the abstract and is based on theoretical analysis rather than empirical intervention or counterfactual testing. The paper discusses causal abstraction as a conceptual framework, which aligns with observational or associational methods (Rung 1) since it does not involve direct intervention on a model or system.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim uses mechanistic language (""provides a fruitful lens on computational implementation"") and implies a deep, structural understanding of how computational implementation works. The term ""causal abstraction"" suggests a framework for identifying unique or necessary mechanisms, which is characteristic of Rung 3 claims. The paper later elaborates on this by discussing how causal abstraction can underpin computational explanations, further supporting the Rung 3 classification.

3. **Hedge Flag (0):** The claim is stated as an established fact (""provides a fruitful lens"") without explicit hedging.

4. **Confidence (4):** The classification is confident but not definitive due to the theoretical nature of the claim. The paper does not present empirical interventions or counterfactual tests to support the claim, which introduces minor ambiguity about whether the claim is purely theoretical (R1) or implicitly mechanistic (R3). However, the mechanistic framing of ""causal abstraction"" strongly suggests Rung 3."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,theoretical_analysis,3,3,0,5,0,"1. **Method Rung (3 - Counterfactual):** The paper explicitly engages in theoretical analysis of causal abstraction, which involves counterfactual reasoning about interventions and their effects (e.g., ""what-if-things-were-different"" questions). The framework of causal abstraction (e.g., exact transformations, constructive abstraction, and abstraction-under-translation) inherently requires counterfactual reasoning to establish unique or necessary mechanisms, aligning with Rung 3 methods.

2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim ""we offer an account of computational implementation grounded in causal abstraction"" is a mechanistic claim. It asserts a theoretical framework for understanding how computational implementation works (i.e., the underlying mechanism), not just correlational or causal effects. The paper argues for a specific, unique account of implementation (""no computation without abstraction""), which is a hallmark of Rung 3 claims.

3. **No Overclaiming:** The claim_rung (3) matches the method_rung (3), as the paper's theoretical framework is designed to address mechanistic and counterfactual questions about computational implementation.

4. **Hedge Flag:** The claim is stated as an established fact (""we offer an account""), with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, hedge_flag is 0.

5. **Confidence:** High confidence (5) due to the clear alignment between the theoretical method (causal abstraction) and the mechanistic nature of the claim."
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,activation_patching,2,3,1,5,0,"1. **Method Rung Classification**: The paper uses activation patching (specifically, patching latents from source to target prompts) to analyze causal effects of representations. Activation patching is classified as a Rung 2 (Interventional) method according to the codebook, as it involves intervening on the model's activations to measure effects on output.

2. **Claim Rung Classification**: The claim states that ""the output language is encoded in the latent at an earlier layer than the concept to be translated."" This is a mechanistic claim about how the model processes information internally, implying a functional and temporal separation of language and concept representations. The use of ""encoded"" (a polysemous term) in this context is interpreted as a mechanistic claim (Rung 3) because:
   - The paper provides interventional evidence (activation patching) to support the claim.
   - The claim goes beyond causal effects (Rung 2) to describe *how* the model processes language and concept information (e.g., temporal ordering, functional separation).
   - The term ""encoded"" is used in a mechanistic sense (e.g., ""the model encodes X in layer Y""), which aligns with Rung 3 linguistic markers.

3. **Overclaiming**: The claim is Rung 3, while the method is Rung 2, resulting in a +1 gap (overclaiming). This aligns with the ""patching → 'THE mechanism'"" pattern described in the codebook, where interventional methods are used to support mechanistic claims.

4. **Hedge Flag**: The claim is stated as an established fact (e.g., ""the output language is encoded...""), with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence**: The classification is straightforward, with clear alignment to the codebook's definitions for both method and claim rungs. Confidence is rated 5."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,activation patching,2,2,0,5,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim states that the authors can change the concept or language independently through activation patching, which aligns with causal effects under specific interventions (e.g., ""intervening on X changes Y""). The claim does not assert uniqueness or a mechanistic narrative, so it does not qualify as Rung 3. The confidence is high (5) because the method and claim are explicitly described and align clearly with Rung 2 criteria."
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,activation_patching,2,2,0,5,0,"1. **Method Used**: The paper explicitly uses ""activation patching"" as its primary method, which is classified as a Rung 2 (Interventional) method according to the codebook. Activation patching involves replacing activations in the model and measuring the effect, which is a causal intervention.

2. **Claim Rung**: The claim states that ""patching with the mean representation of a concept across different languages improves translation."" This is a causal claim about the effect of an intervention (patching) on the model's performance, aligning with Rung 2 linguistic markers such as ""improves"" (indicating a causal effect). There is no uniqueness or mechanistic language (e.g., ""the mechanism,"" ""encodes"") that would elevate this to Rung 3.

3. **Hedge Flag**: The claim is stated as a factual observation (""improves"") without hedging language like ""may,"" ""suggests,"" or ""potentially."" Thus, hedge_flag is 0.

4. **Confidence**: The claim is clearly supported by the method used, and the linguistic markers are unambiguous for Rung 2. Confidence is rated 5."
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,activation_patching,2,3,1,5,0,"1. **Method Used**: The paper primarily employs activation patching (e.g., Figure 2, Section 4.1), which is classified as a Rung 2 (Interventional) method according to the codebook. This method involves intervening on the model's activations to measure causal effects.

2. **Claim Rung**: The claim ""results provide evidence for the existence of language-agnostic concept representations"" uses mechanistic language (""existence of... representations"") and implies a unique or underlying mechanism (language-agnostic concepts). This aligns with Rung 3 (Mechanistic/Counterfactual) claims, as it goes beyond causal effects to suggest a specific internal representation.

3. **Method-Claim Gap**: The method (Rung 2) does not fully support the claim (Rung 3). Activation patching can demonstrate causal effects (e.g., changing output language or concept via interventions) but cannot uniquely establish the *existence* of language-agnostic representations without additional counterfactual or necessity tests (e.g., causal scrubbing or uniqueness proofs).

4. **Hedge Flag**: The claim is stated as an established fact (""results provide evidence""), with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence**: The classification is clear-cut based on the codebook's definitions and examples (e.g., ""encodes,"" ""represents,"" ""existence of"" are Rung 3 markers). Confidence is 5."
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,activation_patching,2,3,1,4,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim states that 'early MLP layers attend not only to the syntactic structure but also process its semantic content,' which implies a mechanistic understanding of how these layers function (e.g., 'process its semantic content'). This language suggests a Rung 3 claim, as it attributes a specific functional role to the layers beyond mere causal influence or correlation. The confidence is rated 4 due to the clear mechanistic framing of the claim, though there is minor ambiguity about whether the authors explicitly test for uniqueness or necessity (which would solidify it as Rung 3). No hedging language is present in the claim."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,activation_patching,2,3,1,4,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim states that ""these layers transform persona tokens into richer representations which are then used by middle MHA layers."" This implies a mechanistic narrative about how the model processes persona-specific information, using terms like ""transform"" and ""used by,"" which are indicative of Rung 3 (Mechanistic/Counterfactual) claims. The paper does not explicitly test for uniqueness or necessity, but the language suggests a functional mechanism beyond mere causal effects. Confidence is rated 4 due to the clear mechanistic framing, though there is minor ambiguity about whether the claim fully asserts uniqueness or necessity."
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,activation_patching,2,3,1,4,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook. The claim states that specific attention heads ""disproportionately attend to racial and color-based identities,"" which implies a mechanistic or functional attribution (e.g., these heads ""attend to"" specific identities). This language aligns with Rung 3 (Mechanistic/Counterfactual) claims, as it suggests a functional role beyond mere correlation or causal effect. The paper does not explicitly test for uniqueness or necessity (which would solidify Rung 3), but the phrasing ""disproportionately attend"" leans toward a mechanistic interpretation. Confidence is rated 4 due to minor ambiguity in whether the claim is purely causal (R2) or mechanistic (R3), but the latter is more likely given the functional verb ""attend."""
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,Activation patching,2,3,1,4,0,"The method used is activation patching, which is classified as Rung 2 (Interventional) according to the codebook, as it involves replacing activations to measure causal effects. 

The claim states that ""associative knowledge is distributed,"" which implies a mechanistic interpretation of how the model represents knowledge. The phrase ""demonstrating that"" suggests a strong causal or mechanistic conclusion, which aligns with Rung 3 (Mechanistic/Counterfactual) claims. The codebook specifies that terms like ""distributed"" or ""localized"" in the context of knowledge representation often imply Rung 3 claims, especially when paired with causal evidence. However, the paper does not explicitly test for uniqueness or counterfactual necessity, which would be required for a definitive Rung 3 classification. 

The confidence is rated 4 due to minor ambiguity in whether the claim is purely causal (Rung 2) or mechanistic (Rung 3). The lack of explicit uniqueness testing slightly reduces confidence, but the mechanistic framing of ""distributed knowledge"" leans toward Rung 3."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation Patching,2,3,1,4,0,"1. **Method Used**: The paper employs activation patching (specifically CLAP), which is classified as a Rung 2 (Interventional) method according to the codebook. This method involves replacing corrupted activations with clean ones and measuring the effect on model behavior, establishing causal effects.

2. **Claim Rung**: The claim states that ""patching the final output layer completely restored accuracy indicating that definitional knowledge is localised."" The phrase ""definitional knowledge is localised"" implies a mechanistic or functional attribution (e.g., the model ""stores"" or ""uses"" this layer for definitional knowledge). This aligns with Rung 3 linguistic markers such as ""encodes,"" ""represents,"" or ""is responsible for.""

   - The codebook specifies that terms like ""localised"" (in the context of knowledge representation) typically imply a Rung 3 claim, as they suggest a unique or mechanistic role for the component (here, the final output layer).
   - The paper does not provide counterfactual or uniqueness tests (e.g., showing no alternative layer could achieve the same effect), which would be required to fully justify a Rung 3 claim. However, the phrasing still leans toward a mechanistic interpretation.

3. **Method-Claim Gap**: The method (Rung 2) establishes that patching the final output layer causally restores accuracy, but the claim (Rung 3) goes further by asserting that this layer ""is localised"" for definitional knowledge. This represents a +1 gap (R2 → R3).

4. **Hedge Flag**: The claim is stated as a factual assertion (""completely restored accuracy indicating that...""), with no hedging language (e.g., ""may,"" ""suggests""). Thus, hedge_flag is 0.

5. **Confidence**: Confidence is rated 4 (high) because:
   - The method (activation patching) is clearly Rung 2.
   - The claim uses mechanistic language (""localised"") that is typically Rung 3.
   - The gap is well-documented in the codebook (e.g., patching → ""THE circuit"" or ""stores"").
   - Minor ambiguity exists because the paper does not explicitly test uniqueness, but the claim’s phrasing still strongly implies Rung 3."
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Activation Patching,2,3,1,4,0,"The method used is activation patching (CLAP), which is classified as Rung 2 (Interventional) according to the codebook, as it involves replacing activations to measure causal effects on model behavior.

The claim ""factual knowledge is more localized and associative knowledge depends on distributed representations"" uses mechanistic language (""is localized,"" ""depends on"") that implies a functional or representational mechanism. This aligns with Rung 3 (Mechanistic/Counterfactual) claims, as it goes beyond causal effects to suggest how knowledge is *organized* within the model. The paper does not provide counterfactual necessity tests or uniqueness proofs, but the language strongly suggests a mechanistic interpretation.

The gap between method_rung (2) and claim_rung (3) is +1, as the method establishes causal effects but the claim implies a stronger mechanistic narrative. Confidence is rated 4 due to the clear mechanistic framing of the claim, though the absence of explicit uniqueness tests introduces minor ambiguity."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,Sparse Autoencoders (SAEs) with universal training objective,1,3,2,4,0,"1. **Method Rung (R1 - Observational/Associational):** The primary method used is training Universal Sparse Autoencoders (USAEs) on activations from multiple models. This involves learning a shared sparse code to reconstruct activations, which is fundamentally an observational/associational technique. While the paper introduces a novel training objective, it does not involve interventions (e.g., ablation, patching) or counterfactual analysis, placing it firmly in Rung 1.

2. **Claim Rung (R3 - Mechanistic/Counterfactual):** The claim uses mechanistic language such as ""jointly learn a universal concept space,"" ""reconstruct and interpret,"" and ""universal concepts."" These terms imply a strong mechanistic interpretation (e.g., ""universal concept space"" suggests a unique, shared representation across models). The paper does not hedge this claim (e.g., no ""may"" or ""suggests""), and the language aligns with Rung 3 markers like ""encodes,"" ""represents,"" and ""the [mechanism/concept space].""

3. **Overclaim Gap:** The method (R1) does not support the mechanistic claims (R3), resulting in a gap of +2. The paper provides correlational evidence (e.g., reconstruction fidelity, concept co-firing) but does not establish causal or counterfactual relationships to justify the mechanistic interpretation.

4. **Confidence:** High confidence (4) due to clear linguistic markers (e.g., ""universal concept space,"" ""interpret"") and the absence of hedging. The method is unambiguously R1, and the claim is unambiguously R3.

5. **Hedge Flag:** 0 (no hedging). The claim is stated as an established fact."
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,Sparse Autoencoder (SAE) feature attribution,1,3,2,4,0,"1. **Method Rung Classification**: The paper uses Universal Sparse Autoencoders (USAEs) to discover and align interpretable concepts across multiple models. The core method involves training a single SAE on activations from multiple models (Section 3, Figure 2). SAE feature attribution is classified as **Rung 1 (Observational/Associational)** because it identifies which features activate on specific inputs without intervening on the model's internal mechanisms.

2. **Claim Rung Classification**: The claim states that ""the learned dictionary captures common factors of variation—concepts—across different tasks, architectures, and datasets."" This language implies a **mechanistic or representational claim** (e.g., ""captures,"" ""common factors""), which aligns with **Rung 3 (Mechanistic/Counterfactual)**. The claim suggests that the dictionary *represents* or *encodes* these concepts, which goes beyond mere correlation or causal sufficiency.

3. **Polysemous Term Handling**: The term ""captures"" is analyzed using the decision tree for ""encodes/represents."" The paper does not provide interventional evidence (e.g., ablation or patching) to support this claim, so the default interpretation is **Rung 3** (mechanistic).

4. **Hedge Flag**: The claim is stated as an established fact (no hedging language like ""may"" or ""suggests""), so **hedge_flag = 0**.

5. **Confidence**: The classification is confident (4/5) because the method (SAE) is clearly Rung 1, and the claim language (""captures common factors"") is unambiguously Rung 3. However, minor ambiguity exists in whether ""captures"" is intended mechanistically or as a correlational observation, though the former is more likely given the paper's framing."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,Sparse Autoencoders (SAEs) with universal training objective,1,3,2,4,0,"1. **Method Rung (R1 - Observational/Associational):** The paper uses Sparse Autoencoders (SAEs) to extract and align concepts across models. SAEs are a form of dictionary learning that identifies features based on activation patterns, which is inherently correlational. The universal training objective (USAE) does not involve interventions or counterfactual testing but rather relies on joint reconstruction of activations from multiple models. This places the method firmly in Rung 1.

2. **Claim Rung (R3 - Mechanistic/Counterfactual):** The claim states that USAEs ""discover semantically coherent and important universal concepts."" The use of ""discover"" and ""universal concepts"" implies a mechanistic interpretation, suggesting that the concepts are not just correlated with model activations but are fundamental to how the models represent information. This language aligns with Rung 3 claims, which often involve functional or mechanistic attributions (e.g., ""encodes,"" ""represents,"" ""the mechanism"").

3. **Overclaim Gap:** The claim (R3) exceeds the method's capabilities (R1), resulting in a +2 gap. The method provides correlational evidence of shared features, but the claim implies a deeper, mechanistic understanding of how models represent concepts.

4. **Hedge Flag:** The claim is stated as an established fact (e.g., ""USAEs discover...""), with no hedging language (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence:** Confidence is rated 4 due to the clear linguistic markers (""discover,"" ""universal concepts"") that align with R3 claims, despite the method being R1. The only minor ambiguity is whether ""semantically coherent"" could be interpreted as a weaker claim, but the overall framing leans toward mechanistic interpretation."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,Sparse Autoencoders (SAEs),1,3,2,4,0,"1. **Method Rung Classification**: The method used is Sparse Autoencoders (SAEs) for feature attribution. According to the codebook, SAE feature attribution is classified as Rung 1 (Observational/Associational) because it identifies which SAE features activate without intervening on the model.

2. **Claim Rung Classification**: The claim states that SAEs ""facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features."" The phrase ""clarify model behavior"" and ""explaining single-meaning atomic features"" implies a mechanistic or functional understanding of the model's internals. This aligns with Rung 3 (Mechanistic/Counterfactual) linguistic markers such as ""explains,"" which, per the decision tree for polysemous terms, defaults to R3 when used in a mechanistic narrative without explicit interventional evidence.

3. **Overclaim Analysis**: The claim_rung (3) exceeds the method_rung (1), resulting in a +2 gap. This is consistent with the overclaim pattern where SAE-based methods (R1) are used to support mechanistic claims (R3).

4. **Hedge Flag**: The claim is stated as an established fact (e.g., ""SAEs facilitate"") with no explicit hedging (e.g., ""may,"" ""suggests""), so hedge_flag is 0.

5. **Confidence**: The confidence is 4 (confident, minor ambiguity) because the claim clearly uses mechanistic language (""clarify model behavior,"" ""explaining"") without interventional support, but the context does not explicitly test uniqueness or necessity (which would further solidify R3)."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,Sparse Autoencoders (SAEs) with concept-specific interpretability evaluation,1,3,2,4,0,"1. **Method Rung Classification**: The paper uses Sparse Autoencoders (SAEs) to decompose internal representations of LLMs into atomic features. This is primarily an observational/associational method (Rung 1) because it involves training SAEs on activations and evaluating concept-specific interpretability through metrics like L0,t and ICDF, which are correlational in nature. No direct intervention (e.g., ablation, patching) is mentioned for this specific claim.

2. **Claim Rung Classification**: The claim ""Safe-SAIL systematically identifies SAE with best concept-specific interpretability"" implies a mechanistic understanding of how SAEs capture interpretable features. The use of ""systematically identifies"" and ""concept-specific interpretability"" suggests a stronger claim than mere correlation, leaning toward a mechanistic (Rung 3) interpretation. However, the paper does not provide counterfactual or uniqueness evidence to fully justify a Rung 3 classification.

3. **Decision Tree for Polysemous Terms**: The term ""identifies"" is ambiguous. The paper does not clarify whether it means ""selects based on correlational metrics"" (R1) or ""discovers the underlying mechanism"" (R3). Given the context of interpretability frameworks and the emphasis on ""concept-specific"" features, the claim is closer to R3 but lacks explicit interventional or counterfactual evidence.

4. **Hedge Flag**: The claim is stated as a factual assertion (""systematically identifies"") with no hedging language (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence**: Confidence is 4 due to minor ambiguity in the term ""identifies"" and the lack of explicit interventional evidence to fully support a Rung 3 claim. The claim is more mechanistic than observational but not definitively R3."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,SAE feature attribution,1,3,2,4,0,"The method used is SAE (Sparse Autoencoder) feature attribution, which is classified as Rung 1 (Observational/Associational) according to the codebook. This method identifies which SAE features activate on specific inputs but does not involve interventions or causal manipulations.

The claim ""we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors"" uses mechanistic language (""capture high-risk behaviors"") and implies a functional or representational understanding of the model's internals. This aligns with Rung 3 (Mechanistic/Counterfactual) claims, as it suggests the features encode or represent specific behaviors rather than merely correlating with them.

The gap between the method (R1) and the claim (R3) is +2, indicating overclaiming. The confidence is rated 4 due to the clear linguistic markers (""capture,"" which implies representation) and the lack of interventional evidence to support the mechanistic interpretation."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,observational and causal mediation analyses,2,3,1,4,0,"1. **Method Rung Classification**: The paper explicitly states the use of ""observational and causal mediation analyses"" (e.g., activation patching, attention knockout, and CountScope probing). These methods involve interventions (e.g., patching activations, ablating tokens) to establish causal effects, placing them in **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim ""latent counts are computed and stored in the final item representations of each part"" uses mechanistic language (""computed and stored""). This implies a functional or representational mechanism, which is characteristic of **Rung 3 (Mechanistic/Counterfactual)**. The paper does not explicitly test uniqueness or counterfactual necessity, but the language suggests a stronger claim than mere causal mediation.

3. **Overclaim Gap**: The method (Rung 2) does not fully support the mechanistic claim (Rung 3), resulting in a gap of +1.

4. **Hedge Flag**: The claim is stated as a factual assertion (no hedging language like ""may"" or ""suggests""), so **hedge_flag = 0**.

5. **Confidence**: The classification is clear based on the paper's explicit description of methods and the linguistic markers in the claim. Minor ambiguity exists around whether ""stored"" is used in a mechanistic or correlational sense, but the context leans toward mechanistic, justifying **confidence = 4**."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,"attention analysis, causal mediation analysis (activation patching)",2,3,1,4,0,"1. **Method Rung Classification**: The paper uses **attention analysis** (observational, Rung 1) and **causal mediation analysis via activation patching** (interventional, Rung 2). The claim specifically references ""dedicated attention heads,"" which are identified through **causal mediation techniques** (e.g., attention knockout, activation patching). These methods establish causal effects of specific components (attention heads) on the model's behavior, placing the method in **Rung 2 (Interventional)**.

2. **Claim Rung Classification**: The claim states that ""counts are transferred to intermediate steps via dedicated attention heads."" This implies a **mechanistic narrative** about how the model internally processes and routes information, which is characteristic of **Rung 3 (Mechanistic/Counterfactual)**. The phrase ""via dedicated attention heads"" suggests a functional attribution (e.g., these heads *perform* the transfer), which goes beyond causal sufficiency (Rung 2) to imply a specific, unique mechanism (Rung 3).

3. **Overclaim Analysis**: The method (Rung 2) does not fully support the claim (Rung 3). While activation patching and attention knockout can show that specific heads are *causally necessary* for the behavior, they do not establish that these heads are the *unique* or *dedicated* mechanism for transferring counts. The term ""dedicated"" implies exclusivity, which would require counterfactual or necessity tests (Rung 3) to rule out alternative pathways.

4. **Hedge Flag**: The claim is stated as a factual assertion (no hedging like ""may"" or ""suggests""), so **hedge_flag = 0**.

5. **Confidence**: The classification is confident (4/5) because the claim and method are clearly described, and the gap between Rung 2 and Rung 3 is well-defined in the codebook. However, minor ambiguity exists around whether ""dedicated"" is intended to imply uniqueness (Rung 3) or just causal involvement (Rung 2). The codebook's decision tree for polysemous terms (e.g., ""dedicated"") defaults to Rung 3 for mechanistic language unless explicitly qualified."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,"causal mediation analysis, attention analysis, activation patching, cross-context activation patching",2,3,1,4,0,"1. **Method Rung Classification (Rung 2 - Interventional):**
   - The paper employs multiple interventional methods to support its claims, including:
     - **Causal mediation analysis** (e.g., activation patching, attention knockout, zero ablation) to identify causal pathways and components necessary for counting behavior.
     - **Cross-context activation patching** to test how partition-level information is combined, which establishes causal effects of specific interventions.
   - These methods go beyond observational/associational evidence (Rung 1) by directly intervening on model activations and measuring effects, but they do not establish counterfactual necessity or uniqueness (Rung 3).

2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**
   - The claim ""this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting"" implies a mechanistic narrative about how the System-2 strategy *overcomes* inherent model limitations. This suggests a functional and structural understanding of the model's behavior, which aligns with Rung 3.
   - The phrase ""surpass architectural limitations"" implies a counterfactual or mechanistic claim: the model *would not* achieve high accuracy without the strategy, and the strategy *uniquely* addresses the limitation. This goes beyond causal sufficiency (Rung 2) to a stronger mechanistic interpretation.

3. **Overclaim Gap:**
   - The method rung (R2) does not fully support the claim rung (R3). While the paper provides strong causal evidence for the effectiveness of the System-2 strategy, it does not establish that this is the *only* or *unique* mechanism by which LLMs could achieve high accuracy on large-scale counting. The claim implies a mechanistic uniqueness that the methods do not fully justify.

4. **Hedge Flag:**
   - The claim is stated as an established fact (""enables LLMs to surpass..."") with no explicit hedging (e.g., ""may,"" ""suggests""). Thus, hedge_flag = 0.

5. **Confidence:**
   - Confidence is rated 4 (high but not maximal) due to:
     - Clear alignment of methods with Rung 2 (interventional).
     - Clear linguistic markers for Rung 3 (e.g., ""surpass limitations,"" which implies a mechanistic narrative).
     - Minor ambiguity about whether the claim is strictly mechanistic or could be interpreted as causal sufficiency (Rung 2). However, the phrasing leans toward Rung 3."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,theoretical_analysis,1,2,1,4,0,"1. **Method Rung Classification**: The paper uses theoretical analysis and generalization bounds derived from statistical learning theory to support its claims. This is primarily an observational/associational method (Rung 1) because it establishes structural parallels between neuron identification and machine learning without direct interventions on the model or counterfactual testing.

2. **Claim Rung Classification**: The claim ""neuron identification can be viewed as the inverse process of machine learning"" suggests a causal or functional relationship (e.g., inversion implies a directional or transformative process). While the claim does not explicitly assert a mechanistic or counterfactual relationship (Rung 3), it goes beyond mere correlation (Rung 1) by proposing a structural analogy that implies a causal or procedural link. Thus, it is classified as Rung 2.

3. **Hedge Flag**: The claim is stated as a factual assertion without hedging language (e.g., ""can be viewed"" is not a hedge in this context).

4. **Confidence**: The classification is confident (4/5) due to the clear linguistic markers (""inverse process"") and the paper's explicit framing of the analogy as a causal or procedural relationship. However, minor ambiguity exists in whether ""inverse process"" strictly implies causality or a looser structural parallel."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,theoretical_analysis_generalization_bounds,1,2,1,5,0,"1. **Method Rung Classification**: The paper derives generalization bounds for similarity metrics (e.g., accuracy, AUROC, IoU) using tools from statistical learning theory. This is an observational/associational method (Rung 1) because it establishes theoretical guarantees based on probabilistic analysis of probing datasets, without direct intervention on the model or counterfactual testing.

2. **Claim Rung Classification**: The claim ""we derive generalization bounds for widely used similarity metrics to guarantee faithfulness"" is a causal claim (Rung 2). The term ""guarantee faithfulness"" implies that the bounds provide evidence for a causal relationship between the identified concepts and the neuron's function, albeit through theoretical guarantees rather than direct intervention. The paper does not claim to establish unique or counterfactual mechanisms (Rung 3).

3. **Hedge Flag**: The claim is stated as a factual assertion (""we derive"") with no hedging language.

4. **Confidence**: High confidence (5) due to the clear theoretical framing and the explicit use of generalization bounds, which align with Rung 1 methods and Rung 2 claims."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,bootstrap ensemble procedure,2,2,0,5,0,"The method described is a bootstrap ensemble procedure, which involves resampling the probing dataset and applying neuron identification algorithms to each resampled dataset. This is an interventional method (Rung 2) because it systematically varies the dataset to measure the effect on the identification results, establishing causal effects of the probing dataset on the stability of explanations.

The claim states that this procedure ""quantifies stability along with guaranteed coverage probability."" This is a claim about the causal effect of the method (how the procedure affects stability and coverage), not a mechanistic or counterfactual claim. The language does not imply uniqueness, necessity, or a deeper mechanistic understanding (e.g., no use of terms like ""encodes,"" ""represents,"" or ""the mechanism""). Thus, the claim rung is appropriately Rung 2, matching the method rung.

No hedging is present in the claim, and the confidence is high due to the clear alignment between the method and claim."
