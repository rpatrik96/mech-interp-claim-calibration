{
  "timestamp": "2026-02-19T10:21:53.486974+00:00",
  "paper_id": "2512.13568",
  "claim_id": "2512.13568-04",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2512.13568\nTitle: Superposition as Lossy Compression: Measure with SAE\nFull text:\n             Published in Transactions on Machine Learning Research (12/2025)\n\n\n\n       Superposition as Lossy Compression: Measure with Sparse\n       Autoencoders and Connect to Adversarial Vulnerability\n\n\n          Leonard Bereska1∗, Zoe Tzifa-Kratira1, Reza Samavi2,3†, Efstratios Gavves1\n               1University of Amsterdam\n              2Toronto Metropolitan University\n              3Vector Institute for Artificial Intelligence\n\n\n\n           Reviewed on OpenReview: https: // openreview. net/ forum? id= qaNP6o5qvJ\n          View as HTML: https: // leonardbereska. github. io/ blog/ 2025/ superposition/\n2025\nDec                                          Abstract\n                    Neural networks achieve remarkable performance through superposition: encoding multiple\n                       features as overlapping directions in activation space rather than dedicating individual neu-15\n                     rons to each feature. This phenomenon challenges interpretability: when neurons respond\n                      to multiple unrelated concepts, understanding network behavior becomes difficult. Yet de-\n                        spite its importance, we lack principled methods to measure superposition. We present\n                  an information-theoretic framework measuring a neural representation’s effective degrees of\n                      freedom. We apply the Shannon entropy to sparse autoencoder activations to compute the\n                  number of effective features as the minimum number of neurons needed for interference-free[cs.LG]\n                     encoding. Equivalently, this measures how many “virtual neurons” the network simulates\n                    through superposition. When networks encode more effective features than they have actual\n                     neurons, they must accept interference as the price of compression. Our metric strongly cor-\n                        relates with ground truth in toy models, detects minimal superposition in algorithmic tasks\n                         (effective features approximately equal neurons), and reveals systematic reduction under\n                     dropout. Layer-wise patterns of effective features mirror studies of intrinsic dimensionality\n                  on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature\n                      consolidation during the grokking phase transition.  Surprisingly, adversarial training can\n                       increase effective features while improving robustness, contradicting the hypothesis that su-\n                      perposition causes vulnerability. Instead, the effect of adversarial training on superposition\n                   depends on task complexity and network capacity: simple tasks with ample capacity allow\n                      feature expansion (abundance regime), while complex tasks or limited capacity force feature\n                     reduction (scarcity regime). By defining superposition as lossy compression, this work en-\n                      ables principled, practical measurement of how neural networks organize information under\n                    computational constraints, in particular, connecting superposition to adversarial robustness.arXiv:2512.13568v1\n\n\n         1  Introduction\n\n               Interpretability and adversarial robustness could be two sides of the same coin (Räuker et al., 2023). Adver-\n                sarially trained models learn more interpretable features (Engstrom et al., 2019; Ilyas et al., 2019), develop\n              representations that transfer better (Salman et al., 2020), and align more closely with human perception\n             (Santurkar et al., 2019). Conversely, interpretability-enhancing techniques improve robustness: input gradi-\n             ent regularization (Ross & Doshi-Velez, 2017; Boopathy et al., 2020), attribution smoothing (Etmann et al.,\n              2019), and feature disentanglement (Augustin et al., 2020) all defend against adversarial attacks.  Even\n              architectural choices that promote interpretability, such as lateral inhibition (Eigen & Sadovnik, 2021) and\n\n                 ∗Corresponding author. Email: leonard.bereska@gmail.com\n                 †Equal supervision\n\n\n                                                           1\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n neurons           features                 superposition measure\n                   ψ                  =\n\n\n\n   (a) Observed network      (b) Hypothetical disentangled model       (c) Superposition as features per neuron\n\nFigure 1: Defining superposition for a neural network layer. (a) Observed network with compressed repre-\nsentation where multiple features share neuronal dimensions. (b) Hypothetical disentangled model where\neach effective feature occupies its own neuron without interference (Elhage et al., 2022b). (c) Superposition\nmeasure ψ quantifies effective features per neuron.  Here, the network simulates twice as many effective\nfeatures as it has neurons. Figure adapted from (Bereska & Gavves, 2024).\n\n\n\n\nsecond-order optimization (Tsiligkaridis & Roberts, 2020), yield more robust models. This pervasive duality\ndemands mechanistic explanation.\n\nThe superposition hypothesis offers a potential mechanism. Elhage et al. (2022b) showed that neural net-\nworks compress information through superposition: encoding multiple features as overlapping activation\npatterns. When features share dimensions, their interference creates attack surfaces that adversaries might\nexploit.  If, by this mechanism, superposition caused adversarial vulnerability, this would explain i.) adver-\nsarial transferability as shared feature correlations (Liu et al., 2017), ii.) the robustness-accuracy trade-off\nas models sacrificing representational capacity for orthogonality (Tsipras et al., 2019), and iii.)  robust\nmodels becoming more interpretable by reducing feature entanglement (Engstrom et al., 2019). Also, this\nsuperposition-vulnerability hypothesis predicts that adversarial training should reduce superposition.\n\nTesting this prediction requires measuring superposition in real networks. While Elhage et al. (2022b) used\nweight matrix Frobenius norms, this approach requires ground truth features; available only in toy models.\nWe need principled methods to quantify superposition without knowing the true features.\n\nWe solve this through information theory applied to sparse autoencoders (SAEs). SAEs extract interpretable\nfeatures from neural activations (Cunningham et al., 2024; Bricken et al., 2023), decomposing them into\nsparse dictionary elements. We measure each feature’s share of the network’s representational budget through\nits activation magnitude across samples.\n\nThe exponential of the Shannon entropy quantifies how many interference-free channels would transmit\nthis feature distribution, the network’s effective degrees of freedom. We call this count effective features\nF (Figure 1b): the minimum neurons needed to encode the observed features without interference. We\ninterpret this as F “virtual neurons”: the network simulates this many independent channels through its\nN physical neurons (Figure 1b). The feature distribution compresses losslessly down to exactly F neurons;\ncompress further and interference becomes unavoidable.\n\nWe measure superposition as ψ = F/N (Figure 1c), counting virtual neurons per physical neuron. At ψ = 1,\nthe network operates at its interference-free limit (no superposition). At ψ = 2, it simulates twice as many\nchannels as it has neurons, achieving 2× lossy compression. Thus, we define superposition as compression\nbeyond the lossless limit.\n\nOur findings contradict the simple superposition-vulnerability hypothesis. Adversarial training does not uni-\nversally reduce superposition; its effect depends on task complexity relative to network capacity (Section 7).\nSimple tasks with ample capacity permit abundance: networks expand features for robustness. Complex\ntasks under constraints force scarcity: networks compress further, reducing features. This bifurcation holds\nacross architectures (MLPs, CNNs, ResNet-18) and datasets (MNIST, Fashion-MNIST, CIFAR-10).\n\nWe validate the framework where superposition is observable. Toy models achieve r = 0.94 correlation\nthrough the SAE extraction pipeline (Section 5.1), and under SAE dictionary scaling the measure con-\n\n\n                                              2\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nverges with appropriate regularization (Section 5.2). Beyond adversarial training, systematic measurement\nacross contexts generates hypotheses about neural organization: dropout seems to act as capacity constraint,\nreducing superposition (Section 6.1), compressing networks trained on algorithmic tasks seems to not cre-\nate superposition (ψ ≤1) likely due to lack of input sparsity (Section 6.2), during grokking, we capture\nthe moment of algorithmic discovery through sharp drop in superposition at the generalization transition\n(Section 6.3), and Pythia-70M’s layer-wise compression peaks in early MLPs before declining (Section 6.4);\nmirroring intrinsic dimensionality studies (Ansuini et al., 2019).\n\nThis work makes superposition measurable. By grounding neural compression in information theory, we en-\nable quantitative study of how networks encode information under capacity constraints, potentially enabling\nsystematic engineering of interpretable architectures.\n\n\n2  Related Work\n\nSuperposition and polysemanticity.  Neural networks employ distributed representations, encoding\ninformation across multiple units rather than in isolated neurons (Hinton, 1984; Olah, 2023). The discovery\nthat semantic relationships manifest as directions in embedding space, exemplified by vector arithmetic like\n“king - man + woman = queen” (Mikolov et al., 2013), established the linear representation hypothesis\n(Park et al., 2023). Building on this geometric insight, Elhage et al. (2022b) formulated the superposition\nhypothesis: networks encode more features than dimensions by representing features as nearly orthogonal\ndirections.  Their toy models revealed phase transitions between monosemantic neurons (one feature per\nneuron) and polysemantic neurons (multiple features per neuron), governed by feature sparsity.  Recent\ntheoretical work proves networks can compute accurately despite the interference inherent in superposition\n(Vaintrob et al., 2024; Hänni et al., 2024).\n\nWhile superposition (more effective features than neurons) inevitably creates polysemantic neurons through\nfeature interference, polysemanticity (multiple features sharing a neuron) also emerges by other means:\nrotation of features relative to the neuron basis, incidentally (Lecomte et al., 2023) (e.g. via regularization),\nor forced by noise (such as dropout) as redundant encoding (Marshall & Kirchner, 2024) (as we show in\nSection 6.1, dropout shows the opposite effect on superposition). Scherlis et al. (2023) analyzed how features\ncompete for limited neuronal capacity, showing that importance-weighted feature allocation can explain\nwhich features become polysemantic under resource constraints.\n\nSparse autoencoders for feature extraction.  Sparse autoencoders (SAEs) tackle the challenge of ex-\ntracting interpretable features from polysemantic representations by recasting it as sparse dictionary learning\n(Sharkey et al., 2022; Cunningham et al., 2024). SAEs decompose neural activations into sparse combina-\ntions of learned dictionary elements, effectively reversing the superposition process. Recent architectural\ninnovations such as gated SAEs (Rajamanoharan et al., 2024), TopK variants (Gao et al., 2024; Bussmann\net al., 2024), and Matryoshka SAEs (Bussmann et al., 2025) improve feature recovery. While our experiments\nemploy vanilla SAEs for conceptual clarity, our entropy-based framework remains architecture-agnostic: im-\nproved feature extraction yields more accurate measurements without invalidating the theoretical foundation.\n\nSAEs scale to state-of-the-art models: Anthropic extracted millions of interpretable features from Claude\n3 Sonnet (Templeton et al., 2024), while OpenAI achieved similar results with GPT-4 (Gao et al., 2024).\nCrucially, these features are causally relevant: activation steering produces predictable behavioral changes\n(Marks et al., 2024). Applications span attention mechanism analysis (Kissane et al., 2024), reward model\ninterpretation (Marks et al., 2023), and automated feature labeling (Paulo et al., 2024), establishing SAEs\nas foundational for mechanistic interpretability (Bereska & Gavves, 2024).\n\nInformation theory and neural measurement.  Information-theoretic principles provide rigorous foun-\ndations for understanding neural representations. The information bottleneck principle (Tishby et al., 2000),\nwhen applied to deep learning (Shwartz-Ziv & Tishby, 2017), reveals how networks balance compression with\nprediction. Each neural layer acts as a bandwidth-limited channel, forcing networks to develop efficient codes\n(i.e. superposition) to transmit information forward (Goldfeld et al., 2019). This perspective recasts super-\nposition as an optimal solution to rate-distortion constraints.\n\n\n                                              3\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n                                                             increasing                                                                            sparsity                                                                 S sparsity                                                                    z       S  f                              f                                                            S =                                            S = 0                                                   S = 0.7                                                                             0.9   S = 0.99    Wsae                                      W−sae      Z     p    F ψ           Wtoy W−toy                                               x            x⊤             x                    x1                                                                                                 1.       2.        3.\n                                                         x2\n                                                                     1.0\n                             W⊤                                                         toyWtoy   increasingimportance                                                         −1.0                                      ψ               ReLU                                  = 1   ψ = 1.5  ψ = 2  ψ = 2.5   ReLU            l1 norm\n\n  (a) Toy model architecture        (b) Effect of sparsity             (c) SAE             (d) Measurement\n\nFigure 2: From toy model to practical superposition measurement. (a) Toy model bottlenecks features f\nthrough fewer neurons x, with importance gradient determining allocation. (b) Sparsity enables interference-\nbased compression: matrices WtoyWtoyT      show off-diagonal terms growing as ψ increases from 1 to 2.5. (c)\nSparse autoencoders learn sparse codes z reconstructing activations x. (d) Measurement: extract activations\nZ, derive probabilities p, compute F = eH(p), measure ψ = F/N.\n\n\nMost pertinent to our work, Ayonrinde et al. (2024) connected SAEs to minimum description length (MDL).\nBy viewing SAE features as compression codes for neural activations, they showed that optimal SAEs\nbalance reconstruction fidelity against description complexity. Our entropy-based framework extends this\nperspective, measuring the effective “alphabet size” networks use for internal communication.\n\nQuantifying feature entanglement.  Despite its theoretical importance, measuring superposition re-\nmains unexplored. Elhage et al. (2022b) proposed a dimensions per feature metric for analyzing uniform\nimportance settings in toy models, which when inverted could measure features per dimension. But this\napproach requires knowing the ground truth feature-to-neuron mapping matrix, limiting its applicability to\ncontrolled settings.  Traditional disentanglement metrics from representation learning (Carbonneau et al.,\n2022; Eastwood & Williams, 2018) assess statistical independence rather than the representational compres-\nsion characterizing superposition. Other dimensionality measures like effective rank (Roy & Vetterli, 2007)\nand participation ratio (Gao et al., 2017) quantify the number of significant dimensions in a representation\nbut do not directly measure feature-to-neuron compression ratios.\n\nEntropy-based measures have proven effective across disciplines facing similar measurement challenges. Neu-\nroscience employs participation ratios (form of entropy, see Appendix A.3 for connection to Hill numbers) to\nquantify how many neurons contribute to population dynamics (Gao et al., 2017). Economics uses entropy\nto quantify portfolio concentration (Fontanari et al., 2021). Quantum physics applies von Neumann entropy\nto count effective pure states in entangled systems (Nielsen & Chuang, 2011). Recent work applies entropy\nmeasures to neural network analysis (Lee et al., 2023; Shin et al., 2024). Across fields, entropy naturally\ncaptures how information distributes across components: exactly what we need for measuring superposition.\n\n\n3  Background on Superposition and Sparse Autoencoders\n\nNeural networks must transmit information through layers with fixed dimensions. When neurons must encode\ninformation about many more features than available dimensions, networks employ superposition—packing\nmultiple features into shared dimensions through interference. This compression mechanism enables repre-\nsenting more features than available neurons at the cost of introducing crosstalk between them. Superposition\nis compression beyond the lossless limit.\n\nWe examine toy models where superposition emerges under controlled bandwidth constraints, making inter-\nference patterns directly observable (Section 3.1). For real networks where ground truth remains unknown,\nwe extract features through sparse autoencoders before measurement becomes possible (Section 3.2).\n\n\n3.1  Observing Superposition in Toy Models\n\nTo understand how neural networks represent more features than they have dimensions, Elhage et al. (2022b)\nintroduced minimal models demonstrating superposition under controlled conditions. The toy model com-\n\n\n                                              4\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\npresses a feature vector f ∈RM through a bottleneck x ∈RN where M > N (Figure 2a):\n\n                            x = Wtoyf,\n                                f ′ = ReLU(WtoyxT  + b)                                        (1)\n\nHere M counts input features, N counts bottleneck neurons, and Wtoy ∈RN×M maps between them.\nThe model must represent M features using only N dimensions; impossible unless features share neuronal\nresources.\n\nEach input feature fi samples uniformly from [0, 1] with sparsity S (probability of being zero) and importance\nweight ωi. Training minimizes importance-weighted reconstruction error L(f) = PMi=1 ωi∥fi−fi∥2,′    revealing\nhow networks optimally allocate limited bandwidth.\n\nAs sparsity increases, the model packs features into shared dimensions through nearly-orthogonal arrange-\nments (Figure 2b). The interference matrix WtoyWtoyT       reveals this geometric solution: at low compression,\nstrong diagonal with minimal off-diagonal terms; at high compression, substantial off-diagonal interference as\nfeatures share space. These interference terms quantify the distortion networks accept for increased capacity.\nThe ReLU nonlinearity proves essential, suppressing small interference to maintain reconstruction despite\nfeature overlap.\n\nElhage et al. (2022b) proposed measuring “dimensions per feature” as D∗= N/∥Wtoy∥2Frob for analyzing\nuniform importance settings, where the Frobenius norm ∥Wtoy∥2Frob = Pi,j W ij2 aggregates weight magni-\ntudes. While this metric was not intended for general superposition measurement, we nevertheless adopt\nits inverse as a baseline, as it provides the only existing weight-based comparison point for our toy model\nvalidation:\n                                            ∥Wtoy∥2Frob                                          ψFrob =                                                       (2)\n                               N\n\nThis weight-based approach requires knowing the true feature-to-neuron mapping (unavailable in real net-\nworks) and lacks scale invariance (multiplying weights by any constant arbitrarily changes the measure). We\nneed a principled framework quantifying compression without ground truth features.\n\n\n3.2  Extracting Features Through Sparse Autoencoders\n\nReal networks do not reveal their features directly. Instead, we must untangle them from distributed neural\nactivations.  Sparse autoencoders (SAEs) decompose activations into sparse combinations of learned dic-\ntionary elements, effectively reverse-engineering the toy model’s feature representation (Cunningham et al.,\n2024; Bricken et al., 2023).\n\nGiven layer activations x ∈RN, an SAE learns a higher-dimensional sparse code z ∈RD where D > N\n(Figure 2c):\n                                 z = ReLU(Wencx + b)                                        (3)\n\nThe reconstruction combines these sparse features:\n\n                                       D\n                                          x′ = Wdecz = X zidi                                         (4)\n                                                          i=1\nwhere columns di of Wdec form the learned dictionary.\n\nTraining balances faithful reconstruction against sparse activation:\n\n                                  L(x, z) = ∥x −x′∥22 + λ∥z∥1                                     (5)\n\nThe ℓ1 penalty creates explicit competition: the bound on total activation Pi |zi| forces features to justify\ntheir magnitude by contributing to reconstruction. This implements resource allocation where larger |zi|\nindicates greater consumption of the network’s limited representational budget (see Appendix A.2 for rate-\ndistortion derivation).\n\n\n                                              5\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\nSAE design choices. We tie encoder and decoder weights (Wdec = Wenc)T   to enforce features as directions\nin activation space, maintaining conceptual clarity at potential cost to reconstruction (Bricken et al., 2023).\nWeight tying can also prevent feature absorption artifacts (Chanin et al., 2024a). We omit decoder bias\nfollowing Cunningham et al. (2024) for a transparent baseline, accepting slight performance degradation.\nThe ℓ1 regularization provides clean budget semantics, though alternatives like TopK (Gao et al., 2024)\ncould work within our framework.\n\nIf networks truly employ superposition, SAEs should recover the underlying features enabling measurement.\nRecent work shows SAE features causally affect network behavior (Marks et al., 2024), suggesting they cap-\nture genuine computational structure. Our measurement framework remains architecture-agnostic: improved\nSAE variants enhance accuracy without invalidating the theoretical foundation.\n\n\n4  Measuring Superposition Through Information Theory\n\nWe quantify superposition by determining how many neurons would be required to transmit the ob-\nserved feature distribution without interference. Information theory provides a precise answer: Shannon’s\nsource coding theorem establishes that any distribution with entropy H(p) can be losslessly compressed to\neH(p) uniformly-allocated channels. This represents the minimum bandwidth for interference-free transmis-\nsion—the network’s effective degrees of freedom.\n\nWe formalize superposition as the compression ratio ψ = F/N, where N counts physical neurons and\nF = eH(p) measures effective degrees of freedom extracted from SAE activation statistics (Figure 2d)1. When\nψ = 1, the network operates at the lossless boundary. When ψ > 1, features necessarily share dimensions\nthrough interference. For instance, in Figure 2b, 5 features represented in 2 neurons yields ψ = 2.5.\n\nFeature probabilities from resource allocation.  Consider a layer with N neurons whose activations\nhave been processed by an SAE with dictionary size D. Across S samples, the SAE produces sparse codes\nZ = ReLU(WsaeX) ∈RD×S where X ∈RN×S contains the original activations. Each feature’s probability\nreflects its share of total activation magnitude2:\n\n                    PSs=1 |zi,s|      budget allocated to feature i                             pi =          =                                                       (6)\n                PDj=1 PSs=1 |zj,s|    total representational budget\n\nThe SAE’s ℓ1 regularization ensures these allocations reflect computational importance. Features activating\nmore frequently or strongly consume more capacity, with optimal |zi| proportional to marginal contribution\nto reconstruction quality (derivation in Appendix A.2).\n\nEffective features as lossless compression limit.  Shannon entropy quantifies the information content\nof this distribution: H(p) = −Pi pi log pi. Its exponential:\n\n                                 F = eH(p)                                               (7)\n\nmeasures effective degrees of freedom, the minimum neurons needed to encode p without interference. This\nis the network’s lossless compression limit: the feature distribution could be transmitted through F neurons\nwith no information loss. Using fewer than F neurons guarantees interference as features must share dimen-\nsions; using exactly F achieves the interference-free boundary; the actual layer width N determines whether\ncompression remains lossless (N ≥F) or becomes lossy (N < F). The ratio\n\n                                     F\n                                  ψ =                                                    (8)\n                              N\n\n   1In toy models where the input dimension M is known, F ranges from N to M depending on sparsity; in real networks M\nis undefined and we estimate F directly.\n  2Why not measure SAE weights instead of activations? Weight magnitude ∥wi∥indicates potential representation but misses\nactual usage: “dead features” may exist in the dictionary without ever activating. Empirically, a weight-based measure succeeds\nonly in toy models (Figure 3a); and small toy transformer models already require our activation-based approach (Section 6.2).\n\n\n                                              6\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n                                                                                                      regulriztion    scling model     scling dictionry      4.0                    non-liner                                                     ψ1\n                                                                                                                  1.0      3.0                                                 dt points\n                                                                        liner regression\n                                                                    perfect correltion                                                  0.6\n                                                                                                                                                                           ωElhage(Wsae)ψOurs(Wtoy)2.0        r = 0.99          r = 0.88           r = 0.95         r = 0.94                    correltion               ωOurs(Wsae)                                                                                                                  0.2\n      1.0                   wrong scle                                                                                           ωOurs(Z)\n         1.0     2.0     3.0       0.1       0.4      0.7 1.0    2.0    3.0    4.0    2.0  3.0  4.0                 10−3   10−1   101      21      23     25        21      23     25\n                                                                                         coeﬃcient           input dim       dict size / input dim          ψFrob(Wtoy)            ψFrob(Wsae)              ψOurs(Wsae)          ψOurs(Z)              ψ1\n\n             (a) Correlation with observable patterns                  (b) Robustness across hyperparameters\n\nFigure 3: Validation of superposition metrics. (a) Our measure maintains high correlation whether applied\nto toy weights (r = 0.99) or SAE activations (r = 0.94), while the Frobenius norm fails on SAE weights.\n(b) Performance remains stable across ℓ1 regularization, model scale, and dictionary size variations. Shaded\nregions show 95% confidence intervals across 100 model-SAE pairs.\n\n\nthen measures superposition as lossy compression.\n\nWhile the SAE extracts D interpretable features, semantic concepts humans might recognize, our measure\nquantifies F effective features, the interference-free channel capacity required for their activation distribution.\nA network might use D = 1000 interpretable features but need only F = 50 effective features if most activate\nrarely.\n\nOur measure inherits desirable properties from entropy. i.) For any D-component distribution, the output\nstays bounded 1 ≤F(p) ≤D, bounded by single-feature dominance and uniform distribution.  ii.) Unlike\nthreshold-based counting, features contribute according to their information content: rare features matter\nless than common ones, weak features less than strong ones. This enables the interpretation as effective\ndegrees of freedom, beyond “counting features”.\n\nIn practice, we use sufficient samples until convergence (see convergence analysis in Section 6.4). For convo-\nlutional layers, we treat spatial positions as independent samples, measuring superposition across the channel\ndimension (Appendix A.7). While, in general, the data distribution for extracting SAE activations should\nreflect the training distribution, for studying adversarial training’s effect, we evaluate on both clean inputs\nand adversarially perturbed inputs for contrast.\n\nThis framework enables quantifying superposition without ground truth by measuring each layer’s compres-\nsion ratio; how many virtual neurons it simulates relative to its physical dimension.\n\n5  Validation of the Measurement Framework\n\n5.1  Toy Model of Superposition\n\nWe validate our measure using the toy model of superposition (Elhage et al., 2022b), where interference\npatterns are directly observable.  This controlled setting tests whether sparse autoencoders can recover\naccurate feature counts from superposed representations.\n\nFollowing Elhage et al. (2022b), we generate 100 toy models with sparsity S ∈[0.001, 0.999]. Each model\ncompresses 20 features through a 5-neuron bottleneck, with importance weights decaying as ωi = 0.7i.\nAfter training to convergence, we extract 10,000 activation samples and train SAEs with 40-dimensional\ndictionaries (8× expansion) and ℓ1 coefficient 0.1. This two-stage process mimics real-world measurement\nwhere ground truth remains unknown.\n\nValidation strategy.  Our validation proceeds in two steps. First, we establish reference values by mea-\nsuring superposition directly from Wtoy, where the interference matrix WtoyWtoyT       reveals compression lev-\nels:  diagonal dominance indicates orthogonal features; off-diagonal terms show interference (Figure 2b).\nBoth our entropy-based measure and the Frobenius norm baseline (Eq. 2) achieve near-perfect correlation\n(r = 0.99 ± 0.01) when applied to toy model weights, confirming both track these observable patterns\n(Figure 3a).\n\n\n                                               7\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n                                                                                    h=16                  ψ1 = 0.01                 60                                                                                h=32     1.0                   no dropout    29       ψ1 = 0.1                                                            h=64\n             =                    1                                                                                   h=128                                                               0.9                                           neurons             =                    10                  ψ1 count 27       ψ1                                                         count 40                                                                                                                     fetures\n    25                                                          0.8     h=16        semi-stble region                                                                                                             ccurcy          h=32                dropout\n                                                                                                    h=64                                                                                                   keep most  feture 23                                                                 feture 20                             0.7                                                                                                          h=128\n               27       25                                                                                    0.4                       29             0.0                                                 0.4                                                            0.8          0.0                                                                                               0.8                        importntfetures         dictionry                      size                                    dropout                                                    rtio                                                                 dropout                                                                                       rtio\n\n\n  (a) Dictionary scaling         (b) Dropout reduces features and performance        (c) Dropout effect on features\n\nFigure 4: Measurements on multi-task sparse parity dataset. (a) Dictionary scaling plateaus with proper\nregularization (ℓ1 ≥0.1), validating intrinsic structure measurement. Weak regularization (ℓ1 = 0.01) shows\nunbounded growth through arbitrary subdivision. (b) Dropout monotonically reduces effective features and\naccuracy. (c) Capacity-dependent response: larger networks show reduced sensitivity while narrow networks\nexhibit sharp feature reduction, distinguishing polysemanticity (neurons encoding multiple features) from\nsuperposition (compression beyond lossless limit).\n\n\nSecond, we test whether each metric recovers these reference values when given only SAE outputs, the\nrealistic scenario for measuring real networks. Here the Frobenius norm fails catastrophically on SAE weights,\nproducing nonlinear relationships and incorrect scales (0.1–0.7 versus expected 1–4); the ℓ1 regularization\nfundamentally alters weight statistics. Our activation-based approach maintains strong correlation (r =\n0.94 ± 0.02) with the reference values even through the SAE bottleneck.\n\nHyperparameter stability. We test sensitivity across three axes: ℓ1 strength (10−3 to 101), model scale\n(8–32 input dimensions), and dictionary expansion (2× to 32×). Figure 3b shows stable performance across\nmost configurations. Correlation degrades when extreme regularization (ℓ1 = 10) suppresses features, when\ndictionaries lack capacity to represent the feature set, when toy models are too small or too large to train\nreliably, or when very large dictionaries enable feature splitting (see Section 5.2). These failure modes reflect\nlimitations of the toy model or SAE training rather than the measure itself.\n\n\n5.2  Dictionary Scaling Convergence\n\nMeasuring a natural coastline with a finer ruler yields a longer measurement; potentially without bound\n(Mandelbrot, 1967). As SAE dictionaries grow, might we discover arbitrarily many features at finer scales?\n\nWe test convergence using multi-task sparse parity (Michaud et al., 2023) (3 tasks, 4 bits each) where ground\ntruth bounds meaningful features. Networks with 64 hidden neurons trained across dictionary scales (0.5×\nto 16× hidden dimension) and ℓ1 strengths (0.01 to 10.0).\nFigure 4a reveals two regimes. With appropriate regularization (ℓ1 ≥0.1), feature counts plateau despite\ndictionary expansion, indicating we measure the network’s representational structure and not arbitrary\ndecomposition (i.e.  feature splitting (Chanin et al., 2024b)). Weak regularization (ℓ1 = 0.01) permits\ncontinued growth across all tested scales—this reflects feature splitting rather than genuine superposition,\nwhere the SAE decomposes single computational features into spurious fine-grained components. Excessive\nregularization (ℓ1 = 10.0) suppresses features entirely.\n\nThe dependence on dictionary size means absolute counts vary with SAE architecture, but comparative\nmeasurements remain valid: networks analyzed under identical configurations yield meaningful relative dif-\nferences, even as changing those configurations shifts all measurements systematically.\n\n\n6  Applications and Findings\n\nWe measure superposition across four neural compression phenomena: capacity constraint under dropout\n(Section 6.1), algorithmic tasks that resist superposition despite compression (Section 6.2), developmental\n\n\n                                              8\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n   transformer                trined-from-scrtch     compressed            trined-from-scrtch     compressed\n                               F 30   F = N   compression    F 20                   F = N                                                                               sorting\n                                                         count                                                                                           count 20                                             lyer\n                                                                     ccurcy                                                                                                                                                                                                                    < 1                                10                                                                                 lyer                                                                                           1  ReLU           ttention          MLP                                                                                                                                                                                                                       1\n                                                                                 lyer                                                                          2                                                                                             < 1                           10                                              ccurcylyerlyer                                                                                                                                                                               2       W−  W−W  W−  W                               feture                          lyer 3        reversing              feture                                                          lyer 3\n    W                         W− input                                                                             lyer 4          residul strem                                                                                 output       0   10   20  30  40    0   10   20  30  40        0  10  20  30  40  50    10  20  30  40  50                                             residul strem dimension N                    residul strem dimension N\n\n     (a) Architecture              (b) Task: sequence reversal                    (c) Task: sequence sorting\n\nFigure 5: Algorithmic tasks under compression. (a) Compression architecture projects activations through\nReLU(W ⊤W x) to force features into fewer dimensions. (b) Sequence reversal: progressive compression from\nnative 45D increases superposition from ψ ≈0.3 toward ψ = 1 (leftmost to rightmost points approaching\nF=N line). Once reaching the F=N boundary, further compression causes performance degradation and\nthen collapse (× markers) rather than superposition beyond ψ = 1. (c) Sorting exhibits identical dynamics\nwith F ≈N throughout compression. Both tasks resist genuine superposition (ψ > 1), operating at the\nlossless limit where each neuron encodes one effective feature, likely due to lack of input sparsity for sequence\noperations.\n\n\ndynamics during learning transitions (Section 6.3), and layer-wise representational organization in language\nmodels (Section 6.4).\n\nEach finding here is a preliminary, exploratory analysis on specific architectures and tasks. Our primary con-\ntribution remains the measurement tool itself. These findings illustrate its potential utility while generating\ntestable hypotheses for future systematic investigation across broader experimental conditions.\n\n\n6.1  Dropout Reduces Features Through Redundant Encoding\n\nWe investigate how dropout affects feature organization using multi-task sparse parity (3 tasks, 4 bits each)\nwith simple MLPs across hidden dimensions h ∈{16, 32, 64, 128} and dropout rates [0.0, 0.1, ..., 0.9].\n\nMarshall & Kirchner (2024) showed dropout induces polysemanticity through redundancy: features must\ndistribute across neurons to survive random deactivation. One might expect this redundancy to increase\nmeasured superposition. Instead, dropout monotonically reduces effective features by up to 50% (Figure 4b).\n\nWe propose this reflects the distinction between polysemanticity and superposition (Figure 4c).  If dropout\nforces each feature to occupy multiple neurons for robustness, this redundant encoding would consume\ncapacity, leaving room for fewer total features within the same dimensional budget. Under this interpretation,\nnetworks respond by pruning less essential features, consistent with Scherlis et al. (2023)’s competitive\nresource allocation framework.\n\nThe capacity dependence supports this account:  larger networks show reduced dropout sensitivity while\nnarrow networks exhibit sharp feature reduction, suggesting capacity constraints mediate the effect.\n\n\n6.2  Algorithmic Tasks Resist Superposition Despite Compression\n\nTracr compiles human-readable programs into transformer weights with known computational structure\n(Lindner et al., 2023). We examine sequence reversal (“123” →“321”) and sorting (“213” →“123”), com-\nparing compiled models at their original dimensionality (compression factor 1×) against compressed variants\nand transformers trained from scratch with matching architectures.\n\nFollowing Lindner et al. (2023), we compress models by projecting residual stream activations through\nlearned compression matrices. Our compression scheme (Figure 5a) applies ReLU(W ⊤W x) where W ∈\nRN×M, compressing from originally M dimensions to N. The ReLU activation, absent in the original Tracr\ncompression, allows small interference terms to cancel out following the toy model rationale (Elhage et al.,\n2022b).\n\n\n                                              9\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n )                              mlp lyer 1                   1.0   10                     mlpresidul   6                                                                               20                                                                                                                                                                                                                                                            phse      170                                    ttention                                                                                                                                                                                    phse −1000                                                                                      40 ( 8                            embedding                             4                                                                                                                                                                                                                                                                                                                                                                                          feture                                                                                                                         LLC                                                                                                                              loss                                                                  0.5           trin                                                                                                                                                           loss\n                                                                                                                              test                                                                                                                             loss                                                                                                                                                                                                     feture                                                                                                                                                                                       count                                                                                      30   count 6                                                                                                                                                                                                            ccurcy                                                                                                                       trin                                                                                                     ccurcy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        trnsition      160                                                                                                                                                        LLC                                                                                                                                                                                                                                                                                                                                                                        trnsition     10    4                       2                                                                                                                              test                                                                                                    ccurcy                                                                                                                                                                                                                                                                              count                                                                                                                                                                   feture                                                                                                                                                         count                                                                                                             150\n                                                                                                                             LLC    feture 2                                                            0.0                     0       20        1    2    3    4    5    6  1   10   102   103   104            0         100                 0         100                   lyer             number of smples                 epoch                          epoch\n\n           (a) Pythia-70M layer analysis                                (b) Grokking dynamics\n\nFigure 6: (a) Non-monotonic feature organization across Pythia-70M. MLP layer 1 peaks at 10,000 features\n(20× neurons). Convergence analysis shows saturation after 2 × 104 samples. (b) Feature dynamics during\ngrokking on modular arithmetic. Sharp consolidation at generalization transition (epoch 60) follows smoother\nLLC decay. Strong correlation (r = 0.908, p < 0.001) with LLC suggests feature count functions as a measure\nof model complexity.\n\n\n\nThe compression dynamics reveal limits on superposition in these algorithmic tasks (Figure 5b, 5c). Both\ncompiled Tracr models and transformers trained from scratch converge to 12 features for reversal and 10 for\nsorting3–far below their original compiled dimensions (45D for reversal), revealing substantial dimensional\nredundancy in Tracr’s compilation.\n\nAs compression reduces dimensions from 45D toward the task-intrinsic boundary, superposition increases\nfrom ψ ≈0.3 toward ψ = 1.  However, compression stops increasing superposition once models reach\nthe F = N diagonal: further dimensional reduction causes linear drop in effective features and eventually\nperformance collapse (× markers) rather than superposition beyond ψ = 1, resisting genuine superposition\n(ψ > 1) entirely.\n\nThis resistance likely stems from algorithmic tasks violating the sparsity assumption required for lossy com-\npression (Elhage et al., 2022b). The toy model of superposition requires features to activate sparsely across\ninputs: most features remain inactive on most samples, keeping interference managable. Algorithmic tasks\nbreak this assumption; sequence operations require consistent activation patterns across inputs. Without\nsparsity, interference becomes destructive rather than enabling compression. While we originally anticipated\nthis setting would enable controlled validation across superposition levels, the systematic F ≈N tracking,\ncoupled with performance collapse when dimensions drop below this boundary, instead provides indirect\nevidence that our measure captures genuine capacity constraints, detecting minimal superposition as the\nsparsity prerequisite fails.\n\n\n6.3  Capturing Grokking Phase Transition\n\nGrokking (sudden perfect generalization after extended training on algorithmic tasks) provides an ideal\ntestbed for developmental measurement (Power et al., 2022). We investigate whether feature count dynamics\ncan detect this phase transition and how they relate to the Local Learning Coefficient (LLC) from singular\nlearning theory (Hoogland et al., 2024).\n\nWe train a two-path MLP on modular arithmetic (a+b) mod 53. Figure 6b reveals distinct dynamics: while\nLLC shows initial proliferation followed by smooth decay throughout training, our feature count exhibits\nsharp consolidation precisely at the generalization transition.\n\nThis pattern suggests the measures capture different aspects of complexity evolution. During memorization,\nthe model employs numerous superposed features to store input-output mappings. The sharp consolida-\ntion coincides with algorithmic discovery, where the model reorganizes from distributed lookup tables into\ncompact representations that capture the modular arithmetic rule (Nanda et al., 2023). Strong correlation\n\n\n   3While we generally recommend comparative interpretation due to measurement limitations (Section 8), the systematic\nF = N boundary tracking and performance decline when violated suggest our measure may provide meaningful absolute\neffective feature counts in sufficiently constrained computational settings.\n\n\n                                              10\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n     2                                                                      2                                                                                                                    MNIST                                                                                                                                                                                                                                                   MNIST rtio  1                                                                                                      rtio  1                                         bseline (ψ = 0 )                                                                                                                                            bseline (ψ = 0 )\n count 0.5    2-clss        3-clss        10-clss                count 0.5    2-clss        3-clss        10-clss     2                                                                2\n     1              conv 1                                                        1       liner 1 feture         conv 2      clen                                                                                               feture           liner 2      clen     0.5       liner       dversril                                                    0.5       liner 3      dversril                                                                                                                                                                                                                                                                                      FshionMNIST                                                                                                                                                                                                                                                                                                    FshionMNIST         0.0  0.1   0.2   0.3      0.1   0.2   0.3      0.1   0.2   0.3                0.0  0.1   0.2   0.3      0.1   0.2   0.3      0.1   0.2   0.3\n           dversril trining strength ()ψ                      dversril trining strength ()ψ\n\n         (a) 3-layer CNN on (Fashion-)MNIST                      (b) 3-layer MLP on (Fashion-)MNIST\n\nFigure 7: Higher task complexity shifts adversarial training’s effect from feature expansion toward reduction.\nEach panel shows results varying dataset (MNIST top, Fashion-MNIST bottom) and number of classes (2,\n3, 10).  (a) CNNs show clear complexity-dependent transitions:  simple tasks enable feature expansion\nwhile complex tasks (10 classes) force reduction below baseline. (b) MLPs exhibit similar patterns with\nmore pronounced layer-wise variation. Fashion-MNIST consistently amplifies the reduction effect compared\nto MNIST, suggesting that representational demands drive defensive strategies beyond mere class count.\nDashed lines:  clean data; solid lines:  adversarial examples.  Feature count ratios normalized to ϵ = 0\nbaseline. Error bars show standard error across 3 seeds.\n\n\n(r = 0.908, p < 0.001) between feature count and LLC positions superposition measurement as a develop-\nmental tool for detecting emergent capabilities through their information-theoretic signatures.\n\n\n6.4  Layer-wise Organization in Language Models\n\nWe analyze Pythia-70M using pretrained SAEs from Marks et al. (2024), measuring feature counts across all\nlayers and components. Convergence analysis (Figure 6a) shows saturation after 2 × 104 samples. Feature\nimportance follows power-law distributions: while 21,000 SAE features activate for MLP 1, our entropy-based\nmeasure yields 5,600 effective features, automatically downweighting rare activations.\n\nMLPs store the most features, followed by residual streams, with attention maintaining minimal counts,\nconsistent with MLPs as knowledge stores and attention as routing (Geva et al., 2021). Features grow in\nearly layers (MLP 1 achieves 20× compression), compress through middle layers, then re-expand before final\nconsolidation.\n\nThis non-monotonic trajectory parallels intrinsic dimensionality studies (Ansuini et al., 2019): both reveal\n“hunchback” patterns peaking in early-middle layers.  Intrinsic dimensionality measures geometric mani-\nfold complexity (minimal dimensions describing activation structure), while we count effective information\nchannels (minimal dimensions for lossless encoding), both measuring aspects of representational complexity.\n\n\n7  Connection between Superposition and Adversarial Robustness\n\nTesting the superposition-vulnerability hypothesis.  The superposition-vulnerability hypothesis pro-\nposed by Elhage et al. (2022b) predicts that adversarial training should universally reduce superposition, as\nnetworks trade representational efficiency for orthogonal, robust features. We test this prediction systemat-\nically across diverse architectures and conditions, finding that the direction of the effect—expansion versus\nreduction—depends on task complexity and network capacity.\n\nWe employ PGD adversarial training (Madry et al., 2018) across architectures ranging from single-layer to\ndeep networks (MLPs, CNNs, ResNet-18) on multiple datasets (MNIST, Fashion-MNIST, CIFAR-10). Task\ncomplexity varies through both classification granularity (2, 3, 5, 10 classes) and dataset difficulty. Network\ncapacity varies through hidden dimensions (8–512 for MLPs), filter counts (8–64 for CNNs), and width\nscaling (1/4×–2× for ResNet-18). For convolutional networks, we measure superposition across channels\n\n\n                                              11\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n      2                          N          N          N                   convlyer 1                                                       2                                                                                                              lyer                                                                                                                                                                                          2                                                                                                                                                                   1-lyer  rtio  1     h=8                                                                                                       rtio                                                                                                                                                                                                                  lyer                                                                                                                                                                                 3               h=32\n                                                                                                                                                                                                                  lyer 4      0.5     h=128                                                     MLP                     2              4  count               h=512 2-clss      3-clss       10-clss           1      2                                                                                            count\n                                                                0.5                                                                                                  clen   feture  1     c=8c=16                                                                                                                               1-lyer                                                                                                                                                feture                                                                                                             dversril                            clen      0.5     c=32                              dversril                                         CNN               c=64                                                                          0.00     0.01     0.02     0.03        0.01     0.02     0.03        0.01     0.02     0.03         0.0  0.1   0.2   0.3      0.1   0.2   0.3      0.1   0.2   0.3           dversril trining strength ()ψ                         dversril trining strength ()ψ\n\n    (a) Widening 1-layer NNs on MNIST                      (b) Narrowing ResNet-18 on CIFAR-10\n\nFigure 8: Higher network capacity shifts adversarial training’s effect from feature reduction toward expan-\nsion. (a) Single-layer networks on MNIST demonstrate capacity-dependent transitions: MLPs with hidden\ndimensions h ∈{8, 32, 128, 512} (top) and CNNs with filter counts c ∈{8, 16, 32, 64} (bottom) show that\nnarrow networks reduce features while wide networks expand them across task complexities. (b) ResNet-18\non CIFAR-10 with width scaling (1×, 1/2×, 1/4×) reveals layer-wise specialization: early layers reduce fea-\ntures while deeper layers (layer 3–4) expand dramatically, with this pattern dampening as width decreases.\nDashed lines: clean data; solid lines: adversarial examples. Feature count ratios normalized to baseline.\n\n\n\nby reshaping activation tensors to treat spatial positions as independent samples (see Appendix A.7 for\ndetails). All SAEs use 4× dictionary expansion with ℓ1 = 0.1. Measurements on adversarial examples match\nthe training distribution; models trained with ϵ = 0.2 are evaluated on ϵ = 0.2 attacks.\n\n\nStatistical methodology.  To quantify adversarial training effects, we extract normalized slopes repre-\nsenting how feature counts change per unit increase in adversarial training strength (ϵ ∈{0.0, 0.1, 0.2, 0.3}).\nPositive slopes indicate adversarial training increases features; negative slopes indicate reduction. For each\nexperimental condition, we fit linear regressions to feature counts across epsilon values, pooling clean and\nadversarial observations to increase statistical power. These slopes are normalized by baseline (ϵ = 0) feature\ncounts, making effects comparable across layers with different absolute scales.\n\nSince networks contain multiple layers, we aggregate layer-wise measurements using parameter-weighted aver-\naging, where layers with more parameters receive proportionally higher weight. This reflects the assumption\nthat computationally intensive layers better represent overall network behavior. For simple architectures, pa-\nrameter counts include all weights and biases; for ResNet-18, we implement detailed counting that accounts\nfor convolutions, batch normalization, and skip connections.\n\n\n   Testing Adversarial Training Effects on Superposition\n\n  We test three formal hypotheses:\n\n     • H1 (Universal Reduction): Adversarial training uniformly reduces superposition across\n               all conditions, directly testing Elhage et al. (2022b)’s original prediction.\n\n     • H2 (Complexity ↓): Higher task complexity shifts adversarial training’s effect from fea-\n            ture expansion toward reduction. We encode complexity ordinally (2-class=1, 3-class=2,\n            5-class=3, 10-class=4) and test for negative linear trends in the adversarial training slope.\n\n     • H3 (Capacity ↑): Higher network capacity shifts adversarial training’s effect from feature\n           reduction toward expansion. We test for positive log-linear relationships between capacity\n          measures and adversarial training slopes.\n\n\n\nAll statistical tests use inverse-variance weighting to account for measurement uncertainty, with random-\neffects meta-analysis when significant heterogeneity exists across conditions.\n\n\n                                              12\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n         MLP           CNN               1-layer MLP            1-layer CNN            ResNet-18\n                                                  2                        1.0                     80\n               bseline 2                    2                      1                        0.5\n      per                                             0                      0                     40 slope  0                    0                                                                      −0.5                                       −1                   normlized                                                                                          0\n       2     3     5    10  2     3     5    10    8    32   128  512    8    16   32    64    1/4   1/2   1     2\n         number of clsses        number of clsses          hidden dimension           chnnel dimension          lyer width multiplier\n\n (a) Complexity ↓(number classes + dataset)         (b) Complexity ↓Capacity ↑              (c) Capacity ↑\n\nFigure 9: Statistical analysis of adversarial training effects on superposition. Normalized slopes quantify fea-\nture count changes per unit adversarial strength ϵ; positive slopes indicate adversarial training increases fea-\ntures, negative slopes indicate reduction. (a) Task complexity (number of classes + dataset difficulty) shows\nconsistent negative relationship with slopes: higher complexity yields more negative slopes. Fashion-MNIST\n(green) produces systematically lower slopes than MNIST (yellow), consistent with its greater difficulty. (b)\nSingle-layer networks on MNIST show capacity-dependent transitions: narrow networks (8–32 units) have\nnegative slopes regardless of task complexity, while wide networks (128–512 units) have positive slopes. (c)\nResNet-18 on CIFAR-10 demonstrates log-linear scaling: wider networks show dramatically more positive\nslopes. Error bars show standard errors.\n\n\n\nComplexity shifts adversarial training toward feature reduction (H2 supported).  Contrary to\nH1’s prediction of universal reduction, adversarial training produces bidirectional effects whose direction\ndepends systematically on task complexity (Figures 7 and 9a). Our meta-analysis reveals significant het-\nerogeneity across conditions (Q = 8.047, df = 3, p = 0.045), necessitating random-effects modeling. The\ncombined effect confirms H2: a negative relationship between task complexity and the adversarial training\nslope (slope = −0.745 ± 0.122, z = −6.14, p < 0.001), meaning higher complexity shifts the effect from\nexpansion toward reduction.\n\nBinary classification consistently yields positive slopes, with feature counts expanding up to 2× baseline.\nNetworks appear to develop additional defensive features when task demands are simple. Ten-class problems\nshow negative slopes, with feature counts decreasing by up to 60%, particularly in early layers. Three-class\ntasks exhibit intermediate behavior with inverted-U curves: moderate adversarial training (ϵ = 0.1) initially\nexpands features before stronger training (ϵ = 0.3) triggers reduction.\n\nDataset difficulty amplifies these effects. Fashion-MNIST produces systematically more negative slopes than\nMNIST (mean difference = −1.467 ± 0.156, t(7) = −2.405, p = 0.047, Cohen’s d = −0.85), consistent with\nits design as a more challenging benchmark (Xiao et al., 2017). This suggests that representational demands,\nbeyond mere class count, drive defensive strategies.\n\nLayer-wise patterns differ between architectures: MLP first layers reduce most while CNN second layers\nreduce most. We lack a mechanistic explanation for this divergence.\n\n\nCapacity shifts adversarial training toward feature expansion (H3 supported).  Network capacity\nexhibits a positive relationship with the adversarial training slope, strongly supporting H3 (Figures 8, 9b,\nand 9c). Single-layer networks demonstrate clear capacity thresholds (meta-analytic slope = 0.220 ± 0.037,\nz = 5.90, p < 0.001). Networks with minimal capacity (8 hidden units for MLPs, 8 filters for CNNs) show\nnegative slopes—reducing features across all task complexities—while high-capacity networks (512 units/64\nfilters) show positive slopes, expanding features even for 10-class problems.\n\nThis capacity dependence scales dramatically in deep architectures.  ResNet-18 on CIFAR-10 exhibits a\nstrong log-linear relationship between width multiplier and adversarial training slopes (slope = 31.0 ± 2.0\nper log(width), t(2) = 15.7, p = 0.004, R2 = 0.929). An 8-fold width increase (0.25× to 2×) produces a\n65-unit change in normalized slope. At minimal width (0.25×), adversarial training barely affects feature\ncounts; at double width, networks show massive feature expansion with slopes approaching 80.\n\n\n                                              13\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nThe layer-wise progression in ResNet-18 reveals hierarchical specialization: early layers (conv1, layer1) reduce\nfeatures by up to 50%, middle layers remain stable, while deep layers (layer3, layer4) expand up to 4×.\nSystematically narrowing the network dampens this pattern: at 1/4 width, late-layer expansion vanishes\nwhile early-layer reduction persists but weakens. This could reflect vulnerability hierarchies, where early\nlayers processing low-level statistics are easily exploited by imperceptible perturbations, necessitating feature\nreduction, while late layers encoding semantic information can safely expand their representational repertoire.\n\nTwo regimes of adversarial response.  Our findings reveal a more nuanced relationship between su-\nperposition and adversarial vulnerability than originally theorized. Rather than universal feature reduction,\nadversarial training operates in two distinct regimes determined by the ratio of task demands to network\ncapacity.\n\n\n    Bifurcation Driven by Task Complexity to Network Capacity Ratio\n\n   Adversarial training’s effect on superposition depends on the ratio of task demands to network ca-\n    pacity:\n\n  • Abundance regime (low complexity / high capacity): Adversarial training increases effective\n       features. Networks add defensive features, achieving robustness through elaboration.\n\n  • Scarcity regime (high complexity / low capacity):  Adversarial training decreases  effective\n       features.  Networks prune to fewer, potentially more orthogonal features, as predicted by the\n       superposition-vulnerability hypothesis.\n\n\n\nUnexplained patterns.  Several patterns in our data remain unexplained. We observe non-monotonic\ninverted-U curves where moderate adversarial training (ϵ = 0.1) expands features while stronger training (ϵ =\n0.3) reduces them below baseline. The gap between clean and adversarial feature counts varies unpredictably;\nsometimes negligible, sometimes substantial. Some results contradict our complexity hypothesis, with 2-class\nMLPs occasionally showing lower feature counts than 3-class. CNN experiments consistently yield stronger\nstatistical significance (p < 0.02) than equivalent MLP experiments (p ≈0.09) for unknown reasons.\n\nImplications for interpretability.  Our findings complicate simple accounts of why robust models often\nappear more interpretable (Engstrom et al., 2019).  If interpretability benefits arose purely from reduced\nrepresentational complexity, we would expect universal feature reduction under adversarial training. The\nexistence of an abundance regime where feature counts increase suggests alternative mechanisms: perhaps\nnon-interpretable shortcut features are replaced by richer, more human-aligned representations, or perhaps\ninterpretability benefits are confined to the scarcity regime. Resolving this requires interpretability metrics\nbeyond the scope of our current framework.\n\nThe bidirectional relationship between robustness and superposition suggests that achieving robustness with-\nout capability loss may require ensuring sufficient capacity for defensive elaboration. While our experiments\ndemonstrate that increased robustness can coincide with either increased or decreased superposition depend-\ning on the regime, establishing the exact causal connection between superposition and robustness remains\nan important direction for future work.\n\n\n8  Limitations\n\nOur superposition measurement framework is limited by its dependence on sparse autoencoder quality,\ntheoretical assumptions about neural feature representation, and should be interpreted as proxy for repre-\nsentational complexity rather than literal feature count:\n\nSparse autoencoder quality.  Our approach inherently depends on sparse autoencoder feature extrac-\ntion quality. While recent architectural advances (gated SAEs (Rajamanoharan et al., 2024), TopK variants\n\n\n                                              14\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n(Gao et al., 2024), and end-to-end training (Braun et al., 2024)) have substantially improved feature recov-\nery, fundamental challenges remain. SAE training exhibits sensitivity to hyperparameters, particularly ℓ1\nregularization strength and dictionary size, with different initialization or training procedures potentially\nyielding different feature counts for identical networks. Ghost features, i.e. SAE artifacts without computa-\ntional relevance (Gao et al., 2024), can artificially inflate measurements, while poor reconstruction quality\nmay deflate them.\n\nAssumptions on feature representation.  Our framework rests on several assumptions that real net-\nworks systematically violate. The linear representation assumption (that features correspond to directions\nin activation space) has been challenged by recent discoveries of circular feature organization for tempo-\nral concepts (Engels et al., 2024) and complex geometric structures beyond simple directions (Black et al.,\n2022). Our entropy calculation assumes features contribute independently to representation, but neural\nnetworks exhibit extensive feature correlations, synergistic information where feature combinations provide\nmore information than individual contributions, and gating mechanisms where some features control others’\nactivation. The approximation that sparse linear encoding captures true computational structure breaks\ndown in hierarchical representations where low-level and high-level features are not substitutable, and in\nnetworks with substantial nonlinear feature interactions that cannot be decomposed additively.\n\nComparative rather than absolute count.  Our measure quantifies effective representational diversity\nunder specific assumptions rather than providing literal feature counts. This creates several interpretational\nlimitations. The measure exhibits sensitivity to the activation distribution used for measurement. SAE\ntraining distributions must match the network’s operational regime to avoid systematic bias. Feature granu-\nlarity remains fundamentally ambiguous: broader features may decompose into specific ones in wider SAEs,\ncreating uncertainty about whether we’re discovering or creating features. Our single-layer analysis poten-\ntially misses features distributed across layers through residual connections or attention mechanisms. Most\ncritically, we measure the effective alphabet size of the network’s internal communication channel rather\nthan counting distinct computational primitives, making comparative rather than absolute interpretation\nmost appropriate.\n\nThe limitations largely reflect active research areas in sparse dictionary learning and mechanistic inter-\npretability. Each advance in SAE architectures, training procedures, or theoretical understanding directly\nbenefits measurement quality. Within its scope—comparative analysis of representational complexity un-\nder sparse linear encoding assumptions—the measure enables systematic investigation of neural information\nstructure previously impossible.\n\n\n9  Future Work\n\nCross-model feature alignment.  Following Anthropic’s crosscoder approach (Templeton et al., 2024),\ntraining joint SAEs across clean and adversarially-trained models would enable direct feature comparison.\nThis could reveal whether the abundance regime involves feature elaboration (creating defensive variants)\nversus feature replacement (substituting vulnerable features with robust ones).\n\nMulti-scale and cross-layer measurement.  Current layer-wise analysis may miss features distributed\nacross layers through residual connections. Matryoshka SAEs (Bussmann et al., 2025) already capture fea-\nture hierarchies at different granularities within single layers; extending this to cross-layer analysis could\nreveal how abstract features decompose into concrete features through network depth. Applying our en-\ntropy measure at each scale and depth would quantify information organization across both dimensions.\nImplementation requires developing new SAE architectures that span multiple layers.\n\nFeature co-occurrence and splitting.  Our independence assumption breaks when features consistently\nco-activate, yet this structure may be crucial for resolving feature splitting across dictionary scales. As\nwe expand SAE dictionaries, single computational features can decompose into multiple SAE features -\nartificially inflating our count.  Features that always co-occur likely represent such spurious decomposi-\ntions rather than genuinely independent components. We initially attempted eigenvalue decomposition of\n\n\n                                              15\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nfeature co-occurrence matrices to identify such dependencies, but this approach faces a fundamental rank\nconstraint: covariance matrices have rank at most N (the neuron count), making it impossible to detect\nsuperposition beyond the physical dimension. Alternative approaches include mutual information networks\nbetween features or hierarchical clustering of co-occurrence patterns. Combining these with Matryoshka\nSAEs’ multi-scale dictionaries could reveal which features remain coupled across granularities (likely repre-\nsenting single computational primitives) versus those that split independently (likely representing distinct\nfeatures). This would provide a principled solution to the dictionary scaling problem: count only features\nthat disentangle across scales.\n\nCausal intervention experiments.  While we demonstrate correlation between adversarial training and\nsuperposition changes, establishing causality requires targeted interventions:  i.)  artificially constraining\nsuperposition via architectural modifications (e.g., softmax linear units (Elhage et al., 2022a)) then measuring\nrobustness changes; ii.)  directly manipulating feature sparsity in synthetic tasks; iii.)  using mechanistic\ninterpretability tools to trace how specific features contribute to adversarial vulnerability.\n\nValidation at scale.  Testing our framework on contemporary architectures (billion-parameter LLMs,\nVision Transformers, diffusion models) would reveal whether findings generalize. Scale might expose new\nphenomena in adversarial training: very large models may escape capacity constraints entirely, or scaling\nlaws might reveal limits on compression efficiency while maintaining robustness.  If validated, our metric\ncould guide architecture search for interpretable models by incorporating superposition measurement into\ntraining objectives or architecture design.\n\nConnection to model compression.  Our lossy compression perspective parallels findings in model com-\npression research (Pavlitska et al., 2023). Both superposition (internal compression) and model compression\n(parameter reduction) force networks to optimize information encoding under constraints. Formalizing this\nconnection through rate-distortion theory could yield theoretical bounds on the robustness-compression\ntradeoff, explaining when compression helps versus hurts.\n\n\n10  Conclusion\n\nThis work provides a precise, measurable definition of superposition. Previous accounts characterized su-\nperposition qualitatively, as networks encoding “more features than neurons”, we formalize it as lossy com-\npression: encoding beyond the interference-free limit.  Applying Shannon entropy to sparse autoencoder\nactivations yields the effective degrees of freedom: the minimum neurons required for lossless transmission of\nthe observed feature distribution. Superposition occurs when this count exceeds the layer’s actual dimension.\n\nThe framework enables testing previously untestable hypotheses. The superposition-vulnerability hypoth-\nesis (Elhage et al., 2022b) predicts that adversarial training should universally reduce superposition as\nnetworks trade representational efficiency for orthogonality. We find instead that the effect depends on the\nratio of task demands to network capacity: an abundance regime where simple tasks permit feature expan-\nsion, and a scarcity regime where complexity forces reduction. By grounding superposition in information\ntheory, this work makes quantitative what was previously only demonstrable in toy settings.\n\n\n\n\n\n                                              16\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\nAuthor Contributions\n\nL.B. conceived the project, developed the theoretical framework, designed and conducted all experiments\nexcept grokking, performed statistical analyses, and wrote the manuscript. Z.T.-K. conducted the grokking\nexperiment and LLC comparison. R.S. and E.G. supervised the research.\n\nAcknowledgments\n\nWe thank Daniel Sadig for insightful discussions on adversarial training mechanisms and Hamed Karimi for\ndetailed feedback on the manuscript that improved clarity and presentation. We are grateful to Jacqueline\nBereska for valuable suggestions on manuscript organization and prioritization. We thank the anonymous\nTMLR reviewers for their rigorous feedback, particularly on statistical methodology and theoretical founda-\ntions, which substantially strengthened this work.\n\nPart of this research was conducted during L.B.’s visit to the Trustworthy AI Lab (TAILab) at Toronto\nMetropolitan University. We are grateful for the stimulating research environment that facilitated the\ndevelopment of the core conceptual framework.\n\n\n\n\n\n                                              17\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\nReferences\n\nKartik Anand, Ginestra Bianconi, and Simone Severini. The shannon and the von neumann entropy of\n  random networks with heterogeneous expected degree. Phys. Rev. E, March 2011. 23\n\nAlessio Ansuini, Alessandro Laio, Jakob Macke, and Davide Zoccolan. Intrinsic dimension of data represen-\n  tations in deep neural networks. NeurIPS, May 2019. 3, 11\n\nMaximilian Augustin, Alexander Meinke, and Matthias Hein.  Adversarial robustness on in- and out-\n  distribution improves explainability. ECCV, July 2020. 1\n\nKola Ayonrinde, Michael T. Pearce, and Lee Sharkey.  Interpretability as compression: Reconsidering sae\n  explanations of neural activations with mdl-saes. CoRR, October 2024. 4\n\nLeonard Bereska and Efstratios Gavves.  Mechanistic interpretability for ai safety — a review. TMLR,\n  August 2024. 2, 3\n\nSid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Carlos Ramón\n  Guevara, Beren Millidge, Gabriel Alfour, and Connor Leahy. Interpreting neural networks through the\n  polytope lens. CoRR, November 2022. 15, 24\n\nAkhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, and Luca Daniel.\n  Proper network interpretability helps adversarial robustness in classification. ICML, October 2020. 1\n\nDan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey. Identifying functionally important\n  features with end-to-end sparse dictionary learning. ICML MI Workshop, May 2024. 15\n\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L.\n  Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\n  Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E. Burke,\n  Tristan Hume, Shan Carter, Tom Henighan, and Chris Olah. Towards monosemanticity: Decomposing\n  language models with dictionary learning. Transformer Circuits Thread, October 2023. 2, 5, 6\n\nBart Bussmann, Patrick Leask, and Neel Nanda. Batchtopk sparse autoencoders. CoRR, 2024. 3\n\nBart Bussmann, Noa Nabeshima, Adam Karvonen, and Neel Nanda.  Learning multi-level features with\n  matryoshka sparse autoencoders. CoRR, 2025. 3, 15\n\nMarc-André Carbonneau, Julian Zaidi, Jonathan Boilard, and Ghyslain Gagnon. Measuring disentanglement:\n A review of metrics. IEEE Trans. Neural Netw. Learn. Syst., May 2022. 4\n\nDavid Chanin, Tomáš Dulka, Hardik Bhatnagar, and Joseph Bloom. Toy models of feature absorption in\n  saes. LessWrong, October 2024a. 6\n\nDavid Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Satvik Golechha, and Joseph Bloom.\n A is for absorption: Studying feature splitting and absorption in sparse autoencoders. CoRR, September\n  2024b. 8\n\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\n  highly interpretable features in language models. ICLR, January 2024. 2, 3, 5, 6\n\nCian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled\n  representations. ICLR, February 2018. 4\n\nHenry Eigen and Amir Sadovnik. Topkconv: Increased adversarial robustness through deeper interpretability.\n  ICMLA, December 2021. 1\n\n\n                                              18\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nNelson Elhage, Tristan Hume, Olsson Catherine, Nanda Neel, Tom Henighan, Scott Johnston, Sheer\n  ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal\n  Ndousse, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jack-\n  son Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-\n  Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah.\n  Softmax linear units. Transformer Circuits Thread, 2022a. 16\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac\n  Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. Transformer\n  Circuits Thread, 2022b. 2, 3, 4, 5, 7, 9, 10, 11, 12, 16, 24\n\nJoshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, and Max Tegmark. Not all language model features\n  are linear. CoRR, May 2024. 15, 24\n\nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry.\n  Adversarial robustness as a prior for learned representations. CoRR, September 2019. 1, 2, 14\n\nChristian Etmann, Sebastian Lunz, Peter Maass, and Carola-Bibiane Schönlieb. On the connection between\n  adversarial robustness and saliency map interpretability. ICML, May 2019. 1\n\nAndrea Fontanari, Iddo Eliazar, Pasquale Cirillo, and Cornelis W. Oosterlee. Portfolio risk and the quantum\n  majorization of correlation matrices. IMA Journal of Management Mathematics, 2021. 4\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\n  He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse\n  text for language modeling. CoRR, December 2020. 26\n\nLeo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan\n  Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. CoRR, June 2024. 3, 6, 15, 24\n\nPeiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya\n  Ganguli. A theory of multineuronal dimensionality, dynamics and measurement. bioRxiv, November 2017.\n  4\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\n  memories. CoRR, September 2021. 11\n\nZiv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and\n  Yury Polyanskiy. Estimating information flow in deep neural networks. ICML, May 2019. 3\n\nKaarel Hänni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of computation\n  in superposition. ICML MI Workshop, August 2024. 3\n\nMark O. Hill. Diversity and evenness: A unifying notation and its consequences. Ecology, March 1973. 22\n\nGeoffrey E Hinton. Distributed representations. Carnegie Mellon University, 1984. 3\n\nJesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, and Daniel Murfet.\n  The developmental landscape of in-context learning. CoRR, February 2024. 10\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.\n  Adversarial examples are not bugs, they are features. NeurIPS, August 2019. 1\n\nEdwin T. Jaynes. Information theory and statistical mechanics. Phys. Rev., May 1957. 23\n\nFrederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet. K. Baker.  Perplexity—a measure of the\n   difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, December 1977.\n  22\n\nLou Jost. Entropy and diversity. Oikos, May 2006. 22, 23\n\n\n                                              19\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nConnor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, and Neel Nanda. Interpreting\n  attention layer outputs with sparse autoencoders. CoRR, June 2024. 3\n\nVictor Lecomte, Kushal Thaman, Trevor Chow, Rylan Schaeffer, and Sanmi Koyejo. Incidental polyseman-\n   ticity. CoRR, 2023. 3\n\nSangyun Lee, Hyukjoon Kwon, and Jae Sung Lee. Estimating entanglement entropy via variational quantum\n   circuits with classical neural networks. CoRR, December 2023. 4\n\nDavid Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, and Vladimir Miku-\n   lik. Tracr: Compiled transformers as a laboratory for interpretability. CoRR, 2023. 9, 25\n\nYanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and\n  black-box attacks. CoRR, February 2017. 2\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\n  deep learning models resistant to adversarial attacks. CoRR, 2018. 11\n\nBenoit Mandelbrot. How long is the coast of britain?  statistical self-similarity and fractional dimension.\n  Science, May 1967. 8\n\nLuke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, and Fazl Barez. Interpreting reward\n  models in rlhf-tuned language models using sparse autoencoders. CoRR, October 2023. 3\n\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.  Sparse\n  feature circuits: Discovering and editing interpretable causal graphs in language models. CoRR, March\n  2024. 3, 6, 11, 26\n\nSimon C. Marshall and Jan H. Kirchner. Understanding polysemanticity in neural networks through coding\n  theory. CoRR, January 2024. 3, 9\n\nEric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural scaling.\n  NeurIPS, March 2023. 8, 25\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of\n  words and phrases and their compositionality. NeurIPS, October 2013. 3\n\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.  Progress measures for\n  grokking via mechanistic interpretability. ICLR, January 2023. 10\n\nMichael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information. Cambridge\n  University Press, January 2011. 4, 23\n\nChris Olah. Distributed representations: Composition & superposition. Transformer Circuits Thread, 2023.\n  3\n\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of\n  large language models. NeurIPS Workshop on Causal Representation Learning, November 2023. 3\n\nGonccalo Paulo, Alex Troy Mallen, Caden Juang, and Nora Belrose. Automatically interpreting millions of\n  features in large language models. CoRR, October 2024. 3\n\nSvetlana Pavlitska, Hannes Grolig, and J. Marius Zöllner.  Relationship between model compression and\n  adversarial robustness: A review of current evidence. CoRR, November 2023. 16\n\nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization\n  beyond overfitting on small algorithmic datasets. CoRR, January 2022. 10\n\nSenthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár,\n  Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. CoRR, April\n  2024. 3, 14\n\n\n                                              20\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nTilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey\n  on interpreting the inner structures of deep neural networks. TMLR, August 2023. 1\n\nAndrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of\n  deep neural networks by regularizing their input gradients. AAAI, November 2017. 1\n\nOlivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality.  2007 15th\n  European Signal Processing Conference, September 2007. 4\n\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially\n  robust imagenet models transfer better? NeurIPS, December 2020. 1\n\nShibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry.\n  Image synthesis with a single (robust) classifier. NeurIPS, August 2019. 1\n\nAdam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, and Buck Shlegeris.  Polysemanticity and\n  capacity in neural networks. CoRR, July 2023. 3, 9\n\nErwin Schrödinger. Discussion of probability relations between separated systems. Mathematical Proceedings\n  of the Cambridge Philosophical Society, 1935. 23\n\nLee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\n  AI Alignment Forum, 2022. 3\n\nMyeongjin Shin, Seungwoo Lee, Junseo Lee, Mingyu Lee, Donghwa Ji, Hyeonjun Yeo, and Kabgyun Jeong.\n  Disentangling quantum neural networks for unified estimation of quantum entropies and distance measures.\n  Phys. Rev. A, December 2024. 4\n\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.\n  CoRR, April 2017. 3\n\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, and Brian Chen. Scaling\n  monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread,\n  2024. 3, 15\n\nNaftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. CoRR, April\n  2000. 3\n\nTheodoros Tsiligkaridis and Jay Roberts. Second order optimization for adversarial robustness and inter-\n  pretability. CoRR, September 2020. 2\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness\n  may be at odds with accuracy. ICLR, September 2019. 2\n\nDmitry Vaintrob, Jake Mendel, and Kaarel Hänni. Toward a mathematical framework for computation in\n  superposition. AI Alignment Forum, 2024. 3\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf.  Fashion-mnist: a novel image dataset for benchmarking\n  machine learning algorithms. CoRR, September 2017. 13\n\n\n\n\n\n                                              21\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\nA  Theoretical Foundations\n\nA.1  Networks as Resource-Constrained Communication Channels\n\nNeural networks must transmit information through layers with limited dimensions. Each layer acts as a\ncommunication bottleneck where multiple features compete for neuronal bandwidth. When a network needs\nto represent F features using only N < F dimensions, it uses lossy compression (:= superposition).\n\nThis resource scarcity creates a natural analogy to communication theory. Just as telecommunications sys-\ntems multiplex multiple signals through shared channels, neural networks multiplex multiple features through\nshared dimensions. Our measurement framework formalizes this intuition by quantifying how efficiently net-\nworks allocate their limited representational budget across competing features.\n\n\nA.2  L1 Norm as Optimal Budget Allocation\n\nThe sparse autoencoder’s ℓ1 regularization creates an explicit budget constraint on feature activations:\n\n                              LSAE = ∥h −Wsaez∥2T    2 + λ∥z∥1                                    (9)\n\nThe penalty term λ∥z∥1 = λ Pi |zi| enforces that the total activation budget Pi |zi| remains bounded. This\ncreates competition where features must justify their budget allocation by contributing to reconstruction\nquality.\n\nFrom the first-order optimality conditions of SAE training, the magnitude |zi| for any active feature satisfies:\n\n                                         1\n                                                                               i (h −W−iz−i)|T                                         (10)                                                    |zi| = λ|wT\n\nwhere W−i excludes feature i. This reveals that |zi| measures the marginal contribution of feature i to re-\nconstruction quality—exactly the budget allocation that optimally balances reconstruction accuracy against\nsparsity. Our probability distribution therefore has meaning as “relative feature strength”:\n\n                                      E[|zi|]     expected budget allocation to feature i\n                            pi =       =                                                            (11)\n            Pj E[|zj|]         total representational budget\n\nThis fraction represents how much of the network’s limited representational resources are optimally allocated\nto feature i under the SAE’s constraints. Alternative norms fail to preserve this budget interpretation. The\nℓ2 norm E[z2i ] overweights outliers and breaks the linear connection to reconstruction contributions through\nsquaring. The ℓ∞norm captures only peak activation while ignoring frequency of use. The ℓ0 norm provides\nbinary active/inactive information but loses the magnitude data essential for measuring resource allocation\nintensity.\n\n\nA.3  Shannon Entropy as Information Capacity Measure\n\nGiven the budget allocation distribution p, the exponential of Shannon entropy provides the theoretically\noptimal feature count. The exponential of Shannon entropy, exp(H), is formally known as perplexity in\ninformation theory and the Hill number (order-1 diversity index) in ecology (Hill, 1973; Jost, 2006):\n\n\n                                  !    n\n                            PP(p) = exp − X pi log pi = Y p−pii                               (12)\n                                                                                i              i=1\n\nThis quantifies the effective number of outcomes in a probability distribution: how many equally likely\noutcomes would yield identical uncertainty. In information theory, it represents the effective alphabet size\nof a communication system (Jelinek et al., 1977). In ecology, it quantifies the effective number of species in\n\n\n                                              22\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nan ecosystem (Jost, 2006). In statistical physics, it relates to the number of accessible states in a system\n(Jaynes, 1957). In quantum mechanics, it corresponds to the effective number of pure quantum states in a\nmixed state (Schrödinger, 1935).\n\nShannon entropy uniquely satisfies the mathematical properties required for principled feature counting\n(Anand et al., 2011). The measure exhibits coding optimality, equaling the minimum expected code length\nfor optimal compression. It satisfies additivity for independent feature sets through H(p⊗q) = H(p)+H(q).\nSmall changes in feature importance yield small changes in measured count through continuity. Uniform\ndistributions where all features are equally important maximize the count. Adding features with positive\nprobability monotonically increases the count.  These axioms uniquely characterize Shannon entropy up\nto a multiplicative constant, making exp(H(p)) the theoretically principled choice for aggregating feature\nimportance into an effective count.\n\nIn quantum systems, von Neumann entropy S(ρ) = −Tr(ρ log ρ) measures entanglement, with eS(ρ) rep-\nresenting effective pure states participating in a mixed quantum state (Nielsen & Chuang, 2011). Neural\nsuperposition exhibits parallel structure: just as quantum entanglement creates non-separable correlations\nthat cannot be decomposed into independent subsystem states, neural superposition creates feature repre-\nsentations that cannot be cleanly separated into individual neuronal components. Both phenomena involve\ncompressed encoding of information: quantum entanglement distributes correlations across subsystems re-\nsisting local description, while neural superposition distributes features across neurons resisting individual\ninterpretation. Our measure eH(p) captures this compression by quantifying the effective number of fea-\ntures participating in the neural representation, analogous to how eS(ρ) quantifies effective pure states in an\nentangled quantum mixture.\n\nHigher-order Hill numbers provide different sensitivities to rare versus common features:\n\n                                              n   !1/(1−q)\n                               qD = X pqi                                               (13)\n                                                   i=1\n\nwhere q = 1 gives our exponential entropy measure (via L’Hôpital’s rule), q = 0 counts non-zero components,\nand q = 2 gives the inverse Simpson concentration index (participation ratio in statistical mechanics).\n\n\nA.4  Rate-Distortion Theoretical Foundation\n\nOur measurement framework emerges from two nested rate-distortion problems that formalize the intuitive\nresource allocation perspective. The neural network layer itself solves:\n\n\n                         RNN(D) =      min      I(X; H)                              (14)\n                                                   p(h|x):E[d(y,f(h))]≤D\n\nwhere the layer width N constrains the mutual information I(X; H) that can be transmitted, while D\nrepresents acceptable task performance degradation. When the optimal solution requires representing F > N\nfeatures, superposition emerges naturally as the rate-optimal encoding strategy.\n\nThe sparse autoencoder solves a complementary problem:\n\n\n                           RSAE(D) =      min       E[∥z∥1]                              (15)\n                                                       p(z|h):E[∥h−ˆh∥22]≤D\n\nwhere sparsity ∥z∥1 acts as the rate constraint and reconstruction error as distortion.  This dual struc-\nture justifies SAE-based measurement: we quantify the effective rate required to represent the network’s\ncompressed internal information under sparsity constraints.\n\nThe SAE optimization can be viewed as an information bottleneck problem balancing information preser-\nvation E[∥h −g(z)∥22] against information cost λE[∥z∥1]. Under this interpretation, E[|zi|] represents the\n\n\n                                              23\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\ninformation cost of including feature i in the compressed representation, making our probability distribution\na natural measure of information allocation across features.\n\n\nA.5   Critical Assumptions and Failure Modes\n\nOur method measures effective representational diversity under sparse linear encoding, which approximates\nbut does not exactly equal the number of distinct computational features. We must carefully assess the\nconditions under which this approximation holds.\n\nFeature Correspondence Assumption. We assume SAE dictionary elements correspond one-to-one\nwith genuine computational features. This assumption fails through feature splitting where one computa-\ntional feature decomposes into multiple SAE features, artificially inflating counts. Feature merging combines\nmultiple computational features into one SAE feature, deflating counts. Ghost features represent SAE ar-\ntifacts without computational relevance (Gao et al., 2024). Incomplete coverage occurs when SAEs miss\ncomputationally relevant features entirely.\n\nLinear Representation Assumption. We assume features combine primarily through linear superpo-\nsition in activation space.  Real networks violate this through hierarchical structure where low-level and\nhigh-level features aren’t interchangeable. Gating mechanisms allow some features to control whether oth-\ners activate (Elhage et al., 2022b). Combinatorial interactions emerge when meaning comes from feature\ncombinations rather than individual contributions (Black et al., 2022).\n\nMagnitude-Importance Correspondence. We assume |zi| reflects feature i’s computational impor-\ntance. This breaks when SAE reconstruction preserves irrelevant details while missing computational es-\nsentials, when features interact nonlinearly in downstream processing (Engels et al., 2024), or when feature\nimportance depends heavily on context rather than magnitude.\n\nIndependent Information Assumption. We assume Shannon entropy correctly aggregates information\nacross features. This fails when correlated features don’t contribute independent information, when synergis-\ntic information means feature pairs provide more information together than separately, or when redundant\nencoding has multiple features encoding identical computational factors.\n\nThe approximation captures genuine signal about representational complexity under specific conditions. The\nmeasure works best when features combine primarily through linear superposition, activation patterns are\nsparse with balanced importance, SAEs achieve high reconstruction quality on computationally relevant\ninformation, and representational structure is relatively flat rather than hierarchical. The approximation\ndegrades with highly hierarchical representations, dense activation patterns with complex feature interac-\ntions, poor SAE reconstruction quality, or extreme feature importance skew. Despite these limitations, the\nmeasure provides principled approximation rather than exact counting, with primary value in comparative\nanalysis across networks and training regimes.\n\n\nA.6  Why Eigenvalue Decomposition Fails for SAE Analysis\n\nFollowing the quantum entanglement analogy, one might consider eigenvalue decomposition of the covariance\nmatrix:\n\n\n                                              1\n                         Σ = nAAT                                           (16)\n\nwhere A represents the activation matrix. Eigenvalues {λ1, λ2, . . . , λn} represent explained variance along\nprincipal components, normalized to form a probability distribution:\n\n\n                                                        λi\n                                                    pi =                                                   (17)\n                             Pni=1 λi\n\n                                              24\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nThis approach faces fundamental rank deficiency when applied to SAEs. Expanding from lower dimension\n(N neurons) to higher dimension (D > N dictionary elements) yields covariance matrices with rank at most\nN, making detection of more than N features impossible regardless of SAE capacity.\n\nOur activation-based approach circumvents this limitation by directly measuring feature utilization through\nactivation magnitude distributions rather than intrinsic dimensionality. This enables superposition quantifi-\ncation with overcomplete SAE dictionaries.\n\n\nA.7  Adaptation to Convolutional Networks\n\nConvolutional neural networks organize features across channels rather than spatial locations.  For CNN\nlayers with activations X ∈RB×C×H×W , we measure superposition across the channel dimension while\naccounting for spatial structure.\n\nWe extract features from each spatial location’s channel vector independently, then aggregate when comput-\ning feature probabilities:\n\n                    Pb,h,w |zb,i,h,w|\n                                             pi =                                                          (18)\n                       PDj=1 Pb,h,w |zb,j,h,w|\n\nwhere zb,i,h,w represents feature i’s activation at spatial position (h, w) in sample b.\n\nThis aggregation treats the same semantic feature activating at different spatial locations (e.g., edge detectors\nfiring everywhere) as evidence for a single feature’s importance rather than separate features.\n\n\nB  Experimental Details\n\nB.1  Tracr Compression\n\nWe compile RASP programs using Tracr’s standard pipeline with vocabulary {1, 2, 3, 4, 5} and maximum\nsequence length 5. The sequence reversal program uses position-based indexing, while sorting employs Tracr’s\nbuilt-in sorting primitive with these parameters.\n\nFollowing Lindner et al. (2023), we train compression matrices using a dual objective that ensures compressed\nmodels maintain both computational equivalence and representational fidelity:\n\n                        L = λoutLout + λlayerLlayer                                      (19)\n                                  Lout = KL(softmax(yc), softmax(yo))                              (20)\n                                      1  L\n                                      Llayer = X ∥h(o)i  −h(c)i  ∥22                                        (21)                            L\n                                              i=1\n\nwhere yc and yo denote compressed and original logits, and h(o)i   , h(c)i   represent original and compressed\nactivations at layer i.\n\nHyperparameters: λout = 0.01, λlayer = 1.0, learning rate 10−3, temperature τ = 1.0, maximum 500 epochs\nwith early stopping at 100% accuracy. We use Adam optimization and train separate compression matrices\nfor each trial. For each compressed model achieving perfect accuracy, we extract activations from all residual\nstream positions across 5 trials. SAEs use fixed dictionary size 100, L1 coefficient 0.1, learning rate 10−3,\ntraining for 300 epochs with batch size 128. We analyze the final layer activations (post-MLP) for consistency\nacross compression factors.\n\n\nB.2  Multi-Task Sparse Parity Experiments\n\nDataset Construction. We use the multi-task sparse parity dataset from Michaud et al. (2023) with 3\ntasks and 4 bits per task. Each input consists of a 3-dimensional one-hot control vector concatenated with\n\n\n                                              25\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\n12 data bits (total dimension 15). For each sample, the control vector specifies which task is active, and\nthe label is computed as the parity (sum modulo 2) of the 4 bits corresponding to that task. This creates a\ndataset where ground truth bounds the number of meaningful features while maintaining task complexity.\n\nModel Architecture.  Simple MLPs with architecture Input(15) →Linear(h) →ReLU →Linear(1),\nwhere h ∈{16, 32, 64, 128, 256} for capacity experiments. We apply interventions (dropout) to hidden\nactivations before the ReLU nonlinearity. Training uses Adam optimizer (lr=0.001), batch size 64, for 300\nepochs with BCEWithLogitsLoss. Dataset split: 80% train, 20% test with stratification by task and label.\n\nIntervention Protocols.  Dropout experiments: Applied to hidden activations with rates [0.0, 0.1,\n0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. Dictionary scaling: Expansion factors [0.5, 1.0, 2.0, 4.0, 8.0, 16.0]\nrelative to hidden dimension, with L1 coefficients [0.01, 0.1, 1.0, 10.0], maximum dictionary size capped at\n1024. Each configuration tested across 5 random seeds with 3 SAE instances per configuration for stability\nmeasurement.\n\nSAE Architecture and Training.  Standard autoencoder with tied weights: z = ReLU(Wencx + b),\nx′ = Wdecz where Wdec = WTenc. Dictionary size typically 4× layer width unless specified otherwise.\nLoss function: L = ||x −x′||22 + λ||z||1 with L1 coefficient λ = 0.1 (unless testing λ sensitivity). Adam\noptimizer (lr=0.001), batch size 128, 300 epochs.  For stability analysis, we train 3-5 SAE instances per\nconfiguration with different random seeds and report mean ± standard deviation.\n\n\nB.3  Grokking\n\nTask and Architecture.  Modular arithmetic task: (a + b) mod 53 using sparse training data (40% of all\npossible pairs, 60% held out for testing). Model architecture: two-path MLP with shared embeddings.\n\n                                     ea = Embedding(a, dim = 12)                                 (22)\n                                        eb = Embedding(b, dim = 12)                                 (23)\n                            h = GELU(W1ea + W2eb)                                  (24)\n                                          logits = W3h                                                    (25)\n\nwhere W1, W2 ∈R48×12 and W3 ∈R53×48.\n\nTraining Configuration.  25,000 training steps, learning rate 0.005, batch size 128, weight decay 0.0002.\nModel checkpoints saved every 250 steps (100 total checkpoints). Random seed 0 for reproducibility.\n\nLLC Estimation Protocol.  Local Learning Coefficient estimated using Stochastic Gradient Langevin\nDynamics (SGLD) with hyperparameters: learning rate 3 × 10−3, localization parameter γ = 5.0, effective\ninverse temperature nβ = 2.0, 500 MCMC samples across 2 independent chains. Hyperparameters selected\nvia 5 × 5 grid search over epsilon range [3 × 10−5, 3 × 10−1] ensuring ε > 0.001 for stability and nβ < 100\nfor β-independence.\n\n\nB.4  Pythia-70M Analysis\n\nData Sampling and Preprocessing.  20,000 samples from Pile dataset (Gao et  al., 2020), shuffled\nwith seeds [42, 123, 456] for reproducibility.  Text preprocessing: truncate to 512 characters before tok-\nenization to prevent memory issues.  Tokenization using model’s native tokenizer with max_length=512,\ntruncation=True, no padding. Samples with empty text or tokenization failures excluded.\n\nModel and SAE Configuration.  Pythia-70M model with layer specifications: embedding layer, and\n{attn_out, mlp_out, resid_out} for layers 0–5. Pretrained SAEs from Marks et al. (2024) with dictionary\nsize 64 × 512 = 32, 768 features per layer. SAE weights loaded from subdirectories following pattern:\nlayer_type/10_32768/ae.pt.\n\n\n                                              26\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\nActivation Processing.  Activations extracted using nnsight tracing with error handling for failed for-\nward passes.  Feature activations accumulated across all token positions and samples:  feature_sumi =\nPsamples,positions |zi|. Feature count computed from accumulated sums using entropy-based measure. Mem-\nory management: explicit cleanup of activation tensors and CUDA cache clearing between seeds.\n\n\nB.5  Adversarial Robustness\n\nB.5.1  Model Architectures\n\nSimple Models (Single Hidden Layer)\n\n   • SimpleMLP: Input(784) →Linear(h) →ReLU →Linear(output)\n\n        – Hidden dimensions h ∈8, 32, 128, 512\n\n   • SimpleCNN: Input →Conv2d(h, 5×5) →ReLU →MaxPool(2) →Linear(output)\n\n        – Filter counts h ∈8, 16, 32, 64\n\n\nStandard Models\n\n   • StandardMLP: Input(784) →Linear(4h) →ReLU →Linear(2h) →ReLU →Linear(h) →ReLU\n       →Linear(output)\n\n        – Base dimension h = 32, yielding layer widths [128, 64, 32]\n\n   • StandardCNN: LeNet-style architecture\n\n        – Conv2d(1, h, 3×3) →ReLU →MaxPool(2)\n        – Conv2d(h, 2h, 3×3) →ReLU →MaxPool(2)\n        – Linear(4h) →ReLU →Linear(output)\n        – Base dimension h = 16\n\n\nCIFAR-10 Models\n\n   • CIFAR10CNN: Three-block CNN with batch normalization\n\n        – Conv2d(3, h, 3×3) →BN →ReLU →MaxPool(2)\n        – Conv2d(h, 2h, 3×3) →BN →ReLU →MaxPool(2)\n        – Conv2d(2h, 4h, 3×3) →BN →ReLU →MaxPool(2)\n        – Dropout(0.2) →Linear(output)\n        – Base dimension h = 32\n\n   • ResNet-18: Modified for CIFAR-10\n\n        – Initial: Conv2d(3, 64, 3×3, stride=1, padding=1)\n        – MaxPool replaced with Identity\n        – Standard ResNet-18 blocks [2, 2, 2, 2]\n\n   • WideResNet: ResNet-18 with variable width\n\n        – Width factors: 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8\n        – Initial channels: 16 × width factor\n        – Block channels: 16, 32, 64, 128 × width factor\n\n\n                                              27\n\nPublished in Transactions on Machine Learning Research (12/2025)\n\n\n\n\n\nB.5.2  Training Protocols\n\nMNIST/Fashion-MNIST:\n\n   • Optimizer: SGD with momentum 0.9\n\n   • Learning rate: 0.01, MultiStep decay at epochs [50, 75]\n\n   • Weight decay: 10−4\n\n   • Epochs: 100\n\n   • Batch size: 128\n\n   • PGD: 40 steps, step size α = 0.01\n\n   • FGSM: Single step, α = ϵ\n\n\nCIFAR-10:\n\n   • Optimizer: SGD with momentum 0.9\n\n   • Learning rate: 0.1, MultiStep decay at epochs [100, 150]\n\n   • Weight decay: 5 × 10−4\n\n   • Epochs: 200\n\n   • Batch size: 128\n\n   • PGD: 10 steps, step size α = 2/255\n\n   • FGSM: Single step, α = ϵ\n\n\nB.6  SAE Configuration\n\n   • Dictionary size: 4N (4× layer width)\n\n   • L1 coefficient: 0.1\n\n   • Optimizer: Adam, learning rate 10−3\n\n   • Training: 800 epochs with early stopping (patience 50)\n\n   • Activation collection: 10,000 samples from test set\n\n   • Separate SAEs trained per layer\n\n\n\n\n\n                                              28\n\n## Claim to Classify\n\"adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "sparse autoencoder feature attribution with intervention analysis (adversarial training)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "1. **Method Rung Classification (Rung 2 - Interventional):**\n   - The paper uses sparse autoencoders (SAEs) to extract features from neural activations, which is primarily an observational method (Rung 1). However, the core evidence for the claim comes from *interventional* experiments involving adversarial training (e.g., PGD, FGSM). Adversarial training modifies the model's weights and activations to test causal effects on robustness and feature organization. This intervention establishes causal relationships between training regimes and model properties, placing the method in Rung 2.\n\n2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**\n   - The claim uses mechanistic language: \"adversarial training can increase effective features while improving robustness.\" The phrase \"contradicting the hypothesis that superposition causes vulnerability\" implies a counterfactual or mechanistic narrative about how neural networks organize information under computational constraints. This goes beyond merely stating a causal effect (Rung 2) and suggests a deeper, explanatory mechanism (Rung 3).\n   - The claim also invokes a *uniqueness* or *exclusivity* implication (e.g., contradicting an existing hypothesis), which is characteristic of Rung 3 claims.\n\n3. **Hedge Flag:**\n   - The claim is stated as a factual observation (\"adversarial training can increase...\") with no explicit hedging (e.g., \"may,\" \"suggests\"). Thus, `hedge_flag` is 0.\n\n4. **Confidence:**\n   - Confidence is 4 (high but with minor ambiguity). The method (adversarial training) is clearly interventional (Rung 2), and the claim is mechanistic (Rung 3). However, there is slight ambiguity in whether the claim fully meets the Rung 3 criteria of establishing *unique* or *necessary* mechanisms, as it primarily refutes a hypothesis rather than proving an alternative mechanism."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 30181,
      "completion_tokens": 487,
      "total_tokens": 30668
    },
    "finish_reason": "stop"
  }
}