{
  "timestamp": "2026-02-19T10:18:29.920473+00:00",
  "paper_id": "2301.04709",
  "claim_id": "2301.04709-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2301.04709\nTitle: Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability\nFull text:\n                    Journal of Machine Learning Research 26 (2025) 1-63             Submitted 1/23; Revised 12/24; Published 5/25\n\n\n\n\n                             Causal Abstraction:\n       A Theoretical Foundation for Mechanistic Interpretability\n\n\n              Atticus Geiger∗♢, Duligur Ibeling♠, Amir Zur♢, Maheep Chaudhary♢,\n\n             Sonakshi Chauhan♢, Jing Huang♠, Aryaman Arora♠, Zhengxuan Wu♠,\n\n           Noah Goodman♠, Christopher Potts♠, Thomas Icard∗♠\n\n\n            ♢Pr(Ai)2R Group   ♠Stanford University2025\n             ∗Corresponding authors: atticusg@gmail.com; icard@stanford.edu\nMay\n                Editor: Jin Tian\n8\n\n                                          Abstract\n                    Causal abstraction provides a theoretical foundation for mechanistic interpretability, the\n                           field concerned with providing intelligible algorithms that are faithful simplifications of[cs.AI]               the known, but opaque low-level details of black box AI models. Our contributions are\n                       (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard\n                  and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old\n                   mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for\n                     the core concepts of polysemantic neurons, the linear representation hypothesis, modular\n                        features, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability\n                   methods in the common language of causal abstraction, namely, activation and path patching,\n                       causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure,\n                       sparse autoencoders, differential binary masking, distributed alignment search, and steering.\n                 Keywords:  Mechanistic Interpretability, Causality, Abstraction, Explainable AIarXiv:2301.04709v4\n\n\n\n\n\n               ©2025 Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman\n                    Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, Thomas Icard.\n\n                      License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\n                   http://jmlr.org/papers/v26/23-0058.html.\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nContents\n\n\n1  Introduction                                                      4\n\n\n2  Causality and Abstraction                                          6\n    2.1  Deterministic Causal Models with Implicit Graphical Structure  .  .  .  .  .  .  .    6\n    2.2  Intervention Algebras    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    9\n    2.3  Exact Transformation with Interventionals .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   13\n         2.3.1   Bijective Translation  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   14\n         2.3.2  Constructive Causal Abstraction   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   16\n         2.3.3  Decomposing Alignments Between Models .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   17\n    2.4  Approximate Transformation   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   20\n    2.5  Interchange Interventions   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   21\n    2.6  Example: Causal Abstraction in Mechanistic Interpretability   .  .  .  .  .  .  .  .   23\n    2.7  Example: Causal Abstraction with Cycles and Infinite Variables   .  .  .  .  .  .   27\n\n\n3 A Common Language for Mechanistic Interpretability                 30\n    3.1  Polysemantic Neurons, the Linear Representation Hypothesis, and Modular\n        Features via Intervention Algebras   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   30\n    3.2  Graded Faithfulness via Approximate Abstraction   .  .  .  .  .  .  .  .  .  .  .  .  .  .   32\n    3.3  Behavioral Evaluations as Abstraction by a Two Variable Chains  .  .  .  .  .  .   32\n         3.3.1  LIME: Behavioral Fidelity as Approximate Abstraction   .  .  .  .  .  .  .   32\n         3.3.2   Single Source Interchange Interventions from Integrated Gradients   .   34\n         3.3.3  Estimating the Causal Effect of Real-World Concepts   .  .  .  .  .  .  .  .   34\n    3.4  Patching Activations with Interchange Interventions   .  .  .  .  .  .  .  .  .  .  .  .  .   35\n         3.4.1  Causal Mediation as Abstraction by a Three-Variable Chain .  .  .  .  .   36\n         3.4.2  Path Patching as Recursive Interchange Interventions   .  .  .  .  .  .  .  .   37\n    3.5  Ablation as Abstraction by a Three Variable Collider .  .  .  .  .  .  .  .  .  .  .  .  .   37\n         3.5.1  Concept Erasure   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   38\n         3.5.2  Sub-Circuit Analysis  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   39\n         3.5.3  Causal Scrubbing .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   40\n    3.6  Modular Feature Learning as Bijective Transformation  .  .  .  .  .  .  .  .  .  .  .  .   41\n         3.6.1  Unsupervised Methods .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   42\n         3.6.2  Aligning Low-Level Features with High-Level Variables    .  .  .  .  .  .  .   42\n         3.6.3  Supervised Methods   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   43\n    3.7  Activation Steering as Causal Abstraction  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   44\n    3.8  Training AI Models to be Interpretable .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   44\n\n\n4  Conclusion                                                       45\n\n\n                                           2\n\n           Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n\nSymbol                                 Meaning                                 Location\n\nDomain(f)                           the domain of a function f\n\nv 7→f(v)                                  the function f\n1[φ]            An indicator function that outputs 1 if φ is true, 0 otherwise.\n\nV                      A complete set of variables                        Definition 2\n\nValX              A function mapping variables X ⊆V to values              Definition 2\n\nΣ                  A signature (variables V and values Val)                 Definition 2\n\nv                       A total setting v ∈ValV                         Definition 3\n\nX, x                 A variable X ∈V and a value x ∈ValX                 Definition 3\n\nX, x            A set of variables X ⊆V and a partial setting x ∈ValX         Definition 3\nProjX(y)          The restriction of a partial setting y to variables X ⊆Y ⊆V      Definition 4\nProj−1Y (x)              The set of partial settings y where ProjX(y) = x             Definition 4\n{FX}X∈V                     Mechanisms for the variables in V                    Definition 5\n\n≺            An ordering of V by causal dependency as induced via {FX}X∈V    Definition 5\n\nM            A causal model (a signature Σ and mechanisms {FX}X∈V)       Definition 5\n\nSolve(M)           The set of total settings that are solutions to a model M        Definition 5\n\ni ∈Hard           A hard intervention that fixes the variables I ⊆V            Definition 9\n\n{IX}X∈X ∈Soft    A soft intervention that replaces the mechanisms of X ⊆V      Definition 10\n{IX}X∈X ∈Func      An interventional that edits the mechanisms of X ⊆V        Definition 11\n\nFuncV                The set of interventionals that edit all mechanisms          Definition 11\n\nΦ               The interventionals in FuncV equivalent to hard interventions.     Definition 11\n\nα                       A sequence of elements                        Definition 11\n\nΨ                       A set of interventionals                        Definition 11\nτ              A map from total settings of M to total settings of M∗        Definition 25\nω             A map from interventionals in M to interventionals of M∗      Definition 25\n\nH, L                             High-level and low-level causal models                 Definition 33\n\nΠXH           A partition cell of VL assigned to the variable XH ∈VH       Definition 31\nπXH            A function mapping from values of ΠXH to values of XH       Definition 31\n\n\n\n\n\n                                        3\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n1 Introduction\n\nWe take the fundamental aim of explainable artificial intelligence to be explaining why a\nmodel makes the predictions it does. For many purposes the paradigm of explanation is\ncausal explanation (Woodward, 2003; Pearl, 2019), elucidating counterfactual difference-\nmaking details of the mechanisms underlying model behavior. However, not just any causal\nexplanation will be apt. There is an obvious sense in which we already know all the low-level\ncausal facts about a deep learning model. After all, we can account for every aspect of\nmodel behavior in terms of real-valued vectors, activation functions, and weight tensors.\nThe problem, of course, is that these low-level explanations are typically not transparent to\nhumans—they fail to instill an understanding of the high-level principles underlying model\nbehavior (Lipton, 2018; Creel, 2020) that can guide human action (Karimi et al., 2021, 2023).\n    In many contexts it can be quite straightforward to devise simple algorithms for tasks that\noperate on human-intelligible concepts. The crucial question is, under what conditions does\nsuch a transparent algorithm constitute a faithful interpretation (Jacovi and Goldberg, 2020)\nof the known, but opaque, low-level details of a black box model? This is the motivating\nquestion for the explainable AI subfield known as interpretability. The question takes on\nparticular significance for mechanistic interpretability,1 which, in contrast to behavioral\ninterpretability, is precisely aimed at reverse engineering the internals of a black box model\nin terms of a transparent algorithm.\n   Mechanistic interpretability research is quite analogous to the problem that cognitive\nscientists face in understanding how the human mind works. At one extreme, we can try to\nunderstand minds at a very low level, e.g., of biochemical processes in the brain. At the\nother extreme, we can focus just on the input-output facts of the system, roughly speaking,\non ‘observable behavior.’  Analogously, for a deep learning model, we can focus either\non low-level features (weight tensors, activation functions, etc.), or on what input-output\nfunction is computed. In both cases, however, it can be illuminating to investigate the\nmediating processes and mechanisms that transform input to output at a slightly higher-level\nof abstraction. This is what Marr (1982) famously called the algorithmic level of analysis.\nTo the extent that these algorithmic-level hypotheses are transparent to the scientist, we\nmay have a useful elucidation of the agent’s inner workings.\n   However, it is crucial that mechanistic interpretability methods avoid telling ‘just-so’\nstories that are completely divorced from the internal workings of the model. To clarify what\nthis means exactly, we need a common language for explicating and comparing methodologies,\nand for precisifying core concepts. We submit that the theory of causal abstraction provides\nthis common language.\n   In some ways, modern deep learning models are like the weather or an economy: they\ninvolve large numbers of densely connected ‘microvariables’ with complex, non-linear dynam-\nics. One way of reining in this complexity is to find ways of understanding these systems in\n\n\n 1. Saphra and Wiegreffe (2024) argue that the term ‘mechanistic interpretability’ often signals not a single\n    research program, but rather a cultural identity within the AI alignment community (Olah et al., 2020;\n   Elhage et al., 2021; Wang et al., 2023; Nanda et al., 2023a). However, they also note that there is a\n    natural ‘narrow’ technical reading of the term, where the concern is specifically with causal analyses of\n   models’ internal mechanisms (Vig et al., 2020; Geiger et al., 2020, 2021, 2023; Finlayson et al., 2021;\n   Meng et al., 2022; Stolfo et al., 2023; Todd et al., 2024; Prakash et al., 2024; Mueller et al., 2024). We\n   embrace that causality is core to mechanistic interpretability.\n\n\n                                           4\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nterms of higher-level, more abstract variables (‘macrovariables’). For instance, the many\nmicrovariables might be clustered together into more abstract macrovariables. A number of\nresearchers have been exploring theories of causal abstraction, providing a mathematical\nframework for causally analyzing a system at multiple levels of detail (Chalupka et al.,\n2017; Rubenstein et al., 2017; Beckers and Halpern, 2019; Beckers et al., 2019; Rischel and\nWeichwald, 2021; Massidda et al., 2023, 2024). These methods tell us when a high-level\ncausal model is a simplification of a (typically more fine-grained) low-level model. To date,\ncausal abstraction has been used to analyze weather patterns (Chalupka et al., 2016), human\nbrains (Dubois et al., 2020a,b), physical systems (Kekic et al., 2023), batteries (Zennaro\net al., 2023a), epidemics (Dyer et al., 2023), and deep learning models (Chalupka et al.,\n2015; Geiger et al., 2021; Hu and Tian, 2022; Geiger et al., 2023; Wu et al., 2023).\n   Macrovariables will not always correspond to sets of microvariables. Just as with neural\nnetwork models of human cognition (Smolensky, 1986), this is the typical situation in\nmechanistic interpretability, where high-level concepts are thought to be represented by\nmodular ‘features’ distributed across individual neural activations (Harradon et al., 2018;\nOlah et al., 2020; Huang et al., 2024). For example, the linear subspaces of activation space\nlearned from distributed alignment search (Geiger et al., 2023) and the output dimensions\nof sparse autoencoders (Bricken et al., 2023; Cunningham et al., 2023) are features that are\ndistributed across overlapping sets of neural activations.\n   Our first contribution is to extend the theory of causal abstraction to remove this\nlimitation, building heavily on previous work. The core issue is that typical hard and soft\ninterventions replace variable mechanisms entirely, so they are unable to isolate quantities\ndistributed across overlapping sets of microvariables. To address this, we consider a very\ngeneral type of intervention—what we call interventionals—that maps from old mechanisms\nto new mechanisms. While this space of operations is generally unconstrained, we isolate\nspecial classes of interventionals that form intervention algebras, satisfying two key modularity\nproperties. Such classes can essentially be treated as hard interventions with respect to a\nnew (‘translated’) variable space. We elucidate this situation, generalizing earlier work by\nRubenstein et al. (2017) and Beckers and Halpern (2019).\n   Our second contribution is to show how causal abstraction provides a solid theoretical\nfoundation for the field of mechanistic interpretability. We leverage our general presentation\nto provide flexible, yet mathematically precise, definitions for the core mechanistic inter-\npretability concepts of polysemantic neurons, the linear representation hypothesis, modular\nfeatures, and graded faithfulness. Furthermore, we unify a wide range of existing inter-\npretability methodologies in a common language, including activation and path patching,\ncausal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure,\nsparse autoencoders, differential binary masking, distributed alignment search, and activa-\ntion steering. We also connect the behavioral interpretability methods of LIME, integrated\ngradients, and causal effect estimation to the language of causal abstraction.\n  We are optimistic about productive interplay between theoretical work on causal ab-\nstraction and applied work on mechanistic interpretability. In stark contrast to weather,\nbrains, or economies, we can measure and manipulate the microvariables of deep learning\nmodels with perfect precision and accuracy, and thus empirical claims about their structure\ncan be held to the highest standard of rigorous falsification through experimentation.\n\n\n\n\n                                           5\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n2 Causality and Abstraction\n\nThis section presents a general theory of causal abstraction. Although we build on much\nexisting work in the recent literature, our presentation is in some ways more general, and in\nother ways less so. Due to our focus on (deterministic) neural network models, we do not\nincorporate probability into the picture. At the same time, because the operations employed\nin the study of modern machine learned system go beyond ‘hard’ and ‘soft’ interventions\nthat replace model mechanisms (see Def. 9 and 10 below), we define a very general kind of\nintervention, the interventional, which is a functional mapping from old mechanisms to new\nmechanisms (see Def. 11). In order to impose structure on this unconstrained class of model\ntransformations, we establish some new results on classes of interventionals that form what\nwe will call intervention algebras (see especially Theorems 20, 21).\n   Next, we explore key relations that can hold between causal models. We begin with exact\ntransformations (Rubenstein et al., 2017), which characterize when the mechanisms of one\ncausal model are realized by the mechanisms of another (‘causal consistency’). We generalize\nexact transformation from hard interventions to interventionals that form intervention\nalgebras (see Def. 25). A bijective translation (see Def. 28) is an exact transformation that\nretains all the details in the original model, staying at the same level of granularity. On the\nother hand, a constructive causal abstraction (see Def. 33) is a ‘lossy’ exact transformation\nthat merges microvariables into macrovariables, while maintaining a precise and accurate\ndescription of the original model. Also, we (1) decompose constructive causal abstraction\ninto three operations, namely marginalization, variable merge, and value merge (see Prop.\n40), and (2) provide a framework for approximate transformations (see Def. 41).\n    Lastly, we define a family of interchange intervention operations, which are central to\nunderstanding mechanistic interpretability through the lens of causal abstraction. We begin\nwith simple interchange interventions (see Def. 44), where a causal model with input and\noutput variables has certain variables fixed to values they would have under different input\nconditions. We extend these to recursive interchange interventions (see Def. 45), which allow\nvariables to be fixed based on the results of previous interchange interventions. Crucially,\nwe also define distributed interchange interventions that target variables distributed across\nmultiple causal variables and involve a bijective translation to and from a transformed\nvariable space (see Def. 46). We conclude by explicating how to construct an alignment\nfor interchange intervention analysis and how to use interchange intervention accuracy to\nquantify approximate abstractions.\n\n\n2.1 Deterministic Causal Models with Implicit Graphical Structure\n\nWe start with some basic notation.\n\nRemark 1 (Notation throughout the paper) Capital letters (e.g., X) are used for\nvariables and lower case letters (e.g., x) are used for values. Bold faced letters (e.g. X or x)\nare used for sets of variables and sets of values. When a variable (or set of variables) and a\nvalue (or set of values) have the same letter, the values correspond to the variables (e.g.,\nx ∈ValX or x ∈ValX).\n\nDefinition 2 (Signature) We use V to denote a fixed set of variables, each X ∈V coming\nwith a non-empty range ValX of possible values. Together Σ = (V, Val) are called a signature.\n\n\n                                           6\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\nDefinition 3 (Partial and Total Settings) We assume ValX ∩ValY = ∅whenever X ̸=\nY , meaning no two variables can take on the same value.2 This assumption allows representing\nthe values of a set of variables X ⊆V as a set x of values, with exactly one value x ∈ValX\nin x for each X ∈X. When we need to be specific about the choice of variable X, we\ndenote this element of ValX as x. We thus have x ⊆S X∈X ValX, and we refer to the values\nx ∈ValX as partial settings. In the special case where v ∈ValV, we call v a total setting.\n\n   Another useful construct in this connection is the projection of a partial setting:\n\nDefinition 4 (Projection) Given a partial setting y for a set of variables Y ⊇X, we\ndefine ProjX(y) to be the restriction of y to the variables in X. Given a partial setting x,\nwe define the inverse:\n\n                         Proj−1Y (x) = {y ∈ValY : ProjX(y) = x}.\n\nDefinition 5 A (deterministic) causal model is a pair M = (Σ, {FX}X∈V), such that Σ is\na signature and {FX}X∈V is a set of mechanisms, with FX : ValV →ValX assigning a value\nto X as a function of the values of all the variables, including X itself. We write FX for\n{FX}X∈X. We will often break up the input argument to a mechanism into partial settings\nthat form a total setting, e.g., FX(y, z) for Y ∪Z = V.\n\nRemark 6 (Inducing Graphical Structure) Observe that our definition of causal model\nmakes no explicit reference to a graphical structure defining a causal ordering on the variables.\nWhile the mechanism for a variable takes in total settings, it might be that the output of a\nmechanism depends only on a subset of values. This induces a causal ordering among the\nvariables, such that Y ≺X—or Y  is a parent of X—just in case there is a setting z of the\nvariables Z = V \\ {Y }, and two settings y, y′ of Y such that FX(z, y) ̸= FX(z, y′) (see, e.g.,\nWoodward 2003). The resulting order ≺captures a notion of direct causation, which can be\nextended to indirect causation by taking its transitive closure ≺∗. Throughout the paper,\nwe will define mechanisms to take in partial settings of parent variables, though technically\nthey take in total settings and depend only on the parent variables.\n\n  When ≺∗is irreflexive, we say the causal model is acyclic. Most of our examples of\ncausal models will have this property. However, it is often also possible to give causal\ninterpretations of cyclic models (see, e.g., Bongers et al. 2021). Indeed, the abstraction\noperations to be introduced generally create cycles among variables, even from initially\nacyclic models (see Rubenstein et al. 2017, §5.3 for an example). In Section 2.7, we provide\nan example where we abstract a causal model representing the bubble sort algorithm into a\ncyclic model where any sorted list is a solution satisfying the equations.\n\nRemark 7 (Acyclic Model Notation) Our example in Section 2.6 will involve causal\nabstraction between two finite, acyclic causal models. We will call variables that depend on\nno other variable input variables (XIn), and variables on which no other variables depend\noutput variables (XOut). The remaining variables are intermediate variables. We can\n\n 2. To allow the same value to occur multiple times, we can simply take any causal model where variables\n    share values, and then ‘tag’ the shared values with variable names to make them unique.\n\n\n                                           7\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\nintervene on input variables XIn to ‘prime’ the model with a particular input.3 As such, the\nconstant functions for input variables in our examples will be overwritten.\n\n   As M can also be interpreted simply as a set of equations, we can define the set of\nsolutions, which may be empty.\n\nDefinition 8 (Solution Sets) Given M = (V, {FX}X∈V), the set of solutions, called\nSolve(M), is the set of all v ∈ValV such that all the equations ProjX(v) = FX(v) are\nsatisfied for each X ∈V. When M is acyclic, there is a single solution and we use Solve(M)\nto refer interchangeably to a singleton set of solutions and its sole member, relying on context\nto disambiguate.\n\n  We give a general definition of intervention on a model (see, e.g., Spirtes et al. 2000;\nWoodward 2003; Pearl 2009). For the following, assume we have fixed a signature Σ.\n\nDefinition 9 (Intervention) Define a hard intervention to be a partial setting i ∈ValI\nfor finite I ⊆V. Given a model M with signature Σ, define Mi to be just like M, except\nthat we replace FX with the constant function v 7→ProjX(i) for each X ∈I.  Define\nHardX = ValX to be the set of all hard interventions on X.\n\nDefinition 10 (Soft Intervention) We define a soft intervention on some finite set of\nvariables X ⊆V to be a family of functions {IX}X∈X where IX  : ValV →ValX for each\nX ∈X. Given a model M with signature Σ, define MI to be just like M, except that\nwe replace each function FX with the function IX. Define SoftX to be the set of all soft\ninterventions on X.\n\nSoft interventions generalize hard interventions. The hard intervention i is equivalent to a\nconstant soft intervention I = {v 7→x}x∈i.\n\nDefinition 11 (Interventional) We define an interventional on some finite set of variables\nX ⊆V to be a family of functions I = {IX}X∈X where IX : SoftX →SoftX for each X ∈X.\nWe define MI to be just like M, except that we replace each function FX with the function\nIX⟨FX⟩. Define FuncX to be the set of interventionals on X.\n\n    Interventionals generalize soft interventions. The soft intervention I  is a constant\ninterventional—namely, the family of functions that, for each X ∈X, sends any element of\nSoftX to IX. When only one variable is targeted, we say that the intervention(al) is atomic.\n\nRemark 12 (On Terminology) What we are calling hard interventions are sometimes\ncalled structural interventions (Eberhardt and Scheines, 2007). Our soft interventions are\nessentially the same as the soft interventions studied, e.g., in Tian (2008); Bareinboim and\nCorrea (2020), although as mentioned, we set aside probabilistic aspects of causal models\nin this paper. The main difference between soft interventions and interventionals is that\nthe latter ‘mechanism replacements’ can depend on the previous mechanisms in the model.\nThis type of dependence has also been studied, e.g., in the setting of so-called parametric\ninterventions (Eberhardt and Scheines, 2007).\n\n 3. In some parts of the literature what we are calling input variables are designated as ‘exogenous’ variables.\n\n\n                                           8\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nRemark 13 (Interventionals as Unconstrained Model Transformations) The set\nFuncV contains the interventionals that targets all variables V. This set of interventionals\nis quite general, containing every function that maps from and to causal models with the\nsame signature.\n\nRemark 14 (Composing Intervention(al)s) Interventionals containing families of con-\nstant functions to constant functions are exactly the set of hard interventions. Similarly,\ninterventionals containing families of constant functions are exactly the set of soft interven-\ntions. As such, we treat all three types of interventions as interventionals on all variables,\ni.e., S X⊆V HardX ⊆S X⊆V SoftX ⊆S X⊆V FuncX ⊆FuncV. This allows us to understand\nthe composition of interventions as function composition. We simplify notation by writing,\ne.g., x ◦y for the composition of hard interventions (setting X to x and then Y to y).4\n\nThe unconstrained space of interventionals FuncV is unruly, and there is no guarantee that\nan interventional can be thought of as isolating a natural model component. We want to\ncharacterize spaces of interventionals that ‘act like hard interventions’ insofar as they possess\na basic algebraic structure. We elaborate on this in the next section.\n   The following is an example of a causal model with hard interventions, soft interventions,\nand interventionals defined on it.\n\nExample 1 Define a signature Σ with variables V = {X, Y } with values ValX = {0, . . . , 9}\nand ValY = {True, False}. Define a model M with mechanisms FX(v) = 0 (recall that\ninput variables have no parents and are mapped to constants per Remark 7) and FY (v) =\n[ProjX(v) > 5]. The graphical structure of M has one directed edge from X to Y because\nX ≺Y . Per Remark 6, we could define the mechanism for Y as FY (x) = [x > 5] omitting\nthe value that Y does not depend on, namely its own.\n   Define the hard intervention y = {True} ∈HardY , the soft intervention I = v 7→\n[ProjX(v) ≤5] ∈SoftY , and the interventional I = FY 7→(v 7→¬FY (v)) ∈FuncY . The\nmodel My has mechanisms FX(v) = 0 and FY (v) = True and a graphical structure with\nno edges. The models MI, MI◦I, and MI all have the mechanisms FX(v) = 0 and\nFY (x) = [x ≤5] and the same graphical structure as M. The model MI◦I is identical to M.\n   Define the interventional J = {FX, FY } 7→{v →6 × 1[ProjY (v)], v 7→[¬FY (v)]} ∈\nFuncV. The model MJ has mechanisms FX(y) = 6 × 1[y] and FY (x) = [x ≤5] with a\ncyclic graphical structure where X ≺Y and Y ≺X. This model has no solutions. The\nmodel MJ◦J has mechanisms FX(y) = 6 × 1[y] and FY (x) = [x > 5] with a cyclic graphical\nstructure where X ≺Y and Y ≺X. The solutions to this model are {6, True} and {0, False}.\n\n\n2.2 Intervention Algebras\n\nWe are interested in the subsets of FuncV that are well-behaved in the sense that they share\nan algebraic structure with hard interventions under the operation of function composition.\nThe relevant algebraic structure is captured in the next definition.\n\nDefinition 15 Let Λ be a set and ⊕be a binary operation on Λ. We define (Λ, ⊕) to be an\nintervention algebra if there exists a signature Σ = (V, Val) such that (Φ, ◦) ≃(Λ, ⊕)—that\n\n 4. Note that we write composition in the opposite order from common notation for function composition,\n    following the more intuitive order of intervention composition adopted, e.g., in Rubenstein et al. (2017).\n\n\n                                           9\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nis, these structures are isomorphic—where Φ is the set of all constant functionals mapping to\nconstant functions (i.e., hard interventions) for signature Σ, and ◦is function composition.\n\nAs a matter of fact, intervention algebras can be given an intuitive characterization based\non two key properties of hard interventions.\n\nDefinition 16 (Commutativity and Left-Annihilativity) Hard interventions x, y ∈Φ\nunder function composition have the following properties:\n\n  (a) If hard interventions target different variables, ◦is commutative:\n         if X ̸= Y then x ◦y = y ◦x;\n\n  (b) If hard interventions target the same variable, ◦is left-annihilative:\n         if X = Y , then x ◦y = y;\n\nNote equality signifies the compositions are the very same functions from models to models.\n\nThese two properties highlight an important sense in which hard interventions are modular:\nwhen intervening on two different variables, the order does not matter; and when intervening\non the same variable twice, the second undoes any effect of the first intervention.\n  We can use commutativity and left-annihilativity to build an equivalence relation that\nwe will show captures the fundamental algebraic structure of hard interventions.\n\nDefinition 17 Let A be any set with equivalence relation ∼, and define (A∗, ·) to be the\nfree algebra generated by elements of A under concatenation. Define ≈to be the smallest\ncongruence on A∗extending the following:\n\n                      {⟨x · y, y · x⟩: x ̸∼y} ∪{⟨y · x, x⟩: x ∼y},                      (1)\n\nfor all x, y ∈A, where · is concatenation in A∗.\n\nAs it turns out, ≈can be obtained constructively as the result of two operations that define\na normal form for sequences of atomic hard interventions.\n\nDefinition 18 (Normal form) Let A be a set equipped with equivalence relation ∼. Fix\nan order ⋖on ∼-equivalence classes. Define Collapse : A∗→A∗to take a sequence and\nremove every element that has an ∼-equivalent element that occurs to its right. Define\nSort : A∗→A∗to take a sequence and sort it according to ⋖. For any element α ∈A∗, we\ncall Sort(Collapse(α)) the normal form of α. This normal form clearly exists and is unique.\n\nLemma 19 For α, β ∈A∗, we have α ≈β iff Sort(Collapse(α)) = Sort(Collapse(β)), that is,\niff α and β have the same normal form.\n\nProof Let us write α ≡β when Sort(Collapse(α)) = Sort(Collapse(β)). First note that ≡is\na congruence extending the relation in (1), and hence ≈⊆≡.\n   For the other direction, it suffices to observe that both Collapse(c) ≈c and Sort(c) ≈c,\nfor any c ∈A∗. This follows from the fact that ≈is a congruence extending (1). Then if\nα ≡β, we have Sort(Collapse(α)) ≈Sort(Collapse(β)) (by reflexivity of ≈), and moreover:\n\n                       α ≈  Sort(Collapse(α))\n                     ≈  Sort(Collapse(β))\n                     ≈  β,\n\n\n                                          10\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nand hence ≡⊆≈. Consequently, α ≈β iff α and β have the same normal form.\n\nThe foregoing produces a representation theorem for intervention algebras.\n\nTheorem 20 The quotient (A∗/≈, ⊙) of (A∗, ·) under ≈, with ⊙defined so\n\n                           [α]≈⊙[β]≈ =  [α · β]≈,\n\nis a intervention algebra.\n\nProof  Define a signature Σ where the variables V are the ∼-equivalence classes of A and\nthe values of each variable are the members of the respective ∼-equivalence class. We need\nto show that (A∗/≈, ⊙) is isomorphic to (Φ, ◦), the set of all (finite) hard interventions with\nfunction composition.\n   Begin by defining the map ι∗: A∗→Φ, where ι∗(x1 · ... · xn) = x1 ◦... ◦xn. First observe:\n\n                      ι∗(α) = ι∗(Collapse(α)) = ι∗(Sort(Collapse(α)))                    (2)\n\nEq. (2) follows from commutativity and left-annihilativity (Def. 16).\n   Moreover, again by commutativity and left-annihilativity, if X = Y but X /∈Z, then\nx ◦z ◦y = z ◦y, which is precisely what justifies the first equality.\n    Finally, define ι : A∗/≈→Φ so that ι([α]≈) = ι∗(α). This is well-defined by Eq. (2) and\nLemma 19. It is surjective because ι∗is surjective. It is also injective:  if Sort(Collapse(α)) ̸=\nSort(Collapse(β)), then pick the ⋖-least ∼-equivalence class of A—that is, the ⋖-least variable\nX—such that Sort(Collapse(α)) and Sort(Collapse(β)) disagree on X. Without loss, we can\nassume ι∗(Sort(Collapse(α))) assigns X to some value x, but ι∗(Sort(Collapse(β))) does not\nassign X to x (either because it does not assign X to any value or because it assigns X\nto a different value). In any case, if [α]≉= [β]≈, then ι([α]≈) = ι∗(Sort(Collapse(α))) ̸=\nι∗(Sort(Collapse(β))) = ι([β]≈).\n   That ι is an isomorphism follows from the sequence of equalities below:\n\n   ι([α]≈⊙[β]≈) = ι([α · β]≈)                         By the definition of ⊙\n          = ι(Sort(Collapse(α · β)))                      By Lemma 19\n          = ι∗(Sort(Collapse(α · β)))                 By the definition of ι\n          = ι∗(α · β)                               By Equation (2)\n          = ι∗(α) ◦ι∗(β)                        By the definition of ι∗\n          = ι∗(Sort(Collapse(α))) ◦ι∗(Sort(Collapse(β)))      By Equation (2)\n          = ι(Sort(Collapse(α))) ◦ι(Sort(Collapse(β)))    By the definition of ι\n          = ι([α]≈) ◦ι([β]≈)                           By Lemma 19\n\nThis concludes the proof of Theorem 20.\n\n\n    Sets of atomic soft interventions also form intervention algebras:\n\nTheorem 21 Suppose Ψ0 is a set of atomic soft interventions with signature Σ. Let Ψ be\nthe closure of Ψ0 under function composition. Then (Ψ, ◦) is an intervention algebra.\n\n\n                                          11\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nProof  Just as in the proof of Theorem 20 we can consider the free algebra generated by\nA = Ψ0, quotiented under ≈, to obtain (A/≈, ⊙). The proof that this algebra is isomorphic\nto a set of hard interventions follows exactly as in the proof of Theorem 20, relying on the\nfact that soft interventions also satisfy the key properties (a) and (b) in Definition 16:\n\n   (i) If soft interventions target different variables, ◦is commutative:  if X ̸= Y then\n    IX ◦IY = IY ◦IX;\n\n  (ii) If interventions target the same variable, ◦is left-annihilative:   if X = Y , then\n    IX ◦IY = IY .\n\nConsequently, we know that there exists a signature Σ∗such that hard interventions on Σ\nare isomorphic to the soft interventions Ψ with respect to function composition. Specifically,\nwhere ΨX0 ⊆Ψ0 is the set of atomic soft interventions that target X, the variables of Σ∗are\nV∗= {X∗: ΨX0  ̸= ∅} and, for each X∗∈V∗, the values are ValX∗= ΨX0 .\n\n\n\nRemark 22 While both hard and soft interventions give rise to intervention algebras, the\nmore general class of interventionals satisfies weaker algebraic constraints. For instance, it is\neasy to see that left-annihilativity (see (b) and (ii) above) often fails. Consider a signature\nΣ with a single variable X that takes on binary values {0, 1}. Define the interventional\nI⟨FX⟩= x 7→1 −FX(x). Observe that I ◦I ̸= I because I ◦I is the identity function, and so\nleft-annihilativity fails.\n\n   While interventionals do not form intervention algebras in general, particular classes of\ninterventionals can form intervention algebras.\n\nExample 2 Consider a signature Σ with variables {X1, . . . , Xn} that take on integer values.\nDefine Ψ to contain interventionals that fix the pth digit of a number to be the digit q,\nwhere % is modulus and // is integer division:\n\n           I⟨FXk⟩= v 7→FXk(v) − (FXk(v)//10p) % 10  · 10p + q · 10p\n\nThis class of interventionals is isomorphic to hard interventions on a signature Σ∗with vari-\nables {Y0p , Y1p , . . . , Ynp : p ∈{0, 1, . . . }} that take on values {0, 1, . . . , 9}. The interventional\n                                                                                                 p\nfixing the pth digit of Xk to be the digit q corresponds to the hard intervention fixing Yk to\nthe value q.\n\nRemark 23 (Assignment mutations) As a side remark, it is worth observing that an-\nother setting where intervention algebras appear is in programming language semantics and\nthe semantics of, e.g., first-order predicate logic. Let D be some domain of values and Var\na set of variables. An assignment is a function g : Var →D. Let gxd be the mutation of g,\ndefined so that gxd(y) = g(y) when y ̸= x, and gxd(y) = d when y = x. Then a mutation\n( · )xd can be understood as a function from the set of assignments to the set of assignments.\nWhere M is the set of all mutations, it is then easy to show that the pair (M, ◦) forms an\nintervention algebra. Furthermore, every intervention algebra can be obtained this way (up\nto isomorphism), for a suitable choice of D and Var.\n\n\n                                           12\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nDefinition 24 (Ordering on Intervention Algebras) Let (Λ, ⊕) be a intervention al-\ngebra. We define an ordering ≤on elements of Λ as follows:\n\n                         λ ≤λ′   iff  λ′ ⊕λ = λ′.\n\nSo, in particular, if the intervention algebra contains hard interventions—if it is of the form\n(Φ, ◦)—then this amounts to the ordering from Rubenstein et al. (2017):\n\n                   x ≤y   iff X ⊆Y and x = ProjX(y)\n                                       iff  x ⊆y.\n\nNote that ≤is defined as in a semi-lattice, except that ⊕(or ◦) is not commutative in\ngeneral. So the order matters.\n\n\n2.3 Exact Transformation with Interventionals\n\nResearchers have been interested in the question of when two models—potentially defined\non different signatures—are compatible with one another in the sense that they could\nboth accurately describe the same target causal phenomena. The next definition presents a\ndeterministic variant of the notion of ‘exact transformation’ from Rubenstein et al. (2017) (see\nalso Def. 3.5 in Beckers and Halpern 2019), generalized to interventionals. The other notions\nwe study in this paper—namely, bijective translation and constructive abstraction—are\nspecial cases of exact transformation.\n\nDefinition 25 Let M, M∗be causal models and let (Ψ, ◦) and (Ψ∗, ◦) be two intervention\nalgebras where Ψ and Ψ∗are interventionals on M and M∗, respectively. Furthermore,\nlet τ  : ValV →ValV∗and ω  : Ψ →Ψ∗be two partial surjective functions where ω is\n≤-preserving; that is, ω(I) ≤ω(I′) whenever I ≤I′.\n   Then M∗is an exact transformation of M under (τ, ω) if for all I ∈Domain(ω), the\nfollowing diagram commutes:\n\n                               ω\n                                     I                    ω(I)\n\n\n                                            τ\n                            Solve MI             Solve M∗ω(I)\n\nThat is to say, the interventional ω(I) on M∗results in the same total settings of V∗as\nthe result of first determining a setting of V from I and then applying the translation τ to\nobtain a setting of V∗. In a single equation:5\n\n                             τ Solve(MI)  =  Solve M∗ω(I)  .                           (3)\n\n\n\n\n 5. When evaluating whether τ(S) = T, as in, e.g. (3), it is possible that not every element of S is in the\n   domain of τ, since τ is partial. Simply map such points to some distinguished element ⊥.\n\n\n                                           13\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\nThis definition captures the intuitive idea that M and M∗are consistent descriptions of the\nsame causal situation.\n\nRemark 26 Definition 25 is a variant of exact transformation in Rubenstein et al. (2017)\nwith intervention algebras. However, their definition of exact transformation includes an\nexistential quantifier over ω, stating that M∗is an exact transformation of M under τ  if an\nω exists that satisfies the commuting diagram. Our definition tells us when a particular pair\nτ and ω constitute an exact transformation. We believe this difference to be inessential.\n\nRemark 27 The composition of exact transformations is an exact transformation. That\nis, if (τ1, ω1) and (τ2, ω2) are exact transformation, then so is (τ1 ◦τ2, ω1 ◦ω2), when these\ncompositions are all defined. (See Lemma 5 from Rubenstein et al. 2017.)\n\n2.3.1 Bijective Translation\n\nExact transformations can be ‘lossy’ in the sense that M may involve a more detailed, or\nfiner-grained, description than M∗. For example, the model M could encode the causal\nprocess of computer hardware operating on bits of memory while the model M∗encodes\nthe fully precise, but less detailed, process of assembly code the hardware implements. In\ncontrast, when τ is a bijective function, there is an important sense in which M and M∗\nare just two equivalent (and inter-translatable) descriptions of the same causal setup.\n\nDefinition 28 (Bijective Translation) Fix signatures Σ and Σ∗. Let M be a causal\nmodel with signature Σ and mechanisms FV. Let τ  : ValV →ValV∗be a bijective map\nfrom total settings of Σ to total settings of Σ∗. Define τ(M) to be the causal model with\nsignature Σ∗and mechanisms\n\n                       FX∗(v∗) = ProjX∗(τ(FV(τ −1(v∗))))\n\nfor each variable X∗∈V∗. We say that τ(M) is the bijective translation of M under τ.\n\nRemark 29 (Bijective Translations Define a Canonical ω) Let Φ∗be the interven-\ntion algebra formed by hard interventions on Σ∗. We will now construct an intervention\nalgebra Ψ consisting of interventionals on Σ and define a function ω : Ψ →Φ∗.\n   For each i∗∈HardI∗with I∗⊆V∗, define the interventional using notation from\nDefinition 11,\n                                      !\n                I⟨FV⟩  =  v 7→τ −1  ProjV∗\\I∗  τ FV(v)   ∪i∗   .\n\nWe add I to Ψ and define ω(I) = i∗. The interventional I takes in a set of mechanisms FV for\nall of the variables and outputs a new set of mechanisms I⟨FV⟩for all of the variables. To\nretrieve mechanisms for individual variables, a projection must be applied. By construction\n(Ψ, ◦) is isomorphic to (Φ∗, ◦) with the ≤-order preserving (and reflecting) map ω, so (Ψ, ◦)\nis an intervention algebra.\n\nTheorem 30 (Bijective Translations are Exact Transformations) The bijective\ntranslation M∗= τ(M) is an exact transformation of M under (τ, ω), relative to Ψ and\nΦ∗as constructed above.\n\n\n                                           14\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\nProof  Choose an arbitrary I ∈Ψ with corresponding hard intervention i∗∈Φ∗where\nω(I) = i∗. Let F∗be the mechanisms of M∗and G∗be the mechanisms of M∗i∗.\n   Fix an arbitrary solution v ∈Solve(MI). The following string of equalities shows that\nτ(v) is in Solve(M∗i∗) and therefore τ(Solve(MI)) ⊆Solve(M∗i∗).\n  τ(v) = τ(I⟨FV⟩(v))                                  By the definition of a solution\n                          !\n    = τ  τ −1  ProjV∗\\I∗  τ FV(v)   ∪i∗                     By the definition of I\n\n    = ProjV∗\\I∗  τ FV(v)   ∪i∗                                              Inverses cancel\n\n    = ProjV∗\\I∗ F∗V∗(τ(v))  ∪i∗           M∗is a bijective translation of M under τ\n    = G∗V∗(τ(v))                             By the definition of hard interventions.\n\n   Fix an arbitrary solution v∗∈Solve(M∗i∗). The following string of equalities show that\nτ −1(v∗) is in Solve(MI) and therefore τ(Solve(MI)) ⊇Solve(M∗i∗).\n   τ −1(v∗) = τ −1(G∗V∗(v∗))                              By the definition of a solution\n\n      = τ −1  ProjV∗\\I F∗V∗(v∗)  ∪i∗          By the definition of hard interventions\n\n      = τ −1  ProjV∗\\I  τ FV(τ −1(v∗))   ∪i∗     M∗is a bij. trans. of M under τ −1\n\n      = I⟨FV⟩(τ −1(v∗))                                  By the definition of I\n\nThus, τ(Solve(MI)) = Solve(M∗i∗) for an arbitrary I, and we can conclude M∗= τ(M) is\nan exact transformation of M under (τ, ω).\n\n\n\nExample 3  If the variables of a causal model form a vector space, then a natural bijective\ntranslation is a rotation of a vector. Consider the following causal model M that computes\nboolean conjunction.\n\n                            Z\n\n                                     Y1   Y2\n\n                             X1  X2\n\n   The variables X1 and X2 take on binary values from {0, 1} and have constant mechanisms\nmapping to 0. The variables Y1 and Y2 take on real-valued numbers and have mechanisms\ndefined by a 20◦rotation matrix\n                                    ⊤  cos(20◦)  −sin(20◦)                                           x1\n                FY1(x1, x2)  FY2(x1, x2) =                                           x2     sin(20◦)   cos(20◦)\n\n\n                                          15\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nThe variable Z takes on binary values from {0, 1} and has the mechanism\n\n          FZ(y1, y2) = 1[(sin(20◦) + cos(20◦))y1 + (cos(20◦) −sin(20◦))y2 = 2]\n\nNote that Z = X1 ∧X2, since FZ un-rotates Y1 and Y2 before summing their values.\n   The variables Y1 and Y2 perfectly encode the values of the variables X1 and X2 using a\ncoordinate system with axes that are tilted by 20◦. We can view the model M through this\ntilted coordinate system using a bijective translation. Define the function\n\n    x1   x1 ⊤   1        0         0          0         0  \n           x2       x2       0        1         0          0         0       \n      τ   y1  =  y1       0        0     cos(−20◦)  −sin(−20◦)     0       \n      y2      y2       0        0     sin(−20◦)   cos(−20◦)     0                                                                                                                                                                                                                   \n           z         z        0        0         0          0         1\n\nand consider the model τ(M). This model will have altered mechanisms for Y1, Y2, and Z.\nIn particular, these mechanisms are\n\n                                          ⊤\n                                                  x1    1  0\n                       FY1(x1, x2)  FY2(x1, x2) =\n                                                  x2    0  1\n\nand FZ(y1, y2) = 1[y1 + y2 = 2].\n\n2.3.2 Constructive Causal Abstraction\n\nSuppose we have a ‘low-level model’ L = (ΣL, FL) built from ‘low-level variables’ VL and\na ‘high-level model’ H = (ΣH, FH) built from ‘high-level variables’ VH. What structural\nconditions must be in place for H to be a high-level abstraction of the low-level model L?\nAt a minimum, this requires that the high-level interventions represent the low-level ones, in\nthe sense of Def. 25; H should be an exact transformation of L. What else must be the case?\n  A prominent further intuition about abstraction is that it may involve associating specific\nhigh-level variables with clusters of low-level variables. That is, low-level variables are to\nbe clustered together in ‘macrovariables’ that abstract away from low-level details. To\nsystematize this idea, we introduce alignment between a low-level and a high-level signature:\n\nDefinition 31 (Alignment) An alignment between signatures ΣL and ΣH is given by a\npair ⟨Π, π⟩of a partition Π = {ΠXH}XH∈VH∪{⊥} and a family π = {πXH}XH∈VH of maps,\nsuch that:\n\n   1. The partition Π of VL consists of non-overlapping, non-empty cells ΠXH ⊆VL for\n     each XH ∈VH, in addition to a (possibly empty) cell Π⊥;\n\n   2. There is a partial surjective map πXH : ValΠXH →ValXH for each XH ∈VH.\n\nIn words, the set ΠXH consists of those low-level variables that are ‘aligned’ with the\nhigh-level variable XH, and πXH tells us how a given setting of the low-level cluster ΠXH\ncorresponds to a setting of the high-level variable XH. The remaining set Π⊥consists of\nthose low-level variables that are ‘forgotten’, that is, not mapped to any high-level variable.\n\n\n                                          16\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\nRemark 32 An alignment ⟨Π, π⟩induces a unique partial function ωπ that maps from\nlow-level hard interventions to high-level hard interventions. We only define ωπ on low-\nlevel interventions that target full partition cells, excluding Π⊥. For xL ∈ValΠXH where\nXH ⊆VH and ΠXH = S X∈XH ΠX, we define\n\n                                                                                                       .                       (4)                     ωπ(xL)   def=  [  πXH ProjΠXH(xL)\n                            XH∈VH\n\nAs a special case of Eq. (4), we obtain a unique partial function τ π : ValVL →ValVH that\nmaps from low-level total settings to high-level total settings. To wit, for any vL ∈ValVL:\n\n                          τ π(vL) =  [  πXH ProjΠXH(vL)  .                       (5)\n                            XH∈VH\n\nThus, the cell-wise maps πXH canonically give us these partial functions (τ π, ωπ).\n\nDefinition 33 (Constructive Abstraction) We say that H is a constructive abstraction\nof L under an alignment ⟨Π, π⟩iff H is an exact transformation of L under (τ π, ωπ).\n\nSee Section 2.6 for an example of constructive causal abstraction.\n\nRemark 34 Though the idea was implicit in much earlier work (going back at least to\nSimon and Ando 1961 and Iwasaki and Simon 1994), Beckers and Halpern (2019) and\nBeckers et al. (2019) explicitly introduced the notion of a constructive abstraction in the\nsetting of probabilistic causal models.6 As Examples 3.10 and 3.11 from Beckers and Halpern\n(2019) show, there are exact transformations that are not also constructive abstractions,\neven when we restrict attention to hard interventions.\n\nRemark 35 In the field of program analysis, abstract interpretation is a framework that\ncan be understood as a special case of constructive causal abstraction where models are\nacyclic and high-level variables are aligned with individual low-level variables rather than\nsets of low-level variables (Cousot and Cousot, 1977). The functions τ and τ −1 are the\nabstraction and concretization operators that form a Galois connection, and the commuting\ndiagram summarized in Equation 3 guarantees that abstract transfer functions are consistent\nwith concrete transfer functions.\n\n2.3.3 Decomposing Alignments Between Models\n\nGiven the importance and prevalence of this relatively simple notion of abstraction, it is\nworth understanding the notion from different angles. Abstraction under the alignment\n⟨Π, π⟩can be decomposed via the following three fundamental operations on variables.\nMarginalization removes a set of variables. Variable merge collapses a partition of variables,\ni.e., each partition cell becoming a single variable. Value merge collapses a partition of\nvalues for each variable, i.e., each partition cell becoming a single value. The first and third\noperations relate closely to concepts identified in the philosophy of science literature as being\ncritical to addressing the problem of variable choice (Kinney, 2019; Woodward, 2021).\n\n 6. Beckers and Halpern (2019) require each π is total, while we allow partial functions for more flexibility.\n\n\n                                           17\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n    First, marginalization removes a set of variables X. As an alignment, the variables X\nare placed into the cell Π⊥, while each other variable Y /∈X is left untouched; in a model\nthat is an abstraction under this alignment, the parents and children of each marginalized\nvariable are directly linked.\n\nDefinition 36 (Marginalization) Define the marginalization of X ⊂V to be an align-\nment from the signature Σ = (V, Val) to the high-level signature Σ∗= (V \\ X, Val): We\nset the partitions ΠY = {Y } for Y ∈V \\ X and Π⊥= X, while the functions are identity,\nπY = y 7→y for Y ∈V \\ X.\n\n    Marginalization is essentially a matter of ignoring a subset X of variables.\n    Next, variable merge collapses each cell of a partition into single variables. Variables are\nmerged according to a partition {ΠX∗}X∗∈V∗with cells indexed by new variables V∗. In\na model that is an abstraction under a variable merge, these new variables depend on the\nparents of their partition and determine the children of their partition.\n\nDefinition 37 (Variable Merge) Let {ΠX∗}X∗∈V∗be a partition of V indexed by new\nhigh-level variables V∗where Val∗X∗= ValΠX∗for each X∗∈V∗. Then the variable\nmerge of V into V∗is an alignment to the new signature Σ∗= (V∗, Val∗) with partition\nΠ = {ΠX∗}X∗∈V∗∪{Π⊥} where Π⊥= ∅and functions πX∗(y) = y for each X∗∈V∗.\n\n    Finally, value merge alters the value space of each variable, potentially collapsing values:\n\nDefinition 38 (Value Merge) Choose some family δ = {δX}X∈V of partial surjective\nfunctions δX  : ValX →Val∗X mapping to new variable values. The value merge is an\nalignment to the new signature Σ∗= (V, Val∗) with partition cells ΠX = {X} for X ∈V,\nΠ⊥= ∅, and functions πX = δX for X ∈V.\n\n   The notion of value merge relates to an important concept in the philosophy of causation.\nThe range of values ValX for a variable X can be more or less coarse-grained, and some\nlevels of resolution seem to be better causal-explanatory targets. For instance, to use a\nfamous example from Yablo (1992), if a bird is trained to peck any target that is a shade\nof red, then it would be misleading, if not incorrect, to say that appearance of crimson (a\nparticular shade of red) causes the bird to peck. Roughly, the reason is that this suggests\nthe wrong counterfactual contrasts:  if the target were not crimson (but instead another\nshade of red, say, scarlet), the bird would still peck. Thus, for a given explanatory purpose,\nthe level of grain in a model should guarantee that cited causes can be proportional to their\neffects (Yablo, 1992; Woodward, 2021).\n   The three operations above are notable not only for conceptual reasons, but also because\nthey suffice to decompose any alignment, as we will now explain. Composition of alignments\nis defined, as expected, via composition of their maps. Formally:\nDefinition 39 Let ⟨Π, π⟩be an alignment from signature (V, Val) to signature (V′, Val′)\nand ⟨Π′, π′⟩be an alignment from signature (V′, Val′) to signature (V′′, Val′′). We define the\ncomposition ⟨Π′, π′⟩◦⟨Π, π⟩as an alignment ⟨Π′′, π′′⟩from signature (V, Val) to signature\n(V′′, Val′′) whose cells Π′′ and maps π′′ are given as follows:\n  Π′′X′′ = [  ΠX′,   Π′′⊥=  [    ΠX′,   π′′X′′(y) = π′X′′ h [  πX′ ProjΠX′(y) i\n         X′∈Π′                   X′′                              X′∈Π′⊥∪{⊥}                       X′∈Π′X′′\n\n\n                                           18\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nWe then have the following:\n\nProposition 40 Let ⟨Π, π⟩be an alignment. Then there is a marginalization, variable\nmerge, and value merge whose composition is ⟨Π, π⟩.\n\nProof  It is straightforward to verify that the composition of alignments corresponding\nto the following three operations gives ⟨Π, π⟩.  First, marginalize the variables X = Π⊥.\nThen, variable merge with V∗as the index set for the partition Π. Lastly, value merge with\nδX = πX for each X ∈V∗.\n\n\n   Here, we give an example of an abstraction under a composition of marginalization,\nvariable merge, and value merges. At each step we give a model that is a constructive\nabstraction of the previous model under the corresponding operation, with the final result\nbeing a high-level model that is a constructive abstraction of the initial model.\n   For succinctness, we will henceforth use marginalization, variable merge, and value\nmerge as operations on models themselves. Thus, rather than saying, e.g., “a constructive\nabstraction of the model under a value merge alignment,” we simply say “a value merged\nmodel,” and so on.\n\n\nExample 4 Consider a causal model M that computes the maximum of two positive\nnumbers.\n                             Z\n\n                                   Y1   Y2   Y3\n\n                             X1  X2\n\n   The variables X1 and X2 take on positive real values and have constant functions\nmapping to 1. The variables Y1, Y2, Y3 take on real-valued numbers and have the following\nmechanisms:\n              FY1(x1, x2)           ⊤        !                                            x1     1 −1  1\n                      FY2(x1, x2)   ReLU                    =         x2   −1   1  1\n                      FY3(x1, x2)\n\n   The variable Z takes on a real-number value and has the mechanism FZ(y1, y2, y3) =\n12(y1 + y2 + y3). Observe that ProjZ(Solve(Mx1◦x2)) = max(x1, x2); only one of Y1 and Y2\ntakes on a positive value, depending on whether X1 or X2 is greater.\n       Z                Z              Z             Z\n\n    Y1   Y2   Y3         Y1 Y2              Y ∗              Y ∗\n\n     X1  X2             X1  X2          X1  X2          X1  X2\n              Marginalization         Variable Merge        Value Merge\n\n   The marginalization of M with Π⊥= {Y3} removes the mechanism of Y3 and changes\nthe mechanism of Z to FZ(x1, x2, y1, y2) = 12(y1 + y2 + x1 + x2).\n\n\n                                          19\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n   The variable merge of the marginalized model with ΠY ∗= {Y1, Y2} constructs a new\nvariable Y ∗with mechanism\n\n                                  FY1(x1, x2)            x1 −x2\n               FY ∗(x1, x2) =        = ReLU\n                                  FY2(x1, x2)            x2 −x1\n\nand alters the mechanism of Z to FZ(x1, x2, (y∗1, y∗2)) = 12(y∗1 + y∗2 + x1 + x2).\n    Finally, we value merge the marginalized and variable merged model. Define δY ∗: R2 →\n{0, 1} where δY ∗(y1, y2) = 1[y1 ≥0], which creates a binary partition over the values of Y . In\nthe case that y1 = x1 −x2, then δY ∗(h) = 1, meaning that x1 ≥x2; in the other possible case\nwhere y = 0 and x1 < x2, then δY ∗(h) = 0. After value merge with δ, the mechanism of Y ∗\nis FY ∗(x1, x2) = 1[x1 ≥x2] and the mechanism of Z is FZ(x1, x2, y∗) = y∗· x1 + (1 −y∗)x2.\n\n2.4 Approximate Transformation\n\nConstructive causal abstraction and other exact transformations are all-or-nothing notions;\nthe exact transformation relation either holds or it doesn’t. This binary concept prevents us\nfrom having a graded notion of faithful interpretation that is more useful in practice. We\ndefine a notion of approximate abstraction (Beckers et al., 2019; Rischel and Weichwald,\n2021; Zennaro et al., 2023b) that can be flexibly adapted:\n\nDefinition 41 (Approximate Transformation) Consider causal models M, M∗and\nintervention algebras (Ψ, ◦), (Ψ∗, ◦) with signatures Σ and Σ∗, respectively. Furthermore,\nlet τ  : ValV →ValV∗and ω be surjective partial functions where ω : Ψ →Ψ∗is ≤order\npreserving. Finally, we also need:\n\n   1. a (‘distance’) function Sim that maps two total settings of M∗to a real number,\n\n   2. a probability distribution P over Domain(ω) used to describe which interventionals are\n      expected,\n\n   3. a real-valued statistic S for the random variable Sim τ(Solve(MI)), Solve(M∗ω(I))\n     where I ∼P.\n\nTaken together, we can construct a metric that quantifies the degree to which M∗is an\napproximate transformation of M in a single number:\n\n                        SI∼P[Sim τ(Solve(MI)), Solve(M∗ω(I))  ]                        (6)\n\n   This metric is a graded version of Equation 3 in the definition of exact transformation.\nIf this number is above a particular cutoff η, we can say that M∗is an η-approximate\nabstraction of M.\n\nRemark 42 When M is a model with inputs and outputs, we might consider probability\ndistributions that only give mass to interventionals that assign all input variables. This\nensures that the default value for input variables is not taken into account (See Remark 7).\n\nRemark 43 In a paper on approximate causal abstraction, Beckers et al. (2019) pro-\npose dmax-α-approximate abstraction, which takes the maximum distance over low-level\ninterventions.\n\n\n                                           20\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nExample 5 Let M be a causal model that computes the sum of two numbers with addend\nvariables X1 and X2 that take on values {0, 1, 2, . . . } and a sum variable Y that takes on\nvalues {0, 1, 2, . . . }. Let M∗be a causal model that only sums multiples of 10 with addend\nvariables X∗1 and X∗2 that take on values {0, 10, 20, . . . } and a sum variable Y ∗that takes\non values {0, 10, 20, . . . }.\n  We can quantify the degree to which M∗approximates M where P is uniform over hard\ninterventions targeting exactly X1 and X2, Sim outputs the absolute difference between the\nvalues of Y ∗, and S is expected value. Let (Ψ, ◦) and (Ψ∗, ◦) be hard interventions and\ndefine maps τ(x1, x2, y) = {x1%10, x2%10, y%10} and ω(z) = {ProjZ(z)%10 : Z ∈Z}.\n   The degree to which M∗is an approximate transformation of M is\n\n                              S{x1,x2}∼P[Sim τ(Solve(M{x1,x2})), Solve(M∗ω({x1,x2}))  ] =\n    S{x1,x2}∼P[Sim (x1%10, x2%10, (x1 + x2)%10), (x1%10, x2%10, x1%10 + x2%10)  ] =\n                                   E{x1,x2}∼P[|(x1 + x2)%10) −(x1%10 + x2%10)|] = 4.5\n\n2.5 Interchange Interventions\n\nAn interchange intervention (Geiger et al., 2020, 2021) is an operation on a causal model\nwith input and output variables (e.g., acyclic models; recall Remark 7). Specifically, the\ncausal model is provided a ‘base’ input and an intervention is performed that fixes some\nvariables to be the values they would have if different ‘source’ inputs were provided. Such\ninterventions will be central to grounding mechanistic interpretability in causal abstraction.\n\nDefinition 44 (Interchange Interventions) Let M be a causal model with input vari-\nables XIn ⊆V. Furthermore, consider source inputs s1, . . . , sk ∈ValXIn and disjoint sets of\ntarget variables X1, . . . , Xk ⊆V \\ XIn. Define the hard intervention\n\n                                                                def\n            IntInv(M, ⟨s1, . . . , sk⟩, ⟨X1, . . . , Xk⟩)                            = [  ProjXj(Solve(Msj)).\n                                                1≤j≤k\n\nWe take interchange interventions as a base case and generalize to recursive interchange\ninterventions, where variables are fixed to be the value they would have if a recursive inter-\nchange intervention (that itself may be defined in terms of other interchange interventions)\nwere performed.\n\nDefinition 45 (Recursive Interchange Interventions) Let M be a causal model with\ninput variables XIn ⊆V. Define recursive interchange interventions of depth 0 to simply\nbe interchange interventions. Given s1, . . . , sk ∈ValXIn, disjoint sets of target variables\nX1, . . . , Xk ⊆V∗\\ XIn, and interchange interventions i1, . . . , ik of depth m, we define the\nrecursive interchange interventions of depth m + 1 to be\n\n  RecIntInvm+1(M, ⟨s1, . . . , sk⟩, ⟨X1, . . . , Xk⟩, ⟨i1, . . . , ik⟩) def= [  ProjXj(Solve(Msj◦ij)).\n                                                        1≤j≤k\n\n   Geiger et al. (2023) generalize interchange interventions to distributed interchange inter-\nventions that target variables distributed across multiple causal variables. We define this\noperation using an interventional that applies a bijective translation, performs an interchange\n\n\n                                           21\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nintervention in the new variable space, and then applies the inverse translation to get back\nto the original variable space.\n\nDefinition 46 (Distributed Interchange Interventions) Let M be a causal model\nwith input variables XIn ⊆V and let τ  : ValV →ValV∗be a bijective translation that\npreserves inputs.7  For source inputs s1, . . . , sk ∈ValXIn, and disjoint target variables\nX∗1, . . . , X∗k ⊆V∗\\ XIn, we define\n                       DistIntInv(M, τ, ⟨s1, . . . , sk⟩, ⟨X∗1, . . . , X∗k⟩)\n\nto be an interventional on all variables V that replaces each mechanism for X with the\nfunction\n            v 7→ProjX  τ −1 Solve τ(M)IntInv(M,⟨s1,...,sk⟩,⟨X∗1,...,X∗k⟩)       .\n\n   When conducting a causal abstraction analysis using interchange interventions, a partition\n{ΠX}X∈VH∪{⊥} defined over all high-level variables and mapping {πX}X∈XInH defined over\nhigh-level input variables are together sufficient to fully determine an alignment ⟨Π, π⟩.\n\nRemark 47 (Constructing an Alignment for Interchange Intervention Analysis)\nConsider low-level causal model L and high-level causal model H with input variables\nXInL ⊆VL and XInH ⊆VH, respectively. Suppose we have a partially constructed alignment\n({ΠX}X∈VH∪⊥, {πX}X∈XInH ) with\n\n       X ∈XInH ⇔ΠX ⊆XInL    X ∈VH \\ XInH ⇔ΠX ⊆VL \\ XInL\n\nWe can induce the remaining alignment functions from the partitions and the input alignment\nfunctions, and the two causal models. For YH ∈VH \\ XInH and zL ∈ValΠYH, if there exists\nxL ∈ValXInL such that zL = ProjΠYH(Solve(LxL)), then, with xH = πXInH (xL), we define the\nalignment functions\n                          πYH(zL) = ProjYH(Solve(HxH))\nand otherwise, we leave πYH undefined for zL. In words, map the low-level partial settings\nrealized for a given input to the corresponding high-level values realized for the same input.\n   This is a subtle point, but, in general, this construction is not well defined because two\ndifferent inputs can produce the same zL while producing different ProjYH(Solve(HxH)).\nIf this were to happen, the causal abstraction relationship simply wouldn’t hold for the\nalignment.\n\n   Once an alignment ⟨Π, π⟩is constructed, aligned interventions must be performed to\nexperimentally verify the alignment is a witness to the high-level model being an abstraction\nof the low-level model. Observe that π will only be defined for values of intermediate\npartition cells that are realized when some input is provided to the low-level model. This\ngreatly constrains the space of low-level interventions to intermediate partitions that will\ncorrespond with high-level interventions. Specifically, we are only able to interpret low-level\ninterchange interventions as high-level interchange interventions.\n\n 7. I.e., XIn ⊆V∗and for all x ∈ValXIn and y ∈ValV\\XIn there exists a y∗∈ValV∗\\XIn such that\n   τ(x ∪y) = x ∪y∗.\n\n\n                                           22\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n\n                                                                                             True\n                              Id2(y1, y2)\n\n                       Z                                                    True                 True\n                 Id1(x1, x2)             Id1(x3, x4)\n\n                     Y1                     Y2\n\n            X1       X2       X3       X4              (b) The total setting of L determined by the\n                                          empty intervention.     {     }{     }{     }{    }\n                                                                                             True\n\n          Id1\n                                      Id2   T    F                                  True                 True\n                T    F    F\n                                      T    T    F\n                F    T    F\n                                      F    F    T\n                F    F    T                                (c) The total setting of L determined by the\n                                                   intervention fixing X3, X4, and Y2 to be △, □,\n               (a) The algorithm.\n                                          and True.\n\nFigure 1: A tree-structured algorithm that perfectly solves the hierarchical equality task\nwith a compositional solution.\n\n\n   Geiger et al. (2022b) propose interchange intervention accuracy, which is simply the\nproportion of interchange interventions where the low-level and high-level causal models\nhave the same input–output behavior (see Section 2.6 for an example).\n\nDefinition 48 (Interchange Intervention Accuracy) Consider a low-level causal model\nL aligned to a high-level causal model H, an alignment (Π, π) as defined in Definition 47, and\na probability distribution P over the domain of ωπ. We define the interchange intervention\naccuracy as follows:\n\n                                       h                                    i       IIA(H, L, (Π, π)) = Ei∼P(Domain(ωπ)) 1[ProjXOutH  (τ π(Li)) = ProjXOutH  (Hωπ(i))]  .\n\nInterchange intervention accuracy is equivalent to input-output accuracy if we further restrict\nω to be defined only on interchange interventions where base and sources are all the same\nsingle input.\n\nThis is a special case of approximate causal abstraction (Section 2.4) where Sim(vL, vH) =\n1[ProjXOutH  (τ π(vL)) = ProjXOutH  (vH)] and S is expected value. This analysis can be extended\nto the case of distributed interchange interventions by simply applying a bijective translation\nto the low-level model before constructing the alignment to the high-level model.\n\n\n2.6 Example: Causal Abstraction in Mechanistic Interpretability\n\nWith the theory laid out, we can now present an example of causal abstraction from the field\nof mechanistic interpretability. We begin by defining two basic examples of causal models\nthat demonstrate a potential to model a diverse array of computational processes; the first\ncausal model represents a tree-structured algorithm and the second is fully-connected feed-\nforward neural network. Both the network and the algorithm solve the same ‘hierarchical\nequality’ task.\n  A basic equality task is to determine whether a pair of objects is identical. A hierarchical\nequality task is to determine whether a pair of pairs of objects have identical relations. The\n\n\n                                          23\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\ninput to the hierarchical task is two pairs of objects, and the output is True if both pairs are\nequal or both pairs are unequal, and False otherwise. For illustrative purposes, we define\nthe domain of objects to consist of a triangle, square, and pentagon. For example, the input\n(D,bothD,labeled△, □) isTrue.assigned the output False and the inputs (D, D, △, △) and (D, □, △, □) are\n  We chose hierarchical equality for two reasons. First, there is an obvious tree-structured\nsymbolic algorithm that solves the task: compute whether the first pair is equal, compute\nwhether the second is equal, then compute whether those two outputs are equal. We will\nencode this algorithm as a causal model. Second, equality reasoning is ubiquitous and has\nserved as a case study for broader questions about the representations underlying relational\nreasoning in biological organisms (Marcus et al., 1999; Alhama and Zuidema, 2019; Geiger\net al., 2022a).\n  We provide a companion jupyter notebook that walks through this example.\n\nA Tree-Structured Algorithm for Hierarchical Equality We define a tree struc-\ntured algorithm H consisting of four ‘input’ variables XInH = {X1, X2, X3, X4} each with\npossible values ValXj =   △, □}, two ‘intermediate’ variables Y1, Y2 with values ValYj =\n{True, False}, and one ‘output’{D,     variable XOutH  = {Z} with values ValZ = {True, False}.\n   The acyclic causal graph is depicted in Figure 1a, where each FXi (with no arguments)\nis a constant function to   (which will be overwritten, per Remark 7), and FY1, FY2, FO\n                                                    1 x1 = x2  . A totaleach compute equality overDtheir respective domains, e.g., FY1(x1, x2) =\nsetting can be captured by a vector [x1, x2, x3, x4, y1, y2, z] of values for each of the variables.\n   The default total setting that results from no intervention is           True, True, True]\n(see Figure 1b). We can also ask what would have occurred had[D,weD,intervenedD, D,    to fix X3, X4,\nand Y1 to be △, □, and False, for example. The result is     △, □, False, False, True]\n(See Figure 1c).                                           [D, D,\n\nA Handcrafted Fully Connected Neural Network for Hierarchical Equality  Define\na neural network L consisting of eight ‘input’ neurons XOutH  = {N1, . . . , N8}, twenty-four\n‘intermediate’ neurons H(i,j) for 1 ≤i ≤3 and 1 ≤i ≤8, and two ‘output’ neurons\nXOutH  = {OTrue, OFalse}. The values for each of these variables are the real numbers. We\ndepict the causal graph in Figure 2.\n    Let N, H1, H2, H3 be the sets of variables for the first four layers, respectively. We define\nFNk (with no arguments) to be a constant function to 0, for 1 ≤k ≤8. The intermediate\nand output neurons are determined by the network weights W1, W2 ∈R8×8, and W3 ∈R8×2.\nFor 1 ≤j ≤8, we define\n\n                 FH(1,j)(n) = ReLU((nW1)j)   FH(2,j)(h1) = ReLU((h1W2)j)\n                FOTrue(h2) = ReLU((h3W3)1)   FOFalse(h2) = ReLU((h2W3)2)\n\n   The four shapes that are the input for the hierarchical equality task are represented\nin    n□, n△∈R2 by a pair of neurons with randomized activation values. The network\noutputs True if the value of the output logit OTrue is larger than the value of OFalse, and False  nD,\notherwise. We can simulate a network operating on the input (□,   □, △) by performing\nan intervention setting (N1, N2) and (N5, N6) to n□, (N3, N4) to  D, and (N7, N8) to n△.\n    In Figure 2, we define the weights of the network L, which have been handcrafted to                                                 nD,\nimplement the tree-structured algorithm H.\n\n\n                                           24\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n\n                                                              Z\n\n                                                                             Y1                        Y2\n\n                                                        X1        X2        X3        X4\n\n\n\n                                                                                           OTrue          OFalse\n\n\n                                                     H(2,1)          H(2,2)          H(2,3)          H(2,4)          H(2,5)          H(2,6)          H(2,7)          H(2,8)\n\n\n                                                     H(1,1)          H(1,2)          H(1,3)          H(1,4)          H(1,5)          H(1,6)          H(1,7)          H(1,8)\n\n\n\n                                    N1        N2        N3        N4        N5        N6        N7        N8\n\n                             {      }        {      }        {      }        {      }\n\n            nD = [0.012, −0.301]  n□= [−0.812, 0.456]  n△= [0.682, 0.333]            1  0  1  0  0  0  0  0     1  1  0  1  0  0  0  0        1  0 \n                 0  1  0  1  0  0  0  0         1  1  0  1  0  0  0  0            1  0\n                 1  0  1  0  0  0  0  0         1  1  0  1  0  0  0  0         1 −ϵ  0\n                 0  1  0  1  0  0  0  0         1  1  0  1  0  0  0  0         1 −ϵ  0\n        W1 =                   W2 =                   W3 =\n                 0  0  0  0  1  0  1  0         1  1  1  0  0  0  0  0            0  0\n                 0  0  0  0  0  1  0  1         1  1  1  0  0  0  0  0            0  0\n                 0  0  0  0  1  0  1  0         1  1  1  0  0  0  0  0            0  0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                 0  0  0  0  0  1  0  1         1  1  1  0  0  0  0  0            0  0\n\n\nW3ReLU(W2ReLU(W1[a, b, c, d])) = [||a −b| −|c −d|| −(1 −ϵ)|a −b| −(1 −ϵ)|c −d|, 0]\n\n\nFigure 2: A fully-connected feed-forward neural network that labels inputs for the hierarchical\nequality task. The weights of the network are handcrafted to implement the tree-structured\nsolution to the task.\n\n\n\n\n\n                                                        False                                                                                               True\n\n                                           True                      False                                                                    True                       True\n\n\n\n\n\n                                   False                                                  True\n                                                                         oTrue                oFalse                                                                                                                  o∗True             o∗True\n\n               h(2,0)               h(2,1)               h(2,2)               h(2,3)               h(2,4)               h(2,5)               h(2,6)               h(2,7)           h∗(2,0)           h∗(2,1)           h∗(2,2)           h∗(2,3)           h∗(2,4)           h∗(2,5)           h∗(2,6)           h∗(2,7)\n\n\n               h(1,0)               h(1,1)               h(1,2)               h(1,3)               h(1,4)               h(1,5)               h(1,6)               h(1,7)               h(1,0)               h(1,1)               h(1,2)               h(1,3)             h′(1,4)             h′(1,5)             h′(1,6)             h′(1,7)\n\n\n            n1             n2             n3             n4             n5             n6             n7             n8               n′1               n′2               n′3               n′4               n′5               n′6               n′7               n′8\n\n\nFigure 3: The result of aligned interchange intervention on the low-level fully-connected\nneural network and a high-level tree structured algorithm under the alignment in Figure 2.\nObserve the equivalent counterfactual behavior across the two levels.\n\n\n\n\n\n                                          25\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n\n                                                     OTrue             OFalse\n\n   H(2,1)             H(2,2)             H(2,3)             H(2,4)             H(2,5)             H(2,6)             H(2,7)             H(2,8)                        Z\n\n   H(1,1)             H(1,2)             H(1,3)             H(1,4)             H(1,5)             H(1,6)             H(1,7)             H(1,8)                      Y1                     Y2\n\n   N1          N2          N3          N4          N5          N6          N7          N8\n                                                                 X1       X2       X3       X4\n   {        }          {        }          {        }          {        }\n                                       {     }{     }{     }{    }\n\n\n\n\n\n                                                     OTrue             OFalse                                             Z\n\n                                                                                        Y1                     Y2\n\n                                                                 X1       X2       X3       X4\n   H(1,1)             H(1,2)             H(1,3)             H(1,4)             H(1,5)             H(1,6)             H(1,7)             H(1,8)\n   N1          N2          N3          N4          N5          N6          N7          N8         {     }{     }{     }{    }\n\n   {        }          {        }          {        }          {        }\n\nFigure 4: An illustration of a fully-connected neural network being transformed into a tree\nstructured algorithm by (1) marginalizing away neurons aligned with no high-level variable,\n(2) merging sets of variables aligned with high-level variables, and (3) merging the continuous\nvalues of neural activity into the symbolic values of the algorithm.\n\n\nAn Alignment Between the Algorithm and the Neural Network  The network L\nwas explicitly constructed to be abstracted by the algorithm H under the alignment written\nformally below and depicted visually in Figure 2.\n\n    ΠZ = {OTrue, OFalse}    ΠXk = {N2k−1, N2k}     ΠY1 = {H(1,j) : 1 ≤j ≤4}\n   ΠY2 = {H(1,j) : 5 ≤j ≤8} Π⊥= V \\ (ΠZ ∪ΠY1 ∪ΠY2 ∪ΠX1 ∪ΠX2 ∪ΠX3 ∪ΠX4)\n                               □          (n2k−1, n2k) = n□\n                               \n           (True   oTrue > oFalse                       (n2k−1, n2k) =\n  πZ(oTrue, oFalse) =                       πXk(n2k−1, n2k) =\n                       False  otherwise                                △D          (n2k−1, n2k) = n△nD                                                                                                                            Undefined  otherwise\n\nWe follow Definition 47 to define πY1 and πY2 on all interchange interventions. For each\ninput n       n□, n△}4, let x = πXInH (n) and {h(1,j)  : 1 ≤j ≤8} = ProjH(Solve(Ln)).\nThen define      ∈{nD,\n                       πY1(h(1,1), h(1,2), h(1,3), h(1,4)) = ProjY1(Solve(Hx))\n                       πY2(h(1,5), h(1,6), h(1,7), h(1,8)) = ProjY2(Solve(Hx))\nOtherwise leave πY1 and πY2 undefined.\n   Consider an intervention i in the domain of ωπ. We have a fixed alignment for the\ninput and output neurons, where i can have output values from the real numbers and input\nvalues from     n□, n△}4. The intermediate neurons are assigned high-level alignment by\nstipulation; i can only have intermediate variables that are realized on some input intervention          {nD,\nLn for n       n□, n△}4 (i.e., interchange interventions). Constructive abstraction will\nhold only if these stipulative alignments to intermediate variables do not violate the causal       ∈{nD,\nlaws of L.\n\n\n                                           26\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nThe Algorithm Abstracts the Neural Network  Following Def. 47, the domain of\nωπ is restricted to 34 input interventions, (34)2 single-source hard interchange interventions\nfor high-level interventions fixing either Y1 or Y2, and (34)3 double-source hard interchange\ninterventions for high-level interventions fixing both Y1 and Y2.\n   This low-level neural network was hand crafted to be abstracted by the high-level\nalgorithm under the alignment ⟨Π, π⟩. This means that for all i ∈Domain(ωπ) we have\n\n                             τ π Solve Li  =  Solve Hωπ(i)                            (7)\n\nIn the companion jupyter notebook, we provide code that verifies this is indeed the case.\nIn Figure 3, we depict an aligned interchange intervention performed on L and H with the\nbase input     △, □) and a single source input (□,  △, △). The central insight is that\nthe network(D,andD,algorithm have the same counterfactualD, behavior.\n\nDecomposing the Alignment  Our decomposition of the alignment object in Section 2.3.2\nprovides a new lens through which to view this result. The network L can be transformed\ninto the algorithm H through a marginalization, variable merge, and value merge. We\nvisually depict the algorithm H being constructed from the network L in Figure 4.\n\nA Fully Connected Neural Network Trained on Hierarchical Equality  Instead of\nhandcrafting weights, we can also train the neural network L on the hierarchical equality\ntask. Looking at the network weights provides no insight into whether or not it implements\nthe algorithm H. A core result of Geiger et al. (2023) demonstrates that it is possible to learn\na bijective translation τ of the neural model L such that the algorithm H is a constructive\nabstraction of the transformed model τ(L). This bijective translation is in the form of an\northogonal matrix that rotates a hidden vector into a new coordinate system. The method\nis Distributed Alignment Search which is covered in Section 3.6.3 and the result is replicated\nin our companion jupyter notebook.\n\n\n2.7 Example: Causal Abstraction with Cycles and Infinite Variables\n\nCausal abstraction is a highly expressive, general purpose framework. However, our example\nin Section 2.6 involved only finite and acyclic models. To demonstrate the framework’s\nexpressive capacity, we will define a causal model with infinitely many variables with infinite\nvalue ranges that implements the bubble sort algorithm on lists of arbitrary length. Then,\nwe show this acyclic model can be abstracted into a cyclic process with an equilibrium state.\n\nA Causal Model for Bubble Sort  Bubble sort is an iterative algorithm. On each\niteration, the first two members of the sequence are compared and swapped if the left\nelement is larger than the right element; then the second and third member of the resulting\nlist are compared and possibly swapped, and so on until the end of the list is reached. This\nprocess is repeated until no more swaps are needed.\n    Define the causal model M to have the following (countably) infinite variables and values\n\n                      j\n                                                                                              j = {1, 2, 3, 4, 5, . . . } ∪{⊥}  V = {Xji , Yi , Zji  : i, j ∈{1, 2, 3, 4, 5, . . . }}  ValXji                              = ValYi\n                                 ValZji = {True, False, ⊥}\n\n   The causal structure of M is depicted in Figure 5a. The ⊥value will indicate that\na variable is not being used in a computation, much like a blank square on a Turing\n\n\n                                          27\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n                                                                                        ...                                              ...                                              ...                                              ...\n                     X31          X32          X33          X34          . . .\n\n                                     Y12           Y22           Y32          . . .\n\n                              Z21            Z22            Z23                        . . .\n\n                     X21          X22          X23          X24          . . .\n\n                                     Y11           Y21           Y31          .. .. ..\n\n                              Z11            Z12            Z13                        . . .\n\n                     X11          X12          X13          X14          . . .\n\n                   (a) A causal model that represents the bubble sort algorithm.\n                                                                                         ...                                              ...                                              ...                                              ...\n                     X31          X32          X33          X34          . . .\n\n\n                     X21          X22          X23          X24          . . .\n\n\n                     X11          X12          X13          X14          . . .\n\n         (b) The causal model from Figure 5a with the variables Yji and Zij marginalized.\n\n\n\n\n                     X∗1          X∗2          X∗3          X∗4          . . .\n\n\n\n                     X11          X12          X13          X14          . . .\n\n(c) The causal model from Figure 5b with the variables X2i , X3i , . . . merged for all i > 0. The values\nof these new variables contain the full history of the algorithm, e.g., the value of X∗1 a sequence\ncontaining the first element in the list after each bubbling iteration.\n\n                     X∗1          X∗2          X∗3          X∗4          . . .\n\n\n\n                     X11          X12          X13          X14          . . .\n\n(d) The causal model from Figure 5c with the values of each variable X∗j merged for all j > 0, e.g.,\nthe value of X∗1 is the first element in the sorted list.\n\n                Figure 5: Abstractions of the bubble sort causal model.\n\n\n\n\n                                           28\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\nmachine. The (countably) infinite sequence of variables X11, X12, . . . contains the unsorted\ninput sequence, where an input sequence of length k is represented by setting X11, . . . , X1k to\nencode the sequence and doing nothing to the infinitely many remaining input variables Xj\nfor j > k.\n   For a given row j of variables, the variables Zji store the truth-valued output of the\n                                                             j\ncomparison of two elements, the variables Yi  contain the values being ‘bubbled up’ through\nthe sequence, and the variables Xji are partially sorted lists resulting from j −1 passes\nthrough the algorithm. When there are rows j and j + 1 such that Xji and Xj−1i   take on\nthe same value for all i, the output of the computation is the sorted sequence found in both\nof these rows.\n  We define the mechanisms as follows. The input variables X1i have constant functions to\n⊥. The variable Z11 is ⊥if either of X11 or X12 is ⊥, True if the value of X11 is greater than\nX12, and False otherwise. The variable Y11 is ⊥if Z11 is ⊥, X11  if Z11 is True, and X12  if Z11 is\nFalse. The remaining intermediate variables can be uniformly defined:\n\n             xj−1i+1  zj−1i  = True           xji+1   zji = True                                                                                                                                                   j (yji−1, zji , xji+1) =   yji−1   zji = False         i−1 , zj−1i    , xj−1i+1) =  yj−1i−1   zj−1i  = False FYi  FXji (yj−1\n                          ⊥     zj−1i  = ⊥             ⊥     zji = ⊥\n\n                    (yji < xji+1  yji ̸= ⊥and xji+1 ̸= ⊥                                           i , xji+1) =                 FZji (yj                     ⊥          otherwise\n\n   This causal model is countably infinite, supporting both sequences of arbitrary length\nand an arbitrary number of sorting iterations.\n\nAbstracting Bubble Sort  Suppose we aren’t concerned with how, exactly, each iterative\npass of the bubble sort algorithm is implemented. Then we can marginalize away the variables\nZ = {Zji } and Y = {Yij } and reason about the resulting model instead (Figure 5b). Define\n                                                                       1    , xj−12   ) = Min(xj−11    , xj−12   )the mechanisms of this model recursively with base case FXj1(xj−1\nfor j > 1 and recursive case\n\n                                                                        1    , xj−12    , . . . , xj−1i   )))     FXji (xj−1              1    , xj−12    , . . . , xj−1i+1) = Min(xj−1i+1, Max(xj−1i    , FXji−1(xj−1\n\n   Suppose instead that our only concern is whether the input sequence is sorted. We can\n                                                              {Xji  : j ∈further abstract the causal model using variable merge with the partition ΠX∗i =\n{2, 3, . . . }} for each i ∈{1, 2, . . . }. The result is a model (Figure 5c) where each variable\nX∗i takes on the value of an infinite sequence. There are causal connections to and from X∗i\nand X∗j for any i ̸= j, because the infinite sequences stored in each variable must jointly be\na valid run of the bubble sort algorithm. This is a cyclic causal process with an equilibrium\npoint.\n    Next, we can value-merge with a family of functions δ where the input variable functions\nδX1i are identity functions and the other functions δX∗i output the constant value to which\nan eventually-constant infinite sequence converges. The mechanisms for the resulting model\n(Figure 5d) simply map unsorted input sequences to sorted output sequences.\n\n\n\n\n                                           29\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n3 A Common Language for Mechanistic Interpretability\n\nThe central claim of this paper is that causal abstraction provides a theoretical foundation for\nmechanistic interpretability. Equipped with a general theory of causal abstraction, we will\nprovide mathematically precise definitions for a handful of core mechanistic interpretability\nconcepts and show that a wide range of methods can be viewed as special cases of causal\nabstraction analysis. A tabular summary of this section can be found in Table 1.\n\n\n3.1 Polysemantic Neurons, the Linear Representation Hypothesis, and Modular\n    Features via Intervention Algebras\n\nA vexed question when analyzing black box AI is how to decompose a deep learning system\ninto constituent parts. Should the units of analysis be real-valued activations, directions in\nthe activation space of vectors, or entire model components? Localizing an abstract concept\nto a component of a black box AI would be much easier if neurons were a sufficient unit of\nanalysis. However, it has long been known that artificial (and biological) neural networks\nhave polysemantic neurons that participate in the representation of multiple high-level\nconcepts (Smolensky, 1986; Rumelhart et al., 1986; McClelland et al., 1986; Thorpe, 1989).\nTherefore, individual neural activations are insufficient as units of analysis in interpretability,\na fact that has been recognized in the recent literature (Harradon et al., 2018; Cammarata\net al., 2020; Olah et al., 2020; Goh et al., 2021; Elhage et al., 2021; Bolukbasi et al., 2021;\nGeiger et al., 2023; Gurnee et al., 2023; Huang et al., 2023a).\n\n   Perhaps the simplest case of polysemantic neurons is where some rotation can be\napplied to the neural activations such that the dimensions in the new coordinate system\nare monosemantic (Smolensky, 1986; Elhage et al., 2021; Scherlis et al., 2022; Geiger et al.,\n2023). Indeed, the linear representation hypothesis (Mikolov et al., 2013; Elhage et al., 2022b;\nNanda et al., 2023b; Park et al., 2023; Jiang et al., 2024) states that linear representations\nwill be sufficient for analyzing the complex non-linear building blocks of deep learning\nmodels. We are concerned that this is too restrictive. The ideal theoretical framework\nwon’t bake in an assumption like the linear representation hypothesis, but rather support\nany and all decompositions of a deep learning system into modular features that each have\nseparate mechanisms from one another. We should have the flexibility to choose the units of\nanalysis, free of restrictive assumptions that may rule out meaningful structures. Whether\na particular decomposition of a deep learning system into modular features is useful for\nmechanistic interpretability should be understood as an empirical hypothesis that can be\nfalsified through experimentation.\n\n   Our theory of causal abstraction supports a flexible, yet precise conception of modular\nfeatures via intervention algebras (Section 2.2). An intervention algebra formalizes the\nnotion of a set of separable components with distinct mechanisms, satisfying the fundamental\nalgebraic properties of commutativity and left-annihilativity (see (a) and (b) in Definition 16).\nIndividual activations, orthogonal directions in vector space, and model components (e.g.\nattention heads) are all separable components with distinct mechanisms in this sense. A\nbijective translation (Section 2.3.1) gives access to such features while preserving the overall\nmechanistic structure of the model. We propose to define modular features as any set of\nvariables that form an intervention algebra accessed by a bijective translation.\n\n\n                                          30\n\n            Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n\n                         Table 1: Interpretability Methods\n\n\nBehavioral Methods\n                        • Feature attribution (Zeiler and Fergus, 2014; Ribeiro et al., 2016;(Section 3.3)\n                         Lundberg and Lee, 2017)\n                        • Integrated gradients (Sundararajan et al., 2017)\n                        • Effects of real-world concepts on models (Goyal et al., 2019; Feder\n                                et al., 2021; Abraham et al., 2022; Wu et al., 2022a)\n\n\nPatching\n                        • Interchange interventions (Geiger et al., 2020; Vig et al., 2020; GeigerActivations with\n                                 et al., 2021; Li et al., 2021; Chan et al., 2022b; Wang et al., 2023; LieberumInterchange\n                               et al., 2023; Huang et al., 2023a; Hase et al., 2023; Cunningham et al.,Interventions\n                             2023; Davies et al., 2023; Tigges et al., 2023; Feng and Steinhardt, 2024;(Section 3.4)\n                          Ghandeharioun et al., 2024; Todd et al., 2024)\n                        • Path patching (Goldowsky-Dill et al., 2023; Wang et al., 2023; Hanna\n                                et al., 2023; Prakash et al., 2024)\n                        • Causal mediation analysis (Vig et al., 2020; Finlayson et al., 2021;\n                      Meng et al., 2022; Stolfo et al., 2023; Mueller et al., 2024)\n\n\nAblation-Based Analysis\n                        • Concept erasure (Ravfogel et al., 2020, 2022, 2023b,a; Elazar et al.,(Section 3.5)\n                             2020; Lovering and Pavlick, 2022; Belrose et al., 2023)\n                        • Sub-circuit analysis (Michel et al., 2019; Sanh and Rush, 2021; Csord´as\n                                 et al., 2021; Cammarata et al., 2020; Olsson et al., 2022; Chan et al., 2022b;\n                             Lepori et al., 2023b,a; Wang et al., 2023; Conmy et al., 2023; Nanda et al.,\n                           2023b)\n                        • Causal scrubbing (Chan et al., 2022b)\n\n\nModular Feature\n                        • Principal Component Analysis (Bolukbasi et al., 2016; Chormai et al.,Learning\n                             2022; Marks and Tegmark, 2023; Tigges et al., 2023)(Section 3.6)\n                        • Sparse autoencoders (Bricken et al., 2023; Cunningham et al., 2023;\n                       Huben et al., 2024; Marks et al., 2024)\n                        • Differential Binary Masking (De Cao et al., 2020, 2021; Csord´as et al.,\n                             2021; Davies et al., 2023; Prakash et al., 2024; Huang et al., 2024)\n                        • Probing (Peters et al., 2018; Tenney et al., 2019; Hupkes et al., 2018)\n                        • Difference of means (Tigges et al., 2023; Marks and Tegmark, 2023)\n                        • Distributed Alignment Search (Geiger et al., 2023; Wu et al., 2023;\n                            Tigges et al., 2023; Arora et al., 2024; Huang et al., 2024; Minder et al.,\n                             2024; Feng et al., 2024; Rodriguez et al., 2024; Grant et al., 2025)\n\n\nActivation Steering         (Giulianelli et al., 2018; Bau et al., 2019; Soulos et al., 2020; Besserve et al.,\n(Section 3.7)              2020; Subramani et al., 2022; Turner et al., 2023; Zou et al., 2023; Vogel,\n                           2024; Li et al., 2024; Wu et al., 2024a,b)\n\nTraining Models to be     (Geiger et al., 2022b; Wu et al., 2022b,a; Elhage et al., 2022a; Hewitt et al.,\nInterpretable              2023; Y¨uksekg¨on¨ul et al., 2023; Chauhan et al., 2023; Huang et al., 2023b;\n(Section 3.8)            Tamkin et al., 2024; G´omez and Cin`a, 2024; Zur et al., 2024; Liu et al., 2024)\n\n\n\n\n\n                                         31\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n     If the linear representation hypothesis is correct, then rotation matrices should be\nsufficient bijective translations for mechanistic interpretability. If not, there will be cases\nwhere non-linear bijective translations will be needed to discover modular features that are\nnot linearly accessible, e.g., the ‘onion’ representations found by Csord´as et al. (2024) in\nsimple recurrent neural networks. Our conception of modular features enables us to remain\nagnostic to the exact units of analysis that will prove essential.\n\n\n3.2 Graded Faithfulness via Approximate Abstraction\n\nInformally, faithfulness has been defined as the degree to which an explanation accurately\nrepresents the ‘true reasoning process behind a model’s behavior’ (Wiegreffe and Pinter,\n2019; Jacovi and Goldberg, 2020; Lyu et al., 2022; Chan et al., 2022a). Crucially, faithfulness\nshould be a graded notion (Jacovi and Goldberg, 2020), but precisely which metric of\nfaithfulness is correct will depend on the situation. It could be that for safety reasons there\nare some domains of inputs for which we need a perfectly faithful interpretation of a black\nbox AI, while for others it matters less. Ideally, we can defer the exact details to be filled in\nbased on the use case. This would allow us to provide a variety of graded faithfulness metrics\nthat facilitates apples-to-apples comparisons between existing (and future) mechanistic\ninterpretability methods.\n   Approximate transformation (Section 2.4) provides the needed flexible notion of graded\nfaithfulness. The similarity metric between high-level and low-level states, the probability\ndistribution over evaluated interventions, and the summary statistic used to aggregate\nindividual similarity scores are all points of variation that enable our notion of approximate\ntransformation to be adapted to a given situation. Interchange intervention accuracy (Geiger\net al., 2022b, 2023; Wu et al., 2023), probability or logit difference (Meng et al., 2022; Chan\net al., 2022b; Wang et al., 2023; Zhang and Nanda, 2024), and KL-divergence all can be\nunderstood via approximate transformation.\n\n\n3.3 Behavioral Evaluations as Abstraction by a Two Variable Chains\n\nThe behavior of an AI model is simply the function from inputs to outputs that the model\nimplements. Behavior is trivial to characterize in causal terms; any input–output behavior\ncan be represented by a model with input variables directly connected to output variables.\n\n3.3.1 LIME: Behavioral Fidelity as Approximate Abstraction\n\nFeature attribution methods ascribe scores to input features that capture the ‘impact’ of a\nfeature on model behavior. Gradient-based feature attribution methods (Zeiler and Fergus,\n2014; Springenberg et al., 2014; Shrikumar et al., 2016; Binder et al., 2016; Lundberg and\nLee, 2017; Kim et al., 2018; Narendra et al., 2018; Lundberg et al., 2019; Schrouffet al., 2022)\nmeasure causal properties when they satisfy some basic axioms (Sundararajan et al., 2017).\nIn particular, Geiger et al. (2021) provide a natural causal interpretation of the integrated\ngradients method and Chattopadhyay et al. (2019) argue for a direct measurement of a\nfeature’s individual causal effect.\n  Among the most popular feature attribution methods is LIME (Ribeiro et al., 2016),\nwhich learns an interpretable model that locally approximates an uninterpretable model.\nLIME defines an explanation to be faithful to the degree that the interpretable model agrees\n\n\n                                          32\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nwith local input–output behavior. While not conceived as a causal explanation method,\nwhen we interpret priming a model with an input as an intervention, it becomes obvious\nthat two models having the same local input–output behavior is a matter of causality.\n    Crucially, however, the interpretable model lacks any connection to the internal causal\ndynamics of the uninterpretable model. In fact, it is presented as a benefit that LIME is\na model-agnostic method that provides the same explanations for models with identical\nbehaviors, but different internal structures. Without further grounding in causal abstraction,\nmethods like LIME do not tell us anything meaningful about the abstract causal structure\nbetween input and output.\n\nDefinition 49 (LIME Fidelity of Interpretable Model) Let L and H be models with\nidentical input and output spaces. Let Distance(·, ·) compute some measure of distance\nbetween outputs. Given an input x ∈ValXInH , let ∆x ⊆ValXInH be a finite neighborhood of\ninputs close to x. The LIME fidelity of using H to interpret L on the input x is given as:\n\n                      1\n   LIME(H, L, ∆x) =  X Distance(ProjXOutL  (Solve(Lx′)), ProjXOutH  (Solve(Hx′)))                     |∆x|\n                           x′∈∆x\n\n   The uninterpretable model L is an AI model with fully-connected causal structure:\n                                H11    H12 . . .  H1l\n                                                                                   . . .                                H21    H22     H2l\n                                                       L                         XInL                         ...                               ...                                    ...    XOut\n                                                                                   . . .                                Hd1    Hd2     Hdl\n\n   The interpretable model H will often also have rich internal structure—such as a decision\ntree model—which one could naturally interpret as causal. For instance:\n                             X1\n                                    X4\n                         XInH   X2                XOutH\n\n                             X3\n\n   However, LIME only seeks to find a correspondence between the input–output behaviors\nof the interpretable and uninterpretable models. Therefore, representing both L and H as a\ncausal models connecting inputs to outputs is sufficient to describe the fidelity measure in\nLIME. To shape approximate transformation to mirror the LIME fidelity metric, define\n\n   1. The similarity between a low-level and high-level total state to be\n\n                   Sim(vL, vH) = Distance(ProjXOutL  (vL), ProjXOutH  (vH))\n\n   2. The probability distribution P to assign equal probability mass to input interventions\n      in ∆x and zero mass to all other interventions.\n\n   3. The statistic S to compute the expected value of a random variable.\n\nThe LIME fidelity metric is the approximate transformation metric (Def. 41) with τ and ω\nas identity functions:\n\n              LIME(H, L, ∆x) = SI∼P[Sim τ(Solve(LI)), Solve(Hω(I))  ]\n\n\n                                          33\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n                         XInL                     XOutL\n\n\n                         XInH                     XOutH\n\n\n3.3.2 Single Source Interchange Interventions from Integrated Gradients\n\nIntegrated gradients (Sundararajan et al., 2017) computes the impact of neurons on model\npredictions. Following Geiger et al. (2021), we can easily translate the original integrated\ngradients equation into our causal model formalism.\n\nDefinition 50 (Integrated Gradients) Given a neural network as a causal model M,\nwe define the integrated gradient value of the ith neuron of an hidden vector Y when the\nnetwork is provided x as an input as follows, where y′ is the so-called baseline value of Y:\n\n                              Z 1  ∂ProjXOut(Mx∪(δy+(1−δ)y′))\n                 IGi(y, y′) = (yi −y′i) ·                               dδ\n                                      δ=0             ∂yi\n\nThe completeness axiom of the integrated gradients method is formulated as follows:\n\n\n                       |Y|\n      X IGi(y, y′) = ProjXOut(Mx∪y) −ProjXOut(Mx∪y′)\n                    i=1\n\n   Integrated gradients was not initially conceived as a method for the causal analysis of\nneural networks. Therefore, it is perhaps surprising that integrated gradients can be used to\ncompute interchange interventions. This hinges on a strategic use of the ‘baseline’ value of\nintegrated gradients; typically, the baseline value is set to be the zero vector, but here we\nset it to an interchange intervention.\n\nRemark 51 (Integrated Gradients Can Compute Interventions) The following is\nan immediate consequence of the completeness axiom\n\n\n                                                              |Y|\n             ProjXOut(Mx∪y′) = ProjXOut(Mx) − X IGi ProjY(Mx), y′\n                                                       i=1\n\nIn principle, we could perform causal abstraction analysis using the integrated gradients\nmethod and taking\n                                   y′ = IntInv(M, ⟨x′⟩, ⟨Y⟩)\n\nHowever, computing integrals is an inefficient way to compute interchange interventions.\n\n3.3.3 Estimating the Causal Effect of Real-World Concepts\n\nThe ultimate downstream goal of explainable AI is to provide explanations with intuitive\nconcepts (Goyal et al., 2019; Feder et al., 2021; Elazar et al., 2022; Abraham et al., 2022)\nthat are easily understood by human decision makers and guide their actions (Karimi et al.,\n2021, 2023; Beckers, 2022). These concepts can be abstract and mathematical, such as\n\n\n                                          34\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\ntruth-valued propositional content, natural numbers, or real valued quantities like height or\nweight; they can also be grounded and concrete, such as the breed of a dog, the education\nlevel of a job applicant, or the pitch of a singer’s voice. A basic question is how to estimate\nthe effect of real-world concepts on the behavior of AI models.\n   The explainable AI benchmark CEBaB (Abraham et al., 2022) evaluates methods on\ntheir ability to estimate the causal effects of the quality of food, service, ambiance, and\nnoise in a real-world dining experience on the prediction of a sentiment classifier, given a\nrestaurant review as input data. Using CEBaB as an illustrative example, we represent\nthe real-world data generating process and the neural network with a single causal model\nMCEBaB.8 The real-world concepts Cservice, Cnoise, Cfood, and Cambiance can take on three\nvalues +, −, and Unknown, the input data XIn takes on the value of a restaurant review\ntext, the prediction output XOut takes on the value of a five star rating, and hidden vectors\nHij can take on real number values.\n                           Cservice\n                                     H11    H12 . . .  H1l\n                           Cnoise                                            . . .                                     H21    H22     H2l\n                             XIn                      ...                               ...                                    ...    XOut\n                         Cfood                                            . . .                                    Hd1    Hd2     Hdl\n                     Cambiance\n\n     If we are interested in the causal effect of food quality on model output, then we can\nmarginalize away every variable other than the real-world concept Cfood and the neural\nnetwork output XOut to get a causal model with two variables. This marginalized causal\nmodel is a high-level abstraction of MCEBaB that contains a single causal mechanism\ndescribing how food quality in a dining experience affects the neural network output.\n\n\n                                                      XOut\n                            Cfood\n\n\n\n3.4 Patching Activations with Interchange Interventions\n\nThere is a diverse body of mechanistic interpretability literature in which interchange\ninterventions (see Section 2.5) are used to analyze neural networks (see Table 1). However,\nthe terminology used within this literature is often inconsistent and can lead to confusion\nregarding the precise techniques being employed. What is called an ‘activation patch’ is\ntypically equivalent to an interchange intervention on a neural network, but the term is\nsometimes used to describe a variety of other intervention techniques. Wang et al. (2023)\nuse ‘activation patching’ to mean (recursive) interchange interventions, while Conmy et al.\n(2023), Zhang and Nanda (2024), and Heimersheim and Nanda (2024) include ablation\ninterventions under this heading (see Section 3.5), and Ghandeharioun et al. (2024) include\narbitrary transformations that are more akin to activation steering (see Section 3.7). We\npropose to use ‘activation patch’ to refer broadly to interventions on hidden vectors in neural\nnetworks, while using ‘interchange intervention’ to pick out a specific type of intervention\non causal models, which may be neural networks in some cases.\n\n\n 8. The models in Abraham et al. (2022) are probabilistic models, but we simplify to the deterministic case.\n\n\n                                           35\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n3.4.1 Causal Mediation as Abstraction by a Three-Variable Chain\n\nVig et al. (2020), Finlayson et al. (2021), Meng et al. (2022), and Stolfo et al. (2023) apply\nthe popular causal inference framework of mediation analysis (Imai et al., 2010; Hicks and\nTingley, 2011) to understand how internal model components of neural networks mediate\nthe causal effect of inputs on outputs. It is straightforward to show that mediation analysis\nis a special case of causal abstraction analysis. Mediation analysis is compatible with both\nablation interventions (see Section 3.5) and interchange interventions. In this section, we\npresent mediation analysis with interchange interventions.\n   Suppose that changing the value of variables X from x to x′ has an effect on a second\nset of variables Y. Causal mediation analysis determines how this causal effect is mediated\nby a third set of intermediate variables Z. The fundamental notions involved in mediation\nare total, direct, and indirect effects, which can be defined with interchange interventions.\n\n\nDefinition 52 (Total, Direct, and Indirect Effects) Consider a causal model M with\ndisjoint sets of variables X, Y, Z ⊂V such that addition and subtraction are well-defined on\nvalues of Y. The total causal effect of changing the values of X from x to x′ on Y is\n\n            TotalEffect(M, x, x′, Y) = ProjY(Solve(Mx′)) −ProjY(Solve(Mx))\n\nThe direct causal effect of changing X from x to x′ on Y around mediator Z is\n\n   DirectEffect(M, x, x′, Y, Z) = ProjY(Solve(Mx′∪IntInv(M,⟨x⟩,⟨Z⟩))) −ProjY(Solve(Mx))\n\nThe indirect causal effect of changing X from x to x′ on Y through mediator Z is\n\n  IndirectEffect(M, x, x′, Y, Z) = ProjY(Solve(Mx∪IntInv(M,⟨x′⟩,⟨Z⟩))) −ProjY(Solve(Mx))\n\n\n   This method has been applied to the analysis of neural networks to characterize how the\ncausal effect of inputs on outputs is mediated by (parts of) hidden vectors, with the goal\nidentifying complete mediators. This is equivalent to a simple causal abstraction analysis.\n\n\nRemark 53 Consider a neural network L with inputs XIn, outputs XOut, and hidden\nvector H. Define ΠX = XIn, ΠY = XOut, ΠZ = H, and Π⊥= V \\ (XIn ∪H ∪XOut).\nApply variable merge and marginalization to L with Π to obtain a high-level model H that\nis an abstraction of L with τ and ω as identity functions. The following are equivalent:\n\n\n   1. The hidden vector H completely mediate the causal effect of inputs on outputs:\n\n                   IndirectEffect(L, x, x′, XOut, H) = TotalEffect(M, x, x′, XOut)\n\n\n   2. H has a structure such that X is not a child of Y :\n                   X     Z     Y\n\n\n                                          36\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n3.4.2 Path Patching as Recursive Interchange Interventions\n\nPath patching (Wang et al., 2023; Goldowsky-Dill et al., 2023; Hanna et al., 2023; Zhang\nand Nanda, 2024; Prakash et al., 2024) is a type of interchange intervention analysis that\ntargets the connections between variables rather than variables themselves. To perform a\npath patch, we use a recursive interchange intervention on a model M processing a base\ninput b that simulates ‘sender’ variables H taking on intervened values from a source input\ns, restricting the effect of this intervention to receiver variables R while freezing variables F.\n   Each receiver variable takes on a value determined by the s input for the sender variables\nH while fixing F to be the value determined by the b input. For receiver variable R ∈R,\ndefine an interchange intervention:\n\n                           iR = IntInv(M, ⟨s, b⟩, ⟨H, F \\ {R}⟩)\n\n   The path patch is a recursive interchange intervention resulting from the receiver\nvariables {R1, . . . , Rk} = R taking on the value determined by each of the basic interchange\ninterventions:\n\n                      j = RecIntInv(M, ⟨b, . . . , b⟩, ⟨R1, . . . , R2⟩, ⟨iR1, . . . , iRk⟩)\n\nThe intervened model Mb∪j has a patched path according to the definition of Wang et al.\n(2023). Simpler path patching experiments will not freeze any variables, meaning F = ∅.\nWe show a visualization below, where H = {H11} is the sender neuron, R = {H32} is the\nreceiver neuron, and F = {H22} is a neuron we would like to keep frozen (meaning it has no\neffect in computing the value of the patched receiver node).\n\n            o                        o′                   o∗                   o∗∗\n       h31   h32   h33      h′31   h′32   h′33     h∗31   h∗32   h∗33      h31   h∗32   h33\n       h21   h22   h23      h′21   h′22   h′23     h∗21   h22   h∗23      h21   h22   h23\n       h11   h12   h13      h′11   h′12   h′13      h′11   h12   h13      h11   h12   h13\n\n         b                   s             b             b\n\n\n\n\n\n3.5 Ablation as Abstraction by a Three Variable Collider\n\nNeuroscientific lesion studies involve damage to a region of the brain in order to determine\nits function; if the lesion results in a behavioral deficit, then the brain region is assumed\nto be involved in the production of that behavior. In mechanistic interpretability, such\ninterventions are known as ablations. Common ablations include replacing neural hidden\nvectors with zero activations (Cammarata et al., 2020; Olsson et al., 2022; Geva et al., 2023)\nor with mean activations over a set of input data (Wang et al., 2023), adding random noise\nto the activations (causal tracing; Meng et al. 2022, 2023), and replacing activations with the\nvalues from a different input (resample ablations; Chan et al. 2022b). To capture ablation\nstudies as a special case of causal abstraction analysis, we only need a high-level model\n\n\n                                           37\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nwith an input variable, an output variable, and a binary valued variable aligned with the\nvariables targeted for ablation.\n\n\n3.5.1 Concept Erasure\n\n\nConcept erasure is a common application of ablations in which an hidden vector H in a\nneural network L is ablated in order to remove information about a particular concept C\n(Ravfogel et al., 2020, 2022, 2023b,a; Elazar et al., 2020; Lovering and Pavlick, 2022; Olsson\net al., 2022; Belrose et al., 2023). To quantify the success of a concept erasure experiment,\neach concept C is associated with some degraded behavioral capability encoded as a partial\nfunction AC : ValXInL →ValXOutL   (e.g., ablating the concept ‘dog’ would be associated with\nthe behavior of inaccurately captioning images with dogs in them). If performing an ablation\non H to erase the concept C leads L to have the degraded behavior AC without changing\nother behaviors, then the ablation was successful.\n\n  We can model ablation on L as abstraction by a three-variable causal model. Define a\nhigh-level signature to be an input variable X taking on values from XInL , output variable Y\ntaking on values from XOutL    , and a binary variable Z that indicates whether the concept C\nhas been erased. The mechanism for X assigns an arbitrary default input, the mechanism\nfor Z assigns 0, and the mechanism for Y produces the degraded behavior if Z is 1, and\nmimics L otherwise:\n\n\n              (AC(x)               z = 1 and x ∈Domain(AC)\n          FY (x, z) =                                                                                   .\n                         ProjXOutL  (Solve(Lx))  Otherwise\n\n\n\n   The map τ from low-level settings to the high-level settings simply sets Z to be 0 exactly\nwhen the low-level input determines the value of the model component H:\n\n\n        ({ProjXIn(v), 0, ProjXOut(v)}  ProjH(v) = ProjH(Solve(LProjXIn(v)))      τ(v) =                                                                                                              .\n                {ProjXIn(v), 1, ProjXOut(v)}  Otherwise\n\n\n\n   The function ω is defined on low-level input interventions and the interventional I that is\nan ablation on H (e.g., setting activations to zero or a mean value, or projecting activations\nonto a linear subspace whose complement is thought to encode the concept C). Low-level\ninput interventions are mapped by ω to identical high-level input interventions, while I is\nmapped by ω to the high-level intervention setting Z to 1.\n\n   The high-level causal model H is an exact transformation of the low-level neural model L\nunder (τ, ω) exactly when the ablation removing the concept C results in degraded behavior\ndefined by AC. We show a visualization below, where H = {H12, H22}.\n\n\n                                           38\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n\n              X        Z            Y\n\n\n\n                            H11    H12    H13\n\n                    XInL   H21    H22    H23   XOutL\n                            H31    H32    H33\n\n   Observe that the high-level model H does not have a variable encoding the concept C\nand the values it might take on. Ablation studies attempt to determine whether a concept\nis used by a model; they do not characterize how that concept is used.\n\n3.5.2 Sub-Circuit Analysis\n\nSub-circuit analysis (Michel et al., 2019; Sanh and Rush, 2021; Csord´as et al., 2021; Cam-\nmarata et al., 2020; Olsson et al., 2022; Chan et al., 2022b; Lepori et al., 2023b,a; Wang\net al., 2023; Conmy et al., 2023) aims to identify a minimal circuit C ⊆V × V between\ncomponents in a model L that is sufficient to perform particular behavior that we represent\nwith a partial function B : ValXInL →ValXOutL   . This claim is cashed out in terms of abla-\ntions, specifically, the behavior B should remain intact when all connections between model\ncomponents C = V × V \\ C are ablated. Define H = {H  : ∃G(G, H) ∈C} as the model\ncomponents with incoming severed connections, G = {G : ∃H(G, H) ∈C} as the model\ncomponents with outgoing severed connections, and let g be the ablation values.\n  We can model sub-circuit analysis on L as abstraction by a three-variable causal model.\nDefine a high-level signature to be an input variable X taking on values from XInL , output\nvariable Y taking on values from XOutL  ∪{⊥}, and a binary variable Z that indicates whether\nthe connections C have been severed. The mechanism for X assigns an arbitrary default\ninput, the mechanism for Z assigns 0, and the mechanism for Y mimics the behavior of L\nwhen Z is 0 and preserves only the behavior B when Z is 1:\n\n               ProjXOutL  (Lx)  z = 0                                           FY (x, z) =  B(x)          z = 1 and x ∈Domain(B)\n                              ⊥             z = 1 and x ̸∈Domain(B)\n\n   The map τ from low-level settings to high-level settings simply sets Z to be 0 exactly\nwhen the low-level input determines the value of the model components H:\n\n        ({ProjXIn(v), 0, ProjXOut(v)}  ProjH(v) = ProjH(Solve(LProjXIn(v)))      τ(v) =\n                {ProjXIn(v), 1, ProjXOut(v)}  Otherwise\n\n   The function ω is defined on low-level input interventions in Domain(B) and the inter-\nventional I that fixes the connections in C to an ablated value and leaves the connections in\n\n\n                                          39\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nC untouched:\n\n          I⟨FH⟩H = v 7→FH  Proj{G:(G,H)̸∈C}(v) ∪  [     ProjG(g)\n                                                     G∈{G:(G,H)∈C}\n\n\nLow-level input interventions are mapped by ω to identical high-level input interventions\nand I is mapped by ω to the high-level intervention setting Z to 1.\n\n   The high-level causal model H is an exact transformation of the low-level neural model L\nunder (τ, ω) exactly when the subcircuit C preserves the behavior B. We show a visualization\nbelow, where:\n\n    C = XIn × {H31} ∪{(H31, H12), (H31, H22), (H31, H32), (H22, H23), (H12, H23)}\n\n\n\n\n\n              X        Z            Y\n\n\n\n                            H11    H12    H13\n\n                    XInL   H21    H22    H23   XOutL\n                            H31    H32    H33\n\n\n3.5.3 Causal Scrubbing\n\nCausal scrubbing (Chan et al., 2022b) is an ablation method that proposes to determine\nwhether a circuit C is sufficient for a behavior B. It doesn’t fit into our general paradigm\nof sub-circuit analysis for two reasons.  First, the minimal circuit C is determined by a\nhigh-level causal model H with identical input and output spaces as L and a surjective partial\nfunction δ : VL →VH assigning each low-level variable a high-level variable. Specifically,\nthe low-level minimal circuit is the high-level causal graph pulled back into the low-level\nC = {(G, H) : δ(G) ≺δ(H)}. Second, the connections in the minimal circuit C are intervened\nupon in addition to connections in C. This means that every single connection in the network\nis being intervened upon.\n\n   Given a base input b, causal scrubbing recursively intervenes on every connection in the\nnetwork. The connections in C are replaced using randomly sampled source inputs, and the\nconnections (G, H) ∈C are replaced using randomly sampled source inputs that set δ(H)\nto the same value in H. Chan et al. (2022b) call these interchange interventions—where\nthe base and source input agree on a high-level variable—resampling ablations. The exact\nintervention value for each targeted connection is determined by a recursive interchange\nintervention performed by calling the algorithm Scrub(b, XOutL   ) defined below.\n\n\n                                          40\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n Algorithm 1: Scrub(b, H)\n\n  1 h ←{}\n  2 for H ∈H do\n  3       if H ∈XInL then\n  4      h ←h ∪ProjH(b)\n  5       continue\n\n  6    g ←{}\n  7     for G ∈{G : (G, H) ̸∈C} do\n  8        s ∼ValXInL\n  9       g ←g ∪Scrub(s, {G})\n\n 10       if H ∈Domain(δ) then\n 11        s ∼{s ∈ValXInL  : Projδ(H)(Solve(Hs)) = Projδ(H)(Solve(Hb))}\n 12       g ←g ∪Scrub(s, {G : (G, H) ∈C})\n\n 13   h ←h ∪ProjH(Solve(Lg))\n 14 return h\n\n   While causal scrubbing makes use of a high-level model H, the only use of this model in\nthe algorithm Scrub is to sample source inputs s that assign the same value to a variable\nas a base input b. No interventions are performed on the high-level model and so no\ncorrespondences between high-level and low-level interventions are established. This means\nthat we can still model causal scrubbing as abstraction by the same three-variable causal\nmodel we defined in Section 3.5.2, which we will name H∗. The map τ from low-level settings\nto the high-level settings simply sets Y to be 0 exactly when the low-level input determines\nthe value of the total setting\n\n            ({ProjXIn(v), 0, ProjXOut(v)}  v = Solve(LProjXIn(v))             τ(v) =\n                       {ProjXIn(v), 1, ProjXOut(v)}  Otherwise\n\n   The function ω is defined to map low-level input interventions to identical high-level input\ninterventions and map any interventional I resulting from a call to Scrub to the high-level\nintervention setting Z to 1.\n   Chan et al. (2022b) propose to measure the faithfulness of H by appeal to the proportion of\nperformance maintained by L under interventionals determined by Scrub. This is equivalent\nto the approximate transformation metric for high-level causal model H∗and low-level\ncausal model L under (τ, ω) (Definition 41) with P as a random distribution over inputs\nand interventionals from Scrub, Sim as a function outputting 1 only if the low-level and\nhigh-level outputs are equal, and S as expected value.\n\n\n3.6 Modular Feature Learning as Bijective Transformation\n\nA core task of mechanistic interpretability is disentangling a vector of activations into a set\nof modular features that correspond to human-intelligible concepts. We construe modular\nfeature learning as constructing a bijective translation (Def. 28). Some methods use a\nhigh-level causal model as a source of supervision in order to construct modular features\nthat localize the concepts encoded in the high-level intermediate variables. Other methods\nare entirely unsupervised and produce modular features that must be further analyzed\n\n\n                                          41\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nto determine the concepts they might encode. Formalizing modular feature learning as\nbijective transformation provides a unified framework to evaluate commonly used mechanistic\ninterpretability methods through distributed interchange interventions (see Huang et al.\n2024 for a mechanistic interpretability benchmark based in this framework).\n\n3.6.1 Unsupervised Methods\n\nPrincipal Component Analysis  Principal Component Analysis (PCA) is a technique\nthat represents high-dimensional data in a lower-dimensional space while maximally pre-\nserving information in the original data. PCA has been used to identify subspaces related\nto human-interpretable concepts, e.g., gender (Bolukbasi et al., 2016), sentiment (Tigges\net al., 2023), truthfulness (Marks and Tegmark, 2023), and visual concepts involved in object\nclassification (Chormai et al., 2022).\n   The modular features produced by PCA are simply the principal components. Given\na model M and k inputs, to featurize an n-dimensional hidden vector H we first create a\nk × n matrix with a column ProjH(Solve(MxIn)) for each input xIn. Then we use PCA to\ncompute an n × n matrix P whose rows are principal components. The bijective translation\nτ : V →V is defined as\n                         τ(v) = ProjV\\H(v) ∪PT ProjH(v).\n\nSparse Autoencoders  Sparse autoencoders are tools for learning to translate an n-\ndimensional hidden vector H into a sparse, k-dimensional feature space with k ≫n (Bricken\net al., 2023; Cunningham et al., 2023; Huben et al., 2024; Marks et al., 2024). However,\ninstead of learning a single invertible function that translates activations into a new feature\nspace, sparse autoencoders separately learn an encoder fenc and decoder fdec, each typically\nparameterized by a single layer feed-forward network. These two functions are optimized to\nreconstruct the activations ValH while creating a sparse feature space, with a hyperparameter\nλ balancing the two terms:\n\n     ℓ= X      fdec  fenc(ProjH(Solve(MxIn)))  −ProjH(Solve(MxIn))\n             xIn∈XIn                                                                    2\n                                            !\n                                            +λ∥fenc(ProjH(Solve(MxIn)))∥1\n\n\n\nGiven a sparse autoencoder that perfectly reconstructs the activations, i.e., one with a\nreconstruction loss of zero, we can view the encoder and decoder as the bijective translation\n      τ(v) = ProjV\\H(v) ∪fenc(ProjH(v));  τ −1(v) = ProjV\\H(v) ∪fdec(ProjH(v))\n\nHowever, in practice, reconstruction loss is never zero and sparse autoencoders are approxi-\nmate transformations of the underlying model.\n\n3.6.2 Aligning Low-Level Features with High-Level Variables\n\nOnce a space of features has been learned, there is still the task of aligning features with\nhigh-level causal variables. Supervised modular feature learning techniques learn a feature\n\n\n                                          42\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nspace with an explicit alignment to high-level causal variables already in mind. However,\nin unsupervised modular feature learning an additional method is needed to align features\nwith high-level causal variables.\n\nSparse Feature Selection A simple baseline method for aligning features with a high-\nlevel variable is to train a linear probe with a regularization term to select the features most\ncorrelated with the high-level variable (Huang et al., 2024).\n\nDifferential Binary Mask  Differential binary masking selects a subset of features for\na high-level variable by optimizing a binary mask with a training objective defined using\ninterventions (De Cao et al., 2020; Csord´as et al., 2021; De Cao et al., 2021; Davies et al.,\n2023; Prakash et al., 2024; Huang et al., 2024). Differential binary masking has often been\nused to select individual neurons that play a particular causal role, but can just as easily be\nused to select features as long as the bijective translation is differentiable.\n\n3.6.3 Supervised Methods\n\nProbes  Probing is the technique of using a supervised or unsupervised model to determine\nwhether a concept is present in a hidden vector of a separate model. Probes are a popular\ntool for analyzing deep learning models, especially pretrained language models (Hupkes\net al., 2018; Conneau et al., 2018; Peters et al., 2018; Tenney et al., 2019; Clark et al., 2019).\n   Although probes are quite simple, they raise subtle methodological issues and our\ntheoretical understanding of probes has greatly improved since their recent introduction into\nthe field. From an information-theoretic point of view, we can observe that using arbitrarily\npowerful probes is equivalent to measuring the mutual information between the concept and\nthe hidden vector (Hewitt and Liang, 2019; Pimentel et al., 2020). If we restrict the class of\nprobing models based on their complexity, we can measure how usable the information is\n(Xu et al., 2020; Hewitt et al., 2021).\n    Regardless of what probe models are used, successfully probing a hidden vector does not\nguarantee that it plays a causal role in model behavior (Ravichander et al., 2020; Elazar\net al., 2020; Geiger et al., 2020, 2021). However, a linear probe with weights W trained to\npredict the value of a concept from an hidden vector H can be understood as learning a\nfeature space for activations that can be analyzed with causal abstraction. Let r1, . . . , rk\nbe a set of orthonormal vectors that span the rowspace of W and uk+1, . . . , un be a set of\northonormal vectors that span the nullspace of W. The bijective transformation is\n\n                   τ(v) = ProjV\\H(v) ∪[r1 . . . rkuk+1 . . . un]ProjH(v)\n\nSince the probe is trained to capture information related to the concept C, the rowspace of\nthe projection matrix W (i.e., the first k dimensions of the new feature space) might localize\nthe concept C. If we have a high-level model that has a variable for the concept C, that\nvariable should be aligned with the first k features.\n\nDistributed Alignment Search  Distributed Alignment Search (DAS) finds linear sub-\nspaces of an n-dimensional hidden vector H in model L that align with high-level variables\nX1, . . . , Xk in model H by optimizing an orthogonal matrix Q ∈Rn×n with a loss objective\ndefined using distributed interchange interventions (Section 2.5). This method has been\nused to analyze causal mechanisms in a variety of deep learning models (Geiger et al., 2023;\n\n\n                                           43\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nWu et al., 2023; Tigges et al., 2023; Arora et al., 2024; Huang et al., 2024; Minder et al.,\n2024). The bijective translation τ : V →V is defined\n\n                         τ(v) = ProjV\\H(v) ∪QT ProjH(v).\n\n  DAS optimizes Q so that Y1, . . . , Yk, disjoint subspaces of H, are abstracted by high-level\nvariables X1, . . . , Xk using the loss\n\nℓ=  X      CE(Lb∪DistIntInv(L,τ,⟨s1,...,sk⟩⟨Y1,...,Yk⟩), Hω(b)∪IntInv(H,⟨ω(s1),...,ω(sk)⟩⟨X1,...,Xk⟩)),\n      b,s1,...,sk∈XInL\n\nwhere ω maps low-level inputs to high-level inputs and CE is cross-entropy loss.\n\n\n3.7 Activation Steering as Causal Abstraction\n\nCausal explanation and manipulation are intrinsically linked (Woodward, 2003);  if we\nunderstand which components in a deep learning model store high-level causal variables, we\nwill be able to control the behavior of the deep learning model via intervention on those\ncomponents. Controlling model behavior via interventions was initially studied on recursive\nneural networks and generative adversarial networks (Giulianelli et al., 2018; Bau et al.,\n2019; Soulos et al., 2020; Besserve et al., 2020). Recently, various works have focused on\nsteering large language model generation through interventions. For instance, researchers\nhave demonstrated that adding fixed steering vectors to the residual stream of transformer\nmodels can control the model’s generation without training (Subramani et al., 2022; Turner\net al., 2023; Zou et al., 2023; Vogel, 2024; Li et al., 2024; Wu et al., 2024a). Additionally,\nparameter-efficient fine-tuning methods such as Adapter-tuning (Houlsby et al., 2019) can\nbe viewed as interventions on model parameters. Between these two types of methods is\nrepresentation fine-tuning (Wu et al., 2024b), where low-rank adapters are attached to a\nsmall number of hidden vectors in order to steer model behavior.\n   While a successful interchange intervention analysis implies an ability to control the\nlow-level model, the reverse does not hold.  Activation steering has the power to bring\nthe hidden vectors of a network off the distribution induced by the input data, potentially\ntargeting hidden vectors that have the same value for every possible model input. An\ninterchange intervention on such a vector will never have any impact on the model behavior.\n   However, activation steering can still be represented in the framework of causal abstraction.\nFor hidden vectors that are useful knobs to steer model generations in specific direction,\nwe can simply define the map ω from low-level interventionals to high-level interventionals\non steering interventions rather than interchange interventions. The crucial point is that\ncausal abstraction analysis that doesn’t define ω on interchange interventions will fail to\nuncover how the network reasons. It may nonetheless uncover how to control the network’s\nreasoning process.\n\n\n3.8 Training AI Models to be Interpretable\n\nOur treatment of interpretability has been largely through a scientific lens; an AI model\nbeing uninterpretable simply makes it an interesting object of study. However, when cracking\nopen the black box is understood as normative goal that could have real positive societal\n\n\n                                           44\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nimpacts, a natural question is whether we can make our job any easier by creating models\nthat are inherently more interpretable. This is an active area of research.\n   General purpose approaches attempt to design training procedure that produces more\ninterpretable representations, such as using a SoLU function as a non-linearity (Elhage\net al., 2022a), collapsing real-valued vector space into a discrete-valued space (Tamkin et al.,\n2024), learning a contextually weighted combination of vectors for different word meanings\n(Hewitt et al., 2023), or replacing MLPs with a new kind of network (Liu et al., 2024). More\ntargeted approaches will construct architectural bottle-necks (Koh et al., 2020; Y¨uksekg¨on¨ul\net al., 2023; Chauhan et al., 2023) or use interchange intervention based loss terms (Geiger\net al., 2022b; Wu et al., 2022b,a; Huang et al., 2023b; Zur et al., 2024) in order to force a\nconcept to be mediated by a particular vector or feature. While these are active avenues of\nexploration, there are, to date, no state-of-the-art models that have architectures or losses\ndesigned around interpretability.\n\n\n4 Conclusion\n\nWe submit that causal abstraction provides a theoretical foundation for mechanistic inter-\npretability that clarifies core concepts and lays useful groundwork for future development of\nmethods that investigate algorithmic hypotheses about the internal reasoning of AI models.\n\n\nAcknowledgements\n\nThank you to the reviewers, Nora Belrose, and Frederik Hytting Jørgensen for their feedback\non earlier drafts of this paper. In particular, we would like to thank Sander Beckers for deep\nand thoughtful engagement as a reviewer, which improved the quality of this work over the\ncourse of the review process. This research is supported by a grant from Open Philanthropy.\n\n\nReferences\n\nEldar David Abraham, Karel D’Oosterlinck, Amir Feder, Yair Ori Gat, Atticus Geiger,\n  Christopher Potts, Roi Reichart, and Zhengxuan Wu. CEBaB: Estimating the causal\n   effects of real-world concepts on NLP model behavior. arXiv:2205.14140, 2022. URL\n  https://arxiv.org/abs/2205.14140.\n\n\nRaquel G Alhama and Willem Zuidema. A review of computational models of basic rule\n   learning: The neural-symbolic debate and beyond. Psychonomic bulletin & review, 26(4):\n  1174–1194, 2019.\n\n\nAryaman Arora, Dan Jurafsky, and Christopher Potts.  CausalGym:  Benchmarking\n  causal interpretability methods on linguistic tasks.   In Lun-Wei Ku, Andre Mar-\n   tins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1:  Long Papers), pages 14638–\n  14663, Bangkok, Thailand, 2024. Association for Computational Linguistics. URL\n  https://aclanthology.org/2024.acl-long.785.\n\n\n                                           45\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nElias Bareinboim and Juan D. Correa. A calculus for stochastic interventions: Causal effect\n  identification and surrogate experiments. In The Thirty-Fourth AAAI Conference on\n  Artificial Intelligence, 2020.\n\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T.\n  Freeman, and Antonio Torralba. Visualizing and understanding gans. In Deep Generative\n  Models for Highly Structured Data, ICLR 2019 Workshop, New Orleans, Louisiana, United\n  States, May 6, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=\n  rJgON8ItOV.\n\nSander Beckers. Causal explanations and XAI. In Bernhard Sch¨olkopf, Caroline Uhler,\n  and Kun Zhang, editors, Proceedings of the First Conference on Causal Learning and\n  Reasoning, volume 177 of Proceedings of Machine Learning Research, pages 90–109. PMLR,\n  11–13 Apr 2022. URL https://proceedings.mlr.press/v177/beckers22a.html.\n\nSander Beckers and Joseph Halpern. Abstracting causal models. In AAAI Conference on\n  Artificial Intelligence, 2019.\n\nSander Beckers, Frederick Eberhardt, and Joseph Y. Halpern. Approximate causal ab-\n  stractions. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference,\n  2019.\n\nNora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and\n  Stella Biderman. LEACE: perfect linear concept erasure in closed form. In Alice Oh,\n  Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,\n  Advances in Neural Information Processing Systems 36: Annual Conference on Neural\n  Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\n  10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n  d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html.\n\nMichel Besserve, Arash Mehrjou, R´emy Sun, and Bernhard Sch¨olkopf. Counterfactuals\n  uncover the modular structure of deep generative models. In 8th International Conference\n  on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n  OpenReview.net, 2020. URL https://openreview.net/forum?id=SJxDDpEKvH.\n\nAlexander Binder, Gr´egoire Montavon, Sebastian Bach, Klaus-Robert M¨uller, and Wojciech\n  Samek. Layer-wise relevance propagation for neural networks with local renormalization\n   layers. CoRR, abs/1604.00825, 2016. URL http://arxiv.org/abs/1604.00825.\n\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.\n Man is to computer programmer as woman is to homemaker?  Debiasing word em-\n  beddings.  In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,\n  Advances in Neural Information Processing Systems, volume 29. Curran Associates,\n   Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/\n  a486cd07e4ac3d270571622f4f316ec5-Paper.pdf.\n\nTolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda B. Vi´egas,\n  and Martin Wattenberg. An interpretability illusion for BERT.  In arXiv preprint\n  arXiv:2104.07143, 2021. URL https://arxiv.org/abs/2104.07143.\n\n\n                                          46\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nStephan Bongers, Patrick Forr´e, Jonas Peters, and Joris M. Mooij. Foundations of structural\n  causal models with cycles and latent variables. The Annals of Statistics, 49(5):2885–2915,\n  2021.\n\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly,\n  Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\n  Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\n  Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,\n Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language\n  models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-\n  circuits.pub/2023/monosemantic-features/index.html.\n\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert,\n  Chelsea Voss, Ben Egan, and Swee Kiat Lim.  Thread:  Circuits.  Distill, 2020.  doi:\n  10.23915/distill.00024. https://distill.pub/2020/circuits.\n\nKrzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning.\n  In Marina Meila and Tom Heskes, editors, Proceedings of the Thirty-First Conference on\n  Uncertainty in Artificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The Nether-\n  lands, pages 181–190. AUAI Press, 2015. URL http://auai.org/uai2015/proceedings/\n  papers/109.pdf.\n\nKrzysztof Chalupka, Tobias Bischoff, Frederick Eberhardt, and Pietro Perona. Unsupervised\n  discovery of el nino using causal feature learning on microlevel climate data. In Alexander T.\n  Ihler and Dominik Janzing, editors, Proceedings of the Thirty-Second Conference on\n  Uncertainty in Artificial Intelligence, UAI 2016, June 25-29, 2016, New York City, NY,\n USA. AUAI Press, 2016. URL http://auai.org/uai2016/proceedings/papers/11.\n  pdf.\n\nKrzysztof Chalupka, Frederick Eberhardt, and Pietro Perona. Causal feature learning: an\n  overview. Behaviormetrika, 44:137–164, 2017.\n\nChun Sik Chan, Huanqi Kong, and Guanqing Liang. A comparative study of faithfulness\n  metrics for model interpretability methods. In Smaranda Muresan, Preslav Nakov, and\n  Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n  22-27, 2022, pages 5029–5038. Association for Computational Linguistics, 2022a. doi:\n  10.18653/v1/2022.acl-long.345. URL https://doi.org/10.18653/v1/2022.acl-long.\n  345.\n\nLawrence Chan, Adri`a Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny\n  Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas.   Causal\n  scrubbing, a method for rigorously testing interpretability hypotheses.  AI Align-\n  ment Forum, 2022b. https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/\n  causal-scrubbing-a-method-for-rigorously-testing.\n\nAditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian.\n  Neural network attributions: A causal perspective. In Proceedings of the 36th International\n  Conference on Machine Learning (ICML), pages 981–990, 2019.\n\n\n                                          47\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nKushal Chauhan, Rishabh Tiwari, Jan Freyberg, Pradeep Shenoy, and Krishnamurthy\n  Dvijotham. Interactive concept bottleneck models. In Brian Williams, Yiling Chen, and\n  Jennifer Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI\n  2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI\n  2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI\n  2023, Washington, DC, USA, February 7-14, 2023, pages 5948–5955. AAAI Press, 2023.\n  doi: 10.1609/AAAI.V37I5.25736. URL https://doi.org/10.1609/aaai.v37i5.25736.\n\nPattarawat Chormai, Jan Herrmann, Klaus-Robert M¨uller, and Gr´egoire Montavon. Disen-\n  tangled explanations of neural network predictions by finding relevant subspaces, 2022.\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does\n BERT look at? An analysis of BERT’s attention.  In Proceedings of the 2019 ACL\n Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages\n  276–286, Florence, Italy, August 2019. Association for Computational Linguistics. URL\n  https://www.aclweb.org/anthology/W19-4828.\n\nArthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a\n  Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In\n  Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey\n  Levine, editors, Advances in Neural Information Processing Systems 36: Annual Confer-\n  ence on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\n USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/\n  2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html.\n\nAlexis Conneau, German Kruszewski, Guillaume Lample, Lo¨ıc Barrault, and Marco Baroni.\n  What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic\n  properties. In Proceedings of the 56th Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pages 2126–2136, Melbourne, Australia, July\n  2018. Association for Computational Linguistics.  doi: 10.18653/v1/P18-1198. URL\n  https://aclanthology.org/P18-1198.\n\nP. Cousot and R. Cousot. Abstract interpretation: a unified lattice model for static analysis\n  of programs by construction or approximation of fixpoints.  In Conference Record of\n  the Fourth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming\n  Languages, pages 238–252, Los Angeles, California, 1977. ACM Press, New York, NY.\n\nKathleen A. Creel. Transparency in complex computational systems. Philosophy of Science,\n  87:568–589, 2020.\n\nR´obert Csord´as, Sjoerd van Steenkiste, and J¨urgen Schmidhuber. Are neural nets modular?\n  inspecting functional modularity through differentiable weight masks. In 9th International\n  Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,\n  2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=7uVcpu-gMD.\n\nR´obert Csord´as, Christopher Potts, Christopher D Manning, and Atticus Geiger. Recurrent\n  neural networks learn to store and generate sequences using non-linear representations.\n  In The 7th BlackboxNLP Workshop, 2024. URL https://openreview.net/forum?id=\n  NUQeYgg8x4.\n\n\n                                          48\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse\n  autoencoders find highly interpretable features in language models. CoRR, abs/2309.08600,\n  2023. doi: 10.48550/ARXIV.2309.08600. URL https://doi.org/10.48550/arXiv.2309.\n  08600.\n\nXander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Dis-\n  covering variable binding circuitry with desiderata. CoRR, abs/2307.03637, 2023. doi:\n  10.48550/ARXIV.2307.03637. URL https://doi.org/10.48550/arXiv.2307.03637.\n\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions\n  emerge across layers in neural models? interpretation with differentiable masking. In\n  Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020\n  Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3243–\n  3255, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/\n  v1/2020.emnlp-main.262. URL https://aclanthology.org/2020.emnlp-main.262.\n\nNicola De Cao, Leon Schmid, Dieuwke Hupkes, and Ivan Titov.  Sparse interventions\n  in language models with differentiable masking. arXiv:2112.06837, 2021. URL https:\n  //arxiv.org/abs/2112.06837.\n\nJulien Dubois, Frederick Eberhardt, Lynn K. Paul, and Ralph Adolphs. Personality beyond\n  taxonomy. Nature human behaviour, 4 11:1110–1117, 2020a.\n\nJulien Dubois, Hiroyuki Oya, Julian Michael Tyszka, Matthew A. Howard, Frederick\n  Eberhardt, and Ralph Adolphs. Causal mapping of emotion networks in the human brain:\n  Framework and initial findings. Neuropsychologia, 145, 2020b.\n\nJoel Dyer, Nicholas Bishop, Yorgos Felekis, Fabio Massimo Zennaro, Anisoara Calinescu,\n  Theodoros Damoulas, and Michael J. Wooldridge. Interventionally consistent surrogates for\n  agent-based simulators. CoRR, abs/2312.11158, 2023. doi: 10.48550/ARXIV.2312.11158.\n URL https://doi.org/10.48550/arXiv.2312.11158.\n\nFrederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy\n  of Science, 74(5):981–995, 2007.\n\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral\n  explanation with amnesic counterfactuals. In Proceedings of the 2020 EMNLP Workshop\n  BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for\n  Computational Linguistics, November 2020. doi: 10.18653/v1/W18-5426.\n\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius\n  Mosbach, Yonatan Belinkov, Hinrich Sch¨utze, and Yoav Goldberg. Measuring causal effects\n   of data statistics on language model’s ’factual’ predictions. CoRR, abs/2207.14251, 2022.\n  doi: 10.48550/arXiv.2207.14251. URL https://doi.org/10.48550/arXiv.2207.14251.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n  Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain,\n  Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane\n\n\n                                          49\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n  Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mc-\n  Candlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer\n  Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott John-\n  ston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernan-\n  dez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao\n  Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly,\n  Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared\n  Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher\n  Olah. Softmax linear units. Transformer Circuits Thread, 2022a. https://transformer-\n  circuits.pub/2022/solu/index.html.\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna\n  Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse,\n  Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher\n  Olah. Toy models of superposition. Transformer Circuits Thread, 2022b.\n\nAmir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. CausaLM: Causal Model Explanation\n  Through Counterfactual Language Models. Computational Linguistics, pages 1–54, 05\n  2021. ISSN 0891-2017.  doi: 10.1162/coli a 00404. URL https://doi.org/10.1162/\n  coli_a_00404.\n\nJiahai Feng and Jacob Steinhardt. How do language models bind entities in context? In\n The Twelfth International Conference on Learning Representations, 2024. URL https:\n  //openreview.net/forum?id=zb3b6oKO77.\n\nJiahai Feng, Stuart Russell, and Jacob Steinhardt. Monitoring latent world states in language\n  models with propositional probes. CoRR, abs/2406.19501, 2024. doi: 10.48550/ARXIV.\n  2406.19501. URL https://doi.org/10.48550/arXiv.2406.19501.\n\nMatthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and\n  Yonatan Belinkov. Causal analysis of syntactic agreement mechanisms in neural language\n  models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings\n  of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\n  International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\n  pages 1828–1843, Online, August 2021. Association for Computational Linguistics. doi:\n  10.18653/v1/2021.acl-long.144. URL https://aclanthology.org/2021.acl-long.144.\n\nAtticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference\n  models partially embed theories of lexical entailment and negation. In Proceedings of the\n  Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,\n  pages 163–173, Online, November 2020. Association for Computational Linguistics. doi:\n  10.18653/v1/2020.blackboxnlp-1.16. URL https://www.aclweb.org/anthology/2020.\n  blackboxnlp-1.16.\n\nAtticus Geiger, Hanson Lu, Thomas F Icard, and Christopher Potts. Causal abstractions of\n  neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan,\n\n\n                                          50\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n  editors, Advances in Neural Information Processing Systems, 2021. URL https://\n  openreview.net/forum?id=RmuXDtjDhG.\n\nAtticus Geiger, Alexandra Carstensen, Michael C. Frank, and Christopher Potts. Relational\n  reasoning and generalization using nonsymbolic neural networks. Psychological Review,\n  2022a. doi: 10.1037/rev0000371.\n\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard,\n  Noah Goodman, and Christopher Potts.  Inducing causal structure for interpretable\n  neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,\n  Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference\n  on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages\n  7324–7338. PMLR, 17–23 Jul 2022b. URL https://proceedings.mlr.press/v162/\n  geiger22a.html.\n\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman.\n  Finding alignments between interpretable causal variables and distributed neural repre-\n  sentations. Ms., Stanford University, 2023. URL https://arxiv.org/abs/2303.02536.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of\n  factual associations in auto-regressive language models.  In The 2023 Conference on\n  Empirical Methods in Natural Language Processing, 2023. URL https://openreview.\n  net/forum?id=F1G7y94K02.\n\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patch-\n  scopes: A unifying framework for inspecting hidden representations of language mod-\n   els. CoRR, abs/2401.06102, 2024.  doi: 10.48550/ARXIV.2401.06102. URL https:\n  //doi.org/10.48550/arXiv.2401.06102.\n\nMario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem H. Zuidema.\n  Under the hood: Using diagnostic classifiers to investigate and improve how language\n  models track agreement information. In Tal Linzen, Grzegorz Chrupala, and Afra Alishahi,\n  editors, Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for\n  NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pages 240–248.\n  Association for Computational Linguistics, 2018.  doi: 10.18653/v1/w18-5426. URL\n  https://doi.org/10.18653/v1/w18-5426.\n\nGabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig\n  Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks.\n   Distill, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.\n\nNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing\n  model behavior with path patching, 2023.\n\nAna Esponera G´omez and Giovanni Cin`a. Interchange intervention training applied to\n  post-meal glucose prediction for type 1 diabetes mellitus patients. In 9th Causal Inference\n  Workshop at UAI 2024, 2024. URL https://openreview.net/forum?id=6sRLazdA1l.\n\n\n                                          51\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nYash Goyal, Uri Shalit, and Been Kim. Explaining classifiers with causal concept effect\n  (cace). CoRR, abs/1907.07165, 2019. URL http://arxiv.org/abs/1907.07165.\n\nSatchel Grant, Noah D. Goodman, and James L. McClelland. Emergent symbol-like number\n  variables in artificial neural networks, 2025. URL https://arxiv.org/abs/2501.06141.\n\nWes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and\n  Dimitris Bertsimas.  Finding neurons in a haystack: Case studies with sparse prob-\n  ing.  In Transactions on Machine Learning Research (TMLR), 2023. URL https:\n  //doi.org/10.48550/arXiv.2305.01610.\n\nMichael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-\n  than?: Interpreting mathematical abilities in a pre-trained language model. In Alice Oh,\n  Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,\n  Advances in Neural Information Processing Systems 36: Annual Conference on Neural\n  Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\n  10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n  efbba7719cc5172d175240f24be11280-Abstract-Conference.html.\n\nMichael Harradon, Jeff Druce, and Brian E. Ruttenberg. Causal learning and explanation of\n  deep neural networks via autoencoded activations. CoRR, abs/1802.00541, 2018. URL\n  http://arxiv.org/abs/1802.00541.\n\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.   Does  localiza-\n  tion inform editing?  surprising differences in causality-based localization vs. knowl-\n  edge editing in language models.   In Alice Oh, Tristan Naumann, Amir Glober-\n  son, Kate Saenko, Moritz Hardt, and Sergey Levine,  editors, Advances in Neu-\n  ral Information Processing Systems 36:  Annual Conference on Neural Information\n  Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10\n  - 16,  2023,  2023.  URL http://papers.nips.cc/paper_files/paper/2023/hash/\n  3927bbdcf0e8d1fa8aa23c26f358a281-Abstract-Conference.html.\n\nStefan Heimersheim and Neel Nanda. How to use and interpret activation patching, 2024.\n\nJohn Hewitt and Percy Liang.  Designing and interpreting probes with control tasks.\n  In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\n  Processing and the 9th International Joint Conference on Natural Language Process-\n  ing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China, November 2019. As-\n  sociation for Computational Linguistics.  doi:  10.18653/v1/D19-1275. URL https:\n  //www.aclweb.org/anthology/D19-1275.\n\nJohn Hewitt, Kawin Ethayarajh, Percy Liang, and Christopher D. Manning. Conditional\n  probing: measuring usable information beyond a baseline. In Marie-Francine Moens,\n  Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the\n  2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,\n  Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 1626–1639.\n  Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.122.\n URL https://doi.org/10.18653/v1/2021.emnlp-main.122.\n\n\n                                          52\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nJohn Hewitt, John Thickstun, Christopher D. Manning, and Percy Liang.  Backpack\n  language models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors,\n  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n  (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 9103–9125.\n  Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.506.\n URL https://doi.org/10.18653/v1/2023.acl-long.506.\n\nRaymond Hicks and Dustin Tingley. Causal mediation analysis. The Stata Journal, 11(4):\n  605–619, 2011. doi: 10.1177/1536867X1201100407. URL https://doi.org/10.1177/\n  1536867X1201100407.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\n  Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.  Parameter-efficient transfer\n  learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings\n  of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,\n  Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,\n  pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.\n  html.\n\nYaojie Hu and Jin Tian. Neuron dependency graphs: A causal abstraction of neural networks.\n  In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan\n  Sabato, editors, Proceedings of the 39th International Conference on Machine Learning,\n  volume 162 of Proceedings of Machine Learning Research, pages 9020–9040. PMLR, 17–23\n  Jul 2022. URL https://proceedings.mlr.press/v162/hu22b.html.\n\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher\n  Potts.  Rigorously assessing natural language explanations of neurons.  In Yonatan\n  Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mo-\n  hebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Inter-\n  preting Neural Networks for NLP, pages 317–331, Singapore, December 2023a. Asso-\n  ciation for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.24. URL\n  https://aclanthology.org/2023.blackboxnlp-1.24.\n\nJing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. Inducing character-\n   level structure in subword-based language models with type-level interchange intervention\n  training.  In Findings of the Association for Computational Linguistics: ACL 2023,\n  pages 12163–12180, Toronto, Canada, July 2023b. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2023.findings-acl.770. URL https://aclanthology.org/2023.\n  findings-acl.770.\n\nJing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. Ravel:\n  Evaluating interpretability methods on disentangling language model representations,\n  2024.\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey.\n  Sparse autoencoders find highly interpretable features in language models. In The Twelfth\n  International Conference on Learning Representations, 2024. URL https://openreview.\n  net/forum?id=F76bwRSLeK.\n\n\n                                          53\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nDieuwke Hupkes, Sara Veldhoen, and Willem H. Zuidema. Visualisation and ’diagnostic\n   classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure.\n  J. Artif. Intell. Res., 61:907–926, 2018. doi: 10.1613/jair.1.11196. URL https://doi.\n  org/10.1613/jair.1.11196.\n\nKosuke Imai, Luke Keele, and Dustin Tingley. A general approach to causal mediation\n  analysis. Psychological Methods, 15(4):309–334, Dec 2010. doi: 10.1037/a0020761.\n\nYumi Iwasaki and Herbert A. Simon. Causality and model abstraction. Artificial Intelligence,\n  67(1):143–194, 1994.\n\nAlon Jacovi and Yoav Goldberg. Towards faithfully interpretable NLP systems: How should\n  we define and evaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and\n  Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association\n  for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4198–4205.\n  Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.386.\n URL https://doi.org/10.18653/v1/2020.acl-main.386.\n\nYibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch.\n  On the origins of linear representations in large language models. CoRR, abs/2403.03867,\n  2024. doi: 10.48550/ARXIV.2403.03867. URL https://doi.org/10.48550/arXiv.2403.\n  03867.\n\nAmir-Hossein Karimi, Bernhard Sch¨olkopf, and Isabel Valera. Algorithmic recourse: From\n  counterfactual explanations to interventions. In Proceedings of the 2021 ACM Conference\n  on Fairness, Accountability, and Transparency, FAccT ’21, page 353–362, New York,\n  NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097.  doi:\n  10.1145/3442188.3445899. URL https://doi.org/10.1145/3442188.3445899.\n\nAmir-Hossein Karimi, Gilles Barthe, Bernhard Sch¨olkopf, and Isabel Valera. A survey of\n  algorithmic recourse: Contrastive explanations and consequential recommendations. ACM\n  Comput. Surv., 55(5):95:1–95:29, 2023. doi: 10.1145/3527848. URL https://doi.org/\n  10.1145/3527848.\n\nArmin Kekic, Bernhard Sch¨olkopf, and Michel Besserve.  Targeted reduction of causal\n  models. CoRR, abs/2311.18639, 2023. doi: 10.48550/ARXIV.2311.18639. URL https:\n  //doi.org/10.48550/arXiv.2311.18639.\n\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas,\n  and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with\n  concept activation vectors (tcav), 2018.\n\nDavid Kinney. On the explanatory depth and pragmatic value of coarse-grained, probabilistic,\n  causal explanations. Philosophy of Science, 86:145–167, 2019.\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been\n  Kim, and Percy Liang. Concept bottleneck models. In Hal Daum´e III and Aarti Singh,\n  editors, Proceedings of the 37th International Conference on Machine Learning, volume\n\n\n                                          54\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\n  119 of Proceedings of Machine Learning Research, pages 5338–5348. PMLR, 13–18 Jul\n  2020. URL https://proceedings.mlr.press/v119/koh20a.html.\n\nMichael A. Lepori, Ellie Pavlick, and Thomas Serre. Neurosurgeon: A toolkit for subnetwork\n  analysis. CoRR, abs/2309.00244, 2023a. doi: 10.48550/ARXIV.2309.00244. URL https:\n  //doi.org/10.48550/arXiv.2309.00244.\n\nMichael A. Lepori, Thomas Serre, and Ellie Pavlick.  Break  it down:  Evidence for\n  structural compositionality in neural networks. In Alice Oh, Tristan Naumann, Amir\n  Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine,  editors, Advances in\n  Neural Information Processing Systems 36: Annual Conference on Neural Informa-\n  tion Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10\n  - 16, 2023, 2023b.  URL http://papers.nips.cc/paper_files/paper/2023/hash/\n  85069585133c4c168c865e65d72e9775-Abstract-Conference.html.\n\nBelinda Z. Li, Maxwell I. Nye, and Jacob Andreas. Implicit representations of meaning in\n  neural language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli,\n  editors, Proceedings of the 59th Annual Meeting of the Association for Computational\n  Linguistics and the 11th International Joint Conference on Natural Language Processing,\n ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages\n  1813–1827. Association for Computational Linguistics, 2021.  doi: 10.18653/v1/2021.\n  acl-long.143. URL https://doi.org/10.18653/v1/2021.acl-long.143.\n\nKenneth   Li,  Oam  Patel,  Fernanda   Vi´egas,  Hanspeter   Pfister,  and  Martin\n  Wattenberg.     Inference-time  intervention:    Eliciting  truthful  answers  from  a\n  language  model.    Advances  in  Neural  Information  Processing  Systems,   36,\n  2024.   URL  https://proceedings.neurips.cc/paper_files/paper/2023/hash/\n  81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html.\n\nTom Lieberum, Matthew Rahtz, J´anos Kram´ar, Neel Nanda, Geoffrey Irving, Rohin Shah,\n  and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple\n  choice capabilities in chinchilla. CoRR, abs/2307.09458, 2023. doi: 10.48550/ARXIV.2307.\n  09458. URL https://doi.org/10.48550/arXiv.2307.09458.\n\nZachary C. Lipton. The mythos of model interpretability. Commun. ACM, 61(10):36–43,\n  2018. doi: 10.1145/3233231. URL https://doi.org/10.1145/3233231.\n\nZiming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Sol-\n   jacic, Thomas Y. Hou, and Max Tegmark. KAN: kolmogorov-arnold networks. CoRR,\n  abs/2404.19756, 2024. doi: 10.48550/ARXIV.2404.19756. URL https://doi.org/10.\n  48550/arXiv.2404.19756.\n\nCharles Lovering and Ellie Pavlick. Unit testing for concepts in neural networks. CoRR,\n  abs/2208.10244, 2022.  doi:  10.48550/arXiv.2208.10244. URL https://doi.org/10.\n  48550/arXiv.2208.10244.\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.\n  In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\n\n\n                                          55\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\n  R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30.\n  Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/\n  file/8a20a8621978632d76c43dfd28b67767-Paper.pdf.\n\nScott M. Lundberg, Gabriel G. Erion, and Su-In Lee. Consistent individualized feature\n  attribution for tree ensembles, 2019.\n\nQing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explana-\n  tion in NLP: A survey. CoRR, abs/2209.11326, 2022. doi: 10.48550/arXiv.2209.11326.\n URL https://doi.org/10.48550/arXiv.2209.11326.\n\nGary F Marcus, Sugumaran Vijayan, S Bandi Rao, and Peter M Vishton. Rule learning by\n  seven-month-old infants. Science, 283(5398):77–80, 1999.\n\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in\n  large language model representations of true/false datasets, 2023.\n\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron\n  Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in\n  language models, 2024.\n\nDavid Marr. Vision. W.H. Freeman and Company, 1982.\n\nRiccardo Massidda, Atticus Geiger, Thomas Icard, and Davide Bacciu. Causal abstraction\n  with soft interventions. In Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing,\n  editors, Conference on Causal Learning and Reasoning, CLeaR 2023, 11-14 April 2023,\n  Amazon Development Center, T¨ubingen, Germany, April 11-14, 2023, volume 213 of\n  Proceedings of Machine Learning Research, pages 68–87. PMLR, 2023. URL https:\n  //proceedings.mlr.press/v213/massidda23a.html.\n\nRiccardo Massidda, Sara Magliacane, and Davide Bacciu. Learning causal abstractions\n  of linear structural causal models. In The 40th Conference on Uncertainty in Artificial\n  Intelligence, 2024. URL https://openreview.net/forum?id=XlFqI9TMhf.\n\nJ. L. McClelland, D. E. Rumelhart, and PDP Research Group, editors. Parallel Distributed\n  Processing. Volume 2: Psychological and Biological Models. MIT Press, Cambridge, MA,\n  1986.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing\n  factual associations in gpt, 2022. URL https://arxiv.org/abs/2202.05262.\n\nKevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau.\n  Mass-editing memory in a transformer. In The Eleventh International Conference on\n  Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n  2023. URL https://openreview.net/pdf?id=MkbcAHIYgyS.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?,\n  2019.\n\n\n                                          56\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous\n  space word representations. In Lucy Vanderwende, Hal Daum´e III, and Katrin Kirch-\n   hoff, editors, Proceedings of the 2013 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies, pages\n  746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL\n  https://aclanthology.org/N13-1090.\n\nJulian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West,\n  and Ryan Cotterell. Controllable context sensitivity and the knob behind it, 2024. URL\n  https://arxiv.org/abs/2411.07404.\n\nAaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash,\n  Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David\n  Bau, and Yonatan Belinkov. The quest for the right mediator: A history, survey, and\n  theoretical grounding of causal interpretability, 2024. URL https://arxiv.org/abs/\n  2408.01416.\n\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress\n  measures for grokking via mechanistic interpretability. In The Eleventh International\n  Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n  OpenReview.net, 2023a. URL https://openreview.net/pdf?id=9XFSbDPmdW.\n\nNeel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in\n  world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap\n  Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of\n  the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,\n  BlackboxNLP@EMNLP 2023, Singapore, December 7, 2023, pages 16–30. Association for\n  Computational Linguistics, 2023b. doi: 10.18653/V1/2023.BLACKBOXNLP-1.2. URL\n  https://doi.org/10.18653/v1/2023.blackboxnlp-1.2.\n\nTanmayee Narendra, Anush Sankaran, Deepak Vijaykeerthy, and Senthil Mani. Explaining\n  deep learning models using causal inference. CoRR, abs/1811.04376, 2018. URL http:\n  //arxiv.org/abs/1811.04376.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan\n  Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001.\n  https://distill.pub/2020/circuits/zoom-in.\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\n  Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn\n  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones,\n  Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,\n  Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads.\n  Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-\n  learning-and-induction-heads/index.html.\n\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the\n  geometry of large language models. CoRR, abs/2311.03658, 2023. doi: 10.48550/ARXIV.\n  2311.03658. URL https://doi.org/10.48550/arXiv.2311.03658.\n\n\n                                          57\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nJudea Pearl. Causality. Cambridge University Press, 2009.\n\nJudea Pearl. The limitations of opaque learning machines. Possible minds: twenty-five ways\n  of looking at AI, pages 13–19, 2019.\n\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih.  Dissecting\n  contextual word embeddings: Architecture and representation. In Ellen Riloff, David\n  Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference\n  on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -\n  November 4, 2018, pages 1499–1509. Association for Computational Linguistics, 2018. doi:\n  10.18653/v1/d18-1179. URL https://doi.org/10.18653/v1/d18-1179.\n\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and\n  Ryan Cotterell. Information-theoretic probing for linguistic structure. In Dan Jurafsky,\n  Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th\n  Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\n  July 5-10, 2020, pages 4609–4622. Association for Computational Linguistics, 2020. doi:\n  10.18653/v1/2020.acl-main.420. URL https://doi.org/10.18653/v1/2020.acl-main.\n  420.\n\nNikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-\n  tuning enhances existing mechanisms: A case study on entity tracking. In The Twelfth\n  International Conference on Learning Representations, 2024. URL https://openreview.\n  net/forum?id=8sKcAWOf2D.\n\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out:\n  Guarding protected attributes by iterative nullspace projection. In Dan Jurafsky, Joyce\n  Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual\n  Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,\n  2020, pages 7237–7256. Association for Computational Linguistics, 2020. doi: 10.18653/\n  v1/2020.acl-main.647. URL https://doi.org/10.18653/v1/2020.acl-main.647.\n\nShauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan Cotterell. Linear adversarial\n  concept erasure, 2022.\n\nShauli Ravfogel, Yoav Goldberg, and Ryan Cotterell.  Log-linear guardedness and its\n  implications. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors,\n  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n  (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 9413–9431.\n  Association for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.ACL-LONG.523.\n URL https://doi.org/10.18653/v1/2023.acl-long.523.\n\nShauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Kernelized concept\n  erasure, 2023b.\n\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. Probing the probing paradigm:\n  Does probing accuracy entail task relevance?, 2020.\n\n\n                                          58\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Explain-\n  ing the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International\n  Conference on Knowledge Discovery and Data Mining, KDD ’16, page 1135–1144, New\n  York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi:\n  10.1145/2939672.2939778. URL https://doi.org/10.1145/2939672.2939778.\n\nEigil F. Rischel and Sebastian Weichwald. Compositional abstraction error and a category\n  of causal models.  In Proceedings of the 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2021.\n\nJuan Diego Rodriguez, Aaron Mueller, and Kanishka Misra. Characterizing the role of\n  similarity in the property inferences of language models. CoRR, abs/2410.22590, 2024. doi:\n  10.48550/ARXIV.2410.22590. URL https://doi.org/10.48550/arXiv.2410.22590.\n\nPaul K. Rubenstein, Sebastian Weichwald, Stephan Bongers, Joris M. Mooij, Dominik\n  Janzing, Moritz Grosse-Wentrup, and Bernhard Sch¨olkopf. Causal consistency of structural\n  equation models.  In Proceedings of the 33rd Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2017.\n\nD. E. Rumelhart, J. L. McClelland, and PDP Research Group, editors. Parallel Distributed\n  Processing. Volume 1: Foundations. MIT Press, Cambridge, MA, 1986.\n\nVictor Sanh and Alexander M. Rush. Low-complexity probing via finding subnetworks. In\n  Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨ur, Iz Belt-\n  agy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,\n  Proceedings of the 2021 Conference of the North American Chapter of the Association\n  for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, On-\n  line, June 6-11, 2021, pages 960–966. Association for Computational Linguistics, 2021.\n  doi: 10.18653/V1/2021.NAACL-MAIN.74. URL https://doi.org/10.18653/v1/2021.\n  naacl-main.74.\n\nNaomi Saphra and Sarah Wiegreffe.  Mechanistic?  CoRR, abs/2410.09087, 2024.  doi:\n  10.48550/ARXIV.2410.09087. URL https://doi.org/10.48550/arXiv.2410.09087.\n\nAdam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, and Buck Shlegeris. Pol-\n  ysemanticity and capacity in neural networks.  CoRR, abs/2210.01892, 2022.   doi:\n  10.48550/ARXIV.2210.01892. URL https://doi.org/10.48550/arXiv.2210.01892.\n\nJessica Schrouff, Sebastien Baur, Shaobo Hou, Diana Mincu, Eric Loreaux, Ralph Blanes,\n  James Wexler, Alan Karthikesalingam, and Been Kim. Best of both worlds: local and\n  global explanations with human-understandable concepts, 2022.\n\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a\n  black box: Learning important features through propagating activation differences. CoRR,\n  abs/1605.01713, 2016. URL http://arxiv.org/abs/1605.01713.\n\nHerbert A. Simon and Albert Ando. Aggregation of variables in dynamic systems. Econo-\n  metrica, 29(2):111–138, 1961. ISSN 00129682, 14680262. URL http://www.jstor.org/\n  stable/1909285.\n\n\n                                          59\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nPaul Smolensky. Neural and conceptual interpretation of PDP models. In James L. McClel-\n  land, David E. Rumelhart, and the PDP Research Group, editors, Parallel Distributed\n  Processing: Explorations in the Microstructure of Cognition: Psychological and Biological\n  Models, volume 2, pages 390–431. MIT Press, 1986.\n\nPaul Soulos, R. Thomas McCoy, Tal Linzen, and Paul Smolensky. Discovering the composi-\n  tional structure of vector representations with role learning networks. In Afra Alishahi,\n  Yonatan Belinkov, Grzegorz Chrupala, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad,\n  editors, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpret-\n  ing Neural Networks for NLP, BlackboxNLP@EMNLP 2020, Online, November 2020,\n  pages 238–254. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.\n  blackboxnlp-1.23. URL https://doi.org/10.18653/v1/2020.blackboxnlp-1.23.\n\nPeter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search.\n MIT Press, 2000.\n\nJost Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for\n  simplicity: The all convolutional net. CoRR, 12 2014.\n\nAlessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. A mechanistic interpretation\n  of arithmetic reasoning in language models using causal mediation analysis. In Houda\n  Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on\n  Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December\n  6-10, 2023, pages 7035–7052. Association for Computational Linguistics, 2023. URL\n  https://aclanthology.org/2023.emnlp-main.435.\n\nNishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering\n  vectors from pretrained language models. arXiv:2205.05124, 2022. URL https://arxiv.\n  org/abs/2205.05124.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.\n  In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\n  pages 3319–3328. JMLR. org, 2017.\n\nAlex Tamkin, Mohammad Taufeeque, and Noah Goodman. Codebook features: Sparse\n  and discrete interpretability for neural networks, 2024. URL https://openreview.net/\n  forum?id=LfhG5znxzR.\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\n  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n  pages 4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics.\n URL https://www.aclweb.org/anthology/P19-1452.\n\nSimon Thorpe. Local vs. distributed coding. Intellectica, 8(2):3–40, 1989. doi: 10.3406/intel.\n  1989.873. URL https://www.persee.fr/doc/intel_0769-4113_1989_num_8_2_873.\n\nJin Tian. Identifying dynamic sequential plans. In Appears in Proceedings of the Twenty-\n  Fourth Conference on Uncertainty in Artificial Intelligence, 2008.\n\n\n                                          60\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representa-\n  tions of sentiment in large language models, 2023.\n\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and\n  David Bau. Function vectors in large language models. In The Twelfth International\n  Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\n  OpenReview.net, 2024. URL https://openreview.net/forum?id=AwyxtyMwaG.\n\nAlexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte\n  MacDiarmid. Activation addition: Steering language models without optimization, 2023.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,\n  and Stuart Shieber. Causal mediation analysis for interpreting neural nlp: The case of\n  gender bias, 2020.\n\nTheia Vogel. repeng, 2024. URL https://github.com/vgel/repeng/.\n\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\n  Interpretability in the wild: a circuit for indirect object identification in GPT-2 small.\n  In The Eleventh International Conference on Learning Representations, 2023. URL\n  https://openreview.net/forum?id=NpsVSN6o4ul.\n\nSarah Wiegreffe and Yuval Pinter.  Attention is not not explanation.  In Kentaro Inui,\n  Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference\n  on Empirical Methods in Natural Language Processing and the 9th International Joint\n  Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,\n  November 3-7, 2019, pages 11–20. Association for Computational Linguistics, 2019. doi:\n  10.18653/v1/D19-1002. URL https://doi.org/10.18653/v1/D19-1002.\n\nJames Woodward. Making Things Happen: A Theory of Causal Explanation.  Oxford\n  university press, 2003.\n\nJames Woodward.  Explanatory autonomy:  the role of proportionality, stability, and\n  conditional irrelevance. Synthese, 198:237–265, 2021.\n\nMuling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao\n  Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang.  Advancing parameter\n  efficiency in fine-tuning via representation editing.  arXiv:2402.15179, 2024a. URL\n  https://arxiv.org/abs/2402.15179.\n\nZhengxuan Wu, Karel D’Oosterlinck, Atticus Geiger, Amir Zur, and Christopher Potts.\n  Causal Proxy Models for concept-based model explanations. arXiv:2209.14279, 2022a.\n URL https://arxiv.org/abs/2209.14279.\n\nZhengxuan Wu, Atticus Geiger, Josh Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard,\n  Christopher Potts, and Noah D. Goodman. Causal distillation for language models. In\n  Proceedings of the 2022 Conference of the North American Chapter of the Association\n  for Computational Linguistics: Human Language Technologies, pages 4288–4295, Seattle,\n  United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/\n  2022.naacl-main.318. URL https://aclanthology.org/2022.naacl-main.318.\n\n\n                                          61\n\nGeiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard\n\n\n\n\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman.\n  Interpretability at scale: Identifying causal mechanisms in alpaca.  In Thirty-seventh\n  Conference on Neural Information Processing Systems, 2023. URL https://openreview.\n  net/forum?id=nRfClnMhVX.\n\nZhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D.\n  Manning, and Christopher Potts. Reft: Representation finetuning for language models.\n CoRR, abs/2404.03592, 2024b. doi: 10.48550/ARXIV.2404.03592. URL https://doi.\n  org/10.48550/arXiv.2404.03592.\n\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory\n  of usable information under computational constraints. In 8th International Conference\n  on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n  OpenReview.net, 2020. URL https://openreview.net/forum?id=r1eBeyHFDH.\n\nStephen Yablo. Mental causation. Philosophical Review, 101:245–280, 1992.\n\nMert Y¨uksekg¨on¨ul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In\n The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\n  Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?\n  id=nA5AZ8CEyow.\n\nMatthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\n  In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer\n  Vision – ECCV 2014, pages 818–833, Cham, 2014. Springer International Publishing.\n  ISBN 978-3-319-10590-1.\n\nFabio Massimo Zennaro, M´at´e Dr´avucz, Geanina Apachitei, Widanalage Dhammika Widan-\n  age, and Theodoros Damoulas. Jointly learning consistent causal abstractions over multiple\n  interventional distributions. In Mihaela van der Schaar and Cheng Zhang and/D Do-\n  minik Janzing, editors, Conference on Causal Learning and Reasoning, CLeaR 2023, 11-14\n  April 2023, Amazon Development Center, T¨ubingen, Germany, April 11-14, 2023, volume\n  213 of Proceedings of Machine Learning Research, pages 88–121. PMLR, 2023a. URL\n  https://proceedings.mlr.press/v213/zennaro23a.html.\n\nFabio Massimo Zennaro, Paolo Turrini, and Theodoros Damoulas. Quantifying consistency\n  and information loss for causal abstraction learning. In Proceedings of the Thirty-Second\n  International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August\n  2023, Macao, SAR, China, pages 5750–5757. ijcai.org, 2023b. doi: 10.24963/IJCAI.2023/\n  638. URL https://doi.org/10.24963/ijcai.2023/638.\n\nFred Zhang and Neel Nanda. Towards best practices of activation patching in language\n  models: Metrics and methods. In The Twelfth International Conference on Learning\n  Representations, 2024. URL https://openreview.net/forum?id=Hf17y6u9BC.\n\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander\n  Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation\n  engineering: A top-down approach to AI transparency. arXiv:2310.01405, 2023. URL\n  https://arxiv.org/abs/2310.01405.\n\n\n                                          62\n\n             Causal Abstraction for Mechanistic Interpretability\n\n\n\n\nAmir Zur, Elisa Kreiss, Karel D’Oosterlinck, Christopher Potts, and Atticus Geiger. Updating\n  CLIP to prefer descriptions over captions.  In Yaser Al-Onaizan, Mohit Bansal, and\n  Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in\n  Natural Language Processing, pages 20178–20187, Miami, Florida, USA, November 2024.\n  Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1125. URL\n  https://aclanthology.org/2024.emnlp-main.1125.\n\n\n\n\n\n                                          63\n\n## Claim to Classify\n\"Causal abstraction provides a theoretical foundation for mechanistic interpretability\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "theoretical_framework",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim \"Causal abstraction provides a theoretical foundation for mechanistic interpretability\" is a high-level, conceptual assertion about the role of causal abstraction in the field of mechanistic interpretability. \n\n1. **Method Rung Classification**: The paper introduces a theoretical framework (causal abstraction) to unify and formalize various mechanistic interpretability methods. Theoretical frameworks, by definition, do not involve direct empirical interventions or observational studies on models. Instead, they provide a conceptual or mathematical basis for understanding or analyzing phenomena. Thus, this falls under **Rung 1 (Observational/Associational)** as it establishes a foundational, non-interventional perspective.\n\n2. **Claim Rung Classification**: The claim uses mechanistic and foundational language (e.g., \"provides a theoretical foundation,\" \"mechanistic interpretability\"). The term \"foundation\" implies a deep, structural, and potentially unique or necessary role for causal abstraction in mechanistic interpretability, which aligns with **Rung 3 (Mechanistic/Counterfactual)** claims. The paper argues that causal abstraction is not just correlated with or sufficient for mechanistic interpretability but is *the* framework that underlies and unifies the field.\n\n3. **Hedge Flag**: The claim is stated as an established fact (\"provides a theoretical foundation\") with no hedging language (e.g., \"may,\" \"suggests\"). Thus, **hedge_flag = 0**.\n\n4. **Confidence**: The claim is explicitly presented as the central thesis of the paper, and the entire manuscript is dedicated to substantiating this assertion. The confidence in this classification is therefore **5 (Very confident)**."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 61890,
      "completion_tokens": 401,
      "total_tokens": 62291
    },
    "finish_reason": "stop"
  }
}