{
  "timestamp": "2026-02-19T10:08:41.537349+00:00",
  "paper_id": "2406.11779",
  "claim_id": "2406.11779-05",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2406.11779\nTitle: Compact Proofs of Model Performance via Mechanistic Interpretability\nFull text:\n              Compact Proofs of Model Performance via\n                        Mechanistic Interpretability\n\n\n\n\n                       Jason Gross∗   Rajashree Agrawal   Thomas Kwa†   Euan Ong†   Chun Hei Yip†\n\n                              Alex Gibson‡           Soufiane Noubir‡          Lawrence Chan\n2024\nDec                 We propose using mechanistic interpretabilityAbstract– techniques for reverse engineering\n                          model weights into human-interpretable algorithms – to derive and compactly\n24                       prove formal guarantees on model performance. We prototype this approach by\n                                formally proving accuracy lower bounds for a small transformer trained on Max-of-\n                       K, validating proof transferability across 151 random seeds and four values of K.\n                    We create 102 different computer-assisted proof strategies and assess their length\n                            and tightness of bound on each of our models. Using quantitative metrics, we find\n                                   that shorter proofs seem to require and provide more mechanistic understanding.\n                             Moreover, we find that more faithful mechanistic understanding leads to tighter[cs.LG]                       performance bounds. We confirm these connections by qualitatively examining a\n                               subset of our proofs. Finally, we identify compounding structureless errors as a\n                             key challenge for using mechanistic interpretability to generate compact proofs on\n                           model performance.\n\n\n                1  Introduction\n\n                 One approach to ensuring the safety and reliability of powerful AI systems is via formally verified\n                      proofs of model performance [48, 11].  If we hope to deploy formal verification on increasingly\n                        large models [24, 27] with powerful emergent capabilities [56] across more diverse and broader\n                     domains [5, 46], we will need compact proofs of generalization bounds on specific models that certify\n                      global robustness. However, existing approaches tend to use proof strategies that suffer from bad\n                       asymptotic complexity, while verifying either generalization properties of training procedures or local\n                       robustness properties of specific models.\n\n                  One key challenge to verification is that neural network architectures are highly expressive [51, 58],\n                    and models with similar training procedure and performance may still have learned significantlyarXiv:2406.11779v14\n                          different weights [38, 9]. This expressivity makes it difficult to adequately compress explanations of\n                       global model behavior in ways that correspond closely enough to the model’s actual mechanisms to\n                     be useful for efficient verification without being too lossy, especially when using only knowledge of\n                        the architecture or training procedure. We propose verifying model performance using understanding\n                       derived from mechanistic interpretability (Section 2) – that is, reverse engineering the specific\n                     implementation of the algorithm from the learned weights of particular models. Knowledge of the\n                         specific implementation allows us to construct less lossy simplifications of the model, and more\n                           efficiently reason about model performance over possible inputs.\n\n                       In this work, we provide a case study of translating mechanistic interpretations into compact proofs.\n               We train an attention-only transformer on a Max-of-K task with 151 random seeds (Section 3), and\n\n                          ∗Corresponding author. Please direct correspondence to jgross@mit.edu.\n                          †These authors contributed equally to this work.\n                          ‡These authors contributed equally to this work.\n\n\n                            Preprint. Under review.\n\n               True Model                     Brute Force Proof                Cubic Proof                 Subcubic Proofs\n\n     Logits      (ℓ0, ℓ1, . . . , ℓ63)                          (ℓ0, ℓ1, . . . , ℓ63)                      (ℓ0, ℓ1, . . . , ℓ63)                     (ℓ0, ℓ1, . . . , ℓ63)\n                                                                                            QK Circuit decomposes\n                                                                                                                                                                         into large “size” and small\n                                                                                                                                                                   . . .                                                           “noise” components\n              Unembed                                                                            Unembed\n                                                                          ...\n\n\n                                                                                                                                                                   Size component                                                                                                                                                         w/\n                                                                                                                                                                      103                                                                                                                                                                           singular                                                                                                                                                                         value 7.4 ×                                                                                                                                                       Path               Circuit                Circuit          OV  QK\n                                                                                                                                                                                                                                   Direct    QK    OV      ?...\n               Embed                                                                                                                       . . .                      Embed                       singularOther componentsvalue < 1.5 have× 101\n\n\n     Input      t0    t1   t2    t3                         t0    t1   t2    t3                    t0    t1   t2    t3                    t0    t1   t2    t3\n\n\n         FLOPs Required:                           1.41 × 1014                       3.51 × 107                       4.68 × 106\n           Accuracy Lower Bound:                      99.92 %                         95.31 %                        28.41 %\n           Unexplained Dimension:                     1.07 × 109                        1.28 × 104                       4.42 × 103\n           Asymptotic Complexity:                  O(dvocab nctx)                  O(dvocab 3 · nctx)            O(dvocab · dmodel 2 · nctx)\n\n\nFigure 1: We construct proofs using different degrees of mechanistic interpretation. (Left) The models we\nconsider in this paper are one-layer attention-only transformers, and so contain three “paths”: the OV circuit, the\nQK circuit, and the direct path. (Right) For the brute-force proof (Section 4.3.1), we treat the model as a black box\nand thus need to check all possible combinations of inputs. For the cubic proof (Section 4.3.1), we decompose\nthe model into its three corresponding paths, but still check the correctness of each path via brute force. Finally,\nin some subcubic proofs (Section 4.3), we use all parts of the mechanistic interpretation presented in Section 3.\n(Bottom) For each of the three categories of proof, we report the number of FLOPs used in computing the\ncertificate (lower=better, Appendix A.6), lower bound on model accuracy (higher=better), effective dimension\nof the unexplained parts of the model (lower=better, Appendix A.5), and asymptotic complexity of the proof\nstrategy as we scale the inputs and model (lower=better). Significantly more compact proofs have vacuous\naccuracy bounds by default. Using more mechanistic understanding allows us to recover some, but not all, of the\naccuracy bounds on these more compact proofs, as our understanding is not fully faithful to the model internals.\n\n\nthen reverse engineer the models using standard mechanistic interpretability techniques. We use\nour understanding to define a set of 102 different computer-assisted proof strategies with varying\ntightness of bound and with different asymptotic complexity and number of required floating-point\noperations (Section 4).4 We validate our technique against an additional 604 models for varying\nvalues of K (Appendix A.2.1).\n\nWe define a quantitative metric to assess the mechanistic understanding used in a proof strategy\nby the dimensionality of the function space that the proof strategy must consider, which we deem\nthe unexplained dimensionality of the proof strategy (Sections 5.1, and A.5). Using this metric, we\nfind a negative relationship between proof length and degree of understanding. We qualitatively\nexamine proof strategies to confirm and explain this relationship, finding that more compact proofs\nboth require and provide more mechanistic understanding. We also find suggestive evidence that\nthe trade-off between proof length and tightness of bound is modulated by the faithfulness of the\nmechanistic understanding used to derive the proof (Section 5.2).5\n\nHowever, we also identify compounding structureless error terms as a key challenge for generating\ncompact proofs on model behavior (Sections 5.3, and G.2.5). The implementation of algorithms\ninside of neural networks may contain components that defy mechanistic understanding and appear\nto us as “noise”. When we don’t know how noise composes across model components, establishing a\nbound requires pessimizing over the ways the composition could occur. Worst-case noise can quickly\ngrow over components even when the empirical noise is small, leading to vacuous performance\nbounds (Appendix G.2.5).\n\n\n2  Mechanistic interpretability for proofs\n\nGeneralization bounds on global performance In the style of prior mechanistic interpretability\nevaluation work [6], we target theorem templates that establish bounds on the expected global\nperformance of the model. Let M : X →Y be a model (here assumed to be a neural network), D be\na probability distribution over input-label pairs (l, t) ∈L × X, notated as D|X when marginalized\n\n   4Our 102 proof strategies are can be broken up as 1 + 1 + 10 × 5 × 2: two standalone strategies, and a class\nof strategies parameterized on three axes of cardinality 10, 5, and 2 (Appendix H).\n   5Code for reproducing our results can be found at https://github.com/JasonGross/guarantees-\nbased-mechanistic-interpretability/. A cache of generated data can be found at https://github.\ncom/JasonGross/guarantees-based-mechanistic-interpretability/tree/with-data.\n\n\n                                       2\n\nover labels, and f : L × Y →R be a scoring function for evaluating the performance of the model.\nThen, we seek to establish lower bounds b on the expected ¯s as the form:\n                                                  ¯s := E(l,t)∼D [f(l, M(t))] ≥b.                                (1)\nAs f can be any metric, this is a fully general template for theorems that can capture any aspect of\nmodel performance for which we have a formal specification. However, in this work we restrict f to\nbe the accuracy and D|X to be uniform, so our theorems lower bound the accuracy of the model. Our\nproof methodology generalizes straightforwardly to other input distributions (Appendix A.8), and\nonly a little work is required to generalize from accuracy to log-loss (Appendix A.11).\n\nProof template The proofs of model performance in this work have two components: a computational\ncomponent C  : model weights →R and a non-computational component Q arguing that for any\nmodel M′, C(M′) ≤E(l,t)∼Df(l, M′(t)), thus implying that C generates a valid lower bound\nfor the performance of M. The whole proof is Q paired with a trace of running C that certifies its\noutput on M.6 Here, b = C(M). As even the size of the model parameters is much larger than any\nreasonable Q, we approximate the length of a proof pair C, Q by the length of a trace of C(M).\n\nProof compactness vs. tightness of bound Different proof strategies make different tradeoffs\nbetween compactness and tightness of bound. For example, consider two extreme proof strategies:\nWe can “prove” a vacuous bound using a null proof. On the other hand, in the brute-force proof, we\nsimply run the model on the entirety of D to achieve b = ¯s, albeit with a very long proof.\n\nWe quantify the length of C(M) using two metrics: the asymptotic time complexity of C as we\nscale the size of the model and the input t, as well as the empirical average number of floating point\noperations required to evaluate C(M′) over a given set of models {Mi}. We measure tightness of\nbound of C(M) using the ratio of the bound to the true accuracy: b/¯s.\n\nProof as pessimal ablation A standard way of assessing the faithfulness of mechanistic interpretabil-\nity is by ablating the parts of the model that your interpretation does not explain [54, 6, 23]. In this\nframework, proofs can be thought of as performing a pessimal ablation over the unexplained parts\nof the model – we set the remaining components of the model (the “noise” or error terms) to values\nover X that minimize the performance of the model. However, the number of ablations required for a\ncomplete argument might be quite high. Thus, we construct relaxations (Appendix A.4) over input\nsequences, such that performing pessimal ablations on a smaller number of relaxed input sequences\nis sufficient to lower bound the performance on D.\n\n3  Experimental setting\n\nWe study our approach to generating compact proofs in a simple toy setting: Max-of-K.\n\nModel Architecture We study one-layer, one-head, attention-only transformers with no biases\nbut with learned positional embeddings, with vocabulary size dvocab, model and head dimension\nd = dmodel = dhead, and context length nctx := k. The model parameters consist of the nctx×dmodel\npositional embedding P; the dvocab × dmodel token embed E; the dmodel × dmodel query, key, value,\nand output matrices of the attention head Q, K, V , and O; as well as the dmodel × dvocab unembed\nmatrix U. We assume (as is standard in language modeling) that dmodel < dvocab.\nFor an nctx × dvocab one-hot encoding x =  [x0, x1, . . . , xnctx−1] of an input sequence t =\n[t0, t1, . . . , tnctx−1], we compute the logits of the model as follows:\n           h(0) = xE + P                             Initial residual stream (nctx × dmodel)\n                 √\n         α = h(0)QKT h(0)T /  d            Attention matrix (nctx × nctx)\n           h(1) = σ∗(α) · h(0)V O + h(0)         Final residual stream (nctx × dmodel)\n      M(t) = ℓ= h(1)nctx−1U                  Final seq. position logits (dvocab)\n\nwhere σ∗is the masked softmax function used in causal attention. Because we only look at outputs\nof the model above the final sequence position i = nctx −1, we also denote this position as the\n\n   6Other components of the proof to account for the difference between floating point numbers and reals are\ndescribed in Appendix A.7. Note that all proofs explicitly given in this paper are of Q only; we do not include\nany traces of running C.\n\n\n                                       3\n\n“query position” and the value of the token in this position as tquery, one-hot encoded as xquery. The\nmodel’s prediction is the token corresponding to the max-valued logit ℓmax.\n\nTask Specifically, we study the setting with nctx = k = 4 because it is the largest sequence length for\nwhich we can feasibly evaluate the brute-force proof. We set hidden dimension dmodel = 32 and a\nvocabulary of size dvocab = 64 comprising integers between 0 and 63 inclusive. For an input sequence\nt = [t0, t1, t2, t3], we denote the true maximum of the sequence by tmax. Outputting the correct\nbehavior is equivalent to outputting logits ℓsuch that ∆ℓt∗:= ℓt∗−ℓmax < 0 for all t∗̸= tmax. We\ntrained 151 models on this task. Models achieved an average accuracy of 0.9992 ± 0.0015 over the\nentire data distribution.\n\nPath decomposition Following prior work [13], we expand the logits of the model and split the paths\nthrough the model into three components – the QK circuit, the OV circuit, and the direct path:\n\n\n                          √\nM(t) = σ∗  (xqueryE + Pquery) QKT (xE + P)T /  d   · (xE + P) V OU + (xqueryE + Pquery) U\n             |            QK {zcircuit             }       |   OV {zcircuit   }  |          direct{zpath      }\n                                                                                                    (2)\n\nIntuitively, the QK circuit determines which tokens the model attends to from a particular query token\nand sequence position, while the OV circuit processes the tokens and sequence positions the model\nattends to. The direct path is simply the skip connection around the attention head.\n\nWe further divide the QK and OV circuits into token (position-independent) and position-dependent\ncomponents. Let Pavg = Pi Pi/nctx be the average position embeds across positions (of size\ndmodel), and let ¯P denote either 1nctx ⊗Pavg or 1dvocab ⊗Pavg depending on context, the result of\nbroadcasting Pavg back into the shape of P or E (that is, nctx ×dmodel or dvocab ×dmodel). Similarly,\nlet Pq = 1dvocab ⊗Pquery be the result of broadcasting Pquery. Then for one-hot encoded x, we can\nrewrite the QK and OV circuits, as well as the direct path, as follows:\n\n         QK circuit = xquery EqQKT ¯ET xT + EqQKT ˆPT\n                                   | EQKE{z   }      |  EQKP{z    }\n          OV circuit = x ¯EV OU + ˆPV OU     Direct Path = xquery EqU\n                             |EVOU{z }  |PVOU{z  }                      |{z}EU\nwhere ˆP = P −¯P and ¯E = E + ¯P and Eq = E + Pq (since h(0) = x¯E + ˆP).\n\n3.1  Mechanistic interpretation of learned models\n\nUsing standard empirical mechanistic interpretability tech-                       Logits  (ℓ0, ℓ1, . . . , ℓ63)\nniques, we interpret one of our learned models (our “mainline”                           Unembed\nmodel) by independently examining the QK and OV circuits        toQKlargerCircuittokensattendsmore\nand the direct path.7 We find that the model outputs the largest\nlogit on the true max token tmax by attending more to larger    OV Circuit performs\n                                                                                                                                                                               Direct                                                                                                                                                                               pathtokens via the QK circuit and copying the tokens it attends to       low-rank copying                                                                                                                                                                          nothing”                                                                                                                   Embed        “does\nvia the OV circuit. We then quantitatively confirm that these\ninterpretations hold for all 151 models by reporting the mean                        Input     t0   t1   t2   t3\nplus minus standard deviation for various summary statistics.\n                                                            Figure 2: The models in our setting\nPlots for this section are available in Appendix B.2.                                                            implement Max-of-K by attending ex-\nQK  circuit By  qualitatively  examining  the  position-  ponentially more to larger tokens and\nindependent QK component EQKE, we find the amount of  copying the attended-to tokens (Sec-\npre-softmax attention paid to a key token is approximately in-  tion 3.1).\ndependent of the value of the query token tquery, and increases monotonically based on the size of the\nkey token. We confirm this hypothesis by performing a singular-value decomposition (SVD) of the\nEQKE matrices (Appendix G.2.3), and find that it contains a single large rank-one component with\nsingular value around 7800 ± 380, around 620 ± 130 times larger than the second largest component\nwith singular value 13 ± 3. The left (query-side) singular vector is approximately constant in all\n\n    7All of our trained models behave similarly; see Appendix B.3.\n\n\n                                        4\n\ndimensions, with value 0.1243 ± 0.0003 ≈1⁄8 = 1/√dvocab. The right (key-side) singular vector√   of this\ncomponent is monotonically increasing as we increase the size of the key token, with (1/  d-scaled)\npre-softmax attention increasing by an average of 1.236 ± 0.056 when the key token increases by 1.8\n           √\nIn comparison, each 1/  d-scaled entry of the position-dependent QK component EQKP has negligi-\nble size (average 0.31 ± 0.18), suggesting that EQKP is unimportant to the functioning of the model.\nWe confirm this by zero ablating EQKP, which changes the models’ accuracies from 0.9992±0.0015\nto 0.9993 ± 0.0011. Combined with our interpretation of EQKE, this implies that the attention\npattern of the model depends only on the token values and not the ordering of the sequence.\n\nOV circuit Then, by qualitatively examining the position-independent OV component EVOU, we\nsee that it has large positive entries along the diagonal. In fact, the entry along the diagonal is the\nlargest in the row for all rows corresponding to t > 6.6 ± 1.2. Since each entry in the sequence is\nuniformly sampled and dvocab = 64, this means that EVOU is a good approximation for the identity\nmatrix for all but ≈(7/64)4 ≈1.2 × 10−2 % of the sequences.\n\nAs with the position-dependent QK component, the position-dependent OV component PVOU also\nhas negligible size and is unimportant to model performance. Taken together with the above results\non EVOU, this suggests that the attention head copies the tokens it attends to.\n\nDirect path As with the two position-dependent components, the entries in EU have small absolute\nmagnitude 2.54 ± 0.20,9 and contribute negligibly to model performance.\n\n4  Proofs of model performance\n\nIn this section we describe intuitions for three categories of proof that are developed around different\nmechanistic interpretations and methods for using the interpretations. The strategies result in proofs\nof different complexities with varying bound tightness (Table 1). We provide detailed theorem\nstatements, proofs, algorithms, and explanations of proof search in Appendices C, D, E, F, and G.\n\nOur theorem statements for Q will all be of the form\n                ∀M′, Cspecific strategy(M′) ≤Et∼D|Xf(tmax, M′(t)).\nWe leave implicit the traces of running Cspecific strategy on our specific models to give the overall\ntheorem. We report the computational complexity or estimated FLOPs of running Cspecific strategy as\napproximations for our proof lengths.\n\n4.1  The brute-force baseline\n\nWe start by considering the brute-force proof (Appendix D), which treats the model as a black box and\nevaluates it on all possible sequences.10 However, this proof strategy has bad asymptotic complexity\nand is untenable for larger models and larger input distributions. So in subsequent sections, we use\nknowledge of the model drawn from the interpretation in Section 3.1 to derive more compact proofs.\n\n4.2 A cubic proof\n\nNext, we use the fact that the model is composed of the direct path and the QK and OV circuits\n(Section 3) to decrease the number of sequences that we need to consider, and the fact that only\nthe position-independent components EQKE and EVOU contribute meaningfully to performance\n(Section 3.1) to pessimize over sequence ordering.\n\nFirst, let a pure sequence ξ be a sequence with at most three distinct tokens: the max token tmax, the\nfinal token tquery ≤tmax, and optionally a third token t′ < tmax, and let Ξpure be the set of all pure\nsequences in X.11 For a given input sequence t, define the adjacent pure sequences Adj(t) as the set\nof sequences that share the same max and query token, and only take on values in t:\n             n                                      o          Adj(t) =  ξ ∈Ξpure max ξi = tmax, ξquery = tquery, ∀i < nctx, ξi ∈t\n                                                     i\n\n   8This implies that the ratio of attention paid to token t and t −1 is approximately exp(1.236) ≈3.442.\n   9For comparison, the average off-diagonal element of EVOU is 21.68 ± 0.83 below the corresponding\ndiagonal element.\n  10Appendix A.10 discusses how to compute the “brute-force” accuracy of a model on an infinite distribution.\n   11In Section 4.3, we will consider a smaller set of “pure sequences”.\n\n\n                                       5\n\nTable 1: We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as\nunexplained dimensionality (Section 5). We round the FLOP and unexplained dimension counts to the closest\npower of 2, and report the mean/standard deviation of the bound averaged across all 151 models. As we include\nmore aspects of the mechanistic interpretation (reflected by a lower number of unexplained dimensions), we get\nmore compact proofs (in terms of both asymptotic complexity and FLOPs), albeit with worse bounds. For space\nreasons, we use k := nctx, d := dmodel, and v := dvocab.\n\n  Description         Complexity Cost              Bound                Est.     Unexplained\n  of Proof                                                   FLOPs  Dimensions\n\n  Brute force         O(vk+1kd)                      0.9992 ± 0.0015      247       230\n  Cubic             O(v3k2)                        0.9531 ± 0.0087      225       214\n  Sub-cubic          O(v2 · k2 + v2 · d)               0.702 ± 0.033        221       213\n    w/o mean+diff                                     0.349 ± 0.080        221       213\n  Low-rank QK       O(v2k2 + vd2 + v2d )           0.675 ± 0.035        222       212\n   SVD only                   |{z}QK   EU&OV|{z}            0.284 ± 0.072        222       212\n  Low-rank EU       O(v2k2 + vd + v2d )           0.633 ± 0.062        221       213\n   SVD only                   |{z}EU   QK&OV|{z}           (3.38±0.06)×10−6   221       213\n  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.610 ± 0.060        221       213\n   SVD only                   |{z}QK    |{z}EU    |{z}OV      (3.38±0.06)×10−6   222       213\n  Quadratic QK       O(v2k2 + vd + v2d )           0.316 ± 0.037        221       212\n                                 |{z}QK   EU&OV|{z}\n  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.283 ± 0.036        221       213\n                              QK&EU|{z}   |{z}OV\n\n\n\nUsing the convexity of softmax and the fact that the model contains three paths, we can show that\none-layer attention-only transformers satisfies a variant of the following convexity property: for a\ngiven t, if M(ξ) is correct for all ξ ∈Adj(t), then M(t) is correct. That is, for these transformers,\nwe can bound the accuracy on all sequences by evaluating M on only the O(dvocab 3(nctx −1)!)\npure sequences. This allows us to bound the accuracy of our actual M on all dvocab nctx sequences,\nwhile evaluating it on O(dvocab 3(nctx −1)!) sequences.\n\nWe can reduce the number of sequences that we need to evaluate by pessimizing over the order of a\nsequence. For a given tuple of (tmax, tquery, t′), there are (nctx −1)! pure sequences, corresponding\nto the permutations of the tuple. Pessimizing over the order of sequences reduces the number of\nsequences to consider for each (tmax, tquery, t′) tuple to the number of t′ in the pure sequence, and\nthe total number of sequences to O(dvocab 3nctx). By precomputing the five component matrices\nEU, EQKE, EQKP, EVOU, PVOU and cleverly caching intermediate outputs, we can reduce the\nadditional work of each sequence to the O(nctx) required to compute the softmax over nctx elements,\nresulting in asymptotic complexity O(dvocab 3nctx2) (Theorem 12, additional details in Appendix E).\n\n4.3  Sub-cubic proofs\n\nWe now consider proofs that are more compact than O(dvocab 3). These require avoiding iteration\nover any set of size O(dvocab 3) (e.g. the set of pure sequences) and performing operations that take\nO(dvocab) time on each of O(dvocab 2) combinations. Unfortunately, some methods of avoiding\nthese operations can lead to vacuous bounds (i.e. accuracy lower bounds near 0%). In order to\nrecover non-vacuous bounds, we introduce two tricks: the “mean+diff trick” to better approximate\nthe sum of two components with unequal variance, and the “max row diff trick” to improve upon\nthe low-rank approximations for EU and EQKE. We consider applying variants of these tricks at\ndifferent locations in the naïve subcubic proof, leading to 100 distinct subcubic proof strategies. See\nAppendix G.2 for a formal description of these strategies.\n\n4.3.1  Removing cubic-time computations\n\nReducing the number of cases by pessimizing over sufficiently small tokens Previously, we\nconsidered Θ(dvocab 3nctx) pure sequences ξ, with ξ parameterized by (tmax, tquery, t′, c). Recall\n\n\n                                       6\n\nfrom our mechanistic interpretation in Section 3.1 that the pre-softmax attention paid from tquery to\na key token t′ is broadly invariant in tquery and increases roughly linearly with the size of t′. This\nallows us to pessimize over the OV circuit over all “sufficiently small” tokens.\nMore formally, suppose we are given some gap g ∈N. For each pure sequence ξ with max token tmax,\nquery token tquery, such that tquery ≤tmax −g, and c copies of the third token type t′ ≤tmax −g,\nwe pessimally ablate the OV circuit over the set Ξpure(tmax, tquery, c; g) of pure sequences ξ′ with\nthe same max and query tokens and c copies of the third token type t′. If the model gets all sequences\nin Ξpure(tmax, tquery, c; g) correct, then we can conclude that it gets ξ correct, otherwise, we treat\nthe model as having gotten ξ wrong. This means that it suffices to only consider the O(dvocab 2nctx)\npessimal pure sequences of each of the O(dvocab 2nctx) sets of the form Ξpure(tmax, tquery, c; g).\nDecoupling and pessimizing computations that require O(dvocab 3) computations Many parts\nof our cubic certificate require iterating through O(dvocab 2) cases parameterized by (tmax, tquery)\nor (tmax, t′). For example, as part of the pessimization procedure over pure sequences, for each\nof the dvocab possible values of tmax, we need to consider the relative effects on the dvocab-sized\nlogits of attending to each of the O(dvocab) other tokens t′ < tmax, and for each tmax and tquery, we\nneed to check that the contribution of the direct path on logits xqueryEU is not sufficiently large as\nto overwhelm the contribution from xmaxEVOU. We independently pessimize over each of these\ncomponents over one of the dvocab-sized axes: for example, instead of computing xmaxEVOU +\nxqueryEU for each tmax, tquery pair, we first pessimally ablate the direct path along the query\ntoken (which takes O(dvocab 2) time as it does not depend on the tmax, and then consider the sum\nxmaxEVOU + maxx′ x′EU. Since this sum no longer depends on tquery, we only need to perform it\nO(dvocab) times, for a total cost of O(dvocab 2).\n\nLow rank approximations to EQKE and EU Recall from Section 3.1 that EQKE is approximately\nrank 1, where the sole direction of variation is the size of the key token. By computing only the low\nrank approximation to EQKE, we can more cheaply compute the most significant component of the\nbehavior in the QK circuit. To bound the remaining error, we can use the fact that after pulling off the\nfirst principal component from each of the four matrices we multiply, very little structure remains.\n\nWe can find the rank 1/2 approximations by performing SVD on EQKE. We can efficiently compute\nthe SVD in O(dvocabdmodel 2) time by using the fact that EQKE can be written as the product of\na dvocab × dmodel matrix and a dmodel × dvocab matrix. This allows us to avoid performing the\nO(dvocab 2dmodel)-cost matrix multiplications to explicitly compute EQKE.\n\nSimilarly, we can more efficiently check that the direct path EU contributes negligibly to the model\noutputs, by using SVD to decompose EU into a sum of rank 1 products (which we can evaluate\nexactly) and a high-rank error term that we can cheaply bound.\n\n4.3.2  Additional subcubic proof strategies\n\nTighter bounds for sums of variables with unequal variance via the “mean+diff trick” Suppose\nwe want to lower bound the minimum of the sum of two functions over three variables h(x, y, z) =\nf(x, y) + g(y, z), while only iterating over two variables at a time. The naïve way is to minimize\nf(x, y) and g(x, y) independently:\n\n                    min h(x, y, z) ≥min f(x, y) + min g(y, z)\n                                x,y,z               x,y              y,z\n\nHere, the error comes from setting the ys in f and g to different values. But in cases where g(y, z)\nvaries significantly with y and only slightly with z, rewriting g as a sum of a component that is\nindependent of z (only varying along y), and a component that depends on z, yields a better lower\nbound:\n         min h(x, y, z) ≥min (f(x, y) + E′zg(y, z′)) + min(g(y, z) −E′zg(y, z′))\n              x,y,z               x,y                                y,z\n\nThis estimate will have error at most ε, while the naïve estimator can have arbitrarily large error. We\nrefer to this rewrite as the “mean+diff trick”.12 From the mechanistic interpretation in Section 3.1,\n\n   12In fact, this is the motivation behind the standard rewrites of QK and OV into position-independent and\nposition-dependent components (Section 3).\n\n\n                                       7\n\n    Bound  1.0                                                                                                brute force (acc: 0.9992 ± 0.0015)\n      0.8                                                                                           cubic (rel acc: 0.9539 ± 0.0080)\n      0.6                                                                                         subcubic (rel acc: 0.700 ± 0.036)\n                                                                                                                                   2 (rel acc: 0.675 ± 0.035)      Accuracy  0.4                                                                                                attention-dvocabdmodel\n                                                                                                             direct-quadratic (rel acc: 0.633 ± 0.062)\n      0.2\n                                                                                                         attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.610 ± 0.060)\n      0.0                                                                                                   attention-quadratic (rel acc: 0.316 ± 0.037)        Normalized                               229                         239                                    attention-quadratic, direct-quadratic (rel acc: 0.283 ± 0.037)\n                          FLOPs to Verify Proof (approximate)                                 brute-force linear baseline\n\n\nFigure 3: For each of the proofs in Section 4, we plot the number of FLOPs used to compute the certificate,\nas well as the normalized accuracy lower-bound (b/¯s). The brute-force proof (Section 4.1) computes the exact\nperformance uses orders of magnitude more compute than other approaches. The cubic proof (Section 4.3)\nuses a small amount of mechanistic understanding and less compute, while still retaining good accuracy lower\nbounds. Finally, subcubic proofs (Section 4.3) require the entirety of the mechanistic interpretation of the model\nto attain non-vacuous bounds; this understanding allows us to further reduces compute costs, but we still achieve\nworse bounds. See Appendix H.2.1 for a detailed description of the various proof strategies.\n\n\nwe know that some of the components barely vary among one or more axes. So we can apply the\nmean+diff trick to get tighter lower bounds.\n\nAvoiding matrix multiplications using the “max row-diff trick” Using properties of linear algebra,\nwe derive a cheap approximation to the max row-diff for the product of matrices AB in terms of the\nproduct of the max row-diff of B and the absolute value of A, which we deem the “max row-diff”\ntrick. We apply this trick to get a better cheap bound on the error terms of low-rank approximations,\nwithout having to multiply out the full matrices. See Appendix G.2.2 for more details.\n\nSee Appendix F for more variants and combinations of these strategies.\n\n5  Results\n\nWe run each of 151 transformers on the various proof strategies of different asymptotic complexity,\nand analyze these proofs to empirically examine the relationship between proof length, bound\ntightness, and degree of understanding. For each proof on each transformer, we approximate the\nlength of the proof by estimating the number of FLOPs used, and plot this against the ratio of certified\nbound the true accuracy b/¯s (Equation 2) in Figure 3. There exists a clear trade-off between bound\ntightness and compactness of the proof – more compact proofs yield looser bounds, and tighter\nbounds are associated with more expensive proofs.\n\n5.1  Compact proofs both require and provide mechanistic understanding\n\nQuantifying mechanistic understanding using unexplained dimensionality We first quantify the\namount of mechanistic understanding used in a proof by measuring its unexplained dimensionality\n– the number of free parameters required to fully describe model behavior, assuming the structural\nassumptions of the proof are correct. More detailed mechanistic interpretations will leave fewer free\nparameters that need to be filled in via empirical observation. (Details in Appendix A.5.) In Figure 5,\nwe plot the two axes and find a suggestive correlation – that is, proofs based on less mechanistic\nunderstanding are longer.\n\nMore mechanistic understanding allows for more compact proofs In addition to the constructions\nin Section 4, the parts of proofs we were unable to compact seem to correspond to components that\nwe do not mechanistically understand. For example, we could not cheaply bound the behavior of\nEVOU without multiplying out the matrices, and this seems in part because we do have a mechanistic\nunderstanding of how EVOU implements low-rank copying.\n\nCompact proofs seem to provide understanding By examining compact proofs, we can extract\nunderstanding about the model. For example, the fact that replacing each row of EU with its average\nacross rows has little effect on the bound implies that EU does not vary much based on tquery.\n\n5.2  Proof length vs. bound tightness trade-off is modulated by faithfulness of interpretation\n\nCompact proofs are less faithful to model internals To derive more compact proofs, we use our\nmechanistic understanding to simplify the model computation in ways that diverge from the original\nmodel internals. For example, in some subcubic proofs (Section 4.3), we approximate EQKE with a\n\n\n                                        8\n\n     1.0\n  Bound  0.8                                                                                mean+max-diffmax-diff\n                                                                                           mean+max-diff-subproduct\n     0.6                                                                                        max-diff-subproduct\n                                                                                                  max-diff-exact   Accuracy\n     0.4                                                                                   svd\n                                                                                                 mean-recursive+max-diff-subproduct-recursive\n     0.2                                                                                       mean+max-diff-subproduct-recursive\n                                                                                                  max-diff-subproduct-recursive   Normalized\n     0.0\n                400               600               800              1000\n                        EPQKE Singular Ratio: σ1/σ2\n\nFigure 4: We plot the normalized accuracy bound versus the ratio of first and second singular values of EQKE,\nfor various types of subcubic proofs that depend on a rank-1 approximation EQKE. For each class of proof, the\ncloser EQKE is to rank-1, the tighter the accuracy bound. This suggests that more faithful interpretations lead to\ntighter bounds even holding proof length fixed. Note that the “svd” proof strategy has the clearest upward trend\n(b/¯s = 0.000 20(σ1/σ2) + 0.44, R2 = 0.41). See Appendix H.2.2 for a detailed description of the various\nproof strategies.\n\n\n     229                                                                                             brute force\n                                                                                                cubic      (Estimated)  226                                                                                      subcubic\n     223                                                                                             attention-dvocabdmodel 2\n                                                                                                          direct-quadratic\n     220     Dimension                                                                                                  attention-dvocabdmodel 2, direct-quadratic\n     217                                                                                                attention-quadratic\n     214                                                                                                  attention-quadratic, direct-quadratic\n      Unexplained               223        227        231        235        239        243        247\n                          FLOPs to Verify Proof (approximate)\n\nFigure 5: We plot, for each proof, the approximate number of flops required to evaluate the proof, versus the\nunexplained dimensionality (Section 5.1). More mechanistic understanding leaves fewer dimensions unexplained.\nWe observe that more compact proofs seem to leave fewer unexplained dimensions, which is indicative of the\nrelationship of mechanistic understanding and compact proofs. See Appendix H.2.1 for a detailed description of\nthe various proof strategies.\n\n\nrank-1 approximation corresponding to the “size direction”. However, while other components are\nsmall, they’re nonzero; this approximation harms model internals.\n\nLess faithful interpretations lead to worse bounds on performance To confirm that faithfulness of\nunderstanding affects the tightness of bound independent of proof length, we plot the normalized\naccuracy bound of subcubic proofs that perform a rank-1 approximation to EQKE, versus the ratio\nof the first two singular components. A larger ratio between the components implies that the rank-1\napproximation is more faithful. In Figure 4, we see a positive correlation between the two axes: when\nthe interpretation is more faithful, the bounds are tighter, even at a fixed proof length.\n\n5.3  Compounding structureless noise is a big challenge for compacting global-behavior proofs\n\nPessimal error terms compound in the absence of known structure The rank-1 approximation of\nEQKE has small error. However, when making rank-1 approximations of each of the constituent\nmatrices E, Q, K, pessimizing over the worst way to composing the individual small error terms\nleads to a bound on the error term of EQKE that is orders of magnitude larger than the actual error\nterm. Because we don’t understand how the matrices compose in a way that doesn’t cause errors to\ncompound (without just multiplying out the matrices), this approximation leads to a trivial bound on\nperformance (Appendix G.2.5). We speculate that in many cases, there is no short human-interpretable\ndescription for why random noise or approximation errors do not compound across layers of neural\nnetworks (e.g., see the error correction results on randomly initialized neural networks from Hänni\net al. [21]), and thus that compounding structureless errors may be an issue in practice.\n\n6  Related Work\n\nGeneralization Bounds Prior work in the PAC-Bayes framework [58, 36, 12] proves generalization\nbounds over learning procedures, which are similar to the global performance bounds we consider in\n\n\n                                       9\n\nthis work. These proofs tend to provide statistical guarantees [25, 26] about the outputs of a known\nstochastic training procedure, while we seek to bound the performance of particular trained models.\n\nFormally verifying neural networks Most prior work formally verifies neural networks either via\nmodel checking [28, 7] or by relaxing the problem setting and taking an automated theorem proving\napproach [17, 50, 18, 35, 43] to verify local robustness properties. These proof strategies tend to be\nderived by examining only the network architecture. We take an approach more akin to interactive\ntheorem proving [22] and verify global performance properties by reverse-engineering the neural\nnetwork weights.\n\nMechanistic Interpretability Finally, mechanistic interpretability is the subfield of the broader field\nof understanding model internals [45], which is too large to faithfully summarize. Our work takes\nmost direct inspiration from efforts to deeply understand how either toy models [38, 9, 53, 2] or small\npretrained text transformers [54, 20] implement algorithmic tasks, generally by performing ablations\nand SVD. In contrast, we formally prove that a transformer implements an algorithm.\n\nNichani et al. [39] proves that, in a significantly simplified 2-layer, 1-head attention-only transformer\nmodel and for the task of in-context bigram statistics, gradient descent will create induction heads [40].\nOur results concern transformers with fixed weights. In concurrent work, Michaud et al. [34] use\ntechniques inspired by mechanistic interpretability to perform automated program synthesis on\n2-dimensional RNNs, while our work works with significantly larger transformer models.\n\n7  Conclusion and Future Work\n\nSummary In this work, we used a Max-of-K setting to prototype the use of mechanistic interpretabil-\nity to derive compact proofs of model behavior. Using varying amounts of understanding, we derived\nmore efficient proof computations lower bounding model accuracy. We found preliminary evidence\nthat mechanistic understanding can compactify proofs. Moreover, we observed that the tightness\nof the lower bound offered by various proof strategies can be used to grade the faithfulness our\nmechanistic interpretation. Finally, we identified compounding structureless errors as a key obstacle\nto deriving compact proofs of model behavior.\n\nLimitations and future work We study one-layer attention-only transformers on a toy algorithmic\ntask. Future work should explore the viability of deriving proofs via interpretability using larger\nmodels featuring MLPs or layernorm on more complex domains. In addition, we were unable to\nsignificantly compact the part of the proof involving the OV circuit, which future work can explore.\nThe proofs we explored in this work also did not lead to qualitatively novel insights; future work\nmay be able to derive such insights with improved techniques. Finally, future work can address the\nproblem of compounding structureless errors, perhaps by relaxing from worst-case pessimal ablations\nto typical-case heuristic guarantees [8].\n\nAcknowledgments and Disclosure of Funding\n\nWe are immensely grateful to Paul Christiano for providing the initial support for this project and for\nhis invaluable research advice, encouragement, and feedback throughout its duration.\n\nAdditionally, we are thankful for clarifying discussions and feedback from Jacob Hilton, Matthew\nCoudron, Adrià Garriga-Alonso, Aryan Bhatt, Leo Gao, Jenny Nitishinskaya, Somsubhro Bagchi,\nGabriel Wu, Erik Jenner, Ryan Greenblatt, Ronak Mehta, Louis Jaburi, and many others. Louis Jaburi\nin particular contributed the text of the final proof of Theorem 11 in Appendix E.\n\nWe are indebted to various organizations for their support:\n\n        • Alignment Research Center for funding this project and making it possible at all\n\n        • Mentorship for Alignment Research Students (MARS) program of the Cambridge AI Safety\n      Hub (CAISH) for setting up the collaboration between a subset of authors, and providing\n        funding for compute and in-person research sprints\n\n        • Constellation and FAR Labs for hosting a subset of the authors and providing an excel-\n          lent research environment, including as part of the Visiting Fellows Program and Astra\n        Fellowship\n\n\n                                       10\n\nAuthor Contributions\n\nJason Gross led the project, including managing the team and conceptualizing the proofs approach.\nHe ran the Max-of-4 experiments, devised the proof strategies, and wrote up the formal proofs. He\nworked on various case studies and developed general methodology for computing complexity and\nlength bounds for proofs. He also developed the particular convex relaxations presented in the paper.\n\nRajashree Agrawal was invaluable in steering the direction of the project, including contributing to\nthe preliminary experiment on Max-of-2 and developing the pessimal ablation approach. She worked\non framing the results, and contributed text to the paper.\n\nThomas Kwa and Euan Ong extended the preliminary experiments to larger values of k and\ncontributed substantially to the cubic proof. Chun Hei Yip, Alex Gibson, and Soufiane Noubir\nworked on case studies other than the Max-of-K task and informed discussion on proof complexity.\n\nLawrence Chan spearheaded the writing of the paper, including turning informal claims into formal\ntheorem statements, creating figures, and writing the core text. He also developed the unexplained\ndimensionality metric for clarifying the takeaway of the paper.\n\n\n\n\n\n                                       11\n\nReferences\n\n [1] Behzad Akbarpour, Amr Abdel-Hamid, Sofiène Tahar, and John Harrison. Verifying a syn-\n     thesized implementation of IEEE-754 floating-point exponential function using HOL. The\n    Computer Journal, 53:465–488, May 2010. doi: 10.1093/comjnl/bxp023.\n\n [2] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning\n     algorithm is in-context learning? investigations with linear models, 2022.\n\n [3] Andrew Appel and Ariel Kellison. VCFloat2: Floating-point error analysis in Coq. In Proceed-\n     ings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs,\n    CPP 2024, pages 14–29, New York, NY, USA, 2024. Association for Computing Machinery.\n    ISBN 9798400704888. doi: 10.1145/3636501.3636953.\n\n [4] Sylvie Boldo and Guillaume Melquiond. Flocq: A unified library for proving floating-point\n     algorithms in Coq. In 2011 IEEE 20th Symposium on Computer Arithmetic, pages 243–252,\n     July 2011. doi: 10.1109/ARITH.2011.40.\n\n [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\n    Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n     Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n      Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\n     Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n     Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n   URL https://arxiv.org/abs/2005.14165.\n\n [6] Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny\n     Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas.  Causal scrub-\n     bing, a method for rigorously testing interpretability hypotheses.  AI Alignment Forum,\n     2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-\n    scrubbing-a-method-for-rigorously-testing.\n\n [7] Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artificial\n     neural networks. In Automated Technology for Verification and Analysis: 15th International\n    Symposium, ATVA 2017, Pune, India, October 3–6, 2017, Proceedings 15, pages 251–268.\n     Springer, 2017.\n\n [8] Paul Christiano, Eric Neyman, and Mark Xu. Formalizing the presumption of independence.\n     arXiv preprint arXiv:2211.06738, 2022. doi: 10.48550/arxiv.2211.06738.\n\n [9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse\n     engineering how networks learn group operations, 2023.\n\n[10] Edmund M. Clarke, William Klieber, Miloš Nováˇcek, and Paolo Zuliani. Model Checking and\n     the State Explosion Problem, pages 1–30. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.\n    ISBN 978-3-642-35746-6. doi: 10.1007/978-3-642-35746-6_1. URL https://doi.org/10.\n    1007/978-3-642-35746-6_1.\n\n[11] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia,\n     Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards guar-\n     anteed safe AI: A framework for ensuring robust and reliable AI systems. arXiv preprint\n     arXiv:2405.06624, 2024.\n\n[12] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds\n      for deep (stochastic) neural networks with many more parameters than training data. Proceedings\n     of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2016, August\n     11–15, 2017, Sydney, NSW, Australia, 2017.\n\n[13] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n    Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n     Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n    Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n     Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n     2021. URL https://transformer-circuits.pub/2021/framework/index.html.\n\n\n                                       12\n\n[14] Martín H. Escardó. Synthetic topology of data types and classical spaces. Electronic Notes in\n     Theoretical Computer Science, 87:21–156, November 2004.\n\n[15] Martín H. Escardó. Infinite sets that admit fast exhaustive search. In Proceedings of the 22nd\n    Annual IEEE Symposium on Logic in Computer Science (LICS 2007), Wrocław, Poland, July\n     2007.\n\n[16] Martín H. Escardó. Seemingly impossible functional programs, 2007. URL https://math.\n    andrej.com/2007/09/28/seemingly-impossible-functional-programs/. Accessed:\n     2024-05-15.\n\n[17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2:\n     Safety and robustness certification of neural networks with abstract interpretation. In 2018\n    IEEE Symposium on Security and Privacy (SP), pages 3–18, Los Alamitos, CA, USA, May\n     2018. IEEE Computer Society. doi: 10.1109/SP.2018.00058.\n\n[18] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan\n     Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval\n    bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715,\n     2018.\n\n[19] Jason S. Gross. Performance Engineering of Proof-Based Software Systems at Scale. PhD\n      thesis, Massachusetts Institute of Technology, February 2021. URL https://dspace.mit.\n    edu/handle/1721.1/130763.\n\n[20] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than.\n     Interpreting mathematical abilities in a pre-trained language model, 2:11, 2023.\n\n[21] Kaarel Hänni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of\n     computation in superposition. In ICML 2024 Workshop on Mechanistic Interpretability, 2024.\n   URL https://openreview.net/forum?id=OcVJP8kClR.\n\n[22] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In\n    Handbook of the History of Logic, volume 9, pages 135–214. Elsevier, 2014.\n\n[23] Stefan Heimersheim and Neel Nanda. How to use and interpret activation patching. arXiv\n     preprint arXiv:2404.15255, 2024.\n\n[24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\n     Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\n     Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\n[25] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo,\n    Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks:\n      Verification, testing, adversarial attack and defence, and interpretability.  arXiv preprint\n     arXiv:1812.08342, 2018.\n\n[26] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek\n     Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei\n    Xu, Dengyu Wu, Andre Freitas, and Mustafa A. Mustafa. A survey of safety and trustworthiness\n     of large language models through the lens of verification and validation, 2023.\n\n[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\n     Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\n     models. arXiv preprint arXiv:2001.08361, 2020.\n\n[28] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An\n      efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th\n     International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings,\n     Part I 30, pages 97–117. Springer, 2017.\n\n[29] A. E. Kellison, A. W. Appel, M. Tekriwal, and D. Bindel. LAProof: A library of formal proofs\n     of accuracy and correctness for linear algebra programs. In 2023 IEEE 30th Symposium on\n    Computer Arithmetic (ARITH), pages 36–43, Los Alamitos, CA, USA, September 2023. IEEE\n    Computer Society. doi: 10.1109/ARITH58626.2023.00021.\n\n\n                                       13\n\n[30] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin,\n    Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell,\n     Harvey Tuch, and Simon Winwood. seL4: Formal verification of an OS kernel. In Proceedings\n      of the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP ’09, pages 207–\n     220, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605587523.\n      doi: 10.1145/1629575.1629596. URL https://doi.org/10.1145/1629575.1629596.\n\n[31] Xavier Leroy. A formally verified compiler back-end. Journal of Automated Reasoning, 43:\n     363–446, 2009.\n\n[32] Chuan Li. OpenAI’s GPT-3 language model: A technical overview, June 2020. URL https:\n    //lambdalabs.com/blog/demystifying-gpt-3. Lambda Labs Blog, accessed October\n     30, 2024.\n\n[33] Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt\n     and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56–61,\n     2010. doi: 10.25080/Majora-92bf1922-00a.\n\n[34] Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge,\n     Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukeli´c, and Max Tegmark. Opening the AI\n     black box: program synthesis via mechanistic interpretability, 2024.\n\n[35] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for\n     provably robust neural networks. In International Conference on Machine Learning, pages\n     3578–3586. PMLR, 2018.\n\n[36] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain\n     generalization in deep learning. Advances in Neural Information Processing Systems, 32, 2019.\n\n[37] Neel  Nanda  and  Joseph  Bloom.     TransformerLens.     https://github.com/\n    TransformerLensOrg/TransformerLens, 2022.\n\n[38] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress\n     measures for grokking via mechanistic interpretability. arXiv preprint, 2023. doi: 10.48550/\n     arXiv.2301.05217.\n\n[39] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with\n     gradient descent. arXiv preprint, 2024. doi: 10.48550/arXiv.2402.14735.\n\n[40] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\n     Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,\n    Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson\n     Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\n    Sam McCandlish, and Chris Olah.  In-context learning and induction heads. Transformer\n     Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/in-context-\n    learning-and-induction-heads/index.html.\n\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\n     Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative\n      style, high-performancedeep learning library.  Advances in neural information processing\n     systems, 32, 2019.\n\n[42] Plotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly.\n\n[43] Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semidefinite relaxations for certifying\n     robustness to adversarial examples. Advances in neural information processing systems, 31,\n     2018.\n\n[44] Tahina Ramananandro, Paul Mountcastle, Benoıˆt Meister, and Richard Lethin. A unified Coq\n    framework for verifying C programs with floating-point computations. In Proceedings of the\n     5th ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2016, pages 15–26,\n    New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341271. doi:\n     10.1145/2854065.2854066.\n\n\n                                       14\n\n[45] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI:\n   A survey on interpreting the inner structures of deep neural networks. In First IEEE Conference\n    on Secure and Trustworthy Machine Learning, 2022. doi: 10.48550/arxiv.2207.13243.\n\n[46] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\n     Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\n     Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\n     Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL\n    https://arxiv.org/abs/2205.06175.\n\n[47] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation.\n     In International Conference on Learning Representations, 2022. URL https://openreview.\n    net/forum?id=oapKSVM2bcj.\n\n[48] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence\n    making AI more trustworthy with a formal methods-based approach to AI system verification\n     and validation.\n\n[49] Alex K. Simpson. Lazy functional algorithms for exact real functionals. In Luboš Brim, Jozef\n     Gruska, and Jiˇrí Zlatuška, editors, Mathematical Foundations of Computer Science 1998, pages\n     456–464, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. ISBN 978-3-540-68532-6.\n\n[50] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain\n     for certifying neural networks. Proc. ACM Program. Lang., 3(POPL), January 2019.  doi:\n     10.1145/3290354.\n\n[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-\n     low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,\n     2013.\n\n[52] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David\n     Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.\n    van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew\n     R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, ˙Ilhan Polat, Yu Feng, Eric W.\n     Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.\n     Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul\n    van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific\n     computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.\n\n[53] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander\n     Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by\n     gradient descent.  In International Conference on Machine Learning, pages 35151–35174.\n    PMLR, 2023.\n\n[54] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\n      Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. arXiv\n      preprint, 2022. doi: 10.48550/arXiv.2211.00593.\n\n[55] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6\n     (60):3021, 2021. doi: 10.21105/joss.03021.\n\n[56] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\n    Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\n     language models. arXiv preprint arXiv:2206.07682, 2022.\n\n[57] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex\n     outer adversarial polytope. In International conference on machine learning, pages 5286–5295.\n    PMLR, 2018.\n\n[58] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\n    deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):\n    107–115, 2021.\n\n\n\n                                       15\n\nA  Subtleties of our approach\n\nIn this section, we address some subtleties and frequently asked questions about our approach.\n\nA.1 Why study this simple task?\n\nFormal reasoning is computationally expensive; very few large software projects have ever been\nverified [31, 30], none of them comparable to large transformer models [10, 19]. Separately, there is\na high fixed cost to taking on any verification project, regardless of computational efficiency of the\nverification itself. Thus, we picked the simplest setting to study the question of interest: Is it even\npossible to formally reason more efficiently than by brute force about model behavior?\n\nA.2  Scalability\n\nIn this section, we address concerns about the scalability of our approach.\n\nA.2.1  Larger input spaces\n\nWe demonstrate that our proof strategies can be reused on larger input spaces while scaling better\nthan the brute force approach does.\n\nWe applied our proof strategies to models trained for Max-of-5, Max-of-10, and Max-of-20. While\nrunning the brute force proof on Max-of-20 would require approximately 2148 FLOPs, which is about\n270× the cost of training GPT-3 [32], our cubic proof achieves bounds of (94.1 ± 1.1) % (Max-of-5),\n(91.4 ± 2.1) % (Max-of-10), and (88.4 ± 4.0) % (Max-of-20) in under two minutes. See Tables 2, 3,\n4, and 5 for more detailed numbers, and Figures 6, and 7 for visualizations. These results demonstrate\nthat proof strategies can be reused on larger input spaces while scaling better than the brute force\napproach does.\n\nA.2.2  Different tasks\n\nIn this paper, we worked on highly optimizing our relaxation to make our bounds as tight as possible\nwhen incorporating as little understanding as possible. This is not necessary for deriving proofs. Our\ngeneral formalization of mechanistic interpretability is replicable: (1) theorem statements are exact\nexpressions for the difference between the actual behavior of the model and the purported behavior,\nand (2) proofs are computations that bound the expression. Furthermore, our convexity theorems and\nproofs are applicable much more generally generally to element retrieval tasks.\n\nA.2.3  More complicated architectures\n\nWe worked on a simple model studied in A Mathematical Framework for Transformer Circuits [13].\nIn follow-up work, we will extend this approach to proving bounds on 1L transformers with ReLU\nMLP trained on modular addition.\n\nA.2.4  Larger models\n\nIt is an open question whether or not the mechanistic interpretability approach to proofs can scale to\nlarger models. However, a large part of this question lies in the feasibility of deriving a high degree of\nfaithful mechanistic understanding from large models — that is, whether mechanistic interpretability\nitself will scale. This is widely recognized in the field, and scaling interpretability approaches while\ngetting both a high degree of mechanistic understanding and assurances that said understanding is\nfaithful to the model is an active area of research. Broadly, we see the compact proofs approach\nas a metric on the quality of mechanistic understanding — we are not purporting to have a general\nsolution to the problem of scaling interpretability, but instead claim that the challenges in proofs are\nin fact challenges in understanding networks.\n\n\n\n\n\n                                       16\n\nTable 2: Version of Table 1 from Section 4.1 with nctx = 5, dvocab = 64. We report the proof complexity,\naccuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).\nUnlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate\nthe bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and\nunexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound\naveraged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.\n\n  Description         Complexity Cost              Bound                Est.     Unexplained\n  of Proof                                                   FLOPs  Dimensions\n\n  Brute force         O(vk+1kd)                      0.9990 ± 0.0018      254       236\n  Cubic             O(v3k2)                        0.941 ± 0.011        226       214\n  Sub-cubic          O(v2 · k2 + v2 · d)               0.705 ± 0.031        222       213\n    w/o mean+diff                                     0.405 ± 0.073        222       213\n  Low-rank QK       O(v2k2 + vd2 + v2d )           0.682 ± 0.033        222       212\n   SVD only                   |{z}QK   EU&OV|{z}            0.335 ± 0.066        222       212\n  Low-rank EU       O(v2k2 + vd + v2d )           0.649 ± 0.055        221       213\n   SVD only                   |{z}EU   QK&OV|{z}             (4.8 ± 0.1) × 10−8    221       213\n  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.628 ± 0.053        222       213\n   SVD only                   |{z}QK    |{z}EU    |{z}OV       (4.8 ± 0.1) × 10−8    222       213\n  Quadratic QK       O(v2k2 + vd + v2d )           0.354 ± 0.034        221       212\n                                 |{z}QK   EU&OV|{z}\n  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.335 ± 0.033        221       213\n                              QK&EU|{z}   |{z}OV\n\n\n\n\n\nTable 3: Version of Table 1 from Section 4.1 with nctx = 10, dvocab = 64. We report the proof complexity,\naccuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).\nUnlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate\nthe bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and\nunexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound\naveraged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.\n\n  Description         Complexity Cost              Bound                Est.     Unexplained\n  of Proof                                                   FLOPs  Dimensions\n\n  Brute force         O(vk+1kd)                      0.9988 ± 0.0013      286       266\n  Cubic             O(v3k2)                        0.914 ± 0.021        228       214\n  Sub-cubic          O(v2 · k2 + v2 · d)               0.674 ± 0.028        223       213\n    w/o mean+diff                                     0.539 ± 0.061        223       213\n  Low-rank QK       O(v2k2 + vd2 + v2d )           0.657 ± 0.028        223       212\n   SVD only                   |{z}QK   EU&OV|{z}            0.469 ± 0.059        223       212\n  Low-rank EU       O(v2k2 + vd + v2d )           0.639 ± 0.032        223       213\n   SVD only                   |{z}EU   QK&OV|{z}            (0 ± 100) × 10−12    222       213\n  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.625 ± 0.031        223       213\n   SVD only                   |{z}QK    |{z}EU    |{z}OV       (2.9 ± 0.1) × 10−17   223       213\n  Quadratic QK       O(v2k2 + vd + v2d )           0.392 ± 0.030        222       212\n                                 |{z}QK   EU&OV|{z}\n  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.390 ± 0.028        222       213\n                              QK&EU|{z}   |{z}OV\n\n\n\n\n\n                                       17\n\nTable 4: Version of Table 1 from Section 4.1 with nctx = 10 and dvocab = 128. We report the proof complexity,\naccuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).\nUnlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate\nthe bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and\nunexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound\naveraged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.\n\n  Description         Complexity Cost              Bound                Est.     Unexplained\n  of Proof                                                   FLOPs  Dimensions\n\n  Brute force         O(vk+1kd)                      0.9972 ± 0.0031      296       277\n  Cubic             O(v3k2)                        0.882 ± 0.012        231       216\n  Sub-cubic          O(v2 · k2 + v2 · d)               0.622 ± 0.031        224       215\n    w/o mean+diff                                     0.390 ± 0.070        224       215\n  Low-rank QK       O(v2k2 + vd2 + v2d )           0.594 ± 0.035        224       214\n   SVD only                   |{z}QK   EU&OV|{z}            0.320 ± 0.053        225       214\n  Low-rank EU       O(v2k2 + vd + v2d )           0.607 ± 0.031        224       215\n   SVD only                   |{z}EU   QK&OV|{z}             (5.4 ± 0.2) × 10−20   224       215\n  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.595 ± 0.030        224       214\n   SVD only                   |{z}QK    |{z}EU    |{z}OV       (5.4 ± 0.2) × 10−20   225       214\n  Quadratic QK       O(v2k2 + vd + v2d )           0.350 ± 0.029        224       214\n                                 |{z}QK   EU&OV|{z}\n  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.384 ± 0.025        224       214\n                              QK&EU|{z}   |{z}OV\n\n\n\n\n\nTable 5: Version of Table 1 from Section 4.1 with nctx = 20, dvocab = 64. We report the proof complexity,\naccuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).\nUnlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate\nthe bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and\nunexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound\naveraged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.\n\n  Description         Complexity Cost              Bound                Est.     Unexplained\n  of Proof                                                   FLOPs  Dimensions\n\n  Brute force         O(vk+1kd)                      0.995 ± 0.015         2148      2126\n  Cubic             O(v3k2)                        0.884 ± 0.040        229       214\n  Sub-cubic          O(v2 · k2 + v2 · d)               0.561 ± 0.043        224       213\n    w/o mean+diff                                     0.486 ± 0.060        224       213\n  Low-rank QK       O(v2k2 + vd2 + v2d )           0.547 ± 0.043        224       212\n   SVD only                   |{z}QK   EU&OV|{z}            0.431 ± 0.060        224       212\n  Low-rank EU       O(v2k2 + vd + v2d )           0.538 ± 0.043        224       213\n   SVD only                   |{z}EU   QK&OV|{z}             (1.0 ± 6.0) × 10−4    224       213\n  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.526 ± 0.041        224       213\n   SVD only                   |{z}QK    |{z}EU    |{z}OV       (1.0 ± 5.0) × 10−4    224       213\n  Quadratic QK       O(v2k2 + vd + v2d )           0.322 ± 0.035        224       212\n                                 |{z}QK   EU&OV|{z}\n  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.321 ± 0.035        224       213\n                              QK&EU|{z}   |{z}OV\n\n\n\n\n\n                                       18\n\n   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9990 ± 0.0018)\n     0.8                                                                                        cubic (rel acc: 0.942 ± 0.010)\n     0.6                                                                                      subcubic (rel acc: 0.702 ± 0.033)\n                                                                                                         direct-quadratic (rel acc:                                                                                                                   0.649 ± 0.055)    Accuracy  0.4\n                                                                                                                               2\n                                                                                                     attention-dvocabdmodel  (rel acc: 0.682 ± 0.033)\n     0.2\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.629 ± 0.053)\n     0.0                                                                                                attention-quadratic (rel acc: 0.354 ± 0.034)     Normalized                               231                        243                        255     attention-quadratic, direct-quadratic (rel acc: 0.336 ± 0.033)\n                         FLOPs to Verify Proof (approximate)\n\n                                           (a) nctx = 5, dvocab = 64\n   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9988 ± 0.0013)\n     0.8                                                                                        cubic (rel acc: 0.915 ± 0.021)\n     0.6                                                                                      subcubic (rel acc: 0.673 ± 0.030)\n                                                                                                         direct-quadratic (rel acc:                                                                                                                   0.640 ± 0.032)    Accuracy  0.4\n                                                                                                                               2\n                                                                                                     attention-dvocabdmodel  (rel acc: 0.658 ± 0.028)\n     0.2\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.626 ± 0.031)\n     0.0                                                                                                attention-quadratic (rel acc: 0.393 ± 0.030)     Normalized                               242                        265                        288\n                         FLOPs to Verify Proof (approximate)\n\n                                        (b) nctx = 10, dvocab = 64\n   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9972 ± 0.0031)\n     0.8                                                                                        cubic (rel acc: 0.885 ± 0.012)\n     0.6                                                                                                 direct-quadratic (rel acc: 0.609 ± 0.030)\n                                                                                              subcubic (rel acc: 0.613 ± 0.037)    Accuracy  0.4\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.596 ± 0.030)\n     0.2                                                                                                                       2\n                                                                                                     attention-dvocabdmodel  (rel acc: 0.595 ± 0.035)\n     0.0                                                                                                  attention-quadratic, direct-quadratic (rel acc: 0.385 ± 0.025)     Normalized                                247                         274\n                         FLOPs to Verify Proof (approximate)\n\n                                          (c) nctx = 10 and dvocab = 128\n   Bound  1.0                                                                                             brute force (estimated) (acc: 0.995 ± 0.015)\n     0.8                                                                                        cubic (rel acc: 0.888 ± 0.036)\n     0.6                                                                                      subcubic (rel acc: 0.563 ± 0.041)\n                                                                                                                               2 (rel acc: 0.549 ± 0.041)    Accuracy  0.4                                                                                             attention-dvocabdmodel\n                                                                                                         direct-quadratic (rel acc: 0.540 ± 0.040)\n     0.2\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.528 ± 0.039)\n     0.0                                                                                                  attention-quadratic, direct-quadratic (rel acc: 0.322 ± 0.034)     Normalized                                263                         2109                                 attention-quadratic (rel acc: 0.323 ± 0.035)\n                         FLOPs to Verify Proof (approximate)\n\n                                        (d) nctx = 20, dvocab = 64\n\nFigure 6: Version of Figure 3 from page 8 with varying nctx and dvocab. The brute-force proof (Section 4.1)\ncomputes the exact performance uses orders of magnitude more compute than other approaches; unlike in\nFigure 3, here we use importance sampling to estimate the bound.\n\n\n\n\n\n                                       19\n\n     1.0\n Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct\n                                                                                           mean+max-diff-subproduct\n     0.6                                                                                        max-diff\n                                                                                       mean+max-diff  Accuracy\n     0.4                                                                                   svd\n                                                                                              mean+max-diff-subproduct-recursive\n     0.2                                                                                           max-diff-subproduct-recursive\n                                                                                                mean-recursive+max-diff-subproduct-recursive   Normalized\n     0.0\n                 400           600           800          1000          1200\n                        EPQKE Singular Ratio: σ1/σ2\n\n(a) nctx = 5, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 15(σ1/σ2) + 0.48,\nR2 = 0.37.\n     1.0\n Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct\n                                                                                           mean+max-diff-subproduct\n     0.6                                                                                        max-diff\n                                                                                       mean+max-diff  Accuracy\n     0.4                                                                                   svd\n                                                                                                  max-diff-subproduct-recursive\n     0.2                                                                                       mean+max-diff-subproduct-recursive\n                                                                                                mean-recursive+max-diff-subproduct-recursive   Normalized\n     0.0\n         400       600       800      1000      1200      1400      1600      1800\n                        EPQKE Singular Ratio: σ1/σ2\n\n(b) nctx = 10, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 074(σ1/σ2)+0.51,\nR2 = 0.23.\n     1.0\n Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct\n                                                                                           mean+max-diff-subproduct\n     0.6                                                                                        max-diff\n                                                                                       mean+max-diff  Accuracy\n     0.4                                                                                   svd\n                                                                                                  max-diff-subproduct-recursive\n     0.2                                                                                         mean-recursive+max-diff-subproduct-recursive\n                                                                                              mean+max-diff-subproduct-recursive   Normalized\n     0.0\n             800      1000      1200      1400      1600      1800      2000\n                        EPQKE Singular Ratio: σ1/σ2\n\n(c) nctx = 10 and dvocab = 128. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 085(σ1/σ2) +\n0.40, R2 = 0.42.\n     1.0\n Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct\n                                                                                           mean+max-diff-subproduct\n     0.6                                                                                mean+max-diff\n                                                                                               max-diff  Accuracy\n     0.4                                                                                   svd\n                                                                                                mean-recursive+max-diff-subproduct-recursive\n     0.2                                                                                       mean+max-diff-subproduct-recursive\n                                                                                                  max-diff-subproduct-recursive   Normalized\n     0.0\n              400         600         800        1000        1200        1400\n                        EPQKE Singular Ratio: σ1/σ2\n\n(d) nctx = 20, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 098(σ1/σ2)+0.41,\nR2 = 0.22.\n\nFigure 7: Version of Figure 4 from page 9 with varying nctx and dvocab. Note that the “svd” proof strategy has\na clear upward trend, especially on early points.\n\n\n\n\n\n                                       20\n\nA.3 Why is more mechanistic understanding correlated with worse bounds?\n\nFigure 3 exhibits Simpson’s Paradox: although more faithful mechanistic understanding is correlated\nwith better bounds within each class of proof (and moreover the most extensive mechanistic under-\nstanding results in the greatest improvement in bound tightness over baseline), when we aggregate\nacross all proof strategies, we find that more mechanistic understanding is correlated with worse\nbounds.\n\nThis relationship is summarized in Figure 8.\n\nFrom the compression perspective, more mechanistic understanding is about having more compres-\nsion. Unless the model is losselessly compressible, we should expect that more compression will\ninherently be more lossy, no matter how good our compression scheme is. Correspondingly, using\nmore understanding to get more compression will often result in a weaker bound, no matter how\ngood our understanding is.\n\nConversely, we can think of the quality of proofs (the combination of tightness of bound, and length of\nproof) as a metric for how good our mechanistic understanding is. From this perspective, the fact that\nmechanistic-interpretability-derived bounds are bad suggests gaps in our mechanistic understanding.\nAs the field matures and we develop tools that enable more faithful and complete understanding of\nmodel behavior, we expect that the quality of bounds we derive from mechanistic understanding will\nimprove.\n\n\nA.4  Convex relaxation\n\nIn this work, we construct convex relaxations to perform the pessimal ablations for our proofs.\n\nIn what sense are we using “convexity”? The intuition is that we are attempting to optimize a function\nf over its domain X by incrementally making local changes to the input sequence, such as replacing\none token by another, or by changing the order of tokens. The reason that convex optimization\nproblems are easy to solve is that all local extrema are global extrema. This is not the case for our\noptimization problem, so we find a relaxation of f and its domain such that all local extrema are in\nfact global extrema.\n\nFurthermore, most convex optimizers perform optimization at runtime by repeatedly stepping towards\nextrema. In this work, we “optimize by hand”, performing the optimization in the proof of our general\ntheorems. The computation of the bound then only needs to instantiate the precomputed possible\nextrema with the actual values of the model’s parameters to determine the the extrema actually are.\n\nWe now give a formal description of what we mean by “convex relaxation”.\nFor a set of inputs Xi, we define a set of “relaxed inputs” Xrelaxedi      with an injection Ti  : Xi ,→\nP(Xrelaxedi      ) mapping input to the model to the set of corresponding relaxed inputs. On the relaxed\ninput, we define a function hi : Xrelaxedi   →R such that for all t ∈Xi and all labels l for which (l, t)\nis supported by (has non-zero probability in) D, we can find trelaxed ∈Ti(t) with f(l, M(t)) ≥\nhi(trelaxed). We proceed by finding a small subset of “boundary” examples Bi ⊂Xrelaxedi         , proving\nthat if hi(trelaxed) ≥bi for all trelaxed ∈Bi then hi(trelaxed) ≥bi for all trelaxed ∈Xrelaxedi         .\nThen, the computational component C of the proof validates that that hi(trelaxed) ≥bi for some bi\nfor all trelaxed ∈Xrelaxedi         . This allows us to conclude that f(l, M(t)) ≥bi for all t ∈Xi.\n\n\nA.5  Computing unexplained dimensionality\n\nWe claim in Figure 5 that we can use unexplained dimensionality as a metric for understanding. Here\nwe describe how we compute the unexplained dimensionality of a proof strategy.\n\nAs in Figure 1, for any given proof, we can separate our treatment of transformer components into\n“black-box” (e.g., matrix multiplication) and “white-box” components (e.g., specifying that the QK\ncircuit is approximately rank one; pessimizing over non-max tokens). Considering the performance\nscore as a large white-box component which may reference black-boxes internally, we define the\nunexplained dimensionality of a single black-box computation as the log-cardinality of it function\nspace (so, e.g, 2 · 64 for a function 64 →R2, whose cardinality is (R2)64, where 64 denotes the finite\n\n\n                                       21\n\n                                                                                   Brute Force Proof\n\n                                  True Performance\n\n\n\n               Performance                                                                          Impractical baseline of\n              Lower Bound       Trivial Proof                           guaranteeing\n                                                                    performance via\n                                                                             inference\n\n                                         FLOPs to Verify Proof\n\n\n                             (a) The baseline of using inference to generate proofs.\n\n\n\n                                   True Performance\n\n\n                                                     Understanding\n                                                               partially recovers\n                Performance                   bound tightness\n              Lower Bound\n\n                                                             Decreased length of proof\n                                                                          leads to looser bounds\n\n\n\n                                         FLOPs to Verify Proof\n\n             (b) Shorter proofs by default have a worse performance bound b. Faithful under-\n             standing allows us to recover significant — but not complete — bound tightness\n            with minimal proof-length overhead.\n\n             Figure 8: The theoretical relationship between proof length and bound tightness.\n\n\nset on 64 elements). The unexplained dimensionality of the entire proof is the sum of the unexplained\ndimensions of all black-box components.\n\nIntuitively speaking, unexplained dimensionality tries to capture the degrees of freedom that we\nhave to check via brute enumeration over black-box computations. Proofs with less unexplained\ndimensionality contain more mechanistic understanding, and vice versa.\n\nA.6  Computing approximate FLOPs\n\nIn Figure 3 and Table 1 on page 6 and on page 8, we display approximate floating point operations.\nWe instrument our code to execute on phantom tensors that track their shape and accumulate an\napproximate count of floating point operations. We compute matrix additions and multiplications in\nthe obvious way. We take the instruction count of SVD to be the cost of verifying that the output of\nSVD is a valid decomposition: that we have a pair of orthonormal bases which when multiplied out\ngive the original basis.\n\nA.7  IEEE 754 vs. R\n\nIn Section 2 we defined C and Q and glossed over whether we were reasoning over reals or floats.\nHere we clarify this point that we’ve so far been sweeping under the rug.\nLet F denote the set of the relevant flavor of IEEE 754 Floating Point numbers (generally 32-bit for\nour concrete models, but everything would hold just as well for 64-bit). Let F∗denote F restricted to\nfinite numbers (that is, without NaNs and without ±∞).\nWe parameterize C, M, and D over the real field13 they operate on, so that,  e.g., CF   :\nmodel weights →F.  Then we have Q establishing that for any model M′, CR(M′R) ≤\nE(l,t)∼DRfR(l, M′R(t)), and we have a trace demonstrating that CF(MF) = b.\n\n   13Technically the floating point numbers are not a field. We gloss over this point here, since they define all of\nthe field operations, even if those operations do not satisfy the field axioms.\n\n\n                                       22\n\nLet i : F∗→R be any injection that maps each floating point number to some real number that it is\n“closest to”. Supposing that b ∈F∗and thus b ∈R, we need two additional components of the proof.\nWe need to find ε, ε′ ∈R+ prove that\n|CR(MR) −i (CF(MF))| < ε    and       E(l,t)∼DRfR(l, MR(t)) −i  E(l,t)∼DFfF(l, MF(t)) < ε′\n\nThen we can chain these proofs to prove that\n                                   i  E(l,t)∼DFfF(l, MF(t)) ≥b −ε −ε′\n\nSuch ε-ball robustness proofs should be well within the scope of existing approaches to formal\nmethods on neural nets, see, e.g., [44, 3, 4, 29, 1, 57]. We leave actually dealing with the gap between\nfloating point numbers and real numbers to future work.\n\nA.8  Non-uniform distributions\n\nIn Equation 1 in Section 2 we defined the expected model performance as the expectation of the\ndistribution D:\n                                                  ¯s := E(l,t)∼D [f(l, M(t))] ≥b.\n\nWe then immediately specialized to the case where the marginalization D|X of D over labels is\nuniform. As we’ll see in Theorem 1 in Appendix D and Algorithm 3 in Appendix E, the bound\ncomputation is modularized between a function that bounds the performance f(l, M(t)) over a\nrestricted collection of inputs, and a much simpler function that combines the bounds on individual\ncases into a bound on the expectation over the entire distribution. The per-input bound computation\nis CORRECTNESS in Algorithm 1 and RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION in\nAlgorithm 3; the expectation computation is BRUTE-FORCE in Algorithm 1 and CUBIC in Algorithm 3.\n\nSince the expectation computation is modularized, it is straightforward to extend our approach to\nnon-uniform distributions simply by adjusting the weighting of each region of inputs. However, if the\ndistribution is too far off from the uniform training distribution, the bound we get may not be very\ngood, as we may not be allocating adequate computation to the high-probability regions of the input\nspace.\n\nA.9  Adversarial robustness via flexibility in D\n\nThere is flexibility inherent in Equation 1. Normally, by out-of-distribution (OOD) or adversarial\ninputs, we suppose that there is a distribution Din that’s used for training and (in-distribution)\nvalidation, and another distribution D′ that is the deployment distribution or generated by an adversary.\nIf we had knowledge of D′, we could compute the expected performance from inputs sampled from\nD′. Even if we don’t have exact knowledge of D′, we can still define a very broad distribution D that\ncovers possible D′s.\nIn this work, D is the distribution of all 644 possible valid input sequences. In addition, as our proofs\npartition D into subdistributions, and bound the performance on each subdistribution, we can bound\nthe model’s performance on any possible distribution over valid input sequences.\n\nA.10   Infinite distributions\n\nIn the brute force proof in Section 4.1, we run the model on the entirety of D. This operation\nis straightforward when X is finite. Perhaps surprisingly, we can do this even if X is infinite\nas long as the PDF L × X →R of D is computable and the natural computational topology of\nX is compact [16, 15, 14], because integration of computable functions on computable reals is\ncomputable [49].\n\nA.11  Using alternate loss functions\n\nBuilding on the point from Appendix A.8, it is also relatively straightforward to extend our approach\nfrom bounding expected accuracy to bounding log-loss. We will see in Figure 12 that the accuracy\nand log-loss share a subterm ∆ℓi. Since we compute this subterm in all of our algorithms, we can\n\n\n                                       23\n\neasily extend our approach to log-loss by combining ∆ℓi directly rather than merely checking that\nthe value is negative as we currently do in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION\nin Algorithm 3. Although this is sufficient for the brute-force and cubic proofs, for the subcubic\nproof using Algorithm 6 in Appendix F, we would additionally have to compute a log-loss bound for\nthe sequences where the largest non-max token is “too close” to the max token, which we currently\nneglect by considering the model to get them wrong in the worst case.\n\nA.12  Proving upper bounds\n\nIn this work, we focus on proving lower bounds on model performance. Most of our theorems,\nfor example in Appendices E, and F, prove two-sided bounds. Most of the other theorems can be\nstraightforwardly adapted to proving upper bounds by swapping uses of min and max. Therefore,\nwe expect that proving upper bounds on model performance should be straightforward.\n\nA.13  What proof system?\n\nLength of proof depends on what proof system we use. We permit any proof system where proof-\nchecking time is linear in the length of the proof. This excludes dependently typed proof systems\nsuch as Martin-Löf type theory, but such proof systems can easily be accommodated by considering\na proof-checking-trace rather than the proof object itself. Alternatively, a more conventional proof\nsystem like ZF, ZFC, or the proof system underlying Isabelle/HOL should suffice.\n\nB  Experimental details\n\nB.1  Training details\n\nTo train each model, we generate 384,000 random sequences of 4 integers picked uniformly at random,\ncorresponding to less than 2.5% of the input distribution. We use AdamW with batch_size = 128,\nlr = 0.001, betas = (0.9, 0.999), weight_decay left at the default 0.01. We train for 1 epoch (3000\nsteps). Over our 151 seeds, models trained with this procedure achieve (99.92 ± 0.15) % train\naccuracy and a loss of (4 ± 8) × 10−3.14 When qualitatively examining a single model (for example\nin Section 3.1 or Appendix H.1), we use the model with config seed 123, model seed15 613947648.\n\nAs our models as sufficiently small, we did not have to use any GPUs to accelerate training our\ninference. Each training run takes less than a single CPU-hour to complete. In total, the experiments\nin this paper took less than 1000 CPU-hours.\n\nWe use the following software packages in our work: Paszke et al. [41], Plotly Technologies Inc.\n[42], Nanda and Bloom [37], Rogozhnikov [47], Virtanen et al. [52], McKinney [33], Waskom [55]\n\nB.2  Additional details supporting our mechanistic interpretation of the model\n\nWe provide heatmaps of the matrices corresponding to the five components described/defined in\nSection 3, for the mainline model.\n\n\n\n\n\n  14Numbers reported as mean across training runs ± std dev across training runs of mean accuracy and loss.\n  15The model seed is deterministically pseudorandomly derived from the seed 123.\n\n\n                                       24\n\n                                                                         0\n\n                0\n\n                                                                                200                                                                                                                 1\n                                                                                                                                                 20\n                                20                                                                                                                                                                                                0\n                                                                                100                                                                                                                                                            token                                                                         token\n                                                                         query 40                                             0                                                                                                                                                                 query 40                                    −1\n                                                                      −100                                                                                       −2\n\n                                60                                        −200                                                                60                                    −3\n                                   0     10    20    30    40    50    60                                                                                  0         1         2         3\n                                                      key token                                                                                                                key position\n\n              (a) EQKE = EqQKT ¯ET                             (b) EQKP = EqQKT ˆPT\n\nFigure 9: The QK circuit can be decomposed into the position-independent and position-dependent components\nEQKE and EQKP. It computes the pre-softmax attention score for the model. The positional contribution to\nthe attention score, as shown in Figure (b), is minimal. In Figure (a), the gradient from left to right along the\nkey axis indicates that the single attention head pays more attention to larger tokens. The uniformity along the\nquery axis suggests that this behavior is largely independent of the query token. Further, the light and dark bands\nimply that some queries are better than others at focusing more on larger tokens.\n\n\n\n\n\n  0                                                                                                                                                               0.8                                                                                    0                                             2\n                                                   30                 0\n                                                                                                                                                               0.6\n                                                   20                                                                                                                                                                                                                       1                                                                                                                                                               0.4\n                                                                                                                                                                        20    20\n                                                   10                 1\n                                                                                                                                                               0.2\n                                                                                                                                                                                                                       0  token                                                0                                                                                                                                  position                                                         0.0                                                                            token\n  input 40                                      −10                                                                           input 2                                            −0.2                                                                        input 40                                    −1\n\n                                           −20                                                                                                                                    −0.4\n                                                                                                                                                                         −2                                           −30                 3    60                                                                                                                                    −0.6                                60\n\n      0     10    20    30    40    50    60                                              0     10    20    30    40    50    60                                             0     10    20    30    40    50    60\n                       output logit token                                                                                  output logit token                                                                                 output logit token\n   (a) EVOU = ¯EV OU                   (b) PVOU = ˆPV OU                    (c) Direct Path = EqU\n\nFigure 10: The OV circuit is a sum of EVOU and PVOU. In Figure (a) we see that EVOU “copies” — with\nthe exception of input tokens ≤5 (6.6 ± 1.2 across all models) — by virtue of the fact that above 5, the diagonal\nis larger than all the other elements in the same row. We see that the range on Figure (b) is much smaller than\nFigure (a), indicating that positional contribution to the copying is minimal. In Figure (c) we see that direct path\nvalues matter a bit more than PVOU, being only ≈20× smaller than the typical EVOU difference. They don’t\nmatter that much, though, being so small. Additionally, the vertical banding indicates that the primary effect of\nthis is a largely-query-independent bias towards larger numbers, reflecting the fact that the input distribution is\nbiased towards larger numbers being the maximum. The weak diagonal pattern indicates a slight bias towards\nupweighting the query token itself as a (possible) maximum token.\n\n\nB.3  Distribution of model mechanisms\n\nWe provide some analysis of the distribution of the mechanisms of the models trained on the same\nconfiguration. At a glance, there is not that much variation across models.\n\nThe statistics of interest are: (1) σ1/σ2, the ratio of the first two singular values of EQKE, a measure\nof the extent to which the attention score computation is low-rank; (2) ¯s, the average score (accuracy)\nof the model across the entire input distribution; (3) bcubic/¯s, the percent-score-recovered accuracy\nbound achieved by the cubic proof from Section 4.2; (4) bsubcubic/¯s, the percent-score-recovered\naccuracy bound achieved by the (per-model best)16 subcubic proof from Section 4.3.\n\nFor each statistic of interest, Table 6 presents an eleven-number summary of the statistic. Plots, seeds,\nand statistic values are shown for models whose values are closest to each of the corresponding\nsummary statistics.17 Additionally, each group contains a boxplot of the summary:\n\n        • the minimum, maximum; the first and third quartiles; the median and mean; percentiles\n        2.15 %, 97.85 %, 8.87 %, and 91.13 %; these are displayed as:\n        • top and bottom of the vertical whisker lines; top and bottom of the box; horizontal line\n         inside the box, and the square; horizontal whisker lines and whisker crosshatches.\n\n\n\n\n   16“Per-model best” here means that for each model seed, we select the variant of the subcubic proof with the\nhighest bound.\n   17If a single model is the closest to two statistics, for example when the mean and median are very similar, the\nmodel is shown only once.\n\n\n                                       25\n\nTable 6: Plots of various models. The statistics of interest are: (1) σ1/σ2, the ratio of the first two singular values\nof EQKE, a measure of the extent to which the attention score computation is low-rank; (2) ¯s, the average score\n(accuracy) of the model across the entire input distribution; (3) bcubic/¯s, the percent-score-recovered accuracy\nbound achieved by the cubic proof from Section 4.2; and (4) bsubcubic/¯s, the percent-score-recovered accuracy\nbound achieved by the (per-model best) subcubic proof from Section 4.3. The y axes are: for EQKE and\nEQKP, the query token; for EVOU, the input token; for EqU, the input query token; for PVOU, the input\nposition. The x axes are: for EQKE, the key token; for EQKP, the key position; for EVOU, PVOU, and\nEqU, the output logit token. All token axes range from 0 at the top (or left) to dvocab −1 = 63 at the bottom\n(or right). All position axes range from 0 at the top (or left) to nctx −1 = 3 at the bottom (or right).\n\n               statistic of interest    EQKE     EQKP    EVOU  PVOU\n  seed                                                      EqU\n           distribution    value  EqQKT ¯ET   EqQKT ˆPT    ¯EV OU   ˆPV OU\n\n\n\n\n                                                          −400       −200         0          200         400       −40        −20          0           20           40\n\n  123        (selected seed)\n\n\n 24262      σ1/σ2    1061.1\n\n 13654           1,000         626.5\n\n\n                           800\n  4810             600         350.0\n\n  6204             400         333.4\n\n\n 12457              ¯s      0.9999\n\n                            1\n\n 19451          0.998         0.9992\n\n\n\n                           0.996\n 15662               0.9932\n                           0.994\n\n 16197          0.992         0.9915\n\n\n 32103       bcubic/¯s    0.971\n\n\n                              0.97\n  6082             0.96          0.954\n\n\n                              0.95\n  6155             0.94          0.926\n\n\n                              0.93\n  2306             0.92          0.922\n\n\n 20415     bsubcubic/¯s   0.80\n\n\n                                0.8\n 29725                 0.70\n                              0.75\n\n 20976              0.7          0.63\n\n\n                              0.65\n  6155               0.6          0.61\n\n\n\n\n\n                                       26\n\nC  Mathematical definitions\n\nWe provide a detailed breakdown of the mathematical notation used in the appendix.\n\nLet                     M         of type X →Y be the model18; sometimes we\n n       be the finite set on n elements; we write                             write\n N<n    := {0, 1, . . . , n −1} when we care about the ele-   ℓ            for the logits of the model M(t)\n           ments of n                      D         be a probability distribution over input-label pairs\n σ(v)    be the softmax function ev/ Pi evi                                     (t, l) ∈X × L\n σ∗(v)   be   the   casually-masked  softmax   function,   D|X       be D marginalized to a distribution over t ∈X\n            σ∗(v)i := evi/ Pj≤i evj                      f          of type L × Y →R be a scoring function for\n  dvocab   be the size of the vocabulary                                        evaluating the performance of the model\n d       be the dimension of the attention head, in our   Pavg      be the average position embeds across positions\n              case, equal to the hidden dimension of the model                   (of size dmodel), nctx1 Pi Pi\n            dmodel (assumption: d < dvocab)                    ¯P         be either 1nctx ⊗Pavg or 1dvocab ⊗Pavg depending\n  nctx    be the context length, the number of tokens in the               on context – that is the result of broadcasting\n             input sequence, equal to K in Max-of-K                       Pavg back into the shape of P or E (that is,\n P       be the nctx × dmodel positional embedding                         nctx × dmodel or dvocab × dmodel)\n E      be the dvocab × dmodel token embed             Pq        be 1dvocab ⊗Pquery, the broadcasting of Pquery\n Q, K,   be the dmodel ×dmodel query, key, value, and output     ˆP, ¯E, Eq   be P −¯P, E + ¯P, and E + Pq respectively\n  V , O     matrices of the attention head\n U       be the dmodel × dvocab unembed matrix            For any vector-valued function v of length dvocab, parameter-\n t       be the input token sequence [t0, t1, . . . , tnctx−1]          ized over the input sequence t, let\n x       be the nctx × dvocab one-hot-encoded input token  ∆vi := vi−vtmax be the difference between the ith element of v\n            sequence [x0, x1, . . . , xnctx−1]                     and the element of v corresponding to the true maximum\n  xquery  := x−1 := xnctx−1 be the query token                   of the input sequence tmax.\n  tquery   := t−1 := tnctx−1 be the one-hot encoded query  For our particular model, we will have\n            token                                                dvocab := 64    d = dmodel := 32     nctx := 4\n tmax    be the true maximum token in the input sequence,   X := (N<dvocab)nctx ∼= 644   L := N<dvocab ∼= 64\n          maxi ti                              D|X := U(0, 1, . . . , dvocab −1)nctx, the uniform distribution\n                                          Figure 11: Preliminary model definitions\n\n                                  E(l,t)∼D [f(l, M(t))] ≥  b      (theorem statement)                                  (3)\n                                                      |{z}bound                                |   ¯s (average{zmodel score) }  lower\n\nWe define the two typical performance functions corresponding to accuracy and log-loss. Note the shared subterm ∆ℓi.\n                       f accuracy(tmax, ℓ) := 1[argmax ℓi = tmax] = 1[0 > max ℓi −ℓtmax ]                          (4)\n                                                                              i                            i̸=tmax\n                                                                              |  ∆ℓi{z  }\n                        f log-loss(tmax, ℓ) := (σ(ℓ))tmax = log(Pi exp(ℓi −ℓtmax ))                                    (5)\n                                                                    |  ∆ℓi{z  }\n\nWe present the model definition in four different regroupings to define via underbrace labels various useful quantities:\n                                  √\n     M(t) = ℓ(t) = σ∗  (xqueryE + P query) QKT (xE + P)T /  d   · (xE + P) V OU + (xqueryE + P query) U     (6)\n                          |            QK {zcircuit             }       |   OV {zcircuit   }  |          direct{zpath      }\n                                  √\n           = σ∗  xquery EqQKT ¯ET xT + EqQKT ˆPT  /  d   · x ¯EV OU + ˆPV OU  +xquery EqU        (7)\n                                 | EQKE{z   }      |  EQKP{z    }            |EVOU{z }  |PVOU{z  }         |{z}EU\n                                                                     |     EPVOU(t){z         }                      |                       α∗(t){z                     }\n           = α∗(t) · x¯EV OU + α∗(t) · ˆPV OU + xqueryEqU                                                    (8)\n                      |   ℓEVOU(t){z      }  |  ℓPVOU(t){z     }  |ℓEU(xquery){z   }\n           =  Pnctx−1i=0   (α∗(t))ixi¯EV OU + (α∗(t))i ˆPiV OU + xqueryEqU                                  (9)\n                                |    ℓEVOU,i(t){z      }  |   ℓPVOU,i(t){z      }    |ℓEU(xquery){z   }\n                                |                    ℓi(t){z               }\n\n\n                                         Figure 12: Definitions of the model behavior\n\n\n\n\n\n                                                  27\n\nD  Brute-force proof\n\nTheorem 1. For BRUTE-FORCE(dvocab, nctx, M) as defined in Algorithm 1,\n\n     Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥BRUTE-FORCE(dvocab, nctx, M)\n                                                i                         i\n\n\nProof. In fact the two sides of the inequality are equal by definition. Hence the inequality follows by\nreflexivity of ≥.\n\n\n\nAlgorithm 1 Counting Correct Sequences By Brute Force\n  1: function CORRECTNESS(M, input-sequence)\n  2:    return MODEL-BEHAVIOR(M, input-sequence) == MAX(input-sequence)\n  3: end function\n  4: function BRUTE-FORCE(dvocab, nctx, M)\n  5:    return                    1   SUM(CORRECTNESS(M, tokens) for tokens ∈(RANGE(dvocab))nctx)                  dvocabnctx\n  6: end function\n\n\nE  Details of cubic proof\n\nIn this section, we prove formally the result used in Section 4.2, A cubic proof.\nAt its heart, the convexity of softmax19 is an extension to a simple idea: a weighted average of scalar\nvalues is extremized by putting 100% of the weight on an extremal value.\n\nUsing this simple version of the theorem, however, gives a useless bound of 0% accuracy: if we pay\nno attention to the maximum of the sequence, of course we’re going to get the wrong answer. Since\nin fact the space of possible weightings we may see in practice is much smaller (finite, in fact, with\nat most dvocabnctx values), we may look for a more general version of this idea that gives us tighter\nbounds that still cover the space of possible weightings.\n\nThe weights are not linearly independently choosable (softmax is non-linear), so extremal values do\nnot necessarily result from putting maximal attention on the worst token. It may be, when trying to\nfind the worst case, that some positions are so dis-preferred that it makes more sense to choose a\ntoken that is “less bad” for those positions, if it draws enough attention away from the correct token.\nSee Lemma 3 for details.\n\nWe thus spend this section characterizing a relaxation of the constraints on weights:\n\n      1. that contains all actually possible weightings,\n\n      2. that is extremized at weights that still correspond to some notion of “put the most weight on\n         the extremal tokens”, and\n\n      3. for which computing the extremal weightings is computationally efficient.\n\nBefore diving in, let’s recall the proof that a weighted average of scalar values is extremized by\nputting 100% of the weight on extremal values:\nTheorem 2 (Warmup: Extremizing weighted averages). Fix a set of values vi ∈R. The weighted\naverage is bounded by the extremal values: for any wi such that Pi wi = 1 and 0 ≤wi ≤1,\n\n                         min vi ≤ X wivi ≤max vi\n                                                    i                              i\n                                                                  i\n\n   18Note that while in the main body, M(t) referred to the pre-softmax output logits, in the appendix we abuse\nnotation and occasionally use it to refer to maximum token indicated by the logits where appropriate.\n   19See Appendix A.4 for the reason that we call this “convexity”. Note that our use of “convexity” is purely\ndescriptive in this section; all theorems are written out explicitly.\n\n\n                                       28\n\nProof. The proof is simple. We have\n\n       X wivi −min vi = X wi(vi −min vj) ≥0\n                                                        i                          j\n                                       i                              i\n\nand\n                 max vi − X wivi = X wi(max vj −vi) ≥0\n                                       i                                   j\n                                                     i                i\nso the result follows.\n\nE.1  Proof strategy\n\nThe model computes the true maximum tmax when its outputs logits ℓare such that ∆ℓt∗:=\n ℓt∗−ℓtmax < 0 for all t∗̸= tmax.20 As a result, it suffices to lower-bound the proportion of\nsequences where (an upper bound on) ∆ℓt∗is negative for all t∗̸= tmax. In particular, we will\nupper-bound the contribution from incorrect tokens t in positions i to the difference ∆ℓi between\nincorrect (t∗) and correct (tmax) output tokens ∆ℓit∗= ℓit∗−ℓitmax.\nWe do this by arguing that the logit difference ∆ℓt∗satisfies a certain notion of convexity over the\nspace of a relaxation of sequences (Theorem 6), and constructing a set of Θ(dvocab3nctx) “extremal”\nrelaxed sequences where the position and token embedding components of attention are pessimized\nindependently.\n\nWe start by first rewriting the contribution of each token through the attention head to the logit\ndifference into the contributions involving PVOU and EVOU:\n\n\n                     ∆ℓtt∗(t) = ∆ℓPVOU,i t∗(t) + ∆ℓEVOU,i t∗(t)\n\n\nWe then upper bound ∆ℓPVOU,it∗(t) by noting that because the softmax attention is a weighted\naverage of PVOU,\n\n\n               ∆ℓPVOU,i t∗(t) = ℓPVOU,i(t)t∗−ℓPVOU,i(t)maxj tj\n                  = α∗i (t)PVOUi,t∗−α∗i (t)PVOUi,maxj tj\n                  = α∗i (t) PVOUi,t∗−PVOUi,maxj tj\n                    ≤α∗i (t) max PVOUi,t∗−PVOUi,maxj tj\n                                                                 i\nSince Pi α∗i (t) = 1, we have\n\n                  nctx−1\n      X ∆ℓPVOU,i t∗(t) ≤max PVOUi,t∗−PVOUi,maxj tj\n                                                                 i\n                   i=0\n\nWe then construct a set Ξpure of “pure sequences” consisting of only three types of tokens in one of\ntwo orders, and show that for each input sequence t and readoff logit t∗, we bound the logit difference\nfrom the token embeddings ∆ℓEVOU,it∗(t) using a small subset X of Ξpure:\n\n                       nctx−1                       nctx−1\n       X ∆ℓEVOU,i t∗(t) ≤max X ∆ℓEVOU,i t∗(ξ)\n                                       ξ∈X\n                       i=0                         i=0\n\nWe construct a set Xrelaxed of relaxed sequences, where each relaxed sequence trelaxed consists of a\nsequence and a position (t, i), where ∆ℓt∗(t, i) is evaluated by separately considering the positional\n\n  20We use the logit difference ∆ℓt∗because: (a) it is shared in the computation of f 0-1 and f log-loss; (b) it is a\nlinear function of the various paths through the model, which can therefore be analyzed separately; (c) it leaves\nopen both the options of pessimizing over output logit before or after combining contributions of various paths\nthrough the model.\n\n\n                                       29\n\ncontribution through attention (that is, the attention weighted PVOU) and the token contribution\n(that is, the attention-weighted EVOU) and direct contribution (the logit difference through the skip\nconnection EU). Note that i indicates the position that we pay 100% of the attention to for the PVOU\ncontribution.\n\nWe argue that ∆ℓt∗(t, i) satisfies a certain notion of convexity over mixtures of sequences, such\nthat we can evaluate it only on a set of Θ(dvocab3nctx) “extremal” sequences in a way that takes\nO(dvocab3nctx) total time to bound ∆ℓt∗(t, i) for every possible input sequence. We then use the\nextremal sequences that the model gets correct to lower bound the proportion of all sequences that\nthe model will get correct. Specifically, we argue that Algorithm 3 provides a valid lower bound on\nthe proportion of sequences the model gets correct.\n\nE.2  Proof outline\n\nWe now proceed to the main results of this section.\nMath fact: For each token t∗, the logit difference ∆ℓt∗for any sequence t can be decomposed into\nthe direct contribution from the embeds ℓEU, the attention-weighted position contribution (PVOU),\nand the attention-weighted token contribution (EVOU). Therefore, it suffices to upper bound each of\nthe three components independently, since summing these upper bounds gives a valid upper bound\non the logit difference.\nWe can compute the direct contribution ℓEU exactly by first computing EU = EqU and then, for each\nmax, subtracting the logit of the max token from each row of the matrix. No theorems needed. For\neach max token, we can bound the position contribution by its maximum over positions (Theorem 6).\n\nIn order to upper bound the token contribution, we argue that any mixed sequence will be upper\nbounded by the maximum of the corresponding pure sequences (Theorem 7). We then argue that for\npure sequences, it suffices to consider orderings where same tokens appear contiguously (Theorem 4).\n\nE.3  Formal proof\n\nFor this subsection, all theorems are parameterized over the following quantities.\nDefinition 1 (Common theorem parameters). Fix a token value function (à la a row difference in\nEVOU) v  : N<dvocab →R and a token attention function (à la EQKE for a fixed query token)\na : N<dvocab →R. Fix a position value function (à la a row difference in PVOU) w : N<nctx →R\nand a position attention function (à la EQKP for a fixed query token) b : N<nctx →R.\n\nIn practice, we’ll take, for fixed query token tquery, fixed output token of interest t∗, and fixed\nmaximum token tmax,\n                                       √\n               vt = EVOUt,t∗−EVOUt,tmax              at = EQKEtquery,t/  d\n                                       √\n            wi = PVOUi,t∗−PVOUi,tmax                 bi = EQKPtquery,i/  d\n\nDefinition 2 (of a sequence via sorted tokens and a position permutation). We can define a sequence\nof tokens via sorted tokens and a position permutation by specifying a non-decreasing sequence of\ntokens t0 ≤· · · ≤tnctx−1 ∈N<dvocab paired with a permutation σ : N<nctx →N<nctx.\nDefinition 3 (sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈\nN<dvocab and a permutation σ : N<nctx →N<nctx define the sequence score st0,...,tnctx−1,σ as:\n\n                  ,\n                           st0,...,tnctx−1,σ := X vtieati+bσ(i) X eati+bσ(i)\n                                     0≤i<nctx            0≤i<nctx\n\nWe will drop the token subscript, writing only sσ, when the token values are unambiguous by context.\n\nThe sequence score here will be computing ∆ℓEVOUt∗for some fixed t∗and tmax. The way we’ve\nset up our definitions, high scores predict t∗(and are thus bad), negative scores predict tmax (and are\nthus good), and more negative the scores, the stronger the prediction of tmax.\nDefinition 4 (swap permutation). Given a permutation σ : N<nctx →N<nctx of the nctx positions\nand two indices 0 ≤i, j < nctx, define the swap permutation σi↔j to be the permutation that is σ\n\n\n                                       30\n\nexcept swapping i and j:\n                     σ(i)    if k = j\n                                                 σi↔j(k) =   σ(j)    if k = i\n                     σ(k)  otherwise\n\nDefine ∆σ,i↔j to be the difference in sequence scores when you swap i and j:\n\n                               ∆σ,i↔j := sσi↔j −sσ\n\nLemma 3 (Characterization of swapping tokens). Fix a non-decreasing sequence of tokens t0 ≤· · · ≤\ntnctx−1 ∈N. Fix σ : N →N be a permutation of the nctx positions. Fix indices 0 ≤i, j < nctx.\nThen there are two cases for sign (∆σ,i↔j):\n\n      1.  If ati = atj then sign (∆σ,i↔j) = −sign  bσ(i) −bσ(j)  sign  vti −vtj  .\n\n                                                                                                       vtieati −vtj e atj\n      2. Otherwise, sign (∆σ,i↔j) = sign  ati −atj  sign  bσ(i) −bσ(j)  sign  sσ −    eati −e atj         .\n\nIntuitively, Lemma 3 says that, if the token contribution to attention is equal between tokens ti and\ntj, then the impact of swapping their positions σ(i) and σ(j) is entirely determined by how much\nattention is paid to the positions of i and j and the relative difference in their value. (Notably, by\nswapping these tokens, we don’t affect the attention paid on other tokens, and so the effect of the\nchange does not depend on the values of the other tokens.) Alternatively, if the attentions are not\nequal, then swapping the positions changes the allocation of attention to other tokens in the sequence,\nand so it may the case that this change in allocation in attention dominates the attention-weighted\nvalues of these two tokens.\n\nProof. First note that the theorem is trivial for i = j.\n\nFor the rest of the proof, we take i ̸= j.\n\nThe proof proceeds just by algebraic manipulation with no deep insight. We first list the facts we use,\nthe proceed to computing sign (∆σ,i↔j). We abbreviate σi↔j as σ′ for brevity.\n\n                          sign  ebσ(i) −ebσ(j) = sign  bσ(i) −bσ(j)\n\nsign (∆σ,i↔j) = sign (sσ′ −sσ)\n\n         P                         0≤p<nctx vtpeatp+bσ′(p)   !\n        = sign                  −sσ\n          P                           0≤p<nctx eatp+bσ′(p)\n\nNow multiply through by the denominator, which is positive\n\n                              \n        = sign X   vtpeatp+bσ′(p) −sσ X eatp+bσ′(p)                              \n                       0≤p<nctx                0≤p<nctx\n\n          \n        = sign X   vtpeatp+bσ(p) −vtieati  ebσ(i) −ebσ′(i)  −vtjeatj  ebσ(j) −ebσ′(j)          \n                       0≤p<nctx\n\n                                                \n                −sσ X   eatp+bσ(p) + sσeati  ebσ(i) −ebσ′(i) + sσeatj  ebσ(j) −ebσ′(j)                                                \n                             0≤p<nctx\n\n          \n        = sign X   vtpeatp+bσ(p) −vtieati  ebσ(i) −ebσ(j)  −vtjeatj  ebσ(j) −ebσ(i)          \n                       0≤p<nctx\n\n                                                \n             − X   vtpeatp+bσ(p) + sσeati  ebσ(i) −ebσ(j) + sσeatj  ebσ(j) −ebσ(i)                                                \n                          0≤p<nctx\n\n\n                                       31\n\n        = sign   vtjeatj −vtieati    ebσ(i) −ebσ(j) + sσ (eati −eatj )  ebσ(i) −ebσ(j)\n        = sign  ebσ(i) −ebσ(j)  sign   vtjeatj −vtieati + sσ (eati −eatj )\n        = sign  bσ(i) −bσ(j)  sign sσ (eati −eatj ) −  vtieati −vtjeatj\n\nDivide through by non-zero values when possible\n\n        = sign  bσ(i) −bσ(j)\n         sign  vti −vtj                                              if ati = atj\n                                   ·                                                              vtieati −vtj e atj\n                                                                  otherwise                    sign (eati −eatj ) sign  sσ −                                                                   eati −e atj         \n        −sign  bσ(i) −bσ(j)  sign  vti −vtj                                         if ati = atj\n                =                                                                                  vtieati −vtj e atj\n                  sign  ati −atj  sign  bσ(i) −bσ(j)  sign  sσ −    eati −e atj       otherwise        \n\n\nDefinition 5 (σ fixes F). Fix a set of fixed indices F ⊆N<nctx and an assignment of token values\nto each of the fixed positions tF  : F →N<dvocab. (F is the set of positions for which we are not\npessimizing over the value of the token in that position.) Fix a non-decreasing sequence of tokens\nt0 ≤· · · ≤tnctx−1 ∈N.\nGiven a permutation σ : N<nctx →Nnctx, say that σ fixes F (relative to t0, . . . , tnctx−1) if ti =\ntF (σ(i)) whenever σ(i) ∈F.\n\nNote that in this section, for the cubic proofs, we will in fact generally take F = {nctx −1}, so that\nwe are fixing the final query token, though in Theorems 7, 8, and 9 F will also contain all positions\nwith the maximum token tmax. In Appendix F, we will take F = ∅or F to be the set of positions of\nthe maximum token. However, none of these theorems are specific to F being subsingleton, and we\nprove them in generality.\nDefinition 6 (position-sorting permutation). Fix a set of fixed indices F ⊆N<nctx and an assignment\nof token values to each of the fixed positions tF  : F →N<dvocab.\nDefine the position-sorting permutation fixing indices in F σs  : N<nctx →N<nctx to be the\npermutation that sorts the indices not in F according to b: for 0 ≤i, j < nctx with i, j ̸∈F, bi ≤bj\nwhenever σs(i) < σs(j); and σs(i) = i for i ∈F.\nDefinition 7 (contiguous on equal tokens). Fix a set of fixed indices F ⊆N<nctx and an assignment\nof token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of\ntokens t0 ≤· · · ≤tnctx−1 ∈N.\nSay that the sequence represented by a permutation σ : N<nctx →N<nctx is contiguous on equal\ntokens if, for all 0 ≤i, j, k < nctx with ti = tj ̸= tk and i, j, k ̸∈σ−1(F), it is never the case that\nσs(σ(i)) < σs(σ(k)) < σs(σ(j)).\nTheorem 4 (Pessimization over sequence ordering is possible and results in contiguous sequences).\nFix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of the fixed positions\ntF  : F →N<dvocab. Fix a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.\nLet σmin, σmax : N →N be permutations of the nctx positions, fixing positions in F, satisfying the\nfollowing property: For all σ : N →N a permutation fixing F, we have\n\n                                       sσmin ≤sσ ≤sσmax                                  (10)\n\n(Such permutations are guaranteed to exist because the permutation group on nctx elements is finite.)\n\nThen σmax and σmin may be taken to be contiguous on equal tokens. That is, there exist σmax and\nσmin satisfying the property of Equation 10 which additionally satisfy the definition of Definition 7.\n\nThe basic idea is that we will assume that one of σmax and σmin cannot be contiguous on equal\ntokens and derive a contradiction. We will pick the extremal permutation that is closest to being\ncontiguous, take a contiguity violation, and then show that either we can correct the contiguity\nviolation without changing the score—thus violating the presumption that the permutation is closest\n\n\n                                       32\n\nto being contiguous—or we will find one swap of indices that decreases the score and another swap\nof indices that increases the score, thus violating the presumption of extremality.\n\nIn slightly more detail, but still informally, we will consider the sign of the difference between\nscores of our purported extremal permutation and a permutation that has swapped some indices. The\ntheorem follows from showing that there exists a triple of indices i, j, k such that the sign of the score\ndifference from swapping i and j is different from the sign of the score difference from swapping j\nand k.\n\nFirst, a definition and some helpful facts about it.\nDefinition 8 (contiguous on equally-attended positions). Fix a set of fixed indices F ⊆N<nctx and\nan assignment of token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing\nsequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.\nSay that a permutation σ is contiguous on equally-attended positions if, for all 0 ≤i < nctx with\ni ̸∈σ−1(F), the sorting order according σs on the contiguous block of positions with contribution\nto the attention score equal to that of σ(i),  σ(j)  bσ(j) = bσ(i) and σ(j) ̸∈F   , is the same as the\nsorting order according to the fraction of tokens equal to tj with b-values greater than bσ(i), with ties\nbroken by the value of tj. Equationally, this second sorting order is defined by the score\n\n                                                                   tj\n     k  tk = tj and bσ(k) > bσ(i) and σ(k) ̸∈F  +           |{k | tk = tj and σ(k) ̸∈F}| .\n                                                     dvocab\n\nMost importantly, any permutation that is contiguous on equally-attended positions has the property\nthat for any indices 0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) and ti = tj ̸= tk and σs(σ(i)) <\nσs(σ(k)) < σs(σ(j)), we will have the strict inequality bσ(i) < bσ(k) < bσ(j). Additionally, we\nmay always sort equally-attended positions to make any permutation contiguous on equally-attended\npositions.\n\nWe will define an additional notion of contiguity-violations which we avoid up-front by arbitrarily\nswapping involved indices without changing the score sσ.\nDefinition 9 (needlessly non-contiguous). Fix a set of fixed indices F ⊆N<nctx and an assignment\nof token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of\ntokens t0 ≤· · · ≤tnctx−1 ∈N.\nSay that a permutation σ is needlessly non-contiguous at i, j, k (for i, j, k ̸∈σ−1(F)) if ∆σ,i↔k = 0\nor ∆σ,j↔k = 0, for 0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) with ti = tj ̸= tk and σs(σ(i)) <\nσs(σ(k)) < σs(σ(j)).\n\nSay that a permutation σ is needlessly non-contiguous if it is needlessly non-contiguous at any\ni, j, k ̸∈σ−1(F).\nLemma 5. Fix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of the\nfixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.\nAny needlessly non-contiguous sequence σ which fixes F can be made into a sequence σ′ which\nstill fixes F and is both simultaneously contiguous on equally-attended positions and not needlessly\nnon-contiguous, and for which sσ = sσ′.\n\n\nProof. First, sort regions of equally-attended positions to make σ contiguous on equally-attended\npositions. If the resulting permutation is not needlessly non-contiguous, then we are done.\n\nOtherwise, we have ∆σ,i↔k = 0 or ∆σ,j↔k = 0 for some i, j, k, for 0 ≤i, j, k < nctx with\ni, j, n ̸∈σ−1(F) and ti = tj ̸= tk and σs(σ(i)) < σs(σ(k)) < σs(σ(j)). Since the sequence is\ncontiguous on equally-attended positions, we have the strict inequality bσ(i) < bσ(k) < bσ(j).\nBy Lemma 3, we have two cases. Noting that ti = tj, we can write them as\n\n\n      1. vtk = vti and ati = atk\n\n                                   vtieati −vtk eatk\n      2. ati ̸= atk and sσ =     eati −eatk\n\n\n                                       33\n\nIn the first case, we may fully freely interchange tokens equal to ti with tokens equal to tk without\nchanging the score; in this case we may use the token value as a sorting tie-breaker and swap tokens\nuntil there are no more needlessly non-contiguous triples falling into case (1).\n\nIn the second case, since swapping tokens does not change sσ, the property will continue to hold for\nthese tokens after the swap. We may then swap tokens, again using token value as a tie-breaker, until\nthere are no more needlessly non-contiguous triples falling into case (2).\n\nWe can now finally make our argument for Theorem 4 more precise.\n\nProof of Theorem 4. Choose σmax and σmin to be contiguous on equally-attended positions and\nnot needlessly non-contiguous, and suppose that we have σ ∈{σmax, σmin} such that for some\n0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) and ti = tj ̸= tk, we have bσ(i) < bσ(k) < bσ(j). We will\nderive a contradiction with the presumption that σ is extremal by showing that we can swap i and k\nto change the score in one direction and that we can swap j and k to change the score in the other\ndirection.\nTake σ′0 to be σ but swapping i and k, and take σ′1 to be σ but swapping j and k.\nNow we will consider the cases for the sign of the score difference ∆0 := sσ′0−sσ and ∆1 := sσ′1−sσ.\nBy the presumption of not being needlessly non-contiguous, ∆z ̸= 0 for z ∈{0, 1}. If we can show\nthat the sign of ∆0 is distinct from the sign of ∆1, then we will have a contradiction with extremality\nbecause we will have either sσ′0 < sσ < sσ′1 or sσ′1 < sσ < sσ′0. That is, we would be able to swap\ni ↔k and j ↔k to get a lower and higher score, making σ not extremal.\n\nNoting that ti = tj,\n\n                  (sign (vtk −vti)                                          if ati = atk\n  sign (∆0) = sign  bσ(i) −bσ(k)                                      vtieati −vtk eatk\n                                   sign (ati −atk) sign  sσ −    eati −eatk       otherwise\n                  (sign (vtk −vti)                                          if ati = atk\n  sign (∆1) = sign  bσ(j) −bσ(k)                                      vtieati −vtk eatk\n                                   sign (ati −atk) sign  sσ −    eati −eatk       otherwise\n\nNoting that the product is non-zero by presumption, that right multiplicand is equal for ∆0 and ∆1,\nand sign  bσ(i) −bσ(k) = −1 and sign  bσ(j) −bσ(k) = 1, we have our desired contradiction.\n\nNote that the proof of Theorem 4 does not go through if we include the position value function w in\nthe score, because we may trade off the position value function against the token value function. We\nnow show that we can independently pessimize over positional attention.\nDefinition 10 (full sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈\nN<dvocab and a permutation σ : N<nctx →N<nctx define the full sequence score s′t0,...,tnctx−1,σ as:\n                    ,\n                    s′                         := X(vti + wσ(i))eati+bσ(i) X eati+bσ(i)                       t0,...,tnctx−1,σ\n                                 0≤i<nctx                      0≤i<nctx\nWe will drop the token subscript, writing only s′σ, when the token values are unambiguous by context.\nThe sequence score here will be computing ∆ℓEPVOUt∗     := ∆ℓEVOUt∗+ ∆ℓPVOUt∗for some fixed\nt∗and tmax. As with Definition 3, with the way we’ve set up our definitions, high scores predict t∗\n(and are thus bad), negative scores predict tmax (and are thus good), and more negative the scores,\nthe stronger the prediction of tmax.\nDefinition 11 (relaxed sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤\ntnctx−1 ∈N<dvocab and a permutation σ : N<nctx →N<nctx define the relaxed sequence scores\nrt0,...,tnctx−1,σ,min and rt0,...,tnctx−1,σ,max as:\n                               rt0,...,tnctx−1,σ,min := st0,...,tnctx−1,σ +0≤i<nctxwimin\n                              rt0,...,tnctx−1,σ,max := st0,...,tnctx−1,σ +0≤i<nctxwimax\nWe will drop the token subscript, writing only rσ,min or rσ,max, when the token values are unambigu-\nous by context.\n\n\n                                       34\n\nTheorem 6 (Independent pessimization over positional contributions is possible). Fix non-decreasing\nsequences of tokens t0 ≤· · · ≤tnctx−1 ∈N and t′0 ≤· · · ≤t′nctx−1 ∈N and permutations\nσ, σ′ : N<nctx →N<nctx. Let rσ,min and rσ,max denote rt0,...,tnctx−1,σ,min and rt0,...,tnctx−1,σ,max;\nlet sσ denote st0,...,tnctx−1,σ; and let sσ′ and s′σ′ denote st′0,...,t′nctx−1,σ′ and s′t′0,...,t′nctx−1,σ′.\nThen we have\n                  0≤i<nctxwimin  = rσ,min −sσ ≤s′                                                   σ′ −sσ′ ≤rσ,max −sσ =0≤i<nctxwi.max\n\nThat is, the difference between the relaxed sequence score and the sequence score of any given\nsequence always bounds the difference between the full sequence score and the sequence score for\nany (related or unrelated) sequence.\n\nProof. This proof follows straightforwardly from the softmax weighting being an affine weighting.\n\nWe have\n                      rσ,min −sσ =0≤i<nctxwσ(i)min   =0≤i<nctxwimin\n                    rσ,max −sσ =0≤i<nctxwσ(i)max   =0≤i<nctxwimax\n                  ,\n                            s′σ′ −sσ′ = X wσ′(i)eat′i+bσ′(i) X eat′i+bσ′(i)\n                                  0≤i<nctx                0≤i<nctx\n                                                                       at′i+bσ′(i)                                              e\n                  = X wσ′(i)\n                                                                              at′j +bσ′(j)                                 0≤i<nctx  P0≤j<nctx e\nSince ex is non-negative for all real x, we have\n\n                                                                                                          at′i+bσ′(i)                                       at′i+bσ′(i)            P       P                                                                     0≤i<nctx e                   0≤i<nctx e\n                                                                                                                                     .                            ≤s′σ′ −sσ′ ≤ 0≤j<nctxwσ′(j)max    0≤j<nctxwσ′(j)min                                       at′j +bσ′(j)                                                                                                          at′j +bσ′(j)       P                          P                   0≤j<nctx e                                                                   0≤j<nctx e\n\nThus we get as desired\n                                                   σ′ −sσ′ ≤ 0≤i<nctxwi.max                                 0≤i<nctxwimin   ≤s′\n\n\n\nNote that we could prove a more fine-grained theorem, that pessimizes over attention paid to positions\nonly for sequences compatible with the chosen fixed tokens F and tF , but since the positional\ncontribution is so small we do not bother.\nTheorem 7 (For a fixed ordering, softmax is convex over token counts and only pure sequences need\nbe considered). Fix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of\nthe fixed positions tF  : F →N<dvocab. Fix a set S ⊆N<dvocab of valid other tokens in the sequence.\n(In our uses of this theorem, S will be the largest subset of N<tmax for which we can guarantee that\nthe model behaves correctly on all sequences compatible with F and tmax and with tokens otherwise\ndrawn from S.)\n\nDefine a comparison on non-negative integers less than dvocab:\n\n          c := X vtF (i)eatF (i)+bi        d := X eatF (i)+bi         f := X ebi\n              i∈F                             i∈F                       0≤i<nctx\n                                                                                                   i̸∈F\n\n   cmp(x, y) := sign d(eaxvx −eayvy) −c(eax −eay) + feax+ay  vxeax+ay −vyeax+ay\nLet tcmp min and tcmp max be the minimum and maximum elements of S according to cmp.21\nFor a given choice of a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N compatible with\nF and S and a given choice of permutation σ : N →N of the nctx positions fixing F (ti = tF (σ(i))\n\n  21We will prove that cmp is transitive in the process of proving this theorem.\n\n\n                                       35\n\nfor σ(i) ∈F; and ti ∈S for σ(i) ̸∈F):  let sσ,min (and sσ,max) denote st0,...,tnctx−1,σ when\nti = tcmp min for all σ(i) ̸∈F (or tcmp max, respectively).\n\nThen for all such choices of sequence-permutation pairs,\n\n                               sσ,min ≤st0,...,tnctx−1,σ ≤sσ,max.\n\nThis theorem follows by chaining two lemmas: that scores are extremized by considering pure\nsequences, and that the extremal pure sequences match the comparison function defined in the\ntheorem statement.\nLemma 8 (Sequences scores are extremized on purer sequences). Fix all the same quantities as in\nTheorem 7.\n\nFor any indices 0 ≤i < j < nctx, token values x, y ∈S, the score for a sequence with ti = x ̸=\ny = tj is bounded on both sides by sequences with ti = tj = x and ti = tj = y.\n\nProof. Let sα,β be the sequence score with ti = α and tj = β, and define the score differences\n∆x := sx,x −sx,y and ∆y := sy,y −sx,y. It suffices to show that sign(∆x∆y) ≤0. To show this,\nwe must only compute the sign of ∆α for α ∈{x, y} and show that whenever both ∆x and ∆y are\nnon-zero, they have opposite signs.\n\nWe proceed by computation after defining some convenience variables for brevity:\n\n          C := X vtkeatk +bσ(k)         D := X eatk +bσ(k)\n                    0≤k<nctx                              0≤k<nctx\n                              k̸=i,j                                               k̸=i,j\n\n\n            x   if α = y                     i    if α = x                     i    if ˜α = x\n            ˜α :=                       iα :=                                  i˜α :=\n              y    if α = x                 j   if α = y                  j   if ˜α = y\n\n                   vαeaα+bσ(i) + vαeaα+bσ(j) + C −vxeax+bσ(i) + vyeay+bσ(j) + Csign (∆α) = sign\n                       eaα+bσ(i) + eaα+bσ(j) + D         eax+bσ(i) + eay+bσ(j) + D\n\n                                                                                                     ˜ + C                   vαeaα+bσ(iα) + vαeaα+bσ(iα)                                                        ˜ + C −vαeaα+bσ(iα) + v˜αeaα+bσ(i˜     α)      = sign\n                       eaα+bσ(iα) + eaα+bσ(iα)˜ + D         eaα+bσ(iα) + eaα+bσ(i˜     α)˜ + D\n\nMultiply through by positive denominators and simplify\n\n                                                                       aα+bσ(i˜     α)˜      = sign C  e                       aα+bσ(i˜     α)˜ −ebσ(iα)+aα˜    + D  vαebσ(iα)+aα                                                                            ˜     −v˜αe\n\n                                                                                      ˜ + ebσ(iα)  eaα+bσ(i˜    α)+aα˜           + vα  e                                  bσ(iα)˜ + ebσ(iα)  eaα+bσ(i˜    α)+aα˜     −v˜α  ebσ(iα)\n\nPulling out ebσ(iα)˜\n\n      = sign  eaα+aα˜     ebσ(iα)˜ + ebσ(iα)  (vα −v˜α) + C (ea                                                           α˜ −eaα) + D (eaαvα −eaαv˜α)˜\n\n\nNote that swapping α and ˜α negates the sign. Hence, we have sign(∆x) = −sign(∆y) and hence\nsx,x ≤sx,y ≤sy,y or sy,y ≤sx,y ≤sx,x as desired.\n\nLemma 9 (Pure sequences are sorted according to cmp in Theorem 7). Fix all the same quantities as\nin Theorem 7.\n\nFix tokens x, y ∈S. Let n := nctx −|F| be the number of non-fixed tokens. Fix sequences with\nn copies of x and y respectively: fix tx,0 ≤· · · ≤tx,nctx−1 ∈N and ty,0 ≤· · · ≤ty,nctx−1 ∈N\ncompatible with F and S and given choices of permutations σx, σy : N →N of the nctx positions\nfixing F: tx,i = tF (σx(i)) for σx(i) ∈F; ty,i = tF (σy(i)) for σy(i) ∈F; tx,i = x for σx(i) ̸∈F;\nand ty,i = y for σy(i) ̸∈F.\n\nThen\n                    sign((sσx,tx,0,...,tx,nctx−1) −(sσy,ty,0,...,ty,nctx−1)) = cmp(x, y)\n\n\n                                       36\n\n                                 √\n                    EQKE(t−1, ti) := t−1EqQKT ¯ET ti T /  d\n                               √\n                    EQKP(t−1, i) := t−1EqQKT ˆPTi /  d\n                       EVOU(ti) := ti¯EV OU\n                       PVOU(i) := ˆPiV OU\n                            ℓEU(t−1) := t−1EqU\n               ∆ℓEU t∗(t−1, max ti) := ℓEU(t−1)t∗−ℓEU(t−1)maxi ti\n                                                     i\n\nFigure 13: Recapitulation of some relevant definitions from Figure 12, parameterized by the arguments they\nactually depend on.\n\n\nProof. The proof goes by straightforward computation.\n\n           sign((sσx,tx,0,...,tx,nctx−1) −(sσy,ty,0,...,ty,nctx−1))\n                vxeaxf + c −vyeayf + c     = sign\n                  eaxf + d     eayf + d\n\nMultiply through by non-negative denominators\n     = sign ((vxeaxf + c) (eayf + d) −(vyeayf + c) (eaxf + d))\n     = sign −cfeax + cfeay + dfvxeax −dfvyeay + f 2vxeax+ay −f 2vyeax+ay\n\nUse f > 0\n     = sign −ceax + ceay + dvxeax −dvyeay + fvxeax+ay −fvyeax+ay\n     = sign c (eay −eax) + d (vxeax −vyeay) + f  vxeax+ay −vyeax+ay\n     = cmp(x, y)\n\n\nCorollary 10. Define the relation ≤cmp by x ≤cmp y if and only if cmp(x, y) ∈{−1, 0}. The\nrelation ≤cmp is always transitive.\n\nProof. Note that by Lemma 9, cmp is comparing two sequence scores. Since ≤is transitive over the\nreals, the relation ≤cmp is also transitive.\n\nFinally, we combine the previous lemmas to complete our proof of Theorem 7:\n\nProof of Theorem 7. Extremal sequences with scores sσ,min and sσ,max are guaranteed to exist\nbecause there are only finitely many elements of S and therefore only finitely many sequences.\nBy Lemma 8, the extremal sequences must be pure (have ti = tj whenever σ(i), σ(j) ̸∈F). By\nLemma 9, the extremal sequences must have tokens that are extremal according to cmp.\n\nWe now have all the tools necessary to prove the following theorem. We refer to Algorithm 3 and\nAlgorithm 4 or the proof of Theorem 11 for a definition of the CUBIC algorithm.\nTheorem 11.\n\n         Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥CUBIC(dvocab, nctx, M)\n                                                      i                         i\n\nBefore we give the proof of this theorem, we introduce some helpful notation.\nDefinition 12. Fix an element (rm, rq, c) ∈{0, ..., dvocab}2 ×{0, ...3} such that rm ≥rq. We define\nX(rm,rq,c) to be the set of tokens t such that\n\n      1. The max token tmax is equal to rm,\n\n\n                                       37\n\nAlgorithm 2 Counting Correct Sequences in Cubic Time: Preliminaries\n  1: function CORRECTNESS(M, input-sequence)\n  2:    return MODEL-BEHAVIOR(M, input-sequence) == MAX(input-sequence)\n  3: end function\n  4: function MODEL-BEHAVIOR(M, input-sequence)\nRequire: input-sequence is a tensor of shape (nctx, ) with values in N<dvocab\n  5:    tmax ←MAX(input-sequence)                               ▷tmax ←max-token\n  6:    t ←input-sequence\n  7:    skip-scoret∗←∆ℓEUt∗(tnctx−1, tmax)\n  8:     attn-weights-unscaledi ←EQKE(tnctx−1, ti) + EQKP(tnctx−1,√              i)\n  9:     attn-weights ←SOFTMAX(attn-weights-unscaled/  d)\n10:     vt ←EVOU(t)\n11:    wi ←PVOU(i)\n12:    ∆vt,t∗←vt,t∗−vt,tmax\n13:    ∆wi,t∗←wi,t∗−wi,tmax\n14:    return maxt∗̸=tmax(skip-scoret∗+ Pnctx−1i=0   (∆vi,t∗+ ∆wi,t∗) · attn-weightsi)\n15: end function\n16: function CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW(M, input-sequence)\n17:    t ←input-sequence\n18:    return ALL(CORRECTNESS(M, perm + [t−1]) for all perm ∈PERMUTATIONS(t0:−1))\n19: end function\n\n\n      2. The query token tquery is equal to rq,\n\n      3. The cardinality of tokens that are not at the query position and not equal to tmax is equal to\n           c.\n\nFor clarity, we list all the possible cases. We always take tquery ≤tmax and let S3 act on sequences\nby permuting the first three factors (i.e. keeping the query position fixed).\n\n      1.  If c = 0, then X(tmax,tquery,0) = {[tmax, tmax, tmax, tquery]},\n      2.  If c = 1, then X(tmax,tquery,1) = S3.{[t1, tmax, tmax, tquery] | t1 < tmax},\n      3.  If c = 2, then X(tmax,tquery,2) = S3.{[t1, t2, tmax, tquery] | ti < tmax},\n      4.  If c = 3, then X(tmax,tmax,3) = S3.{[t1, t2, t3, tmax] | ti < tmax}.\nDefinition 13. Let t ∈X be a sequence. We say t is pure, if it has at most three distinct tokens: the\nmax token tmax, the query token tquery, and optionally a third token t∗< tmax.\nWe denote by Xpure the subset of pure tokens. For any subset Y ⊂X, we set Y pure := Y ∩Xpure.\n\nWe now come to the proof of Theorem 11. We will show how to use the previous theorems to get\nexplicit bounds and explain how CUBIC(dvocab, nctx, M) computes these bounds.\n\nProof of Theorem 11. First of all, we note that the algorithm CUBIC = CUBIC(dvocab, nctx, M)\nyields a lower bound for the accuracy on the set X(tmax,tquery,c). We can therefore compute the bound\non X = `(tmax,tquery,c) X(tmax,tquery,c) by computing it for each such choice (tmax, tquery, c) and\nsumming over them\n\n  Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti ≥ X     CUBIC(X(tmax,tquery,c)).\n                                              i                         i\n                                                                       (tmax,tquery,c)\nSo from now on we will fix one such subset X(tmax,tquery,c).\nWe begin by defining a map\n                         f : X(tmax,tquery,c) →{0, ..., dvocab}c\nwhich sends a sequence to the subsequence of elements which are not at the query position and not\nequal to tmax. Then Theorem 7 can be restated as follows22:\n\n   22In fact, the theorem yields a stronger result, but we will only need the following formulation.\n\n\n                                       38\n\nAlgorithm 3 Counting Correct Sequences in Cubic Time, Part I. Lines are annotated with comments\nindicating the parameters for a cache to avoid duplicate computations.\n  1: function MODEL-BEHAVIOR-RELAXED(M, query-tok, max-tok, non-max-tok, n-copies-\n   nonmax)\n  2:     tquery ←query-tok, tmax ←max-tok, t′ ←non-max-tok, c ←n-copies-nonmax\nRequire: 0 ≤tquery ≤tmax < dvocab, 0 ≤t′ ≤tmax < dvocab, 0 ≤c < nctx\nRequire:  if n-copies-nonmax = 0 then non-max-tok = max-tok\nRequire:  if query-tok ̸= max-tok then n-copies-nonmax < nctx −1\nEnsure: return ≥MODEL-BEHAVIOR(M, t) for all t with specified tquery, c copies of t′ in non-\n    query positions, and the remainder of the tokens equal to tmax\n  3:    skip-scoret∗←∆ℓEUt∗(tquery, tmax)                    ▷Cache by tmax, tquery, t∗\n  4:    wi ←PVOU(i)     for 0 ≤i < nctx                              ▷Cache by i\n  5:    ∆wmax,t∗←max0≤i<nctx(wi,t∗−wi,tmax)                    ▷Cache by tmax, t∗\n  6:     vt ←EVOU(t), ∆vt,t∗←vt,t∗−vt,tmax     for t ∈{tquery, tmax, t′} ▷Cache by tmax, t,\n     t∗             √\n  7:     at ←EQKE(tquery, t)/  d     for t ∈{tquery, tmax, t′}          ▷Cache by tquery, t                      √\n  8:     bnctx−1 ←EQKP(tquery, nctx −1)/  d                        ▷Cache by tquery                       √\n  9:     b0,:−1 ←SORT(EQKP(tquery, : −1))/  d                     ▷Cache by tquery, i\n10:     b1,:−1 ←REVERSE(b0,:−1)\n11:     attn-weights-unscaled:,nctx−1 ←atquery + bnctx−1                 ▷Cache by tquery\n12:     attn-weights-unscaled0,i ←atmax + b0,i   for 0 ≤i < nctx −c −1 ▷Cache by tmax, c, i,\n     tquery\n13:     attn-weights-unscaled1,i ←atmax + b1,i   for 0 ≤i < nctx −c −1 ▷Cache by tmax, c, i,\n     tquery\n14:     attn-weights-unscaled0,i ←at′ + b0,i   for nctx −c −1 ≤i < nctx −1 ▷Cache by t′, c, i,\n     tquery\n15:     attn-weights-unscaled1,i ←at′ + b1,i   for nctx −c −1 ≤i < nctx −1 ▷Cache by t′, c, i,\n     tquery\n16:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0)      ▷Cache by tmax, t′, c, i, tquery\n17:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1)      ▷Cache by tmax, t′, c, i, tquery\n18:       if c = 0 then ▷In this case, attn-weights0,i = attn-weights1,i, so we drop the first subscript\n19:        return   maxt∗̸=tmax(skip-scoret∗ +  ∆wmax,t∗ +  ∆vt−1,t∗attn-weights−1 +\n   ∆vtmax,t∗Pnctx−2i=0    attn-weightsi)\n20:     else\n21:       ∆vi,t∗←∆vtmax,t∗   for 0 ≤i < nctx −c −1\n22:       ∆vi,t∗←∆vt′,t∗   for nctx −c −1 ≤i < nctx −1\n23:        ∆vnctx−1,t∗←∆vtquery,nctx−1      (Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights0,i\n24:        return maxt∗̸=tmax skip-scoret∗+max                                          Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights1,i\n25:    end if\n26: end function\n27: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, tquery, tmax, t′, c)\n28:       ▷runs the model on a relaxed variant of input sequences compatible with the arguments\nEnsure: return is False if CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW(M, t) is False\n    for any t with specified tquery, c copies of t′ in non-query positions, and the remainder of the\n    tokens equal to tmax\n29:    return MODEL-BEHAVIOR-RELAXED(M, tquery, tmax, t′, c) < 0\n30: end function\n\n\n\n\n\n                                       39\n\nAlgorithm 4 Counting Correct Sequences in Cubic Time, Part II\n  1: function CUBIC(dvocab, nctx, M)\n  2:     count ←0                                    ▷# of correct sequences\n  3:     for tmax ∈RANGE(dvocab) do                               ▷tmax ←max-token\n  4:        for 0 ≤tquery ≤tmax do                                   ▷tquery ←query-token\n  5:          cmax ←nctx −1 if tquery = tmax else nctx −2    ▷maximum copies of nonmax\n  6:            for 0 ≤c ≤cmax do          ▷number of valid choices for the non-max token\n  7:            RCPOP(⃗χ) ←RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, ⃗χ)\n  8:                     if c = 0 then\n  9:                    t-count ←1 if RCPOP(tquery, tmax, tmax, 0) else 0\n10:                 else\n11:                    t-count ←Ptmax−1t′=0   1 if RCPOP(tquery, tmax, t′, c) else 0\n12:             end if\n13:               count ←count +  nctx−1c      · (t-count)c         ▷taking 00 = 0 conventionally\n14:          end for\n15:       end for\n16:    end for\n17:    return count ·    1                          dvocabnctx\n18: end function\n\nLet S ⊂{0, ..., dvocab}. Then full accuracy f −1(Sc)pure := Xpure(tmax,tquery,c) ∩f −1(Sc), implies full\naccuracy on f −1(Sc).\nNow instead of computing the output of the model for every element f −1(Sc)pure, we use Theorem 4\n(combined with Theorem 6) to run a relaxed version of this. In particular, we may assume that the pure\nsequence is contiguous on equal tokens. Here contiguous on equal tokens means that for the positional\npart of the attention (i.e. the EQKP part), we have either btmax < {bi, bj} or btmax > {bi, bj}, where\ni, j ∈{0, ..., nctx −1} are indices of tokens not equal to tmax.\nFor the algorithm CUBIC(dvocab, nctx, M) we fix a t∗∈{0, ..., tmax−1} (unless c = 0, in which case\nthere is no such choice). We then run the relaxed accuracy computation RCPOP(tquery, tmax, t′, c)\nas described in Theorem 6. If RCPOP(tquery, tmax, t′, c) < 0, we add t′ to S. If we do, we add t∗to\nS. Therefore by construction of S we know that we get full accuracy on f −1(Sc)pure and therefore\nwe get full accuracy on f −1(Sc).\nNow we count the cardinality of f −1(Sc) and add it to the count of correct sequences.\n\n\n\nTheorem 12. The running time of Algorithm 3, after using caching to avoid duplicate computations,\nis O(dvocab3nctx2).\n\nProof. The nested loops in CUBIC execute the innermost body O(dvocab2nctx) times, and the\nsummation on Line 13 costs O(nctx) per iteration.  What remains is to show that the call\nto RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, tquery, tmax, t′, c) costs O(nctx)\nwhen c ̸= 0 and at most O(dvocabnctx) when c = 0 and t′ = tmax.\nThe matrix multiplications in EQKE, EQKP, EVOU, PVOU, and ℓEU can be cached upfront,\ncosting O(max(dvocab, dmodel, nctx)2dmodel) ≤O(dvocab3) since we assume dvocab > dmodel and\ndvocab > nctx.\nThe sorting on Line 9 can also be cached upfront (per tquery), costing O(dvocabnctx log nctx).\n\nNote that each variable assignment in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION can\nbe cached into a table parameterized over at most three variables which range over dvocab and over at\nmost two variables that range over nctx.\n\nWhat remains is the return statements.\n\nWhen  c =   0, we have on Line  19:   return maxt∗̸=tmax(skip-scoret∗+ ∆wmax,t∗+\n∆vt−1,t∗attn-weights−1 + ∆vtmax,t∗Pnctx−2i=0    attn-weightsi). This is O(dvocabnctx) as desired.\n\n\n                                       40\n\nWhen c ̸= 0, we have on Line 24:\n                          (Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights0,i\nreturn max skip-scoret∗+max        t∗̸=tmax                  Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights1,i\nWe can cache maxt∗̸=tmax skip-scoret∗per tmax and tquery, costing O(dvocab3nctx). We can cache\nmaxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) per tmax and t′ costing O(dvocab3), since each ∆vi,t∗will be\n∆vt,t∗for some t ∈{tquery, tmax, t′}. Finally, we can compute the summation in cost O(nctx) per\nloop iteration, as required.\n\nF  Quadratic counting for a sub-cubic proof\n\nIn this section we fill in the details lacking from Section 4.3.\n\nIn Appendix E we proved an intricate version of convexity of softmax where, modulo pessimizing in\nunrealistic ways over the attention paid to positions for the computation done on positional encodings,\nall extremal relaxed sequences correspond to actual sequences.\nWhen we only get a budget of O(dvocab2nctx) extremal relaxed cases to consider, though, we must\npessimize more, which gives us a simpler version of the convexity theorem and proof. Notably, when\nwe restrict our sequences to have only two tokens (the max token tmax and the non-max token t′),\nmost of the theorems from Appendix E.3 get significantly simpler.\n\nAdditionally, we must pessimize separately over the token value (v) and token attention (b) computa-\ntions in order to allow efficient computation (Theorem 15).\n\nF.1  Proof of baseline sub-cubic result\n\nFor this subsection, all theorems are parameterized over the following quantities.\nDefinition 14 (Common theorem parameters). Fix a total number of tokens nctx. Fix a token value\nfunction (à la a row-difference in EVOU) v : N<dvocab →R and a token attention function (à la\nEQKE for a fixed query token) a : N<dvocab →R. Fix a position value function (à la a row-difference\nin PVOU) w : N<nctx →R and a position attention function (à la EQKP for a fixed query token)\nb : N<nctx →R.\n\nIn practice, as in Appendix E.3, we’ll take, for fixed query token tquery,\n                                       √\n               vt = EVOUt,t∗−EVOUt,tmax              at = EQKEtquery,t/  d\n                                       √\n            wi = PVOUi,t∗−PVOUi,tmax                 bi = EQKPtquery,i/  d\n\nNote that unlike in Appendix E.3, we pessimize independently over the query token and the non-max\ntoken, so the “fixed” query token may not in fact appear in any key-side position in the relaxed\nsequence we consider.\nDefinition 15 (of a sequence via mapping from positions). We can define a sequence of tokens via\nmapping from positions by specifying a subset of valid tokens S ⊆N<dvocab paired with a function\nT  : N<nctx →S specifying which token is in each position.\nDefinition 16 (sequence score). Given a subset of valid tokens S ⊆N<dvocab and a function\nT  : N<nctx →S specifying which token is in each position, define the sequence score\n\n                 ,\n                        sT := X vT (i)eaT (i)+bi X eaT (i)+bi\n                               0≤i<nctx              0≤i<nctx\nDefinition 17 (swapped mapping). Given a subset of valid tokens S ⊆N<dvocab and a function\nT  : N<nctx →S specifying which token is in each position and two indices 0 ≤i, j < nctx, define\nthe swapped mapping Ti↔j be the function that is T except swapping i and j:\n\n                     T(i)    if k = j\n                                                 Ti↔j(k) =  T(j)    if k = i\n                     T(k)  otherwise\n\n\n                                       41\n\nLemma 13 (Characterization of swapping tokens in a two-token sequence). Fix two tokens t0 <\nt1 ∈N and a function T  : N<nctx →{t0, t1} specifying which token is in each position.\nDefine ∆T,i↔j to be the difference in sequence scores when you swap i and j:\n\n                              ∆T,i↔j := sTi↔j −sT\n\nThen\n                     sign (∆T,i↔j) = −sign (bi −bj) sign vT (i) −vT (j)\n\nProof. Lemma 3 gives us the result directly when aT (i) = aT (j). Otherwise, we get\n   sign (∆T,i↔j) = sign aT (i) −aT (j)  sign (bi −bj) sign  sT −vT (i)eaT (i)(i) −vT (j)eaT(j)    (j)                                                                    eaT  −eaT\n\nHence all that remains is to show that\n         sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j) = −sign vT (i) −vT (j)\n\nDefine ¯v := 12(vT (i) + vT (j)) and define ∆v := 12(vT (i) −vT (j)) so that vT (i) =  ¯v + ∆v and\nvT (j) = ¯v −∆v. Assume WLOG that T(i) = 0 and T(j) = 1 so that vT (p) = ¯v + (−1)T (p)∆v for\nall p.\n\nThen we have\n\n sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j)\n= sign (sT (eaT (i) −eaT (j)) −¯v (eaT (i) −eaT (j)) −∆veaT (i) −∆veaT (j))\n     X   vT (p)eaT (p)+bp                             \n= sign  0≤p<nctx               (eaT                                                           (i) −eaT (j)) −¯v (eaT (i) −eaT (j)) −∆v (eaT (i) + eaT (j))        X eaT (p)+bp                                   \n            0≤p<nctx\n     X       ¯v + (−1)T (p)∆v  eaT (p)+bp                             \n     0≤p<nctx                                                     (i)                             (i)                                (i)     = sign                                       (eaT  −eaT (j)) −((((((((¯v (eaT  −eaT (j)) −∆v (eaT  + eaT (j))\n        X eaT (p)+bp                                                                                      \n                   0≤p<nctx\n         eaT (i) X ebp −eaT                                                         (j) X ebp                \n\n                      0≤p<nctx                                      0≤p<nctx                                                                       (p)=T (i)      T                                        (p)=T (j)           T                                                                                                                          (i)                         (i)         (j)= sign(∆v) sign                              (eaT  −eaT (j)) −eaT  −eaT \n          X eaT (p)+bp                                                          \n             0≤p<nctx                       \n\n              eaT (i) X ebp −eaT                                                                      (j) X ebp                \n\n                               0≤p<nctx                                               0≤p<nctx                                                                                          (p)=T (i)      T                                                 (p)=T (j)                T                                                                                                                                              (i)                         (i)         (j)= sign(vT (i) −vT (j)) sign                              (eaT  −eaT (j)) −eaT  −eaT \n               X eaT (p)+bp                                                               \n                  0≤p<nctx                       \n\nDefine\n                     Pi := X ebp                    Pj := X ebp\n                          0≤p<nctx                           0≤p<nctx\n                       T (p)=T (i)                        T (p)=T (j)\n\n\nso that we get\n sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j)\n\n\n                                       42\n\n                            eaT (i)Pi −eaT (j)Pj        (i)                         (i)         (j)= sign(vT (i) −vT (j)) sign                      (eaT  −eaT (j)) −eaT  −eaT\n                            eaT (i)Pi + eaT (j)Pj\n\nMultiply through by the positive denominator and expand out so that we get\n= sign(vT (i) −vT (j)) sign −2eaT (i)+aT (j)Pi −2eaT (i)+aT (j)Pj\n= −sign(vT (i) −vT (j)) sign eaT (i)+aT (j)Pi + eaT (i)+aT (j)Pj\n= −sign(vT (i) −vT (j))\n\n\n\n\nTheorem 14 (Pessimization over sequence ordering for two-token sequences is simple). Let σs :\nN →N denote a permutation of the nctx positions that sorts them according to b: for 0 ≤i, j < nctx,\nbi ≤bj whenever σs(i) < σs(j). Fix two tokens t0 < t1 ∈N.\nLet nt0 be the number of p ∈[0, nctx) with T(p) = t0 and let n1 be the number of p ∈[0, nctx) with\nT(p) = tt1. Note that nt0 + nt1 = nctx.\nDefine tmin := argmint∈{t0,t1} vt and define tmax := argmaxt∈{t0,t1} vt.\nDefine Tmin, Tmax : N<nctx →{t0, t1} to be the assignment of tokens to positions that pays the least\n(respectively, most) attention to tmax:\n\n                                    tmax    if 0 ≤σs(i) < ntmax\n                         Tmin(i) :=\n                                       tmin    if ntmax ≤σs(i) < nctx\n                                       tmin    if 0 ≤σs(i) < ntmin\n                       Tmax(i) :=\n                                    tmax    if ntmin ≤σs(i) < nctx\n\nThen we have that\n                                      sTmin ≤sT ≤sTmax\n\nProof. The extremality of sTmin and sTmax follows straightforwardly from Theorem 4.\nAll that remains is sTmin ≤sTmax.\nThis follows from noting by Lemma 13 that swapping two tokens in Tmin increases the sequence\nscore, while the reverse is true of sTmax, thus showing that it must be sTmin that is the minimum and\nsTmax that is the maximum and not vice versa.\nDefinition 18 (full sequence score). Given a subset of valid tokens S ⊆N<dvocab and a function\nT  : N<nctx →S specifying which token is in each position define the full sequence score s′T :\n\n                  ,\n                            s′T := X(vT (i) + wi)eaT (i)+bi X eaT (i)+bi\n                            0≤i<nctx                     0≤i<nctx\n\nTheorem 15 (Independent pessimization over positional contributions and token attention and token\nvalue is possible). Fix two tokens t0 < t1 ∈N. Let Tmin, Tmax : N<nctx →{t0, t1} and tmax, tmin\nbe as in Theorem 14. Fix a set S of valid tokens with t0, t1 ∈S.\nDefine relaxed versions T max,′   Tmin′     : N<nctx →S of Tmax and Tmin:\n\n                            (Tmax(i)                if Tmax(i) = tmax\n                T max(i)′     :=  argmin j∈S  aj   otherwise\n                                                  j̸=tmax\n                             (Tmin(i)                  if Tmax(i) = tmax\n                T min(i)′     :=  argmax j∈S  aj   otherwise\n                                                   j̸=tmax\nThat is, T max′   replaces tmin with whatever token in S draws the least attention away from tmax, while\nT min′   replaces tmin with whichever token in S draws the most attention away from tmax.\n\n\n                                       43\n\nDefine relaxed extremal sequence scores rTmax, rTmin:\n\n                       ,        \n              rTmin := min  +  X vTmin(i)eaT min(i)+bi′   X eaT min(i)+bi′                       0≤i<nctxwi                      \n                                   0≤i<nctx                   0≤i<nctx\n                        ,        \n             rTmax :=0≤i<nctxwimax  +  X vTmin(i)eaT ′max(i)+bi X eaT ′max(i)+bi \n                                   0≤i<nctx                   0≤i<nctx\n\nThen rTmin ≤s′Tmin and s′Tmax ≤rTmax.\n\nProof. (sketch) Essentially the same as the proof of Theorem 6.\n\nNote that in practice, we take S to be the set of all tokens less than tmax −g for some minimum\ngap g. This allows us to share computation across the various maximum tokens to reduce overall\ncomputational complexity.\n\nAlgorithm 5 Counting Correct Sequences in Subcubic Time, Preliminaries\n  1: function INPUT-SEQUENCE-COMPATIBLE-WITH(input-sequence, dvocab, nctx, tmax, tquery, c,\n    g)\n  2:    t ←input-sequence\n  3:    return False if t ̸∈(N<dvocab)nctx            ▷the sequence is not made of valid tokens\n  4:    return False if t−1 ̸= tquery                               ▷wrong query token\n  5:    return False if maxi ti ̸= tmax                              ▷wrong max token\n  6:    return False if |{i ∈N<nctx | ti ̸= tmax}| ̸= c          ▷wrong count of non-max toks\n  7:    return ALL(ti = tmax or tmax −ti ≥g for 0 ≤i < nctx)   ▷check gap on non-max toks\n  8: end function\n  9: function CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW(M, dvocab, nctx, tmax, tquery, c, g)\n10:    return ALL(CORRECTNESS(M, t) for all t s.t. INPUT-SEQUENCE-COMPATIBLE-WITH(t,\n    dvocab, nctx, tmax, tquery, c, g))\n11: end function\n12: function SUBCUBIC(dvocab, nctx, M, G)\n13:     count ←0                                    ▷# of correct sequences\n14:     Gtmax,tquery,c ←MIN(tmax, MAX(1, Gtmax,tquery,c))             ▷Clip G to valid range\n15:   G∗tmax,c ←mint≤tmax Gtmax,t,c                          ▷Cache running minima\n16:     for tmax ∈RANGE(dvocab) do                               ▷tmax ←max-token\n17:        for 0 ≤tquery ≤tmax do                                   ▷tquery ←query-token\n18:           cmin ←0 if tquery = tmax else 1               ▷minimum copies of nonmax\n                     0             if tmax = 0\n19:          cmax ←                              ▷maximum copies of nonmax\n                         nctx −1  otherwise\n20:            for cmin ≤c ≤cmax do         ▷valid choices for the number of non-max tokens\n21:              g ←Gtmax,tquery,c\n22:         g∗←G∗tmax,c\n23:              q-gap ←tmax −tquery\n24:            RCPOG(⃗χ) ←RELAXED-CORRECTNESS-PESSIMIZING-OVER-GAP(M, ⃗χ)\n25:                     if (q-gap = 0 or q-gap ≥g) and RCPOG(dvocab, nctx, tmax, tquery, c, g, g∗)\n    then\n26:                        c′ ←c if tquery = tmax else c −1      ▷# of non-max non-query tokens\n                                         nctx−127:                  count ←count +      c′   (tmax −g)c′     ▷taking 00 = 1 conventionally\n28:             end if\n29:          end for\n30:       end for\n31:    end for\n32:    return count ·    1                          dvocabnctx\n33: end function\n\n\n\n                                       44\n\nAlgorithm 6 Counting Correct Sequences in Subcubic Time\n  1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗)\nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False\nRequire: 0 ≤g∗≤g ≤tmax\nRequire:  if c = 0 then tquery = tmax\n  2:     skip-score ←maxt∗ℓEU(tquery)t∗−mint∗ℓEU(tquery)t∗            ▷Cache by tquery\n  3:     vt ←EVOU(t)\n  4:    wi ←PVOU(i)\n  5:   ∆wmax,t∗←maxi wi,t∗−wi,tmax                           ▷Cache by tmax, t∗\n  6:    ∆wmax,max ←maxt∗∆wmax,t∗                               ▷Cache by tmax\n  7:    ∆vt ←maxt∗vt,t∗−mint∗vt,t∗                                 ▷Cache by t\n  8:    ∆vmax ←max0≤t≤tmax−g∗∆vt                           ▷Cache by tmax −g∗\n  9:    ∆vtmaxt∗   ←vtmax,t∗−vtmax,tmax                                ▷Cache by tmax\n10:    ∆vtmaxmax ←maxt∗̸=tmax ∆vtmaxt∗                                  ▷Cache by tmax\n11:       if c = 0 then\n12:      ℓt∗←ℓEU(tmax)t∗+ vtmax,t∗+ ∆wmax,t∗\n13:        return maxt∗̸=tmax (ℓt∗−ℓtmax)\n14:    end if                √\n15:     b:,nctx−1 ←EQKP(tquery, nctx −1)/  d                       ▷Cache by tquery                       √\n16:     b0,:−1 ←SORT(EQKP(tquery, : −1))/  d                     ▷Cache by tquery, i\n17:     b1,:−1 ←REVERSE(b0,:−1)                √\n18:     at ←EQKE(tquery, t)/  d                                ▷Cache by tquery, t\n19:     amin,t ←min0≤t′′≤t at′′         ▷Cache by tquery, t, compute in amortized O(dvocab2)\n20:     amax,t ←max0≤t′′≤t at′′        ▷Cache by tquery, t, compute in amortized O(dvocab2)\n21:    ∆amax ←atmax −amin,tmax−g                         ▷Cache by tquery, tmax, c\n22:    ∆amin ←atmax −amax,tmax−g                         ▷Cache by tquery, tmax, c\n23:     idx-set ←{0, . . . , nctx −c −1} if tmax ̸= tquery else {0, . . . , nctx −c −2, nctx −1}\n24:     attn-weights-unscaled0,i ←b0,i + (∆amin if i ∈idx-set else 0)\n25:     attn-weights-unscaled1,i ←b1,i + (∆amax if i ∈idx-set else 0) ▷Cache by tquery, tmax, i,\n    c\n26:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0)        ▷Cache by tquery, tmax, i, c\n27:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1)        ▷Cache by tquery, tmax, i, c\n28:     attn-max0 ←Pi∈idx-set attn-weights0,i\n29:     attn-max1 ←Pi∈idx-set attn-weights1,i\n30:     attn-max ←attn-max0 if ∆vtmaxmax < ∆vmax else attn-max1\n31:                  ▷Recall that ∆vtmaxmax is negative when the model outputs the correct answer\n32:    return skip-score + ∆wmax,max + attn-max · ∆vtmaxmax + (1 −attn-max)∆vmax\n33: end function\n34: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-GAP(M, dvocab, nctx, tmax, tquery, c,\n     g, g∗)\n35:       ▷runs the model on a relaxed variant of input sequences compatible with the arguments\nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False\nEnsure: return is False if CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW(M, t) is False for any\n    t with specified tmax, tquery, and c tokens not equal to tmax\n36:    return MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗) < 0\n37: end function\n\n\n\n\n\n                                       45\n\nTheorem 16. For all G,\n\n     Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥SUBCUBIC(dvocab, nctx, M, G)\n                                                 i                         i\n\nProof. (sketch) Apply preceding lemmas and theorems to Algorithm 6\n\nTheorem 17. The running time of Algorithm 6, after using caching to avoid duplicate computations,\nis O(dvocab2dmodel + dvocab2nctx2).\n\nProof. (sketch) Sum the complexities indicated along the right side of Algorithm 3.  The\ndvocab2dmodel term comes from the precomputing EVOU, EU, and EQKP. The dvocab2nctx2\nterm comes from the softmax over nctx tokens for O(dvocab2nctx) pessimized pure sequences. Con-\nfirming that none of the complexities on the right side exceeds O(dvocab2dmodel + dvocab2nctx2)\ncompletes the proof.\n\n\n\nG  Subcubic proof strategies\n\nIn this section, we present a number of proof strategies that we use to reduce the computational cost\nof the proof, ultimately driving down the cost of EU and EQKE verification to O(dvocabdmodel),\nwhile unfortunately leaving the cost of EVOU verification at O(dvocab2dmodel).\n\nThe three main tricks we cover are the mean+diff trick (Appendix G.1), the max row-diff trick\n(Appendix G.2.2), and the rank one / rank two SVD decomposition of EQKE (Appendix G.2.3).\nWhile the mean+diff trick is useful for getting slightly better bounds, the SVD decomposition of\nEQKE is the place where we get to insert the most understanding (without which we’d have no\nhope of non-vacuous bounds below O(dvocab2dmodel)), and the max row-diff trick is the workhorse\nthat allows us to drive down the error term computations from cubic to quadratic without getting\ncompletely vacuous bounds.\n\n\nG.1  The mean+diff trick\n\nSuppose we have quantities fx,y and gy,z and we want to pessimize (WLOG, suppose minimize) the\nquantity fx,y + gy,z over x, y, and z in time less than O(nxnynz), say we allow O(nxny + nynz +\nnxnz). Also suppose the variation of f over the y axis is much larger than the variation of f over the\nx-axis.\n\nWe can of course say\n                       min fx,y + min gy,z ≤fx,y + gy,z\n                                    x,y          y,z\n\nBut we can do better!\n\nNote that\n                                 fx,y = Exfx,y + (fx,y −Exfx,y)\n\nSuppose that fx,y varies much less over x than it does over y, and much less than gy,z varies over\neither of y and z. This will make the following bound a good approximation, though the bound is\nsound even without this assumption. We can write\n\n                     fx,y + gy,z ≥minx,y,z[fx,y + gy,z]\n                 = x,y,z[Exfx,ymin    + gy,z + fx,y −Exfx,y]\n                   ≥minx,y,z[Exfx,y + gy,z] + x,y,z[fx,ymin    −Exfx,y]\n                 = min[Exfx,y + gy,z] + min[fx,y −Exfx,y]\n                                         y,z                      x,y\n\nBy averaging the variation over certain axes, we have\n\n\n                                       46\n\nTheorem 18 (Mean+Diff).\n             min fx,y + gy,z ≥min[Exfx,y + gy,z] + min[fx,y −Exfx,y]\n                     x,y,z                  y,z                      x,y\n            max fx,y + gy,z ≤max [Exfx,y + gy,z] + max [fx,y −Exfx,y]\n                     x,y,z                  y,z                       x,y\n\nand the RHSs can be computed in time O(nxny + nynz + nxnz) for nx, ny, and nz the number of\npossible values of x, y, and z, respectively.\n\nExample for how this helps with small variation:\n\nTake any function k(y) and then take\n\n                                     fx,y := k(y) + ε1(x, y)\n                                       gy,z := −k(y) + ε2(y, z)\n\nThen we have\n                            x,y,z[fx,ymin   + gy,z] = x,y,z[ε1(x,min      y) + ε2(y, z)]\n                min fx,y + min gy,z = min k(y) + min −k(y) + min ε1(x, y) + min ε2(y, z)\n                         x,y          y,z          y           y              x,y               y,z\n                      = min k(y) −max k(y) + min ε1(x, y) + min ε2(y, z)\n                                               y           y            x,y               y,z\nmin[fx,y −Exfx,y] + min[gy,z + Exfx,y] = min ε1(x, y) + min[ε2(y, z) + Exε1(x, y)]\n x,y                       y,z                      x,y               y,z\n\nIf ε1 and ε2 are small compared to miny k(y) −maxy k(y), then using Exfx,y gives a much better\nbound.\n\nNote, though, that this could be a worse bound if the assumption of small variation does not hold.\nNote also that this trick is not restricted to adding and subtracting Exfx,y. If f is a matrix indexed\nby x and y, we might also try taking SVD and using the first principal component instead. A basic\napplication of the triangle inequality gives the following, more general, result:\nTheorem 19 (Summarize+Diff). For any hy which can be computed in time O(nh),\n                 min fx,y + gy,z ≥min[hy + gy,z] + min[fx,y −hy]\n                          x,y,z                  y,z                 x,y\n               max fx,y + gy,z ≤max [hy + gy,z] + max [fx,y −hy]\n                          x,y,z                  y,z                  x,y\n\nand the RHSs can be computed in time O(nxny + nynz + nh) for nx, ny, and nz the number of\npossible values of x, y, and z, respectively.\n\nWe see that if the variation of f in the x-axis is indeed much smaller than the variation in the y-axis,\nthen letting\n                                          fx,y = hy + εx,y\nwe get\n\n                 min fx,y + gy,z −min[hy + gy,z] −min[fx,y −hy]\n                           x,y,z                  y,z                 x,y\n\n            ≤  x,y,z[fx,ymin   + gy,z] −miny,z [hy + gy,z] + minx,y [εx,y]\n\n            ≤2 max |εx,y|\n                               x,y\n\nso indeed this bound isn’t too much worse and we are able to compute it in quadratic rather than\ncubic time.\n\nG.2  Details of SVD of QK proof\n\nAs discussed in Section 4.3.1, to further reduce the computation cost of proof, we need to avoid\ncomputing the residual stream, EVOU, and EPQKE matrices fully. Using mechanistic insight or\notherwise, we observe that these matrices (apart from EVOU) can be well-approximated by rank one\nmatrices. This will remove the dominant computation cost of O(dvocab2 · dmodel).\n\n\n                                       47\n\nG.2.1  Comments on relationship between mechanistic insight and proof size\n\nUp to this point, we haven’t really said much in our proofs about what the model is doing. All the\nmechanistic insight has been of the form “the model varies more along this axis than this other axis”\nor “the input data is distributed such that handling these inputs is more important than handling these\nother inputs” or, at best, “the model computes the answer by attending to the maximum token of the\nsequence; everything else is noise”.\n\nHere, finally, our proof-size constraints are tight enough that we will see something that we could\nplausibly call “how the model pays attention to the maximum token more than anything else”, i.e., (if\nwe squint a bit) “the model pays more attention to larger tokens in general.\n\nG.2.2  The max row-diff trick\n\nAs stated above, we are breaking matrices into their rank one approximation and some error term. To\nbound the error, i.e. to bound expressions of the form Qi(Ai + Ei) −Qi Ai, where Ei denote the\nmatrix errors, we can use the following trick:\nLemma 20 (Max Row-Diff (vector-matrix version)). For a row vector a and a matrix B,\n               max ((aB)i −(aB)j) ≤ X |ak| max (Bk,i −Bk,j)\n                                  i,j                                             i,j\n                                               k\n\nMoreover, for a collection of n row vectors Ar, if the shape of B is m × p, the right hand side can be\ncomputed for all r in time O(nm + mp).\n\nProof.\n                 max (aB)i −(aB)j\n                                       i,j\n              = max X ak (Bk,i −Bk,j)\n                                           i,j\n                                 k\n              ≤ X max ak (Bk,i −Bk,j)\n                                                i,j\n                             k\n                                  maxi,j (Bk,i −Bk,j)   if ak ≥0              = X ak\n                                    mini,j (Bk,i −Bk,j)    if ak < 0\n                             k\n                                  maxi,j (Bk,i −Bk,j)      if ak ≥0              = X ak\n                             −maxi,j (Bk,i −Bk,j)   if ak < 0\n                             k\n              = X |ak| max (Bk,i −Bk,j)\n                                                      i,j\n                             k\n\nThe asymptotic complexity of computing the result follows from caching the computation of\nmaxi,j (Bk,i −Bk,j) for each k independently of r, as the computation does not depend on Ar.\n\nTheorem 21 (Max Row-Diff). For matrices A and B,\n           max ((AB)r,i −(AB)r,j) ≤max X |Ar,k| max (Bk,i −Bk,j)\n                         r,i,j                              r                    i,j\n                                                   k\n\nProof. By taking the max of Lemma 20 over rows r of A.\n\nLemma 20 can also be applied recursively for a product of more than two matrices.\nLemma 22 (Max Row-Diff (vector-matrix recursive version)). For a row vector a and a sequence of\nn matrices Bp of shapes rp × cp,\n       !       ! \nmax   a Y Bp  −  a Y Bp  X |ak0| · · · X (Bn−1)kn−1,kn max ((Bn)kn,i −(Bn)kn,j)  i,j             ≤                                                   i,j\n            p         i        p        j       k0          kn\n\nMoreover, for a collection of q row vectors Aα, the right hand side can be computed for all α in time\nO(qr0 + Pp rpcp).\n\n\n                                       48\n\nProof. We proceed by induction on n.\n\nFor n = 1, the statement is identical to Lemma 20.\n\nSuppose the theorem holds for all positive n = s; we show the theorem holds for n = s + 1. We\nreassociate the matrix multiplication as\n\n      s+1  !       s+1  ! \nmax   a Y Bp  −  a Y Bp  i,j               \n          p=1        i      p=1       j\n         s+1  !     s+1  ! \n= max    Y Bp  − Y Bp       i,j (aB1)             \n                  p=2        i    p=2       j\n\nUsing the induction hypothesis gives\n\n≤ X X ak0(B1)k0,k1 X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j\n                                                                                            i,j\n    k1   k0                 k2                ks+1\n\nThe triangle inequality gives\n\n≤ X X |ak0(B1)k0,k1| X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j\n                                                                                            i,j\n    k1  k0                  k2                ks+1\n\nand algebra gives\n\n= X |ak0| X |(B1)k0,k1| X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j\n                                                                                              i,j\n    k0        k1              k2                ks+1\n\nThe asymptotic complexity of computing the right hand side also follows straightforwardly by\ninduction.\n\nTheorem 23 (Max Row-Diff (recursive)). For a sequence of n + 1 matrices A0, ..., An,\n\n      !       ! \nmax\n r,i,j    Y Ap   − Y Ap  ≤maxr X |(A0)r,k0| · · · X (An−1)kn−1,kn maxi,j  ((An)kn,i −(An)kn,j)\n          p         r,i      p         r,j            k0               kn\n\nProof. By taking the max of Lemma 22 over rows r of A0.\n\nNote that Theorem 21 is compatible with the mean+diff trick of Appendix G.1.\nTheorem 24 (Combined Mean+Diff and Max Row-Diff). For matrices A and B, and any column-\nwise summary vector Hk of A (for example we may take Hk := ErAr,k)\n\n                              !\nmax ((AB)r,i −(AB)r,j) ≤  max X Hk (Bk,i −Bk,j) +max X |Ar,k −Hk| max (Bk,i −Bk,j)\n r,i,j                                         i,j                                r                             i,j\n                                    k                            k\n\nProof.\n\n       max ((AB)r,i −(AB)r,j)\n                r,i,j\n      = max X Ar,k (Bk,i −Bk,j)\n                    r,i,j\n                   k\n      = max X (Hk + (Ar,k −Hk)) (Bk,i −Bk,j)\n                    r,i,j\n                   k\n                                         !\n      = max X Hk (Bk,i −Bk,j) + max X (Ar,k −Hk) (Bk,i −Bk,j)\n                     i,j                                 r\n                     k                           k\n\n\n                                       49\n\n                      !\n      ≤  max X Hk (Bk,i −Bk,j) + max X max (Ar,k −Hk) (Bk,i −Bk,j)\n                        i,j                                 r           i,j\n                     k                             k\n                      !\n      ≤  max X Hk (Bk,i −Bk,j) + max X |Ar,k −Hk| max (Bk,i −Bk,j)\n                        i,j                                 r                             i,j\n                     k                             k\n\n\nTheorem 25 (Combined Mean+Diff and Vector-Matrix Recursive Max Row-Diff). For a row vector\na, a vector of summaries h corresponding to a (for example, if a is a row of a matrix, h might be the\naverage of the rows), a sequence of n matrices Bp of shapes rp × cp, and a corresponding sequence\nof column-wise summary vectors hp of Bp (for example we may take (hp)k := Er(Bp)r,k),\n        !       ! \n  max   a Y Bp  −  a Y Bp     i,j               \n              p         i        p        j\n                                 !\n  ≤max X hk0 · · · X(Bn−1)kn−1,kn ((Bn)kn,i −(Bn)kn,j)\n         i,j\n              k0         kn\n                                     !\n   + X |ak0 −hk0| · · ·   max X(hn−1)kn ((Bn)kn,i −(Bn)kn,j)\n                                               i,j\n          k0                        kn\n                                                !\n                + X (Bn−1)kn−1,kn −(hn−1)kn max ((Bn)kn,i −(Bn)kn,j)\n                                                                                                 i,j\n                               kn\n\nMoreover, for a collection of q row vectors Aα, the right hand side can be computed for all α in time\nO(qr0 + Pp rpcp).\n\nProof sketch. Apply the triangle inequality recursively, fusing the proofs of Lemmas 22, and 24.\n\nTheorem 26 (Combined Mean+Diff and Recursive Max Row-Diff). For a sequence of n+1 matrices\nA0, . .., An, and corresponding column-wise summary vectors h0, ..., hn−1 of A0, ..., An−1,\n       !       ! \nmax Y Ap   − Y Ap r,i,j              \n           p         r,i      p         r,j\n                                  !\n≤max X(h0)k0 · · · X(An−1)kn−1,kn ((An)kn,i −(An)kn,j)\n     r,i,j\n            k0           kn\n                                        !\n  + X |(A0)k0 −(h0)k0| · · ·   max X(hn−1)kn ((An)kn,i −(An)kn,j)\n                                                     i,j\n        k0                              kn\n                                                   !\n                  + X (An−1)kn−1,kn −(hn−1)kn max ((An)kn,i −(An)kn,j)\n                                                                                                       i,j\n                                    kn\n\nMoreover, if the matrices Ap have shapes rp × cp, the right hand side can be computed in time\nO(Pp rpcp).\n\nProof. By taking the max of Theorem 25 over rows r of A0.\n\nG.2.3  Exploring rank one approximation via SVD\n\nLet us first look at\n                     EQKE := EqQKT ¯ET .\n\nFrom Figure 9a, we see that there is not much variation along long query token direction. We can\nconfirm this by performing a singular value decomposition (SVD) on EQKE, as seen in Figure 14.\n\n\n                                       50\n\n                      Query-Side SVD                                         Singular Values                                      Key-Side SVD\n     0                                                                                              0\n                                                                                                                                                   20\n                                                  7000\n    10                                                                                             10                                              15\n                                                  6000\n                                                                                                                                                   10\n    20                                                                                             20\n  Token 30                                            50004000                                                                                                          Token 30                                              50                                                                                                                     Value\n  Query 40                                            3000                                                               Key 40                                    −5\n                                                  2000                                                                               −10\n    50                                                                                             50\n                                                                                                                            −15\n                                                  1000\n    60                                                                                             60                                       −20\n                                                     0\n       0     10    20    30    40    50    60           0    10    20    30    40    50    60           0     10    20    30    40    50    60\n                         Singular Index                                          Singular Index                                          Singular Index\n\nFigure 14: SVD of EQKE for seed 123, with principal component vectors scaled by the square root of the\ncorresponding singular value. This scaling allows us to see visually that there is not much going on beyond the\nfirst singular component. Numerically: the first singular value is just over 7440, while the second singular value\nis just under 15.\n\n\nThe first singular value is just over 7440 (7800 ± 380 across all seeds), while the second singular\nvalue is just under 15 (13.1 ± 2.8 across all seeds). The ratio across all seeds is 620 ± 130. There’s\nreally not much going on here beyond the first singular component.23\n\nCall the first singular component of EQKE the “query direction” dq and the “size direction” dk on\nthe query-side and key-side, respectively.\n\nThere are two ways that we can decompose EQKE into a low-rank component that we can compute\nexactly, and a full-rank error term that we approximate bounds for.\n\nG.2.4  The simple SVD decomposition of QK\n\nIn time O(dvocabdmodel2) we can perform SVD on each of the four component matrices Eq, Q, K,\n¯E and perform low-rank SVD on the matrix product EqQKT ¯ET .\n\nWe can then bound the difference between two elements in the same row of EQKE by computing\nexactly the difference between the two elements in the same row of the rank one approximation of\nEQKE, and adding to that a bound on the difference between the two elements in the same row of\nthe error term.\nThat is, we can decompose E into a part parallel to dq and a part orthogonal to dq, say Eq = Eq +E⊥q ,\nand similarly ¯E = Ek + E⊥k . Note that Eq and Ek are both rank one, and hence can be multiplied\nwith other matrices of shape dmodel × a in time O(dmodela) rather than time O(dvocabdmodela).\nHence we can define EQKE_err1 (subscript one for “rank one”) and decompose EQKE as\n\n                EQKE = EqQKT (Ek)T + EQKE_err1.\n\nDefine for any vector v\n\n                                    ∆i,jv := vi −vj\n\nso that we get\n\n               ∆i,j(EqQKT (Ek)T )tquery + min ∆i,j(EQKE_err1)tquery\n                                                                  i̸=j\n                              ≤∆i,jEQKEtquery ≤\n               ∆i,j(EqQKT (Ek)T )tquery + max ∆i,j(EQKE_err1)tquery\n                                                                  i̸=j\n\nThen we may use any method we please to pessimize ∆i,j(EQKE_err1)tquery quickly. For example,\nsince for any matrix M we have σ1(M) = supx ∥Mx∥/ ∥x∥, considering vectors with one 1, one\n\n  23We might be tempted to keep analyzing the SVD, and notice that the query direction is mostly uniform,\nwhile the key direction is monotonic (nearly linear, even). But the proof complexity doesn’t demand this level of\nanalysis, yet, and so we can’t expect that any automated compact proof discovery system will give it to us.\n\n\n                                       51\n\n                                   0                                                   1.00\n\n                                                                                                                                     0.75\n\n                                                                                                                                     0.50\n                                                                       20\n                                                                                                                                                                         token                                                       0.250.00\n                                                                                                                                                                         query 40                                            −0.25\n\n                                                                                                                   −0.50\n\n                                                                                                                   −0.75\n                                                                       60\n                                                                                                                   −1.00\n                                                                         0     10    20    30    40    50    60\n                                                                                              key token\n\n                         Figure 15: The error term EQKE_err for seed 123.\n\n\n−1, and zero elsewhere, the maximum difference between elements in a row upper bounded by√\n  2σ1(M):\n                          √\n                       ∆i,jEQKE_err1tquery ≤  2σ1(EQKE_err1)                     (11)\n\nG.2.5  The complicated SVD decomposition of QK\n\nWhile the “most mechanistic” interpretation would proceed with the analysis in terms of Eq and\nEk, perhaps decomposing them further, we can get more bang for our buck by extracting out all the\nlow-rank structure available E, Q, and K, so as to make our error bounds as tight as possible.\nTo this end, we perform SVD on E⊥q , E⊥k , Q, and K and peel off the first singular components so as\nto get the decomposition\n                           Eq = Eq + Eq,2 + E⊥q,2\n                                         ¯E = Ek + Ek,2 + E⊥k,2\n                   Q = Q0 + Q⊥\n                  K = K0 + K⊥\n\nThen EQKE, a product of these four matrices, can be expressed as a sum of 2232 −1 = 35\nrank one products and one high-rank error term. We can compute the sum of the rank one\nproducts in time O(dvocab2) and express EQKE as, say, EQKE2 + E⊥q,2Q⊥(E⊥k,2K⊥)T .  Call\nthe second term EQKE_err (Figure 15). We must now bound for each q and m the quantity\nmaxi≤m−G EQKE_err[q, i] −EQKE_err[q, m].\n\nHow big is this?\n\nEven if we relax to maxi,j EQKE_err[q, i] −EQKE_err[q, j], the maximum such value across all\nrows is under 1.85 (1.99 ± 0.68 across all seeds). And the rows don’t have any particular structure to\nthem; the maximum absolute element of the entire matrix is just barely over 1 (1.12 ± 0.40 across all\nseeds), so doubling that doesn’t give too bad an estimate.\n\nBut we somehow need to compute this value without multiplying out the four matrices.\n\nOne option is to try to use singular value decomposition again. Since σ1(M) = supx ∥Mx∥/ ∥x∥,\nconsidering vectors with one 1, one −1, and zero elsewhere, the maximum difference between                 √\nelements in a row upper bounded by  2σ1(M). The largest singular value of EQKE_err (Figure 16)\nis just under 7.6 (8.4 ± 2.0 across all seeds), giving a row-diff bound of about 10.7 (11.8 ± 2.8 across\nall seeds), which is large but not unusably so.\n\nIf we perform SVD before multiplying out the matrices (Figure 17), however, their first singular                                              √\nvalues are about 4, 1.4, 1.4, and 4, giving a product of about 30, which when multiplied by  2\nis about 43. (Across all seeds, these numbers are 3.79 ± 0.12, 1.525 ± 0.067,√  1.513 ± 0.073, and\n3.78 ± 0.12, giving a product of about 33.1 ± 2.9, which when multiplied by  2 is about 46.8 ± 4.2.)\nThis works because σ1(AB) ≤σ1(A)σ1(B), but note that we can do factored SVD without needing\nto use this technique. This bound is still usable, but pretty big.\nCan we use Frobenius? Note that using anything close to this method to drop below dvocabdmodel2\nmight seem infeasible (it’ll eventually turn out not to be). For example, the best bound we know\non the largest singular value that can be verified even in the worst-case in strictly less time than\n\n\n                                       52\n\n                                 U                                      Singular Values for EQKE_err                           V\n                        0                                                                                        0\n\n                       10                                            7                                           10\n                                                                     6\n                       20                                                                                       20\n                                                                     5\n                       30                                            4                                           30\n\n                       40                                            3                                           40\n\n                       50                                            2                                           50\n\n                                                                     1\n                       60                                                                                       60\n                                                                     0\n                          0     10    20    30    40    50    60        0    10    20    30    40    50    60        0     10    20    30    40    50    60\n\n                            Figure 16: SVD of EQKE_err for seed 123.\n\n\n\n                         U                                         Singular Values for E⊥q,2                             V                                      U                                         Singular Values for Q⊥                              V\n            0                                                 4.0                                            0                                                    0                                                 1.4                                            0\n           10                                                 3.5                                            5                                                    5                                                 1.2                                            5\n           20                                                 3.0                                                                                                     10                                                   10                                                 1.0                                           10\n                                                                 2.5\n           30                                                 2.0                                           15                                                   15                                                 0.8                                           15\n           40                                                 1.5                                           20                                                   20                                                 0.6                                           20\n           50                                                 1.0                                           25                                                   25                                                 0.4                                           25\n                                                                                                                                                                                                                                        0.2                                                                 0.5\n           60\n\n               0     5     10    15    20    25    30                                                                 0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30           30  0     5     10    15    20    25    30                                                                                                                                                                                                                                        0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30\n\n                         U                                         Singular Values for K⊥                              V                                      U                                        Singular Values for E⊥k,2                             V\n            0                                                 1.4                                            0                                                    0                                                 4.0                                            0\n\n            5                                                 1.2                                            5                                                    5                                                 3.5                                           10\n           10                                                 1.0                                           10                                                   10                                                 3.0                                           20\n           15                                                 0.8                                           15                                                   15                                                 2.52.0                                           30\n           20                                                 0.6                                           20                                                   20                                                 1.5                                           40\n           25                                                 0.4                                           25                                                   25                                                 1.0                                           50\n                                                                 0.2\n           30\n\n               0     5     10    15    20    25    30                                                                 0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30           30  0     5     10    15    20    25    30   0.50.0   0     5    10    15    20    25    30    60  0     5     10    15    20    25    30\n\n   Figure 17: SVD of the four component matrices of EQKE_err for seed 123. Matrices look like noise.\n\n\nit takes to compute the full SVD is the Frobenius norm, which is defined as tr(MM T ), can be\ncomputed in dmodeldvocab time, and is equal to the square root of the sum of the squares of the\nsingular values. While the Frobenius norm of EQKE_err is only about 12 (giving a bound of about\n17 on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over 10, 4, 4,\nand 10, giving a product of 1932 and a bound of 2732(!). (Across all seeds, the Frobenius norm of\nEQKE_err is about 13.1 ± 1.9 (giving a bound of about 18.6 ± 2.7 on the row-diff), the Frobenius\nnorms of the four multiplicand matrices are a bit over 9.92 ± 0.19, 4.43 ± 0.01, 4.361 ± 0.095, and\n9.85 ± 0.19, giving a product of 1888 ± 99 and a bound of 2670 ± 140.) This is unusably large.\n\nHowever, we can get a much better bound on the max row-diff of EQKE_err without having to\nmultiply out all four matrices. We can use an approach vaguely similar to the mean+diff trick, as\nfollows.\n\nIf we want to compute the max row-diff of a product of matrices AB, we can compute by Theorem 21\n\n           max ((AB)r,i −(AB)r,j) ≤max X |Ar,k| max (Bk,i −Bk,j)             (12)\n                         r,i,j                              r                    i,j\n                                                   k\n\nor by combining this approximation with Theorem 18 via Theorem 24 we may compute\n\n     max ((AB)r,i −(AB)r,j)\n           r,i,j\n                      !\n    ≤  max X ErAr,k (Bk,i −Bk,j) + max X |Ar,k −ErAr,k| max (Bk,i −Bk,j)\n                   i,j                                     r                                  i,j\n                 k                                 k\n\ntaking whichever bound is better.\n\nThe first gives us a bound of 7.94 on the maximum row-diff, which is better than we can get by doing\nSVD on the product of the matrices! We can get an even better bound by peeling off the first two\nsingular values of all four matrices before multiplying them; this gives us a bound of 5.67. Combining\nit with the avg+diff trick wouldn’t give us much (8.05 and 5.66 respectively), as we’ve effectively\nalready done this by peeling off the leading singular contributions; the mean of EQKE_err over\ndimension zero has norm 0.025 (0.030 ± 0.012 across all seeds).\n\nAlthough this error bound is no longer the leading asymptotic bottleneck, we can peek ahead to what\nwe get if we want to be linear in parameter count. In this case, we can apply the recursive version of\nEquation 12 via Theorem 23, giving a bound of 97.06 on the maximum row-diff.\n\n\n                                       53\n\nThe mechanistic understanding we get here is roughly “for any given basis vector of the residual\nstream, the difference between the overlap of any two input tokens with this direction is small once\nwe factor out the first two singular components”, and this is sufficient to drive a low error term overall\nif we factor out the leading singular components in other places. We don’t mechanistically understand\nhow to combine the EqQKT (without multiplying them out) in a way that allows getting a good\nbound, though, which corresponds to our inability to drop below dvocabdmodel2 here.\n\nIf we use this trick on QK only, and use the mean+diff trick on final attention handling (without\nwhich we lose about 19 %), we can achieve a bound of 0.7840 (0.661 ± 0.035 across all seeds).\n\nIf we use this trick on the skip connection (EU) only, we can achieve a bound of 0.6768 (0.632±0.061\nacross all seed).\n\nUsing this trick on both EU and QK drops us down only to 0.6354 (0.601 ± 0.060 across all seeds).\n\nIf we use this trick on EU and use the recursive version of this trick on QK, we get a bound of 0.2927\n(0.281 ± 0.036 across all seeds).\n\nUnfortunately, it’s not clear how this trick would apply to EVOU. A fancier convex hull checking\nalgorithm seems required, and an analysis thereof is in progress.\n\nG.3  The algorithm\n\nWe now put all of these tricks together into the subcubic algorithm Algorithm 7, which is the full\nversion of Algorithm 6. The format we give here is parameterized over the summarization strategy\n(from Theorem 19 in Appendix G.1), the decomposition of EQKE, and the handling of EQKE_err\nand EU.\n\n\n\n\n\n                                       54\n\nAlgorithm 7 Counting Correct Sequences in Subcubic Time\n  1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗)\nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False\nRequire: 0 ≤g∗≤g ≤tmax\nRequire:  if c = 0 then tquery = tmax\n  2:    skip-scoret∗←SUMMARIZEEU,tquery(ℓEU(tquery)t∗)                  ▷Cache by t∗\n  3:     skip-score ←maxt∗ℓEU(tquery)t∗−mint∗ℓEU(tquery)t∗            ▷Cache by tquery\n  4:     vt ←EVOU(t)\n  5:    wi ←PVOU(i)\n  6:   ∆wmax,t∗←maxi wi,t∗−wi,tmax                           ▷Cache by tmax, t∗\n  7:    ∆wmax,max ←maxt∗∆wmax,t∗                               ▷Cache by tmax\n  8:    ∆vt ←maxt∗vt,t∗−mint∗vt,t∗                                 ▷Cache by t\n  9:    ∆vmax ←max0≤t≤tmax−g∗∆vt                           ▷Cache by tmax −g∗\n10:    ∆vtmaxt∗   ←vtmax,t∗−vtmax,tmax                                ▷Cache by tmax\n11:    ∆vtmaxmax ←maxt∗̸=tmax ∆vtmaxt∗                                  ▷Cache by tmax\n12:       if c = 0 then\n13:      ℓt∗←ℓEU(tmax)t∗+ vtmax,t∗+ ∆wmax,t∗\n14:        return maxt∗̸=tmax (ℓt∗−ℓtmax)\n15:    end if\n16:     b:,nctx−1 ←EQKP(tquery, nctx −1)                           ▷Cache by tquery\n17:     b0,:−1 ←SORT(EQKP(tquery, : −1))                         ▷Cache by tquery, i\n18:     b1,:−1 ←REVERSE(b0,:−1)\n19:    EQKE(1), EQKE_err ←DECOMPOSE(EQKE)\nRequire: EQKE(1)(tquery, t) −EQKE(1)(tquery, tmax) −EQKE_errtquery ≤EQKE(tquery, t) −\n   EQKE(tquery, tmax) ≤EQKE(1)(tquery, t) −EQKE(1)(tquery, tmax) + EQKE_errtquery\n20:     at ←EQKE(1)(tquery, t)                                  ▷Cache by tquery, t\n21:     amin,t ←min0≤t′′≤t at′′         ▷Cache by tquery, t, compute in amortized O(dvocab2)\n22:     amax,t ←max0≤t′′≤t at′′        ▷Cache by tquery, t, compute in amortized O(dvocab2)\n23:    ∆amax ←atmax −amin,tmax−g + EQKE_errtquery          ▷Cache by tquery, tmax, c\n24:    ∆amin ←atmax −amax,tmax−g −EQKE_errtquery          ▷Cache by tquery, tmax, c\n25:     idx-set ←{0, . . . , nctx −c −1} if tmax ̸= tquery else {0, . . . , nctx −c −2, nctx −1}\n26:     attn-weights-unscaled0,i ←b0,i + (∆amin if i ∈idx-set else 0)\n27:     attn-weights-unscaled1,i ←b1,i + (∆amax if i ∈idx-set else 0) ▷Cache by tquery, tmax, i,\n    c                           √\n28:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0/ √ d)    ▷Cache by tquery, tmax, i, c\n29:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1/  d)    ▷Cache by tquery, tmax, i, c\n30:     attn-max0 ←Pi∈idx-set attn-weights0,i\n31:     attn-max1 ←Pi∈idx-set attn-weights1,i\n32:     attn-max ←attn-max0 if ∆vtmaxmax ≥∆vmax else attn-max1\n33:     attn-max ←SUMMARIZEattn,tquery(attn-max)                   ▷Cache by tmax, c\n34:     attn-max′ ←attn-max −attn-max\n35:   summaryt∗←∆wmax,t∗+ skip-scoret∗+ attn-max∆vtmaxt∗  + (1 −attn-max)∆vmax    ▷\n   Cache by tmax, t∗\n36:    return skip-score + attn-max′ · ∆vtmaxmax + (−attn-max′)∆vmax + maxt∗̸=tmax summaryt∗\n37: end function\n\n\n\n\n\n                                       55\n\nH  Comparison of proof strategies\n\nIn this section, we compare the various proof strategies that we have developed in Appendix G. We\ndo some traditional mechanistic interpretability analysis to justify that the choices that we made could\nbe expected to lead to reasonably good bounds in Appendix H.1. We then compare the complexities\nand performance of various proof strategies in Appendix H.2 to line up with the legends of Figures 3,\nand 4. We close with a figure relating the various categories of proof strategies.\n\n\nH.1  Justification of pessimization choices\n\nIn Sections 4.3, F, and G we make a number of choices about which axes of variation are more or\nless important to track at various points in the bound computation.\n\nHere we do some more traditional mechanistic interpretability analysis to justify that the choices that\nwe made could be expected to lead to reasonably good bounds.\n\n\nH.1.1  Justifying the gap\n\nWe take advantage of the fact that attention is mostly monotonically increasing in input integers and\nthat for most sequences, the attentional contribution of the particular query token matters much more\nthan the particular non-max token in the sequence.\n\nWe justify this as follows.\n\nWe can look at the typical diff, when attending to the max token, between the largest non-max logit\nand the max logit. As shown in Figure 18a, the largest difference between an off-diagonal entry of\nEVOU and the diagonal of that row is typically at most −7.24 The typical worst contribution to the\nwrong logit from a non-max token (this is typical over non-max tokens, worst over choice of output\ntoken-logit index) is around 43, as shown in Figure 18b.\n\nThe difference in attention between tokens is approximately linear in the gap between the tokens, as\nseen in Figure 19. The slope of the line, that is, the difference in pre-softmax attention scores divided\nby the gap between the key token and the max token, is approximately 1.2.\n\nExponentiating, the post-softmax attention paid to the max is typically about 3× larger than to the\ntoken one below the max; here the logit difference between the max and non-max token is significant,\ntypically being around 13 (43/3) for the worst output logit. But by the time the gap is 3, this difference\nhas dropped to about 1.1, and by the time the gap is 4 it is around 0.3.\n\n   24“Typically” here means about 96 % of the time.\n\n\n\n               ·106\n          7\n          max\n          6                                                                    20.0\n                given                                                                       17.5          5\n             with                                                                       15.0\n          4\n                                                                                                                                                                            count 12.510.0          3                             sequences                                                                                     7.5\n   #          2\n   ×                                                                           5.0\n                count 1                                                                        2.5\n          0                                                                        0.0\n             −10   −5      0      5      10                          30  35  40  45  50  55  60  65  70\n                                     logit - diag                                                                           logit diff\n\n(a) The attention computation weighted by the number  (b) Histogram of the maximum difference between\nof sequences with the particular max. The computation  two logits contributed by a single row of EVOU.\nis maxj s.t. j̸=i EVOUi,j −EVOUi,i. µ ± σ: −9.9 ±  The computation is, for each i, maxh EVOUi,j −\n2.1; range: (−0 ± 120) × 10−1                   minj EVOUi,j. µ ± σ: 43.4 ± 9.0; range: 52 ± 20\n\n  Figure 18: Plots of the difference in logit for the attention computation, EVOU := ¯EV OU for seed 123.\n\n\n                                       56\n\n                                                                                           ·109\n                                                                        max 1.0\n           75\n                                                                                    0.8           50                                                                                                given\n           25                                                                             with 0.6                     difference\n            0\n                                                                                    0.4        −25                                                                                                                                                                              sequences\n                        #                   attention −50\n                                  ×  0.2\n        −75                                                                                                count                                                                                    0.0\n             −60  −40  −20   0    20    40    60                       −2      0       2       4       6\n                               token gap                                                    attention difference / gap\n                √                       √\n        (a) (EQKEi −EQKEj)/  d vs. i −j         (b) (EQKEi −EQKEj)/(  d(i −j)), weighted by\n                                                 sequence count. µ ± σ = 1.22 ± 0.13\n\nFigure 19: Plots of attention difference vs. token gap, for EQKE := EqQKT ¯ET for seed 123. The difference\nin attention between tokens is approximately linear in the gap between the tokens.\n\n\n                                                                   ·106\n\n                                                     6\n\n                                                     5\n\n                                                     4                                                                                                                                                                                                                              sequences\n                         # 3                         ×\n                                                                                                                            count 2\n\n                                                     1\n\n                                                     0\n                                                         1   2   3   4   5   6   7   8   9\n                                                                            gap\n\nFigure 20: Histogram of the minimum gap between the max token and the largest non-max token, for the seed\n123.\n\n\n\nSo for sequences where the largest non-max and the max are close together, the particular structure\nof the non-max EVOU matters a lot; but when the max is separated from the largest non-max by a\nmodest gap, the structure of the non-max EVOU does not matter so much.\n\nThe upshot is that to handle most sequences, we need only ask an oracle for the minimum gap g > 0\nbetween the max token tmax and largest non-max tokens t′ ̸= tmax, such that the model outputs the\ncorrect answer for all sequences where the non-max, non-query tokens have value at most tmax −g.\n\nWhile computing this gap may be expensive (and indeed the naïve computation of the oracle takes\nlonger than the brute-force proof—though it should be very easy to optimize), we don’t have to\npay the cost of computing the gap in the size of the proof, only the cost of storing the gap table\n(O(dvocab2nctx)) and of verifying the gap. Empirically, gaps are typically 1–5, as seen in Figure 20.\n\nIf we rely on the gaps, this results in leaving behind about 6.9 % of sequences.\n\nPicking up more sequences In this paragraph / bulleted list, we sketch out how we might go about\npicking up more sequences to get a tighter bound. This is not coded up, and is left as future work.\nWe propose computing the following quantities:\n\n        • First, we could build in time (O(dvocab2)) a table indexed on pairs (t, tmax) of the maximum\n        token and a non-maximum token: the table would store pessimal logit contributions from\n          t to maximum output tokens ≤the tmax parameter. The table could be further split to\n        pessimize separately for tokens within and outside of the gap window.\n\n        • Compute a table of pre-softmax attention differences between tokens t and t + 1 in time\n         (O(dvocab2)).\n\n        • Next sort the queries by overlap with the query direction.\n\n\n                                       57\n\n                                  E⊥q,2                                  Q⊥\n\n                         200                                                 120\n                                                   N(−0.00, 0.22)                                     N(−0.01, 0.14)\n                         175\n                                                                             100\n                         150\n                                                                              80\n                         125\n                                                        count 100                                                                                                                  count  60\n\n                          75\n                                                                              40\n                          50\n                                                                              20\n                          25\n\n                           0                                                   0\n                               −0.6 −0.4 −0.2  0.0  0.2  0.4  0.6  0.8            −0.4    −0.2      0.0       0.2       0.4\n                                               matrix element value                                        matrix element value\n\n                               K⊥                                  E⊥k,2\n\n                         120\n                                                   N(−0.00, 0.14)          200                            N(0.00, 0.22)\n\n                         100                                                 175\n\n                                                                             150\n                          80\n                                                                             125\n                                                        count  60                                                                                                                   count 100\n\n                          40                                                  75\n\n                                                                              50\n                          20\n                                                                              25\n\n                           0                                                   0\n                              −0.4    −0.2      0.0       0.2       0.4               −0.6 −0.4 −0.2  0.0   0.2   0.4   0.6   0.8\n                                               matrix element value                                        matrix element value\n\nFigure 21: The distribution of entries of the four residual matrices (after removing two principal components\nfrom Eq and ¯E and one principal component from Q and K). Distributions look pretty close to normal. Plots\nare for the seed 123.\n\n\n        • Compute for each number of queries handled (where we assume we handle all queries with\n         greater overlap than the current one) and for each maximum input token tmax, how many\n        of the query tokens tquery fall strictly below the max tmax (and whether or not the model\n        succeeds when tmax = tquery). This will tell us how many query tokens we can count for a\n        given maximum token.\n        • Compute a table indexed on pairs of # of queries handled and input tokens t which stores\n         the smallest difference in more attention paid to t + 1 than to t (O(dvocab2)).\n        • Compute a table indexed on pairs tmax, t storing an upper bound on amount more attention\n        paid to non-maximum tokens than to tmax by Oracle-permitted query tokens (the Oracle is\n        indexed only on tmax) (O(dvocab2)).\n        • For each # queries permitted: compute for each tmax, t, c, if the non-maximum token t\n         contributes little enough to incorrect logits that even with the worst skip connection the\n       model still gets the correct answer.\n\nH.1.2  Stopping after 1–2 principal components of QK\n\nDid we miss out on any structure in the error term of EQKE? The distribution of entries of the four\nmatrices looks pretty close to normal as seen in Figure 21.\n\n\n\n\n\n                                       58\n\n  Bound  1.0                                                                                             brute force (acc: 0.9992 ± 0.0015)\n     0.8                                                                                        cubic (rel acc: 0.9539 ± 0.0080)\n     0.6                                                                                      subcubic (rel acc: 0.821 ± 0.013)\n                                                                                                                               2 (rel acc: 0.795 ± 0.014)   Accuracy  0.4                                                                                             attention-dvocabdmodel\n                                                                                                         direct-quadratic (rel acc: 0.653 ± 0.060)\n     0.2\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.628 ± 0.060)\n     0.0                                                                                                attention-quadratic (rel acc: 0.390 ± 0.032)   Normalized                    225              231              237              243                     attention-quadratic, direct-quadratic (rel acc: 0.286 ± 0.036)\n                         FLOPs to Verify Proof (approximate)\n  Bound  1.0                                                                                             brute force (acc: 0.9992 ± 0.0015)\n     0.8                                                                                        cubic (rel acc: 0.9539 ± 0.0080)\n     0.6                                                                                      subcubic (rel acc: 0.821 ± 0.013)\n                                                                                                                               2 (rel acc: 0.795 ± 0.014)   Accuracy  0.4                                                                                             attention-dvocabdmodel\n                                                                                                         direct-quadratic (rel acc: 0.653 ± 0.060)\n     0.2\n                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.628 ± 0.060)\n     0.0                                                                                                attention-quadratic (rel acc: 0.390 ± 0.032)   Normalized                    225              231              237              243                     attention-quadratic, direct-quadratic (rel acc: 0.286 ± 0.036)\n                         FLOPs to Verify Proof (approximate)\n\nFigure 22: Recreations of Figure 3 for ease of viewing of the legend. Top is a strict recreation; bottom includes\npoints not on the Pareto frontier.\n\n\nIf we replace the entries of E⊥q,2, E⊥k,2, Q⊥, and K⊥with randomly sampled values, we get (sample\nsize 100) that the maximum row-diff of the product of the matrices is approximately 1.31 ± 0.13\n(sampling without replacement from the empirical distribution) or 1.31 ± 0.14 (sampling from the\nnormal distribution). So in fact our max row-diff is unusually high (by about 4σ).25\n\nH.2  How various combinations of tricks perform\n\nRecall Figures 3 and 4 on page 8 and on page 9 from Section 5, recapitulated here without captions\nfor convenience as Figures 22, and 23.\n\nWe describe what each subcubic proof strategy in the legend means. Note that all subcubic proof\nstrategies (that is, all proof strategies except for “brute force” and “cubic”) use the quadratic counting\nalgorithm of Appendix F.\n\nH.2.1  Proof strategies grouped by complexity\n\nIn Figures 3, and 22, proof strategies are grouped by computational complexity.\n\nThe 102 proof strategies break down into 1 + 1 + 2 × 5 × 10 × 2 strategies.\n\nThe brute force and cubic proofs (1 + 1) were fully covered in Appendices D, and E.\n\nThere are 5 options for handling EU:\n\ndirect-quadratic refers to handling EU in time O(dvocabdmodel) with either the max row-diff trick\n(Appendix G.2.2)26 or the max row-diff trick fused with mean+diff or some other summary statistic\n(Theorem 24)27.\nWhen direct is not mentioned, this indicates that we handle EU in time O(dvocab2dmodel) by first\nmultiplying out EqU and then either taking the maximum row-diff in each row28 or by taking the\nmaximum row-diff across all rows29. The latter is included purely for comparison’s sake, and never\ngives a tighter bound than the former.\n\nThere are 10 options for handling the high-rank attention error term EQKE_err:\n\n   25This shows up in the bias towards having larger values (both positive and negative) in the lower-right\ncorner of the plot, indicating that errors are larger for larger query and key values. We hypothesize that this is\ndue to the distribution of data: larger values are more likely to have more space between the maximum and\nnext-most-maximum token, so a bit of noise matters less for larger maxes than for smaller ones.\n   26This strategy is labeled “max_diff” in the Python source code.\n   27These strategies are labeled “mean_query+max_diff” and “svd_query+max_diff” in the Python source\ncode\n  28“max_diff_exact”\n  29“global_max_diff_exact”\n\n\n                                       59\n\n     1.0\n  Bound  0.8                                                                                           max-diff-exactmean+max-diff-subproduct\n                                                                                               max-diff-subproduct\n     0.6                                                                                        max-diff\n                                                                                       mean+max-diff   Accuracy\n     0.4                                                                                   svd\n                                                                                              mean+max-diff-subproduct-recursive\n     0.2                                                                                           max-diff-subproduct-recursive\n                                                                                                 mean-recursive+max-diff-subproduct-recursive   Normalized\n     0.0\n                400               600               800              1000\n                        EPQKE Singular Ratio: σ1/σ2\n\n                  Figure 23: Recreation of Figure 4 for ease of viewing of the legend.\n\n\nattention-quadratic refers to handling the high-rank attention error term EQKE_err from Ap-\npendix G.2.5 in time O(dvocabdmodel) either with the recursive max row-diff trick (Theorem 23)30 or\nwith the recursive max row-diff trick fused with the mean+diff trick either just on the query side31 or\nthroughout32 (Theorem 26).\nattention-dvocabdmodel2 indicates that we use one of the various O(dvocabdmodel2) strategies for\nhandling   √ EQKE_err1 from Appendix G.2.4 or EQKE_err from Appendix G.2.5. These include\nusing  2σ1—computed via low-rank SVD—as the bound (Equation 11)33, considering all ways of\nmultiplying out a subset of the matrices and taking the maximum row-diff of the resulting pair of\nmatrices34 (Theorem 21), or fusing the max row-diff trick with the mean+diff trick35 (Theorem 24).\n\nWhen attention is not mentioned, this indicates that we handle the attention error term in time\nO(dvocab2dmodel), either by taking the per-row maximum row-diff36 or by using the full rank EQKE\nmatrix and taking the per-row maximum row diff37.\n\nFinally, note that in combining the rank one attention computation with EVOU, PVOU, and EU, we\nmay either use the mean+diff trick38 (Appendix G.1) or not39; this makes up the final factor of 2.\n\nH.2.2  Proof strategies grouped by attention handling\n\nThis section slightly reorganizes the information just covered in Appendix H.2.1, for convenience\nof legend correspondence. Here we group by the strategy used to handle the attention error term.\nStrategies that involve using the full rank EQKE matrix are elided. The dashed descriptors here\ncorrespond to underscore-joined descriptors in footnotes of Appendix H.2.1.\nmax-diff-exact (O(dvocab2dmodel)) corresponds to taking the full rank EQKE_err1 term and taking\nthe maximum row-diff in each row.\n\nmean+max-diff-subproduct (O(dvocabdmodel)) corresponds to fusing the max row-diff trick with\nthe mean+diff trick (Theorem 24) and considering all ways of associating the multiplication of\nEQKE_err.\n\nmax-diff-subproduct (O(dvocabdmodel)) corresponds to using the max row-diff trick (Theorem 21)\nand considering all ways of associating the multiplication of EQKE_err.\nmax-diff (O(dvocabdmodel2)) corresponds to using the max row-diff trick (Theorem 21) on the\nfactored SVD of EQKE_err1.\nmean+max-diff (O(dvocabdmodel2)) corresponds to fusing the max row-diff trick with the mean+diff\ntrick (Theorem 24) and applying it on the factored SVD of EQKE_err1.\n\n  30“max_diff_subproduct_recursive”\n  31“mean+max_diff_subproduct_recursive”\n  32“mean_recursive+max_diff_subproduct_recursive”\n  33“svd”\n  34“max_diff” for EQKE_err1, “max_diff_subproduct” for EQKE_err\n  35“mean+max_diff” for EQKE_err1, “mean+max_diff_subproduct” for EQKE_err\n  36“max_diff_exact”\n  37“exact_EQKE+max_diff_exact”\n  38“mean_query+diff”\n  39“drop_average_query_per_output_logit_reasoning”\n\n\n                                       60\n\n                      √\nsvd (O(dvocabdmodel2)) corresponds to using  2σ1—computed via low-rank SVD—as the bound\n(Equation 11).\n\nmean+max-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the high-rank\nattention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick fused\nwith the mean+diff trick on the query-side only (Theorem 26, taking all but the first summary vector\nto be zero).\n\nmax-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the high-rank attention\nerror term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick (Theorem 23).\n\nmean-recursive+max-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the\nhigh-rank attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick\nrecursively fused with the mean+diff trick (Theorem 26).\n\nH.2.3  What understanding do we get from each proof strategy?\n\nThroughout most of this paper, we talk about doing mechanistic interpretability and using understand-\ning to allow more compact proofs to have tighter bounds. We can also look at the reverse problem:\nwe can take a collection of proof strategies, check by brute force which strategies give the tightest\nbounds for each model, and ask what this implies about how that model works. We do this here.\n\nIn general, which proof methods perform best is an indication of where structure exists in\nthe model.   For example,  in quadratic EU proofs, when max_diff performs worse than\nmean_query+max_diff and svd_query+max_diff, this indicates that E has a relatively strong\nbehavioral component shared across query tokens that U is not that good at filtering out. Sim-\nilarly, when, e.g., mean_recursive+max_diff_subproduct_recursive performs better than\nmax_diff_subproduct_recursive, this indicates that even after removing the first one or two\nprinciple components from Eq, Q, K, and ¯E, there is still enough common structure that it is worth\nfactoring out the mean behavior.\n\n\n\n\n\n                                       61\n\n## Claim to Classify\n\"Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices\"\nLocation in paper: body\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Low-rank approximation via SVD and error analysis",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "1. **Method Rung Classification (Rung 1 - Observational/Associational):**\n   - The method used here involves performing singular value decomposition (SVD) on matrices (e.g., EQKE) to derive low-rank approximations. This is primarily an observational technique, as it involves analyzing and decomposing model weights without intervening on the model's computations or parameters. The analysis of error terms (e.g., EQKE_err) also falls under observational methods, as it involves measuring and characterizing the residuals of the approximation.\n   - No causal interventions (e.g., activation patching, ablation) or counterfactual reasoning are employed in this specific claim.\n\n2. **Claim Rung Classification (Rung 3 - Mechanistic/Counterfactual):**\n   - The claim asserts that \"compounding structureless errors\" pose a challenge when making rank-1 approximations of constituent matrices. This implies a mechanistic understanding of how errors propagate and compound in the model's computations. The term \"compounding\" suggests a dynamic or causal process, and \"structureless errors\" refers to a specific property of the model's internal workings.\n   - The claim goes beyond associational or causal sufficiency (Rung 1 or 2) by attributing a specific *mechanism* (error compounding) to the observed phenomenon. It also implies a counterfactual aspect: if the errors were structured, the challenge would not exist.\n   - The use of terms like \"key challenge\" and the focus on the *process* of error propagation align with Rung 3 claims, which often describe unique or necessary mechanisms underlying model behavior.\n\n3. **Hedge Flag:**\n   - The claim is stated as a factual assertion (\"are a key challenge\") without hedging language (e.g., \"may,\" \"suggests,\" \"potentially\"). Thus, hedge_flag = 0.\n\n4. **Confidence:**\n   - Confidence is rated 4 because the claim is well-supported by the detailed analysis in Appendix G.2.5, where the authors explicitly discuss the challenges of bounding error terms in low-rank approximations. However, there is minor ambiguity in whether the term \"compounding\" strictly implies a mechanistic process or could be interpreted as a correlational observation."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 68268,
      "completion_tokens": 542,
      "total_tokens": 68810
    },
    "finish_reason": "stop"
  }
}