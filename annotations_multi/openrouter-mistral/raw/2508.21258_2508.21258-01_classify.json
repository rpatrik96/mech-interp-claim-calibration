{
  "timestamp": "2026-02-19T10:08:50.678261+00:00",
  "paper_id": "2508.21258",
  "claim_id": "2508.21258-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2508.21258\nTitle: RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching\nFull text:\n              RelP: Faithful and Efficient Circuit Discovery in\n                Language Models via Relevance Patching\n\n\n\n\n                          Farnoush Rezaei Jafari1,2∗        Oliver Eberle1,2       Ashkan Khakzar\n\n\n                                                         Neel Nanda\n\n                                           1Machine Learning Group, Technische Universität Berlin\n                                 2BIFOLD – Berlin Institute for the Foundations of Learning and Data2025\nOct                                            Abstract\n\n                               Activation patching is a standard method in mechanistic interpretability for lo-30\n                                 calizing the components of a model responsible for specific behaviors, but it is\n                              computationally expensive to apply at scale. Attribution patching offers a faster,\n                               gradient-based approximation, yet suffers from noise and reduced reliability in\n                               deep, highly non-linear networks. In this work, we introduce Relevance Patching\n                                 (RelP), which replaces the local gradients in attribution patching with propagation\n                                  coefficients derived from Layer-wise Relevance Propagation (LRP). LRP propa-[cs.LG]                          gates the network’s output backward through the layers, redistributing relevance to\n                                 lower-level components according to local propagation rules that ensure properties\n                             such as relevance conservation or improved signal-to-noise ratio. Like attribution\n                                patching, RelP requires only two forward passes and one backward pass, main-\n                                 taining computational efficiency while improving faithfulness. We validate RelP\n                                across a range of models and tasks, showing that it more accurately approximates\n                                   activation patching than standard attribution patching, particularly when analyzing\n                                 residual stream and MLP outputs in the Indirect Object Identification (IOI) task.\n                             For instance, for MLP outputs in GPT-2 Large, attribution patching achieves a\n                             Pearson correlation of 0.006, whereas RelP reaches 0.956, highlighting the im-\n                            provement offered by RelP. Additionally, we compare the faithfulness of sparse\n                                  feature circuits identified by RelP and Integrated Gradients (IG), showing that RelP\n                               achieves comparable faithfulness without the extra computational cost associated\n                              with IG. Code is available at https://github.com/FarnoushRJ/RelP.git.arXiv:2508.21258v2            1  Introduction\n\n                     Recent advances in machine learning continue to rely on transformer-based language models, which\n                      achieve remarkable performance across a wide range of tasks. Given their widespread adoption,\n                      understanding the internal mechanisms of these models is an important challenge for improving our\n                           ability to interpret, trust, and control them.\n\n                  To address this challenge, the fields of eXplainable Artificial Intelligence (XAI) [1, 2] and, more\n                          recently, mechanistic interpretability [3, 4] have emerged, aiming to reveal the decision-making pro-\n                        cesses of such complex architectures. Early efforts in XAI focused on developing feature attribution\n                     methods, often visualized as heatmaps, to highlight relevant features in classification models. As\n                           state-of-the-art models have grown more complex, approaches that extend beyond input explanations\n\n                          ∗Correspondence to: rezaeijafari@campus.tu-berlin.de\n\n\n                        39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Mechanistic Inter-\n                             pretability Workshop at NeurIPS 2025.\n\nFigure 1: Pearson correlation coefficient (PCC) between activation patching and attribution patching\n(AtP) or relevance patching (RelP), computed over 100 IOI prompts for three GPT-2 model sizes\n(Small, Medium, Large), two Pythia models (70M, 410M), two Qwen2 models (0.5B, 7B), and\nGemma2-2B. A higher value of PCC represents higher alignment with activation patching results.\n\n\n\n\nhave become increasingly important for uncovering their internal strategies. This has motivated\na shift toward exploring the complex internal structure of neural networks through methods such\nas concept-based interpretability [5, 6], higher-order explanations [7, 8, 9], and circuit discovery\n[3, 10]. Herein, a central goal of interpretability research is the reliable localization of specific\nmodel components responsible for particular functions, algorithms or behaviors [3, 4, 11, 12, 13]. To\nidentify components that are causally involved in inference, mechanistic interpretability has proposed\nactivation patching, also known as causal mediation analysis [14, 15, 16].  Activation patching\nreplaces the activations of selected components in an original input with those from a patch input,\nallowing direct causal testing of the contribution of a component to the model’s output.\n\nAs these patching-based interventions are computationally expensive, it is challenging to scale them,\nespecially for larger models. To improve efficiency, attribution patching [17] was introduced as a fast\ngradient-based approximation to activation patching. Using gradients to estimate the influence of each\ncomponent on the model’s output, attribution patching significantly reduces the computational cost\ncompared to traditional perturbation-based approaches. However, this efficiency comes at the expense\nof robustness and accuracy, particularly in deep models like state-of-the-art LLMs [18]. This issue\nis not unique to attribution patching. Many gradient-based attribution methods [19, 20, 21, 22, 23]\nsuffer from a lack of robustness in large networks, often due to noisy and unreliable gradients [24]\nthat undermine the quality of explanations [25, 26, 27].\n\nAlternative attribution methods have been proposed to overcome these limitations. One of the\nmost widely used methods is Layer-wise Relevance Propagation (LRP) [28], developed within the\ntheoretical framework of Deep Taylor Decomposition (DTD) [29]. LRP works by propagating the\nmodel’s output backward through the network, redistributing the relevance of each component to its\ninputs according to layer-specific propagation rules. These rules are designed to enforce desirable\nproperties such as conservation of total relevance, sparsity of explanations, and robustness to noise.\n\nBuilding on these insights, we propose Relevance Patching (RelP), a technique that replaces local\ngradients in attribution patching with propagation coefficients achieved using LRP. RelP enables\nmore faithful localization of influential components in large models, without sacrificing scalability.\nSimilar to attribution patching, RelP requires two forward passes and one backward pass. We\ndemonstrate the effectiveness of RelP in the context of the Indirect Object Identification task, showing\nthat it consistently outperforms attribution patching across a range of architectures and model sizes,\nincluding GPT-2 {Small, Medium, Large} [30], Pythia-{70M, 410M} [31], Qwen2-{0.5B, 7B}\n[32], and Gemma2-2B [33], with the strongest gains on the residual stream and MLP outputs. In\naddition, we apply RelP to recover sparse feature circuits responsible for Subject–Verb Agreement\nin the Pythia-70M model. Compared to Integrated Gradients [21], RelP demonstrates comparable\nfaithfulness in identifying meaningful circuits while also offering greater computational efficiency.\n\n\n                                       2\n\n2  Related Works\n\nActivation and Attribution Patching Techniques  Activation patching is a causal mediation\ntechnique that tests the role of model components by replacing activations from an original run with\nthose from a patch run [34, 35]. It has been widely used in interpretability studies [15, 36, 37, 38],\nwith variants such as causal tracing, which perturbs activations with Gaussian noise [16], and\npath patching, which extends interventions to multi-step computation paths [39, 40]. To improve\nefficiency, attribution patching (AtP) uses gradient-based attribution instead of full interventions,\nrequiring only two forward passes and one backward pass [17], and AtP* further enhances reliability\nwhile maintaining scalability [18]. Building on this line of work, we propose Relevance Patching\n(RelP), which also targets efficiency but replaces the local gradients in attribution patching with\npropagation coefficients computed using Layer-wise Relevance Propagation (LRP) [28], improving\nfaithfulness to activation patching while maintaining the computational advantages of AtP.\n\n\nCircuit Analysis   Circuit analysis methods aim to uncover subnetworks that drive particular model\nbehaviors. Activation patching, used in ACDC [41], identifies critical edges in the computation graph\nbut is computationally expensive. It has been scaled to large models [42] and adapted to study the\neffects of fine-tuning [43]. Sparse Autoencoders (SAEs) offer an alternative by learning disentangled,\ninterpretable latent features [44, 45]. Methods such IFR [46] and EAP [47] instead use gradient-based\nattributions to perform circuit discovery in a more efficient way. Finally, attribution-guided pruning\nwith LRP has been shown to support both circuit discovery and model compression [48]. Our\nproposed Relevance Patching (RelP) also builds on LRP but stays within the patching framework,\nproviding faithful and efficient attribution that approximates activation patching.\n\n\nGradient-Based and Propagation-Based Attribution Methods  Local gradients have long been\nused to explain the behavior of non-linear models [23]. Through backpropagation, they enable\nthe efficient computation of saliency maps [28, 49], which have been extensively studied in the\ncontext of vision models. Saliency maps derived from raw gradients [49] capture the sensitivity of\nmodel predictions to small perturbations in input features. When these gradients are scaled by the\ncorresponding input values, they yield Gradient×Input [22] explanations. Grad-CAM [20] aggregates\nclass-specific gradients over the final convolutional feature maps, yielding coarse but discriminative\nheatmaps. Since raw gradients are often noisy, SmoothGrad [19] averages saliency maps over random\nperturbations, PathwayGrad [50] finds a sub-network (pathway) critical for the output and propagates\ngradients through that pathway, and Integrated Gradients [21] integrates along a baseline-to-input\npath to mitigate saturation and enforce key attribution axioms. Beyond gradient-based methods,\npropagation-based approaches such as LRP [28] and DeepLIFT [51] redistribute the model’s output\nbackward through the network using local propagation rules, which can offer more faithful attribution.\nFor a comprehensive review of explanation methods and attribution techniques, see [1, 2, 52, 53].\n\n\n3  Background\n\nFrom Activation to Attribution Patching  Let M : X →RV be a decoder-only transformer model\nthat maps an input sequence x ∈X := {1, ..., V }T to a vector of output logits over a vocabulary\nof size V . We represent the model as a directed computation graph G = (N, E), where each node\nn ∈N corresponds to a distinct model component, and each edge (n1, n2) ∈E represents a direct\ncomputational dependency. The activation of a component n when processing input x is denoted\nn(x).\n\nLet D be a distribution over prompt pairs (xoriginal, xpatch), where xoriginal is a representative task\nsample that captures the behavior of interest, and xpatch serves as a reference input used to introduce\ncounterfactual perturbations for causal intervention. A metric L : RV →R quantifies the output\nbehavior of the model.\n\nThe causal contribution c(n) of a component n ∈N is defined as the expected effect of replacing its\nactivation in the original input with the corresponding activation from the patch input, i.e.,\n\n       c(n) := E(xoriginal,xpatch)∼D [L (M(xoriginal | do(n ←n(xpatch)))) −L (M(xoriginal))] .     (1)\n\n\n                                       3\n\nThis definition follows the causal intervention framework, where do(n ←n(xpatch)) denotes over-\nwriting the activation of n with its value under the patch input. Evaluating c(n) directly for all\nn ∈N is computationally prohibitive for large-scale models, motivating the development of efficient\napproximation methods such as attribution patching (AtP) [17]:\n\n                       \"                                      #\n      ˆcAtP(n) := E(xoriginal,xpatch)∼D  (n(xpatch) −n(xoriginal))⊤∂L(M(xoriginal))                    .   (2)\n                                               ∂n                                                                                                         n=n(xoriginal)\n\nAtP (Eq. 2) approximates the effect of replacing component n’s activation from the original input\nwith that from the patch input, without explicitly running the patched model, by leveraging gradients.\n\nLayer-wise Relevance Propagation (LRP)  Layer-wise Relevance Propagation (LRP) [28] is a\nframework for interpreting neural network predictions. The central idea is to assign a relevance\nscore R to each component or input feature, quantifying its contribution to the model’s output or to\nany metric defined from the output. These relevance scores are then propagated backward through\nthe network according to layer-specific rules. The propagation follows the principle of relevance\nconservation [28, 29], ensuring that the total relevance at each layer is completely redistributed to the\npreceding layer, ultimately attributing the chosen metric to the input features.\nTo formalize this process, let a(l−1)i    with i ∈{1, ..., n} denote the activations in layer l −1. These\nactivations serve as inputs to a layer represented by f (l)  : Rn →Rm, resulting in outputs a(l)j  ,\nj ∈{1, ..., m}. A local first-order Taylor expansion around a reference point ˜a(l−1) expresses each\noutput as\n                         (l)                                    ∂fj(l)\n              fj  (a(l−1)) ≈ X J(l)ji a(l−1)i  + ˜b(l)j  ,      J(l)ji =                       .\n                                                i                               ∂a(l−1)i       ˜a(l−1)\nHere, J(l)ji  is the local Jacobian, describing how activations a(l−1)i     influences outputs a(l)j  , while ˜b(l)j\ncollects constant terms and approximation errors.\n\nThis decomposition motivates the LRP redistribution step. Each component j in layer l is assigned\na relevance score R(l)j  , which is redistributed to its inputs in proportion to their contributions to\nthe activation of component j. Here, this redistribution from upper-layer activations to component\ni is expressed through a propagation coefficient ρi, which aggregates the contributions from all\nconnected upper-layer components j. The relevance of component i in layer l −1 is then given by\n                                   R(l−1)i   = a(l−1)i     ρi.\n\nThe propagation coefficient ρi depends on the local Jacobian J(l), the activations a(l−1), and the\nrelevance scores at layer l (i.e., R(l)). Intuitively, ρi acts as a filter that determines which parts of the\ninput activations are considered relevant.\n\nThe layer-wise redistribution of relevance can be repeated until the input layer is reached or stopped at\nan intermediate layer of interest. The procedure begins with an initial relevance signal R(L)c    , typically\ndefined using a metric computed from the model outputs, commonly the logit of the true, predicted,\nor any target class c at the output layer [28, 29]. Contrastive approaches have also been proposed,\nwhich explain the difference between class logits to highlight features that strongly influence changes\nin the model’s prediction [26, 54].\n\n4  RelP: Relevance Patching for Mechanistic Analysis\n\nGradient-based attribution methods have been extensively studied in the XAI literature [19, 20, 21, 22,\n23]. Despite their simplicity and model-agnostic applicability, these methods often face challenges\nwhen applied to large neural networks due to noisy and less reliable gradients [25, 27]. LRP mitigates\nthese shortcomings by propagating the network’s output M(x), or any metric L defined from the\noutput, backward through the network in a structured and theoretically grounded manner. Depending\non the layers in a given model architecture, different propagation rules have been proposed. These\nrules are typically derived from gradient analyses of model components [26, 29, 55], with specific\n\n\n                                       4\n\n    Propagation Rules for Transformers\n\n\n     Layer                  Propagation Rule          Implementation Trick\n     LayerNorm / RMSNorm   LN-rule [26]                  yi = [√ϵ+Var[x]]const.xi−E[x]\n    GELU / SiLU               Identity-rule [55]          x ⊙[Φ(x)]const.\n      Linear                       0-rule, ϵ-rule, or γ-rule [52]   –\n      Attention                AH-rule [26]                 yj = Pi xi[Aij]const.\n       Multiplicative Gate        Half-rule [55, 57]           0.5·(x⊙g(x))+0.5·[(x⊙g(x))]const.\n\n    Table 1: A summary of propagation rules for Transformer layers. Components marked with\n      []const. are treated as constants, typically using .detach() in PyTorch. In this table, x and y\n    denote the input and output of a layer, respectively, and A represents the attention weights.\n\n\n\n\nchoices guided by common desiderata for explanations, such as sparsity, reduced noise, and relevance\nconservation [52, 56].\n\nSpecialized propagation rules have been developed to handle diverse nonlinear components (e.g.,\nactivation functions, attention mechanisms, normalization layers) and architectural nuances across\ndeep network families [7, 8, 27, 52, 55, 57, 58]. An overview of relevant rules for transformer models\nis shown in Table 1.\n\nAs attribution patching relies on gradients to approximate the effect of substituting hidden activations\n(“patching in”), it is also susceptible to the noise and approximation errors inherent to gradient-\nbased attribution methods, particularly in very large models [24, 29]. To address this, we introduce\nRelevance Patching (RelP).\n\nFormally, RelP follows the structure of standard attribution patching (AtP). In AtP, the contribution of\na component is computed as the dot product between the change in its activation, caused by replacing\nthe original input with a patch input, and the gradient of the metric L with respect to that component,\nevaluated at the original input. RelP modifies this procedure by substituting the gradient term with\nthe LRP-derived propagation coefficient (introduced in Section 3) for that component. The resulting\ncontribution score is defined as\n\n\n                              h                                              i        ˆcRelP(n) := E(xoriginal,xpatch)∼D  (n(xpatch) −n(xoriginal))⊤ρ(L(M(xoriginal)))\n                                                                                                                              n(xoriginal)\n                              h                           i        = E(xoriginal,xpatch)∼D RRelP(L(M(xoriginal)))               ,                           (3)\n                                                                                           n(xoriginal)\n\nwhere ρ(L(M(xoriginal))) and RRelP(L(M(xoriginal))) denote, respectively, the propagation coeffi-\ncient and the relevance score for component n.\n\nRelP preserves the efficiency of attribution patching while improving faithfulness. Unlike Marks\net al. [44], which relies on Integrated Gradients [21] for more accurate approximations at higher\ncomputational cost, RelP provides scalable and faithful localization of influential components.\n\nPropagation Rules  Table 1 summarizes the propagation rules applicable to core components of\ntransformer architectures. As discussed in Section 3, relevance conservation is a key property in LRP,\nensuring that the sum of relevance scores is preserved across layers. This prevents relevance from\nbeing lost or artificially introduced during propagation. Many of the specialized rules in Table 1 are\ndesigned to address situations where this property could fail, and are often derived from gradient\nanalyses of individual model components.\n\nIn attention heads, attention outputs depend on inputs directly and via attention weights Aij, which\nare also input-dependent. Correlations between these terms can break conservation and the AH-rule\npreserves it by treating Aij as constants, effectively linearizing the heads [26]. LayerNorm’s centering\nand variance-based scaling can cause “relevance collapse”, which the LN-rule mitigates this by\ntreating (pϵ + Var[x])−1 as constant, allowing the operation to be seen as linear and preserving\n\n\n                                       5\n\n   IOI Templates\n\n\n          Template #   Sentence Template\n\n              1        Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A].\n              2       When, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A].\n              3        After [B] and [A] went to the [PLACE], [B] gave a [OBJECT] to [A].\n\n                    Table 2: Templates used for generating the IOI samples.\n\n\n\n\n\nconservation [26]. For multiplicative gates, the Half-rule splits relevance equally to avoid spurious\ndoubling [55, 57]. Activation functions (e.g., GELU, SiLU) can disrupt conservation since their output\nis a nonlinear transformation of the input, the Identity-rule addresses this by treating the non-linear\ncomponent as constant [55]. For linear layers, the 0-rule is equivalent to Gradient ×Input [22]. The\nϵ-rule and γ-rule extend the 0-rule, the ϵ-rule is applied when sparsity and noise reduction are desired,\nwhile the γ-rule is used when it is preferable to emphasize positive contributions over negative ones\n[52]. For additional rules specifically tailored to transformers, we refer the readers to [26, 27, 59].\n\n\n5  Experiments\n\n\nWe evaluate the effectiveness of RelP by benchmarking it against attribution patching (AtP), on the\nIndirect Object Identification (IOI) task, across three GPT-2 variants (Small, Medium, and Large), two\nPythia models (70M and 410M), two Qwen2 models (0.5B and 7B), and Gemma2-2B. Furthermore,\nwe assess RelP’s ability to recover sparse feature circuits underlying Subject–Verb Agreement in the\nPythia-70M model, comparing its performance against Integrated Gradients (IG) as a more accurate\ngradient-based baseline. Additional details of our experimental setup are provided in Appendix A.\n\n\n5.1  Evaluating RelP on the IOI Task\n\nThe objective of this experiment is to evaluate how well attribution patching (AtP) and relevance\npatching (RelP) approximate the ground-truth effects captured by activation patching (AP) on the\nIOI task. This task involves determining which entity in a sentence functions as the indirect object,\ntypically the recipient or beneficiary of the direct object.\n\nTo enable controlled evaluations, we construct 100 prompt pairs. Each pair consists of an original\nprompt (with the correct indirect object) and a patch variant in which the subject and indirect object\nare swapped, keeping all other tokens the same. All prompts are generated from structured templates\n[10] (Table 2) to ensure consistency across examples. We define the metric L as the difference in\nlogits between the original and patch targets. The methods RelP, AtP, and AP are each applied to\nthe residual stream, attention outputs, and MLP outputs. Since the resulting attribution scores are\ncomputed across hidden dimensions, we aggregate them by summing over these dimensions. To\nquantify how well AtP and RelP approximate AP, we compute the Pearson correlation coefficient\n(PCC) between their results and those of AP.\n\nThe results of this experiment are presented in Figure 1. As shown, RelP consistently outperforms AtP\nin a range of architectures and model sizes, including GPT-2 {Small, Medium, Large}, Pythia-{70M,\n410M}, Qwen2-{0.5B, 7B}, and Gemma2-2B. The performance gap is especially pronounced in\nthe MLP output and residual stream analyses. Detailed numerical results are listed in Table 5 in\nAppendix B.\n\nQualitative differences between AtP and RelP are also visible in Figure 2. Nanda [17] suggests\nthat attribution patching fails notably in the residual stream, primarily due to its large activations\nand the nonlinearity introduced by LayerNorm, which significantly disrupts the underlying linear\napproximation.  It also performs poorly on MLP0, since this layer in GPT-2 Small functions as\nan “extended embedding”, and Nanda [17] has shown that linear approximations in this layer may\nbecome unstable, leading to misleading insights. As can be seen in Figure 2, the RelP results align\n\n\n                                       6\n\nFigure 2: Qualitative comparison showing how accurately relevance patching (RelP) and attribution\npatching (AtP) approximate the effects of activation patching in GPT-2 Small. RelP shows notably\nbetter alignment in the residual stream and at MLP0, where AtP’s estimates are less reliable.\n\n\n\nmore closely with those from activation patching, especially when considering the residual stream\nand MLP outputs (e.g., MLP0). Further qualitative results are provided in Appendix C.\n\nOverall, we find that the proposed method, RelP, consistently achieves strong correlations with\nactivation patching and is often clearly superior to standard attribution patching. This underscores\nthe effectiveness of the tailored propagation rules in the LRP framework, which provide a reliable\nand computationally efficient approximation of the computationally expensive activation patching\nmethod across models and components commonly studied in mechanistic interpretability research.\n\n\n5.2  Sparse Feature Circuits for Subject-Verb Agreement\n\nSparse feature circuits are small, interconnected groups of interpretable features that jointly drive\nspecific behaviors in language models. Analyzing these circuits allows us to understand how models\ncombine meaningful components to solve tasks, offering insight into the underlying mechanisms\nbehind their decisions. In this experiment, we aim to uncover feature circuits involved in the subject-\nverb agreement task, a linguistic evaluation that tests a model’s ability to correctly match a verb’s\ninflection (e.g., singular or plural) to the grammatical number of its subject.\n\n\n                                       7\n\nFigure 3: Faithfulness and completeness scores for circuits, evaluated on held-out data. Faint lines\nshow individual circuits for structures from Table 3, while the bold lines indicate the average across\nall structures. An ideal circuit has a faithfulness score of 1 and a completeness score of 0. While\nIntegrated Gradients (IG) requires multiple integration steps (steps=10 in this experiment), RelP\nachieves comparable faithfulness scores without any additional computational cost.\n\n\n    Subject-Verb Agreement Templates\n\n\n      Template Type             Example xoriginal                   Example Output Metric\n\n      Within Relative Clause (RC)   The athlete that the managers             p(likes) −p(like)\n      Across Relative Clause (RC)   The athlete that the managers like        p(do) −p(does)\n      Across Prepositional Phrase   The secretaries near the cars             p(has) −p(have)\n      (PP)\n\n    Table 3: These templates involve contrastive pairs of inputs that differ only in the grammatical\n   number of the subject, with the model’s task being to choose the appropriate verb inflection.\n\n\n\n\n\nTo identify and analyze these circuits, we use features discovered using sparse autoencoders (SAEs),\nwhich provide fine-grained, human-interpretable units. We follow [44] and use their SAEs trained\non Pythia-70M language model and their sparse features in our experiments. These sparse features\nserve as the fundamental units for circuit analysis. To assess their influence on the model’s output,\nMarks et al. [44] employed efficient linear approximations of activation patching, such as Integrated\nGradients (IG), to estimate the contributions of individual features and their interactions. Although\nIG provides greater faithfulness than attribution patching, it requires multiple forward and backward\npasses, resulting in higher computational cost. Our experiments show that RelP achieves faithfulness\non par with attribution patching, without incurring additional computational overhead.\n\nWe use three templatic datasets (Table 3), where tokens at the same positions serve similar roles,\nallowing us to average node and edge effects across examples while retaining positional information.\nConsistent with [44], we assess the identified sparse feature circuits using faithfulness and com-\npleteness evaluation metrics [10]. Additionally, we compare our sparse feature circuits with neuron\ncircuits, constructed by applying the same discovery method directly to individual neurons rather\nthan to SAE features, serving as a baseline for evaluation.\n\nFaithfulness measures how much of the model’s original performance is captured by the discovered\ncircuit, relative to a baseline where the entire model is mean-ablated. It is calculated as L(M)−L(∅),L(C)−L(∅)\nwhere L(C) is the metric when only the circuit C is active, L(∅) is the metric of the fully mean-ablated\nmodel, and L(M) is the metric of the full model.\n\nCompleteness evaluates how much of the model’s behavior is not explained by the discovered\ncircuit. It is defined as the faithfulness of the circuit’s complement, (M\\C). In other words, if the\ncomplement of the circuit still explains a lot of the behavior, then the original circuit is not complete\nin capturing the mechanism.\n\n\n                                       8\n\nWe report faithfulness scores for feature and neuron circuits as the node threshold TN is varied\n(Figure 3). The threshold keeps only nodes with contribution scores above TN, retaining the most\nrelevant ones. Consistent with [44], small feature circuits explain most of the model’s behavior: in\nPythia-70M, about 100 features account for the majority of performance, whereas roughly 1,500\nneurons are needed to explain half.\n\nSAE error nodes summarize reconstruction errors in the SAE, making them fundamentally different\nfrom single neurons. To analyze their impact, we evaluate faithfulness after removing all SAE error\nnodes, as well as those originating from attention and MLP modules. Consistent with findings by\nMarks et al. [44], we observe that removing residual stream SAE error nodes severely disrupts model\nperformance, whereas removing MLP and attention error nodes has a less pronounced effect.\n\nNotably, the circuits identified by our RelP method achieve faithfulness scores comparable to those\nobtained with IG, while offering greater computational efficiency. In our experiments, IG requires 10\nintegration steps, increasing computational overhead. In contrast, RelP matches the efficiency of AtP,\nrequiring only two forward passes and one backward pass, while achieving faithfulness scores on par\nwith IG. In the case of neuron circuits and feature circuits without attention/MLP errors, we observe\nan improvement in the faithfulness of RelP compared to IG. This combination of faithfulness and\nefficiency makes RelP a more effective and practical approach for uncovering sparse feature circuits\nin large language models.\n\nFollowing [44], we also evaluate completeness (Figure 3) and observe that ablating only a few nodes\nfrom feature circuits can drastically reduce model performance, whereas hundreds of neurons are\nneeded for a similar effect for Pythia. This highlights the efficiency of the identified sparse feature\ncircuits in capturing critical model behavior, as well as the usefulness of RelP in locating them.\n\n\n6  Conclusion\n\n\nIn this paper, we introduced Relevance Patching (RelP) as an enhancement over the standard attribu-\ntion patching method. RelP preserves the overall algorithmic structure of attribution patching while\nreplacing its local gradient signal with the propagation coefficient derived from Layer-wise Relevance\nPropagation (LRP). This modification enables RelP to more accurately approximate the causal effects\ncaptured by activation patching, while remaining computationally efficient.\n\nIn our experiments on the Indirect Object Identification task, we demonstrated that RelP exhibits\nstronger alignment with activation patching across residual stream, attention, and MLP outputs\ncompared to attribution patching. Furthermore, in comparing sparse feature circuits identified by\nRelP and Integrated Gradients (IG), we showed that RelP achieves comparable faithfulness without\nincurring the additional computational cost associated with IG. Our study thus demonstrates that\nmethods developed for feature attribution can be effectively integrated into mechanistic interpretability,\nhelping to advance our understanding of modern foundation models.\n\n\n7  Limitations and Future Work\n\n\nApplying RelP to localize relevant model components requires choosing appropriate rules within\nthe LRP framework, which introduces some model-specific overhead compared to model-agnostic\nattribution methods. In our experiments, RelP consistently outperformed attribution patching on\nresidual stream and MLP outputs, while gains for attention outputs were smaller. This suggests that\na deeper analysis of gradient structure in self-attention, combined with more careful rule selection,\ncould improve attribution faithfulness. Prior works have examined these challenges in the context of\ninput-level feature attribution [26, 27, 55], but systematic studies of internal layers remain limited.\nFor our experiments, we used small- and medium-scale open-access models, which allow full access\nto internals and gradients. The tasks were chosen to match standard circuit analysis benchmarks,\nrelying on predefined original–patch input pairs and single-token prediction metrics. As a result,\ncircuit discovery was limited to moderately complex behaviors. Extending RelP to more challenging\nsettings, such as free-form text generation or scenarios without known counterfactual inputs, remains\nan important direction for future work.\n\n\n                                       9\n\nAcknowledgments and Disclosure of Funding\n\nWe acknowledge support by the Federal Ministry of Research, Technology and Space (BMFTR)\nfor BIFOLD (ref. 01IS18037A). F.RJ. was partly supported by MATS 8.0 program during which\nfoundational experiments for this work were completed.\n\n\n\n\n\n                                       10\n\nReferences\n\n [1] Wojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J. Anders, and\n     Klaus-Robert Müller. Explaining deep neural networks and beyond: A review of methods and\n     applications. Proceedings of the IEEE, 109(3):247–278, 2021. doi: 10.1109/JPROC.2021.\n     3060483.\n\n [2] Luca Longo, Mario Brcic, Federico Cabitza, Jaesik Choi, Roberto Confalonieri, Javier Del Ser,\n     Riccardo Guidotti, Yoichi Hayashi, Francisco Herrera, Andreas Holzinger, et al. Explainable\n      artificial intelligence (xai) 2.0: A manifesto of open challenges and interdisciplinary research\n      directions. Information Fusion, 106:102301, 2024.\n\n [3] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan\n      Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001.\n      https://distill.pub/2020/circuits/zoom-in.\n\n [4] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas\n     Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems\n      in mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025.\n\n [5] Pattarawat Chormai, Jan Herrmann, Klaus-Robert Müller, and Grégoire Montavon. Disentangled\n     explanations of neural network predictions by finding relevant subspaces. IEEE Transactions\n    on Pattern Analysis and Machine Intelligence, 46(11):7283–7299, 2024.\n\n [6] Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna Hedström, Marina MC\n     Höhne, and Oliver Eberle. Capturing polysemanticity with PRISM: A multi-concept feature de-\n      scription framework. In The Thirty-ninth Annual Conference on Neural Information Processing\n     Systems, 2025.\n\n [7] Oliver Eberle, Jochen Büttner, Florian Kräutli, Klaus-Robert Müller, Matteo Valleriani, and\n     Grégoire Montavon. Building and interpreting deep similarity models. IEEE Transactions on\n     Pattern Analysis and Machine Intelligence, 44(3):1149–1161, 2020.\n\n [8] Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T. Schütt, Klaus-\n     Robert Müller, and Grégoire Montavon. Higher-order explanations of graph neural networks\n     via relevant walks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):\n     7581–7596, 2022.\n\n [9] Thomas Schnake, Farnoush Rezaei Jafari, Jonas Lederer, Ping Xiong, Shinichi Nakajima, Stefan\n     Gugler, Grégoire Montavon, and Klaus-Robert Müller. Towards symbolic xai — explanation\n     through human understandable logical relationships between features.  Inf. Fusion, 118(C),\n     April 2025. ISSN 1566-2535. doi: 10.1016/j.inffus.2024.102923.\n\n[10] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\n      Interpretability in the wild: a circuit for indirect object identification in GPT-2 small.  In\n     International Conference on Learning Representations (ICLR), 2023.\n\n[11] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza:\n    Two stories in mechanistic explanation of neural networks. Advances in neural information\n     processing systems, 36:27223–27250, 2023.\n\n[12] Takuya Ito, Murray Campbell, Lior Horesh, Tim Klinger, and Parikshit Ram. Quantifying\n      artificial intelligence through algorithmic generalization. Nature Machine Intelligence, pages\n     1–11, 2025.\n\n[13] Oliver Eberle, Thomas Austin McGee, Hamza Giaffar, Taylor Whittington Webb, and Ida\n    Momennejad. Position: We need an algorithmic understanding of generative AI. In Forty-\n     second International Conference on Machine Learning Position Paper Track, 2025.\n\n[14] Lawrence Chan,  Adrià  Garriga-Alonso,  Nicholas Goldwosky-Dill, Ryan  Greenblatt,\n     Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas.  Causal\n     scrubbing,  a method  for  rigorously  testing  interpretability  hypotheses.    AI  Align-\n    ment Forum, 2022.  https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/\n    causal-scrubbing-a-method-for-rigorously-testing.\n\n\n                                       11\n\n[15] Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah\n    Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks.\n     In International Conference on Machine Learning, 2022.\n\n[16] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\n     associations in GPT. In Advances in Neural Information Processing Systems, 2022.\n\n[17] Neel Nanda. Attribution patching: Activation patching at industrial scale. 2022. URL https:\n    //www.neelnanda.io/mechanistic-interpretability/attribution-patching.\n\n[18] János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. Atp*: An efficient and scalable\n    method for localizing llm behaviour to components. arXiv preprint arXiv:2403.00745, 2024.\n\n[19] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smooth-\n     grad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\n[20] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\n     Parikh, and Dhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-\n     based localization. In International Conference on Computer Vision (ICCV), 2017.\n\n[21] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In\n     International Conference on Machine Learning (ICML), 2017.\n\n[22] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through\n     propagating activation differences. In International Conference on Machine Learning (ICML),\n     2017.\n\n[23] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and\n     Klaus-Robert Müller. How to explain individual classification decisions. The Journal of\n    Machine Learning Research, 11:1803–1831, 2010.\n\n[24] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian\n    McWilliams. The shattered gradients problem: If resnets are the answer, then what is the\n     question?  In International Conference on Machine Learning (ICML), volume 70, pages\n     342–350. PMLR, 2017.\n\n[25] Ann-Kathrin Dombrowski, Christopher J. Anders, Klaus-Robert Müller, and Pan Kessel. To-\n    wards robust explanations for deep neural networks. Pattern Recognition, 121:108194, 2022.\n    ISSN 0031-3203.\n\n[26] Ameen Ali, Thomas Schnake, Oliver Eberle, Grégoire Montavon, Klaus-Robert Müller, and\n     Lior Wolf. XAI for transformers: Better explanations through conservative propagation. In\n     International Conference on Machine Learning (ICML), 2022.\n\n[27] Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain,\n    Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek. AttnLRP: Attention-aware\n     layer-wise relevance propagation for transformers. In International Conference on Machine\n     Learning (ICML), 2024.\n\n[28] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert\n     Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by\n     layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\n\n[29] Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-\n     Robert Müller. Explaining nonlinear classification decisions with deep taylor decomposition.\n     Pattern recognition, 65:211–222, 2017.\n\n[30] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n    models are unsupervised multitask learners. OpenAI blog, 2019.\n\n[31] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,\n     Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\n     Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In\n     International Conference on Machine Learning, 2023.\n\n\n                                       12\n\n[32] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\n     Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\n    Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,\n    Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu,\n    Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\n    Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,\n    Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv\n     preprint arXiv:2309.16609, 2023.\n\n[33] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\n     Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé,\n      et al. Gemma 2: Improving open language models at a practical size.  arXiv preprint\n     arXiv:2408.00118, 2024.\n\n[34] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000.\n\n[35] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,\n    and Stuart Shieber.  Investigating gender bias in language models using causal mediation\n      analysis. In Advances in Neural Information Processing Systems, 2020.\n\n[36] Paul Soulos, R. Thomas McCoy, Tal Linzen, and Paul Smolensky. Discovering the compositional\n     structure of vector representations with role learning networks. In Proceedings of the Third\n    BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2020.\n\n[37] Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference\n     models partially embed theories of lexical entailment and negation. In Proceedings of the Third\n    BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2020.\n\n[38] Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and\n    Yonatan Belinkov. Causal analysis of syntactic agreement mechanisms in neural language\n     models. In Proceedings of the 59th Annual Meeting of the Association for Computational\n     Linguistics and the 11th International Joint Conference on Natural Language Processing\n    (Volume 1: Long Papers), 2021.\n\n[39] Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model\n     behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.\n\n[40] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models.\n     arXiv preprint arXiv:1910.04732, 2019.\n\n[41] Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià\n     Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability.  In\n    Advances in Neural Information Processing Systems, 2023.\n\n[42] Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, and\n     Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice\n      capabilities in chinchilla. arXiv preprint arXiv:2307.09458, 2023.\n\n[43] Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning\n     enhances existing mechanisms: A case study on entity tracking. In International Conference on\n     Learning Representations (ICLR), 2024.\n\n[44] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\n     Sparse feature circuits: Discovering and editing interpretable causal graphs in language models.\n     In International Conference on Learning Representations (ICLR), 2025.\n\n[45] Dmitrii Kharlapenko, Stepan Shabalin, Arthur Conmy, and Neel Nanda. Scaling sparse feature\n      circuits for studying in-context learning. In International Conference on Machine Learning\n    (ICML), 2025.\n\n[46] Javier Ferrando and Elena Voita. Information flow routes: Automatically interpreting language\n    models at scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural\n    Language Processing, 2024.\n\n\n                                       13\n\n[47] Aaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated\n      circuit discovery. arXiv preprint arXiv:2310.10348, 2023.\n\n[48] Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Patrick Ka-\n      hardipraja, Thomas Wiegand, Wojciech Samek, and Sebastian Lapuschkin. Attribution-guided\n     pruning for compression, circuit discovery, and targeted correction in llms. arXiv preprint\n     arXiv:2506.13727, 2025.\n\n[49] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\n     Visualising image classification models and saliency maps. In 2nd International Conference on\n     Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track\n     Proceedings, 2014.\n\n[50] Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae\n    Kim, and Nassir Navab. Neural response interpretation through the lens of critical pathways. In\n     Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n    13528–13538, 2021.\n\n[51] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through\n     propagating activation differences. In International conference on machine learning, 2017.\n\n[52] Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-\n     Robert Müller. Layer-Wise Relevance Propagation: An Overview, pages 193–209. Springer\n      International Publishing, Cham, 2019.\n\n[53] Ashkan Khakzar, Pedram Khorsandi, Rozhin Nobahari, and Nassir Navab. Do explanations\n     explain? model knows best. In Proceedings of the IEEE/CVF Conference on Computer Vision\n    and Pattern Recognition, pages 10244–10253, 2022.\n\n[54] Oliver Eberle, Ilias Chalkidis, Laura Cabello, and Stephanie Brandl. Rather a nurse than a\n     physician - contrastive explanations under investigation. In Proceedings of the 2023 Conference\n    on Empirical Methods in Natural Language Processing, 2023.\n\n[55] Farnoush Rezaei Jafari, Grégoire Montavon, Klaus-Robert Müller, and Oliver Eberle. Mam-\n    baLRP: Explaining selective state space sequence models. In Advances in neural information\n     processing systems, 2024.\n\n[56] William R. Swartout and Johanna D. Moore. Explanation in second generation expert systems.\n     In Jean-Marc David, Jean-Paul Krivine, and Reid Simmons, editors, Second Generation Expert\n     Systems, pages 543–585, Berlin, Heidelberg, 1993. Springer Berlin Heidelberg. ISBN 978-3-\n     642-77927-5.\n\n[57] Leila Arras, José Arjona-Medina, Michael Widrich, Grégoire Montavon, Michael Gillhofer,\n     Klaus-Robert Müller, Sepp Hochreiter, and Wojciech Samek. Explaining and interpreting\n    LSTMs. Explainable AI: Interpreting, explaining and visualizing deep learning, pages 211–238,\n     2019.\n\n[58] Simon Letzgus, Patrick Wagner, Jonas Lederer, Wojciech Samek, Klaus-Robert Müller, and\n     Grégoire Montavon. Toward explainable artificial intelligence for regression models: A method-\n     ological perspective. IEEE Signal Processing Magazine, 39(4):40–58, 2022.\n\n[59] Yarden Bakish, Itamar Zimerman, Hila Chefer, and Lior Wolf. Revisiting lrp: Positional attribu-\n      tion as the missing ingredient for transformer explainability. arXiv preprint arXiv:2506.02138,\n     2025.\n\n\n\n\n\n                                       14\n\nA  Experimental Details\n\nA.1  LRP Implementation\n\nWe applied the LN-rule [26] for LayerNorm/RMSNorm, the Identity-rule [55] for nonlinear activation\nfunctions, and the LRP-0 rule [52] for linear transformations. The Half-rule [55, 57] was applied\nonly to the Qwen2 model family and Gemma2-2B, as other models used in our experiments did not\ninclude multiplicative gating operations. No attention-specific rule (e.g., AH-rule) was used. Full\ndetails are given in Table 4.\n\n\n         Propagation Rule  GPT-2 Family   Pythia Family  Qwen2 Family  Gemma2-2B\n\n         LN-rule [26]        ✓        ✓        ✓        ✓\n           Identity-rule [55]     ✓        ✓        ✓        ✓\n           0-rule [52]         ✓        ✓        ✓        ✓\n          Half-rule [55, 57]        ×            ×        ✓        ✓\n         AH-rule [26]           ×            ×            ×            ×\n\nTable 4: Propagation rules applied in our LRP implementation across different model families. A\ngreen tick (✓) indicates that the rule was applied, while a red cross (×) indicates that it was not used.\n\n\n\nA.2  Further Details on Subject-Verb Agreement Experiment\n\nIn this experiment, we used 300 samples, structured according to Table 3, for circuit discovery, and\n100 held-out samples for evaluating faithfulness and completeness. Consistent with [44], the first\none-third of each circuit was excluded from evaluation, since components in early model layers\nare typically responsible for processing specific tokens, which may not consistently appear across\ntraining and test splits.\n\nB  Quantitative results\n\nThe exact numerical values corresponding to the IOI experiment, visualized in Figure 1, are reported\nin Table 5.\n\n                              Residual Stream      Attention Outputs    MLP Outputs\n\n          Model          AtP   RelP (ours)   AtP   RelP (ours)   AtP   RelP (ours)\n\n          GPT-2 small     0.753     0.968     0.979     0.962     0.195     0.992\n          GPT-2 medium   0.671     0.884     0.961     0.982     0.551     0.993\n          GPT-2 large      0.740     0.853     0.461     0.965     0.006     0.956\n           Pythia-70M      0.728     0.898     0.981     0.986     0.507     0.967\n           Pythia-410M     0.695     0.874     0.879     0.903     0.501     0.948\n          Qwen2-0.5B     0.684     0.922     0.933     0.965     0.717     0.981\n          Qwen2-7B       0.230     0.549     0.395     0.569     0.471     0.622\n         Gemma2-2B     0.391     0.769     0.942     0.956     0.910     0.952\n\nTable 5: Pearson correlation coefficient (PCC) between activation patching and attribution patching\n(AtP) or relevance patching (RelP), computed over 100 IOI prompts for three GPT-2 model sizes\n(Small, Medium, Large), two Pythia models (70M, 410M), two Qwen2 models (0.5B, 7B), and\nGemma2-2B. A higher value of PCC represents higher alignment with activation patching results.\n\n\n\nC  Further Qualitative Results\n\nIn Section 5, we presented the qualitative differences between AtP and RelP for the GPT-2 Small\nmodel. In this section, we extend our analysis by providing additional experimental results for\nother models. It is evident that RelP provides a more accurate approximation to activation patching,\nparticularly for the residual stream and MLP outputs.\n\n\n                                       15\n\nFigure 4: Qualitative comparison showing how accurately relevance patching (RelP) and attribution\npatching (AtP) approximate the effects of activation patching in GPT-2 Large. RelP shows notably\nbetter alignment in the residual stream and at MLP0, where AtP’s estimates are less reliable.\n\n\n\n\n\n                                       16\n\nFigure 5: Qualitative comparison showing how accurately relevance patching (RelP) and attribution\npatching (AtP) approximate the effects of activation patching in Pythia-410M. RelP shows notably\nbetter alignment in the residual stream and at MLP0, where AtP’s estimates are less reliable.\n\n\n\n\n\n                                       17\n\n## Claim to Classify\n\"RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Relevance Patching (RelP) and Attribution Patching (AtP)",
    "method_rung": 2,
    "claim_rung": 2,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim states that RelP \"more accurately approximates activation patching\" compared to attribution patching. Activation patching is a Rung 2 (Interventional) method, as it involves replacing activations to measure causal effects. Both RelP and AtP are designed to approximate activation patching, meaning they operate at the same methodological level (Rung 2). The claim itself is about the accuracy of this approximation, which is a comparative performance claim about causal interventions, not a mechanistic or counterfactual claim. Thus, the claim rung aligns with the method rung (Rung 2). No hedging is present, and the confidence is high due to clear methodological alignment and explicit evidence in the paper (e.g., Figure 1 and Table 5)."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 18385,
      "completion_tokens": 217,
      "total_tokens": 18602
    },
    "finish_reason": "stop"
  }
}