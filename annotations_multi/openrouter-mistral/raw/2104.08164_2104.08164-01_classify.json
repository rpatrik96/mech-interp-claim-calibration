{
  "timestamp": "2026-02-19T09:58:23.441244+00:00",
  "paper_id": "2104.08164",
  "claim_id": "2104.08164-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2104.08164\nTitle: Editing Factual Knowledge in Language Models\nFull text:\n                     Editing Factual Knowledge in Language Models\n\n\n                                Nicola De Cao 1,2, Wilker Aziz 1, Ivan Titov 1,2\n                                 1University of Amsterdam, 2University of Edinburgh\n                     { nicola.decao, w.aziz, titov } @uva.nl\n\n\n\n\n                          Abstract                                                KnowledgeEditor   Updated prediction\n\n                                                                                  Regular predictions\n               The factual knowledge acquired during pre-\n                    training and stored in the parameters of Lan-\n                guage Models (LMs) can be useful in down-\n                 stream tasks (e.g., question answering or tex-\n                     tual inference). However, some facts can be\n                                                                                                                       Retain previous knowledge2021             incorrectly induced or become obsolete over\n                   time.  We present KNOWLEDGEEDITOR, a\n                                                                    Figure 1: Left: a model f with parameters θ prefers a\n               method which can be used to edit this knowl-\n                                                                         prediction y for input x (e.g., y is the mode/argmax of aSep           edge and, thus, ﬁx ‘bugs’ or unexpected pre-\n                                                                            discrete distribution parameterized by f(x; θ)). Right:\n8             dictions without the need for expensive re-                                                                   our method uses a hyper-network g to update the pa-\n                    training or ﬁne-tuning.  Besides being com-                                                                     rameters of f to θ′ such that f(x; θ′) prefers an alterna-\n                   putationally efﬁcient, KNOWLEDGEEDITOR                                                                                 tive prediction a without affecting the prediction y′ of\n                 does not require any modiﬁcations in LM pre-                                                              any other input x′ ̸= x. Our model edits the knowledge\n                    training (e.g., the use of meta-learning). In our\n                                                                   about x stored in the parameters of f.\n                  approach, we train a hyper-network with con-[cs.CL]             strained optimization to modify a fact without\n                    affecting the rest of the knowledge; the trained                                                     become a standard practice in NLP. Factual knowl-\n                 hyper-network  is then used  to  predict the\n                                                           edge induced during pre-training can help in down-\n                 weight update at test time. We show KNOWL-\n                                                              stream tasks, but it can also be incorrect or become               EDGEEDITOR’s efﬁcacy with two popular ar-\n                    chitectures and knowledge-intensive tasks: i) a       obsolete over time (e.g., not reﬂecting changes of\n            BERT model ﬁne-tuned for fact-checking, and       heads of states or country populations). Developing\n                          ii) a sequence-to-sequence BART model for        reliable and computationally efﬁcient methods for\n                  question answering. With our method, chang-      bug-ﬁxing models without the need for expensive\n                  ing a prediction on the speciﬁc wording of a                                                                     re-training would be beneﬁcial. See Figure 2 for\n                 query tends to result in a consistent change\n                                                           an example of revising the memory of a model that\n                    in predictions also for its paraphrases. We\n                                                                              initially misremembered Namibia’s capital.               show that this can be further encouraged by ex-\n                    ploiting (e.g., automatically-generated) para-         Unlike conventional Knowledge Bases (KBs)\n                  phrases during training.   Interestingly, our        that explicitly store factual knowledge, neural mod-arXiv:2104.08164v2           hyper-network can be regarded as a ‘probe’ re-        els implicitly memorize facts in their parameters.\n                  vealing which components need to be changed      One cannot easily access and interpret their com-\n                    to manipulate factual knowledge; our analysis       putation and memories (Ribeiro et al., 2016; Be-\n                shows that the updates tend to be concentrated\n                                                                linkov and Glass, 2019; Voita et al., 2019; De Cao\n               on a small subset of components.1\n                                                                           et al., 2020), thus, modifying their knowledge is a\n                                                               challenging problem. Motivated by practical con-          1  Introduction\n                                                                       siderations, we formulate the following desiderata\n           Using pre-trained transformer-based Language    for a method aimed at tackling this problem (see\n           Models (LMs; Vaswani et al., 2017; Devlin et al.,   Section 2 for a more formal treatment):\n            2019; Radford et al., 2019; Lewis et al., 2020; Raf-                                                                                 • Generality: be able to modify a model that\n               fel et al., 2020; Brown et al., 2020) has recently                                                          was not speciﬁcally trained to be editable (i.e.,\n                                                          no need for special pre-training of LMs, such                  1Source code available at https://github.com/\n           nicola-decao/KnowledgeEditor                       as using meta-learning);\n\n                   Semantically equivalent                           Another fact                             Fact to change            Fact that also changes             Another fact\n\n  What is the capital    How is Namibia's     What is the capital            What is the capital    How is Namibia's     What is the capital\n      of Namibia?         capital city called?          of Russia?                        of Namibia?         capital city called?          of Russia?\n\n\n\n   Answers  Scores     Answers  Scores     Answers  Scores             Answers  Scores     Answers  Scores     Answers  Scores\n   Namibia   -0.43     Namibia   -0.32     Moscow   -0.55            Windhoek  -0.06    Windhoek  -0.07     Moscow   -0.56\n     Nigeria    -0.69        Nigeria    -0.79       Nashville   -0.97             Tasman    -1.42      Tasman    -1.50        Ufa      -1.03\n      Nibia     -0.89        Nibia     -0.87        Ufa     -1.22             Windygates  -1.52     Windygates  -1.51       Nashville   -1.04\n   Namibia   -1.08      Tasman    -1.14        Kiev     -1.28              Tasmania   -1.59     Windhoof   -1.53        Kiev     -1.43\n   Tasman    -1.19      Namibia   -1.16      Nashua    -2.09              Windhoof   -1.66     Tasmania   -1.53      Nashua    -2.21\n\n           (a) Model predictions before the update.                         (b) Model predictions with edited parameters.\n\nFigure 2: Predictions from a pre-trained language BART model ﬁne-tuned for closed-book question answering.\nLeft: model top-k predictions from Beam Search. Right: top-k after using our method conditioning on changing\n‘What is the capital of Namibia?’ from ‘Namibia’ (wrong) to ‘Windhoek’ (correct prediction). Changing one fact\nalso changes a semantically equivalent question and keeps the predictions from other facts the same.\n\n\n    • Reliability: be able to successfully update a    not have to select a subset of parameters to update\n     speciﬁc fact without affecting the rest of the    as we let our model learn that by itself. In fact,\n     acquired knowledge;                         our hyper-network can be regarded as a ‘probe’ re-\n    • Consistency: the changes should be consis-   vealing which components of the network need to\n      tent across equivalent formulations of a fact   be changed to manipulate factual knowledge, i.e.\n      (e.g., when asked to update an answer for one    revealing the ‘causal mediation mechanisms’ (Vig\n     question, answers to its paraphrases should    et al., 2020). We observe that the updates end up\n     change accordingly).                          being concentrated in a restricted set of model com-\n                                                     ponents, even though we do not encourage any kindThe problem has been previously tackled in Zhu\n                                                     of sparsity. Interestingly, the most-updated compo-et al. (2020) and Sinitsin et al. (2020), as discussed\n                                                  nents are different from the groups of parametersin detail in Section 3. However, both do not ensure\n                                                     receiving large gradients (see Figure 4).that the edited model will be ‘reliable’, i.e. that the\nrest of the knowledge would not be badly affected,                                               Contributions  Our contributions are as follows:\nand that the changes are ‘consistent’ across equiv-\n                                                                 • we deﬁne the task of knowledge editing and\nalent inputs. Additionally, Sinitsin et al.’s (2020)\n                                                    propose a set of evaluation metrics;\nmethod requires expensive specialized training of\n                                                                 • we propose KNOWLEDGEEDITOR that learns\nthe original network. While re-training the original\n                                                             to modify LMs memories efﬁciently and reli-\nnetwork was feasible in their applications (e.g., in\n                                                        ably while maintaining consistent predictions\nmachine translation), it is problematic when the\n                                                             for semantically equivalent inputs;\nnetwork is a pre-trained LM. We propose a novel\n                                                                 • we verify that our proposed method largelymethod that overcomes these limitations.\n                                                  meets our desiderata—while other baselines\n  We treat editing the memories of a neural model\n                                                   based on ﬁne-tuning fail—testing  it with\nas a learning-to-update problem. We use an efﬁ-\n                                                             different LM architectures on knowledge-\ncient parameterization of a hyper-network that is\n                                                           intensive tasks such as fact-checking and\ntrained to update the LM parameters when provided\n                                                 open-domain question answering;\nwith a single fact that needs to be modiﬁed. We do\n                                                                 • we analyze the updates for KNOWLEDGEEDI-not require meta-learning, re-training or ﬁne-tuning\n                                          TOR and the alternatives.of the original network. We employ constrained\noptimization in training: we constrain the edited\n                                        2  Task\nmodel to retain the same predictions as the original\none regardless of the distance between the original  We want to edit the memory of a neural language\nand updated models in the parameter space. We   model such that when, presented with an input, its\nshow how this framework can be extended to incor-   output reﬂects a revised collection of facts. Un-\nporate (e.g., automatically-generated) paraphrases    fortunately, the knowledge of a language model\nin training, further improving consistency. Figure 1    is typically opaque to us, being stored non-locally\nshows an outline of our method.                     across a large number of parameters and architec-\n   Differently from both previous methods, we do    tural components. Thus, concretely, to operational-\n\nize the task, we seek a change in the model’s pa-        racy of revised predictions for inputs in D;\nrameters that affects predictions from the model      2. retain accuracy: how well θ′ retains the orig-\nonly for a speciﬁc input. For a given input x, the         inal predictions of f, measured as accuracy\nprediction a made by the edited model should differ        wrt input-output pairs in sets Ox;\nfrom the prediction y made by the original model      3. equivalence accuracy: how consistent the pre-\nonly if x is inﬂuenced by one of the revised facts.          dictions of the revised model θ′ are for seman-\n                                                                 tically equivalent inputs, measured as accu-\n2.1  Deﬁnition                                                       racy of the revised predictions for all Px;\nMore formally, we have a model x 7→f(x; θ) with      4. performance deterioration: how much test\ntrained parameters θ, and a dataset of revisions        performance of the updated model deterio-\n⟨x, y, a⟩∈D, i.e., x is an input, y is the prediction          rates.3\npreferred by f(x; θ), and a is an alternative predic-   These values are obtained by comparing predic-\ntion which we would like an edited version of the    tions of f(·; θ) and f(·; θ′) for different subsets of\nmodel to prefer. Concretely, we keep the model ar-   inputs (e.g., D, Ox, Px) and against different tar-\nchitecture f ﬁxed, and seek alternative parameters    gets (e.g., gold-standard, original predictions, or\nθ′ such that for x, f(x; θ′) would prefer the predic-    alternative predictions). While these metrics are\ntion a instead of y while keeping all other predic-   straightforward to compute in principle, some can\ntions unchanged. In practice, we approximate the   be computationally demanding. For example, re-\nset of ‘all other predictions’ using a ﬁnite data set    tain accuracy depends on predictions for all inputs\nOx of pairs ⟨x′, y′⟩with x′ ̸= x. Moreover, pre-  we have access to, which is potentially the entirety\ndictions need not be continuous nor differentiable    of the downstream task’s validation/test data.4\noutputs from the model; instead, they may result      Previous work has evaluated similar versions of\nfrom an arbitrary decision rule based on f(x; θ).    this task differently. Sinitsin et al. (2020) measure\nFor example, when f(x; θ) parameterizes a discrete   performance deterioration and success rate but do\ndistribution pY |X over the output space, the most    not measure retain accuracy nor equivalence accu-\nstandard decision rule is to output the mode of the    racy. A small performance deterioration does not\ndistribution: y = arg maxc∈Y pY |X(c|x, θ).2        guarantee high equivalence accuracy as the former\n                                                                is sensitive to changes in cases where the original\nSemantically equivalent inputs  Optionally, for\n                                            model makes wrong decisions. Assessing accuracy\nsome revision ⟨x, y, a⟩∈D, we may also have\n                                                       against old or revised facts, which Zhu et al. (2020)\na set Px of inputs semantically equivalent to x\n                                                     also do, does not help to measure the retain accu-\n(e.g., automatically-generated paraphrases). Such\n                                                        racy. We argue that preserving model predictions\na set can be used in at least two ways:  i) to ob-\n                                                         for inputs not in D is critical in production settings,\ntain explicit supervision for changes that should\n                                           where model predictions might have been exten-\nbe realized in tandem with ⟨x, y, a⟩; and, indepen-\n                                                      sively analyzed and tested. For x′ ̸∈D, we aim\ndently of that, ii) to evaluate whether an edited\n                                                      to maintain all original predictions as well as the\nmodel makes consistent predictions on semanti-\n                                          model scores f(x′; θ′) itself, effectively avoiding\ncally equivalent inputs. Note that in this work we\n                                                      the need to re-calibrate the models (for example, in\nnever use paraphrases at test time, only for training\n                                                     applications where probability estimates are used\nand evaluation of our approach; generating them\n                                                downstream).\nat test time, while potentially helpful, would have\ncompromised efﬁciency.                                       3  Related work\n\n2.2  Evaluation                              Modifying transformers  The most straightfor-\n                                           ward strategy to edit the knowledge of a modelTo test if a method g, producing edited parameters\nθ′, meets our desiderata, we measure:             would be to re-train it on a new dataset with addi-\n                                                           tional, modiﬁed, or removed facts. This is often\n  1. success rate: how much g successfully up-\n                                                    unfeasible as LMs require large-scale expensive     dates the knowledge in θ′, measured as accu-\n                                                        training that can hardly be reproduced by the most.\n    2Whereas in text classiﬁcation solving this is straightfor-                              of f(·;θ′)\nward (for Y is small), in sequence-to-sequence we resort to        31 −accuracyaccuracy of f(·;θ)\nbeam search to approximate the mode (for Y is too large or        4During training of g, however, we can use sub-sampling\nunbounded).                                                                       (i.e., mini batches) to approximate the metric.\n\nSinitsin et al. (2020) propose a meta-learning ap-   reach the prediction quality of alternatives that re-\nproach (Finn et al., 2017) for model modiﬁcation    trieve and use context. Approaches that incentivize\nthat learns parameters that are easily editable at test   memorization of factual knowledge show to be ben-\ntime (e.g., updating the knowledge of the model    eﬁcial for many downstream tasks suggesting that\nrequires only a few SGD steps from these learned    research on methods that effectively edit the mem-\nparameters). To have a reliable method, they em-   ory of a model is indeed important (Zhang et al.,\nploy a regularized objective forcing the updated   2019; Sun et al., 2019, 2020). Some recent hy-\nmodel not to deviate from the original one. This    brid approaches that use both implicit and explicit\ntechnique suffers from three main limitations: i) it   memory show some beneﬁts for question answer-\nrequires expensive and specialized pre-training, ii)    ing (Févry et al., 2020; Verga et al., 2020). Notably,\nit is sensitive to many hyper-parameters (e.g., the   language models that only rely on internal implicit\nweights of the regularizers and the subset of param-   memory are state-of-the-art for (multilingual-) En-\neters to update), and iii) their multitask objective     tity Linking (De Cao et al., 2021a,b). An effective\ndoes not guarantee reliability (i.e., the model is   mechanism for editing LM’s implicit memory may\npenalized for diverging from the original, rather   be applicable in all these settings.\nthan constrained not to).\n                                            Causal Interventions  Identiﬁcation of minimal   Instead of penalizing an updated model for devi-\n                                              changes to neural networks needed to achieve aating from the original one, Zhu et al. (2020) use\n                                                         certain behaviour has been studied in the context ofconstrained optimization. They use a less com-\n                                                   research in interpreting neural networks (Lakretzputationally expensive procedure as they re-ﬁne-\n                                                           et al., 2019; Vig et al., 2020; Elazar et al., 2021;tune on a speciﬁc downstream task (with altered\n                                                Csordás et al., 2021). The components which need\ndata). Their method employs either an L2 or L∞\n                                                      to be updated can be interpreted as controllingconstraint between the original model’s parame-\n                                                   or encoding the corresponding phenomena (e.g.,ters and the edited ones. However, a norm-based\n                                                     subject-verb agreement). Much of this research fo-constraint on parameters ignores the highly non-\n                                                cused on modifying neuron activations rather thanlinear nature of LMs and how parameters deter-\n                                                weights and on sparse interventions (e.g., modify-mine the outputs of the model. Indeed, a minimal\n                                                  ing one or a handful of neurons). While far fromchange in parameter space may produce a com-\n                                                our goals, there are interesting connections withpletely different output for many datapoints leading\n                                                our work. For example, our analysis of updates into a potentially unreliable method. Additionally,\n                                                 Section 6.4, though very limited, may shed somethey show the need to select a subset of parameters\n                                                          light on how factual knowledge is encoded in theto be updated, which requires extra development\n                                                 parameters of a model.effort. Zhu et al.’s (2020) method is similar to Elas-\ntic Weight Consolidation (Kirkpatrick et al., 2017),\n                                       4  Method\na technique developed for preventing catastrophic\nforgetting in neural network models.          We propose to treat the task of editing the mem-\n                                                ory of a neural model as a learning problem. In-\nKnowledge in Language Models  Petroni et al.                                                    stead of deﬁning a handcrafted algorithm to com-\n(2019) show that pre-trained language models re-                                                 pute the new parameters θ′, we learn a KNOWL-\ncall factual knowledge without ﬁne-tuning, which                                       EDGEEDITOR:  a model that predicts θ′ condi-\nthey do by feeding speciﬁc prompts to LMs. Hand-                                                   tioned on an atomic fact that we want to mod-\ncrafted prompts have been found not to be the best                                                                  ify. Concretely, KNOWLEDGEEDITOR is a hyper-\noption to extract knowledge from LMs, and var-                                             network (Ha et al., 2017)—i.e., a neural network\nious solutions have been proposed to understand                                                        that predicts the parameters of another network.\nwhat LMs ‘know’ (Jiang et al., 2020; Shin et al.,                                                Since the task requires every other prediction\n2020; Liu et al., 2021). Additionally, Roberts et al.                                                      to stay the same—except the one we desire to\n(2020) show that large models can be ﬁne-tuned to                                        change—we cast the learning task as a constrained\naccess their internal memories to answer questions                                                   optimization problem.\nin natural language without any additional context\nand with surprisingly high accuracy—a setting they   Optimization  For an input x, changing the pre-\nreferred to as closed-book question answering. Al-    diction of a model f(·; θ) to a corresponds to min-\nthough performing quite well, these models cannot    imizing the loss L(θ; x, a) incurred when a is the\n\ntarget. Preserving the rest of the knowledge cor-   approximately via Monte Carlo (MC) sampling\nresponds to constraining the updated parameter θ′    (see Appendix A for more details).  Finally, in\nsuch that model outputs f(·; θ′) do not change for   sequence-to-sequence models, assessing KL is\nx′ ∈Ox. Our editor g is a neural network parame-    intractable even for a single data point, as the\nterized by φ which we choose by optimising the fol-   sample space Y is unbounded. In such cases we\nlowing objective for each data-point ⟨x, y, a⟩∈D:   approximate the computation on a subset of the\n                                              sample space obtained via beam search.\n        min X L(θ′; ˆx, a)\n           φ                                    Architecture  Instead of predicting θ′ directly,                   ˆx∈Px                         (1)\n                                                our hyper-network predicts a shift ∆θ such that              s.t.  C(θ, θ′, f; Ox) ≤m ,\n                                                             θ′ = θ + ∆θ. A naive hyper-network implementa-\nwhere Px is the set of semantically equivalent in-    tion might be over-parameterized, as it requires a\nputs to x (for convenience we assume it contains    quadratic number of parameters with respect to the\nat least x), θ′ = θ + g(x, y, a; φ), C is a constraint    size of the target network. Thus, we apply a trick\non the update, and the margin m ∈R>0 is a hy-    similar to Krueger et al. (2017) to make g tractably\nperparameter. The constraint is used to express    predict edits for modern large deep neural networks\nour desire to preserve model outputs unchanged for     (e.g., BERT). Namely, g makes use of the gradient\nx′ ̸= x. Note that only x, but not the rest of Px,   information ∇θL(θ; x, a) as it carries rich informa-\nare provided as input to the editor, as these will    tion about how f accesses the knowledge stored in\nnot be available at test time. In our models, f(x; θ)   θ (i.e., which parameters to update to increase the\nparameterizes a discrete distribution pY |X over the   model likelihood given a).5\noutput sample space Y, hence we choose to con-    We ﬁrst encode ⟨x, y, a⟩, concatenating the\nstrain updates in terms of sums of Kullback-Leibler    text with special separator and feeding  it to a\n(KL) divergences from the updated model to the    bidirectional-LSTM (Hochreiter and Schmidhuber,\noriginal one: CKL(θ, θ′, f; Ox) =                  1997). Then, we feed the last LSTM hidden states\n                                                       to a FFNN that outputs a single vector h that con-\n                          pY |X(c|x′, θ)\n X X pY |X(c|x′, θ) log                  (2)    ditions the further computations. To predict the\n                          pY |X(c|x′, θ′)                           n×m  x′∈Ox c∈Y                                              shift for a weight matrix W    ∈θ, we use\n                                           ﬁve FFNNs conditioned on h that predict vectors\nThe constraint pushes the updated model to pre-                                               α, β ∈Rm, γ, δ ∈Rn and a scalar η ∈R. Then\ndict output distributions identical to the original\none for all x′  ̸=  x. An alternative constraint                         ∆W = σ(η) ·   ˆα ⊙∇W L(W; x, a) + ˆβ    ,\nwe could employ is an Lp norm over the param-                                                    (3)\neter updates such that g is optimized to make a      with   ˆα = ˆσ(α)γ⊤  and   ˆβ = ˆσ(β)δ⊤,\nminimal update to the original model parameter:\nCLp(θ, θ′, f; Ox) = (Pi |θi −θ′i|p)1/p. This con-   where σ is the Sigmoid function (i.e., x 7→(1 +\n                                              exp(−x))−1), and ˆσ indicates the Softmax func-straint was previously used by Zhu et al. (2020).\n                                                      tion (i.e., x 7→exp(x)/ Pi exp(xi)). With thisHowever, such a constraint, expressed purely in\n                                                    formulation, the parameters for the hyper-networkparameter space and without regards to the model\n                                    φ scale linearly with the size of θ. An interpreta-architecture f, does not directly encourage model\n                                                        tion of Equation 3 is that an update ∆W is a gatedoutputs to be close to original ones in function\n                                        sum of a scaled gradient of the objective and a biasspace (i.e., the two functions to be similar). Neural\n                                                   term. The scale for the gradient and the bias aremodels are highly non-linear functions, so we do\n                                                 generated via an outer vector product as it allowsnot expect this type of constraint to be effective.\n                                                        for efﬁcient parameterization of a matrix with justThis will be empirically demonstrated in Section 6.\n                                                     three vectors. The gate lets the model keep some\nTractable  approximations  Non-linear  con-   parameters unchanged.\nstrained  optimization  is  generally  intractable,\nthus we employ Lagrangian relaxation (Boyd   Margin annealing  The margin m is a hyperpa-\net al., 2004) instead. The constraint itself poses a    rameter and therefore ﬁxed. However, i) it is hard to\ncomputational challenge, as it requires assessing   choose since it is task-dependent, and ii) it should\nKL for all datapoints in the dataset at each training                                                  5A version of our hyper-network that does not use gradi-\nstep. For tractability, we evaluate the constraint     ent information converges far too slowly.\n\nbe as small as possible. If the margin is too small,   on a task with a more complex output space: closed-\nhowever, we risk having a small feasible set, and   book question answering (QA). For that we ﬁne-\nthe model may never converge. To address both    tune a BART base model (Lewis et al., 2020) with\nissues, we pick some initial value for the margin    a standard seq2seq objective on the Zero-Shot Rela-\nand anneal it during training conditioned on vali-    tion Extraction (zsRE) dataset by Levy et al. (2017).\ndation performance: when the model successfully  We evaluate on this dataset because it is annotated\nchanges > 90% of the predictions, we multiply the   with human-generated question paraphrases that\nmargin by 0.8. We stop decreasing the margin once   we can use to measure our model’s robustness to\nit reaches a desirable small value. The annealing    semantically equivalent inputs. We create alterna-\nprocedure prevents the model from diverging while    tive predictions for FC simply ﬂipping the labels,\nincreasingly tightening the constraint.              whereas for QA we pick all hypotheses enumerated\n                                                    via beam search except the top-1. The latter en-\n5  Experimental Setting                                                    sures high-probability outcomes under the model\n                                                         distribution. We generate semantically equivalentWe aim to evaluate the effectiveness of KNOWL-\n                                                    inputs with back-translation. See Appendix B forEDGEEDITOR  comparing   to   baselines  on\n                                                      technical details on models and data collection.knowledge-intensive tasks where the importance of\nmodifying the memory of a large LM has a broad\n                                       6  Resultsimpact. We then test our method on closed-book\nfact-checking and closed-book question answering                                                Table 1 reports the main results for fact-checking\nwith the metrics proposed in Section 2.2.                                            and  question  answering.    Overall, KNOWL-\n                                      EDGEEDITOR achieves high performance in all5.1  Baselines\n                                                       metrics. Some other methods also achieve high ac-\nWe compare against two baselines: i) ﬁne-tuning                                                 curacy in some metrics but always sacriﬁcing oth-\nand ii) the method proposed by Zhu et al. (2020).                                                        ers (i.e., never meeting all our desiderata at once).\nFine-tuning corresponds to using standard gradient                                   We compare methods along different metrics (as\ndescent, minimizing the loss for the fact/prediction                                              opposed to a single one), as there is no way to pre-\nwe want to revise. For this, we follow Sinitsin                                                       cisely determine the importance of each of these\net al. (2020) and employ RMSProp (Tieleman and                                                      metrics. To gather more insight, we compute their\nHinton, 2012).6 We set the learning rate to 10−5                                                      stochastic convex combination with coefﬁcients\nand stop upon successfully changing the output                                             sampled from a Dirichlet distribution (with α = 1\nof the model or having reached a maximum of                                                      to ensure a very diverse set of combinations) and\n100 gradient steps. Zhu et al.’s (2020) method                                                     report in Figure 6 in Appendix C an estimate of\nextends ﬁne-tuning with an L∞constraint on pa-                                                    the probability that a system outperforms another\nrameters.7 Following both Sinitsin et al. (2020)                                                   across 1, 000 such combinations. The probability\nand Zhu et al. (2020) we report these baselines                                                   of our full method to outperform all baselines is\nﬁne-tuning all parameters or just a subset of them.                                                  very high for both FC and QA (≈97% and ≈88%,\nWe limit the search to selecting entire layers and                                                         respectively). In Figure 5 in Appendix C, we show\nbase our decision on performance on a subset of                                                    the distributions of the combined scores (i.e., the\nthe validation set. Note that selecting a subset of                                           raw data for the approximation reported in Fig-\nparameters for update requires an extensive search,                                                  ure 6). We then analyze different aspects of our\nwhich KNOWLEDGEEDITOR dispenses with by au-                                           method and the baselines.\ntomatically learning it.\n\n                                                    6.1  Success rate\n5.2  Models and data\n                                             Every method achieves an almost perfect successWe evaluate on closed-book fact-checking (FC)\n                                                         rate on fact-checking. All methods but ours applyﬁne-tune a BERT base model (Devlin et al., 2019)\n                                                 updates in a loop, stopping either when the newon the binary FEVER dataset (Thorne et al., 2018)\n                                          model is successfully updated or after reaching afrom KILT (Petroni et al., 2021). We also evaluate\n                                     maximum number of iterations. The success rate\n   6We tried alternatives, RMSProp was the most effective.                                                         for KNOWLEDGEEDITOR is not 100% because we\n   7We search the hyper-parameter for the penalty m ∈\n                                           do not apply more than one update even in case of{10−3, 5 × 10−4, 10−4, 5 × 10−5, 10−5} selecting the best\nbased on the sum of success rate and retain accuracy.           failure. To this end, we also show an experiment\n\n                                    Fact-Checking                        Question Answering\n\n                          Success   Retain   Equiv.   Perform.   Success   Retain      Equiv.     Perform.\n   Method                 rate ↑    acc ↑    acc ↑      det ↓      rate ↑    acc ↑      acc ↑*        det ↓\n\n    Fine-tune (1st layer)      100.0     99.44    42.24      0.00      98.68     91.43    89.86 / 93.59     0.41\n    Fine-tune (all layers)     100.0     86.95    95.58      2.25      100.0     67.55    97.77 / 98.84     4.50\n   Zhu et al. (1st layer)      100.0     99.44    40.30      0.00      81.44     92.86    72.63 / 78.21     0.32\n   Zhu et al. (all layers)     100.0     94.07    83.30      0.10      80.65     95.56    76.41 / 79.38     0.35\n\n   Ours CL2                99.10     45.10    99.01     35.29      99.10     46.66    97.16 / 99.24     9.22\n\n   KNOWLEDGEEDITOR    98.80     98.14    82.69      0.10      94.65     98.73    86.50 / 92.06     0.11\n    + loop†                100.0     97.78    81.57      0.59      99.23     97.79    89.51 / 96.81     0.50\n    + Px ‡                 98.50     98.55    95.25      0.24      94.12     98.56    91.20 / 94.53     0.17\n    + Px + loop‡           100.0     98.46    94.65      0.47      99.55     97.68    93.46 / 97.10     0.95\n\nTable 1: Accuracy scores on fact-checking and question answering for the metrics presented in Section 2.2. *We\nreport both the accuracy on the set of generated paraphrases (left) and human-annotated (right).†Apply updates in\na loop, stopping when the update is a success or when reaching a maximum number of iterations (only at test time).\n‡Using paraphrases (semantically equivalent inputs) as additional supervision (only at training time).\n\n\nwith our method with multiple updates within a     is ≈98% for both FC and QA). Conversely, as ex-\nloop employing the same stopping criteria as the    pected, our method with CL2 has very low retain\nbaselines. Note that we apply this only at test time    accuracy (always < 50%). CL2 suffers from catas-\n(i.e., we do not train for multiple updates). When    trophic forgetting because it does not enforce the\napplying multiple updates also our method reaches   updated model to be close to the original one in\na 100% success rate on fact-checking and almost    function space (i.e., the two functions to be similar)\nperfect accuracy (> 99%) for QA.8                  but just in parameter space.\n  Closed-book QA is a more challenging task      Fine-tuning all layers is successful but it affects\nsince the output space is text and not just a bi-   the previously acquired knowledge negatively: re-\nnary label. In this setting, KNOWLEDGEEDITOR    tain accuracy is ≈87% and ≈68% for FC and\nachieves high accuracy (≈95% or > 99% with   QA, respectively, while performance deterioration\nthe loop). Among all methods, KNOWLEDGEEDI-    in ≈2% and ≈4%. Fine-tuning a single layer is\nTOR gets the best success rate while also obtaining   more effective as it prevents over-ﬁtting (the best\nthe best retain accuracy. In QA, Zhu et al.’s (2020)   model updates the 1st layer in both FC and QA).\nmethod does not reach a good success rate (≈80%).   However, in FC the updated model does not gener-\nWe searched hyperparameters for their method also    alize on semantic equivalent inputs: the accuracy\nto have high retain accuracy, and indeed that is   on paraphrases is much lower even than versions\nhigher than regular ﬁne-tuning. However, unlike    of our methods which do not use paraphrases in\nfact-checking, regular ﬁne-tuning for QA gets al-    training (42% vs. > 81%), and even more so when\nmost perfect scores but at the expense of the retain   compared to those which use them (> 94%).\naccuracy. Sequence-to-sequence models are more      Fine-tuning with Zhu et al.’s (2020) method does\nsensitive to a slight parameter shift. This happens    not affect performance for FC much, which is not\nbecause minor changes may completely alter the    surprising since standard ﬁne-tuning already gets\ntop-k prediction from beam search (in the case of   almost perfect scores. Differently, in the QA set-\nQA). Differently, in a binary classiﬁer (in the case    ting, using their constrained optimization boosts\nof FC) the probability of a prediction can change    the retain accuracy (up to +4% to normal ﬁne-\nsubstantially without crossing the decision bound-   tuning) but at the cost of a low success rate (≈80%\nary (usually set at 0.5 when not calibrated).         where ﬁne-tuning gets the perfect score).\n\n6.2  Retaining previous knowledge                6.3  Accuracy on paraphrases\n\nKNOWLEDGEEDITOR maintains the predictions in  We evaluate our method both with and without the\nthe validation set almost perfectly (retain accuracy    additional supervision of paraphrases to improve\n                                                   generalization—that corresponds to have Px as the\n    8Even if we do not train for multiple subsequent updates,\n                                                            set of paraphrases of x or Px = {x} in Equation 1,its success opens the possibility to add this at training time.\nWe leave the exploration of this technique to future work.       respectively. Without this additional supervision,\n\n                    Should not flip (correct)        Should flip (correct)                             Should not flip (correct)        Should flip (correct)                             Should not flip (correct)        Should flip (correct)\n                    Should not flip (wrong)         Should flip (wrong)                              Should not flip (wrong)         Should flip (wrong)                              Should not flip (wrong)         Should flip (wrong)\n\n\n     4                                                  4                                                  4\n\n\n     2                                                  2                                                  2model                                                                        model                                                                        model\n\n     0                                                  0                                                  0updatede                                                                                                                  updatede                                                                                                                  updatede\n\n     2                                                  2                                                  2Logits                                                                                      Logits                                                                                      Logits\n\n     4                                                  4                                                  4\n\n\n            4       2      0       2       4                    4       2      0       2       4                    4       2      0       2       4\n                  Logits original model                               Logits original model                               Logits original model\n         (a) Fine-tune (all layers).                           (b) CL2.                              (c) Ours CKL with Px.\n\n Figure 3: Distribution of logits of the original model and updated model on FEVER. Fine-tuning all layers (a)\n leads to many errors, and the probability of the predictions does not stay the same even when they do not cross the\n decision boundary. CL2 (b) successfully ﬂips labels, but it does not force the predictions to stay the same. For our\n full method, CKL with Px (c), errors are mainly concentrated around the origin where the model is uncertain, and\n small perturbations make logits to cross the decision boundary. Better view with colors.\n\n\nKNOWLEDGEEDITOR is already competitive in   methods. With an ideal method, all logits before\n equivalence accuracy. However, employing this   and after an update have to stay the same (except\n additional supervision is clearly beneﬁcial on both    the ones we want to change). From that ﬁgure,\n tasks: we get the same success rate and re-train   we can see distributions of different types of er-\n accuracy but equivalence accuracy improves by    rors such as datapoints whose predictions were\n> 70% on FC and > 30% on QA, respectively   mistakenly ﬂipped (from true to false or the other\n (for generated paraphrases). In FC, although ﬁne-  way around). These errors are mostly concentrated\n tuning of a single layer proved to be optimal in   around the origin, where small perturbations make\n terms of success rate and retain accuracy, it per-    logits cross the decision boundary. When ﬁne-\n forms poorly for paraphrases. That is the model    tuning all layers, we can see a clear impact on\n successfully updates the prediction of a particular    logits, they undergo a lot of change (i.e., points do\n datapoint, but does not update predictions of para-   not concentrate around the diagonal). Indeed, ﬁne-\n phrases. This indicates that ﬁne-tuning to edit the    tuning makes many datapoints cross the decision\n knowledge of a model does not generalize well,   boundary and their probabilities to change from\n and it overﬁts to speciﬁc inputs. On QA, also Zhu    the original ones. The failure of CL2 is visible in\n et al. (2020) performs poorly compared to our or    Figure 3b as this method preserves almost none of\n other methods.                                       the previous predictions. Instead KNOWLEDGEED-\n  When other methods perform on par or better   ITOR preserves almost all of the predicted labels\n than ours on paraphrases, they do not have good re-   as well as their probabilities (most datapoints in\n tain accuracy (e.g., see QA ﬁne-tuning on Table 1).   Figure 3c stay on the diagonal).\n Fine-tuning on QA seems to generalize better than                                  We also report visualizations of the average\n on FC, but does not preserve previous knowledge.                                                weight updates for the QA experiment in Figure 4.\n In Table 1 we also report both the accuracy on the                                 We report the setting with additional supervision\n set of generated and human-generated paraphrases.                                            from paraphrases (but the heatmaps are similar\n Surprisingly, the scores on human-generated para-                                                 without them). There are three main observations\n phrases are higher. We speculate that this happens                                            from this plot. First, gradients are mostly concen-\n because automatic paraphrases are sometimes not                                                           trated on the ﬁrst encoder layer and the last decoder\n semantically equivalent or ﬂuent.                                                          layer. Gradients explain why the best subset of\n                                                 parameters to update is the ﬁrst layer. Secondly,\n 6.4  Analysis of model updates\n                                                  ﬁne-tuning does not preserve gradient magnitudes\n In Figure 3 we plot the distribution of logits of   and updates the whole model almost uniformly.\n the original and updated model on FC for different   That happens because of the optimizer’s adaptive\n\n     Encoder       Decoder          Encoder       Decoder          Encoder       Decoder\n                                                                                        WVselfWVself                WVself                 WVself             WV                                                                                                                                    self                 WVself\n               WQself                                WQself                                WQself\nWQself            WOself                WQself            WOself                WQself            WOself\n                WKself                                 WKself                                 WKself\nWOself                    FF2                WO                                                                                             self                    FF2                WO                                                                                                                                                                                      self                    FF2\n                        FF1                                                 FF1                                                 FF1WKself                                 WKself                                 WKself\n                WVenc                                 WVenc                                 WVenc\n FF2            WQenc                          FF2            WQenc                          FF2            WQenc\n               WOenc                                WOenc                                WOenc\n FF1                                                 FF1                                                 FF1                WKenc                                 WKenc                                 WKenc\n     1 2 3 4 5 6       1 2 3 4 5 6            1 2 3 4 5 6       1 2 3 4 5 6            1 2 3 4 5 6       1 2 3 4 5 6\n      Layer          Layer             Layer          Layer             Layer          Layer\n             (a) Gradients.                          (b) Fine-tune (all layers).                 (c) KNOWLEDGEEDITOR + Px.\n\nFigure 4: Average normalized magnitude of updates on weight matrices across layers for the QA experiment.\nFine-tuning updates all layers uniformly while our updates are more sparse.\n\n\nlearning rate that initially erases the gradient di-   based on a hyper-network that learns to modify\nrection. The gradient direction plays a role only    implicit knowledge stored within LM parameters\nafter a couple of gradient steps, but most of the    efﬁciently and reliably. We provide comprehensive\ntime, the method only needs one step to modify its    evaluations for our models against different vari-\nknowledge. Lastly, our updates are sparser and are    ants of ﬁne-tuning demonstrating the advantage of\nnot consistent with the gradient for changing the   our approach. The magnitude of the updates pre-\npredictions. That indicates that our method learns    dicted by our method may unfold the mechanisms\nto use the gradient in a meaningful way (i.e. ignor-   used by the LMs to encode factual knowledge; we\ning some directions or manipulating its magnitude).   leave such investigation for future work.\nIt is surprising that the knowledge manipulation\nseems to be achieved by primarily modifying pa-\n                                             Ethical Considerations\nrameters affecting the shape of the attention distri-\n       K      Q\nbution (W self and W self) rather than, e.g., values                                             Technology built upon pre-trained LMs inherits\n(W self).V   As we discussed, the hyper-network may                                         some or all of their potential harms (Bender et al.,\nbe regarded as a probe providing insights about the\n                                                 2021). Our technology for editing the knowledge\nmechanism used by the model to encode the knowl-\n                                                   of LMs does not exacerbate their potential harms\nedge (Vig et al., 2020). For example, the focus\n                                              and can, in fact, be used to mitigate harms, as mod-\non the bottom layer is already intriguing, as it con-\n                                                           els can be corrected once problems are discovered.\ntrasts with claims that memorization happens in top\n                                              However, we note that malicious uses of our knowl-\nlayers of image classiﬁcation models (Stephenson\n                                             edge editor are possible. For example, malicious\net al., 2021), hinting at substantial differences in\n                                                  agents may use the techniques presented in this\nthe underlying memorization mechanisms in NLP\n                                          work to inject incorrect knowledge into LMs.\nand vision. Proper investigation is however outside\nof the scope of this study. See Appendix C for\nsome additional analysis.                    Acknowledgments\n\n7  Conclusions                           The authors want to thank Michael Schlichtkrull,\n                                            Lena Voita and Luisa Quarta for helpful discussions\nIn this work, we explore the task of editing the fac-   and support.  This project is supported by SAP\ntual knowledge implicitly stored in the parameters    Innovation Center Network, ERC Starting Grant\nof Language Models. For this task, we formally   BroadSem (678254), the Dutch Organization for\ndeﬁne desiderata, the objective, and a set of metrics    Scientiﬁc Research (NWO) VIDI 639.022.518, and\nto measure the efﬁcacy of different methods. We    the European Union’s Horizon 2020 research and\nconcretely evaluate that on two benchmarks based    innovation programme under grant agreement No\non closed-book fact-checking and question answer-   825299 (Gourmet).\ning. We propose KNOWLEDGEEDITOR, a method\n\nReferences                                                 of the North American Chapter of the Association\n                                                                for Computational Linguistics: Human Language\nYonatan Belinkov and James Glass. 2019.  Analysis                                                           Technologies, Volume 1 (Long and Short Papers),\n  methods in neural language processing: A survey.                                                      pages 4171–4186, Minneapolis, Minnesota. Associ-\n  Transactions of the Association for Computational                                                              ation for Computational Linguistics.\n   Linguistics, 7:49–72.\n                                                    Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\n                                                       Goldberg. 2021. Amnesic probing: Behavioral ex-\n  Major, and Shmargaret Shmitchell. 2021. On the\n                                                            planation with amnesic counterfactuals.  Transac-\n  dangers of stochastic parrots: Can language models\n                                                                tions of the Association for Computational Linguis-\n  be too big? In Proceedings of the 2021 ACM Confer-\n                                                                           tics, 9:160–175.\n  ence on Fairness, Accountability, and Transparency,\n  FAccT ’21, page 610–623, New York, NY, USA. As-                                                        Thibault Févry, Livio Baldini Soares, Nicholas FitzGer-\n   sociation for Computing Machinery.                                                                   ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\n                                                                            tities as experts: Sparse memory access with entityStephen Boyd, Stephen P Boyd, and Lieven Vanden-\n                                                             supervision. In Proceedings of the 2020 Conference   berghe. 2004. Convex optimization. Cambridge uni-\n                                                 on Empirical Methods in Natural Language Process-   versity press.\n                                                          ing (EMNLP), pages 4937–4951, Online. Associa-\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie       tion for Computational Linguistics.\n  Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n                                                    Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.   Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                       Model-agnostic meta-learning for fast adaptation of   Askell,  Sandhini  Agarwal,  Ariel  Herbert-Voss,\n                                                    deep networks.  In Proceedings of the 34th Inter-  Gretchen Krueger, Tom Henighan, Rewon Child,\n                                                           national Conference on Machine Learning, ICML  Aditya Ramesh, Daniel M. Ziegler,  Jeffrey Wu,\n                                                       2017, Sydney, NSW, Australia, 6-11 August 2017,  Clemens Winter, Christopher Hesse, Mark Chen,\n                                                  volume 70 of Proceedings of Machine Learning Re-   Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n                                                            search, pages 1126–1135. PMLR.  Chess, Jack Clark, Christopher Berner, Sam Mc-\n   Candlish, Alec Radford, Ilya Sutskever, and Dario\n                                                 David Ha, Andrew M. Dai, and Quoc V. Le. 2017.  Amodei. 2020. Language models are few-shot learn-\n                                                      Hypernetworks.   In 5th International Conference   ers. In Advances in Neural Information Processing\n                                                 on Learning Representations, ICLR 2017, Toulon,  Systems 33: Annual Conference on Neural Informa-\n                                                        France, April 24-26, 2017, Conference Track Pro-   tion Processing Systems 2020, NeurIPS 2020, De-\n                                                           ceedings. OpenReview.net.  cember 6-12, 2020, virtual.\n\n                                               Sepp  Hochreiter  and  Jürgen  Schmidhuber.  1997.Róbert Csordás, Sjoerd van Steenkiste, and Jürgen\n                                               Long short-term memory.   Neural computation,  Schmidhuber. 2021.   Are neural nets modular?\n                                                       9(8):1735–1780.   inspecting functional modularity through differen-\n   tiable weight masks. In Submitted to International\n                                               Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\n  Conference on Learning Representations.\n                                                      Neubig. 2020. How can we know what language\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and      models know?  Transactions of the Association for\n                                                     Computational Linguistics, 8:423–438.  Fabio Petroni. 2021a.   Autoregressive entity re-\n   trieval.  In International Conference on Learning\n                                              Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke   Representations.\n                                                          Zettlemoyer. 2017.  TriviaQA: A large scale dis-\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker       tantly supervised challenge dataset for reading com-\n  Aziz, and Ivan Titov. 2020.  How do decisions       prehension. In Proceedings of the 55th Annual Meet-\n  emerge across layers in neural models?  interpreta-      ing of the Association for Computational Linguistics\n   tion with differentiable masking. In Proceedings of      (Volume 1: Long Papers), pages 1601–1611, Van-\n   the 2020 Conference on Empirical Methods in Nat-      couver, Canada. Association for Computational Lin-\n   ural Language Processing (EMNLP), pages 3243–       guistics.\n  3255, Online. Association for Computational Lin-\n                                                 Marcin  Junczys-Dowmunt,  Roman  Grundkiewicz,   guistics.\n                                                Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nNicola De Cao, Ledell Wu, Kashyap Popat, Mikel     Tom Neckermann, Frank Seide, Ulrich Germann,\n   Artetxe,  Naman  Goyal,   Mikhail  Plekhanov,     Alham Fikri Aji, Nikolay Bogoychev, André F. T.\n  Luke  Zettlemoyer,  Nicola Cancedda,  Sebastian       Martins, and Alexandra Birch. 2018. Marian: Fast\n   Riedel, and Fabio Petroni. 2021b.   Multilingual       neural machine translation in C++. In Proceedings\n   autoregressive  entity  linking.    arXiv  preprint       of ACL 2018, System Demonstrations, pages 116–\n  arXiv:2103.12528.                                     121, Melbourne, Australia. Association for Compu-\n                                                                  tational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\n   Kristina Toutanova. 2019.  BERT: Pre-training of    Diederik P. Kingma and Jimmy Ba. 2015. Adam: A\n  deep bidirectional transformers for language under-     method for stochastic optimization.  In 3rd Inter-\n   standing.  In Proceedings of the 2019 Conference       national Conference on Learning Representations,\n\n  ICLR 2015, San Diego, CA, USA, May 7-9, 2015,      2021.  GPT Understands, Too.   arXiv preprint\n  Conference Track Proceedings.                         arXiv:2103.10385.\n\nJ. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz,   Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\n   J. Veness, G. Desjardins, Andrei A. Rusu, K. Milan,      Lewis,  Majid Yazdani,  Nicola De Cao,  James\n  John Quan, Tiago Ramalho, Agnieszka Grabska-      Thorne, Yacine Jernite, Vladimir Karpukhin, Jean\n  Barwinska, Demis Hassabis, C. Clopath, D. Ku-      Maillard, Vassilis Plachouras, Tim Rocktäschel, and\n  maran, and Raia Hadsell. 2017. Overcoming catas-      Sebastian Riedel. 2021.  KILT: a benchmark for\n   trophic forgetting in neural networks. Proceedings      knowledge intensive language tasks. In Proceedings\n   of the National Academy of Sciences, 114:3521 –       of the 2021 Conference of the North American Chap-\n  3526.                                                          ter of the Association for Computational Linguistics:\n                                          Human Language Technologies, pages 2523–2544,\nDavid Krueger, Chin-Wei Huang, Riashat Islam, Ryan                                                          Online. Association for Computational Linguistics.\n   Turner, Alexandre Lacoste, and Aaron Courville.\n  2017.  Bayesian hypernetworks.   arXiv preprint\n                                                    Fabio  Petroni, Tim Rocktäschel,  Sebastian Riedel,\n  arXiv:1710.04759.\n                                                             Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\n                                                      Alexander Miller. 2019. Language models as knowl-Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n                                                    edge bases?   In Proceedings of the 2019 Confer-   ﬁeld, Michael Collins, Ankur Parikh, Chris Al-\n                                                      ence on Empirical Methods in Natural Language   berti, Danielle Epstein, Illia Polosukhin, Jacob De-\n                                                        Processing and the 9th International Joint Confer-   vlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n                                                      ence on Natural Language Processing (EMNLP-  Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\n                                                   IJCNLP), pages 2463–2473, Hong Kong, China. As-  Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n                                                              sociation for Computational Linguistics.   Natural questions: A benchmark for question an-\n  swering research.  Transactions of the Association\n   for Computational Linguistics, 7:453–466.            Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\n                                                      Dario Amodei, and Ilya Sutskever. 2019. Language\nYair Lakretz, German Kruszewski, Theo Desbordes,      models are unsupervised multitask learners. OpenAI\n  Dieuwke Hupkes, Stanislas Dehaene, and Marco Ba-       blog, 1(8):9.\n   roni. 2019.  The emergence of number and syn-\n   tax units in LSTM language models.  In Proceed-   Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\n   ings of the 2019 Conference of the North American       ine Lee, Sharan Narang, Michael Matena, Yanqi\n  Chapter of the Association for Computational Lin-      Zhou, Wei Li, and Peter J. Liu. 2020.  Exploring\n   guistics: Human Language Technologies, Volume 1       the limits of transfer learning with a uniﬁed text-to-\n  (Long and Short Papers), pages 11–20, Minneapolis,       text transformer. Journal of Machine Learning Re-\n  Minnesota. Association for Computational Linguis-      search, 21(140):1–67.\n   tics.\n                                                   Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau      Percy Liang. 2016. SQuAD: 100,000+ questions for\n  Yih, Hao Ma, and Madian Khabsa. 2020. Language      machine comprehension of text. In Proceedings of\n  models as fact checkers?   In Proceedings of the       the 2016 Conference on Empirical Methods in Natu-\n  Third Workshop on Fact Extraction and VERiﬁca-       ral Language Processing, pages 2383–2392, Austin,\n   tion (FEVER), pages 36–41, Online. Association for      Texas. Association for Computational Linguistics.\n  Computational Linguistics.\n                                             Marco  Tulio  Ribeiro,  Sameer  Singh,  and  Carlos\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\n                                                            Guestrin. 2016.  Model-agnostic interpretability of\n   Zettlemoyer. 2017. Zero-shot relation extraction via\n                                                   machine learning. International Conference on Ma-\n   reading comprehension. In Proceedings of the 21st\n                                                         chine Learning (ICML) Workshop on Human Inter-\n  Conference on Computational Natural Language\n                                                                  pretability in Machine Learning.\n  Learning (CoNLL 2017), pages 333–342, Vancou-\n   ver, Canada. Association for Computational Linguis-\n                                       Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.   tics.\n                                       How much knowledge can you pack into the param-\nMike  Lewis,  Yinhan  Liu,  Naman  Goyal,  Mar-       eters of a language model?  In Proceedings of the\n   jan Ghazvininejad, Abdelrahman Mohamed, Omer      2020 Conference on Empirical Methods in Natural\n  Levy,  Veselin Stoyanov,  and Luke Zettlemoyer.     Language Processing (EMNLP), pages 5418–5426,\n  2020. BART: Denoising sequence-to-sequence pre-      Online. Association for Computational Linguistics.\n   training for natural language generation, translation,\n  and comprehension. In Proceedings of the 58th An-   Rico Sennrich, Barry Haddow, and Alexandra Birch.\n  nual Meeting of the Association for Computational      2016.  Improving neural machine translation mod-\n   Linguistics, pages 7871–7880, Online. Association        els with monolingual data.  In Proceedings of the\n   for Computational Linguistics.                         54th Annual Meeting of the Association for Compu-\n                                                                tational Linguistics (Volume 1: Long Papers), pages\nXiao  Liu,  Yanan  Zheng,  Zhengxiao  Du,  Ming      86–96, Berlin, Germany. Association for Computa-\n  Ding,  Yujie Qian,  Zhilin Yang,  and  Jie Tang.       tional Linguistics.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,      and VERiﬁcation.   In Proceedings of the 2018\n   Eric Wallace, and Sameer Singh. 2020. AutoPrompt:      Conference  of  the North American Chapter  of\n   Eliciting Knowledge from Language Models with       the  Association  for  Computational  Linguistics:\n  Automatically Generated Prompts.   In Proceed-    Human Language Technologies, Volume 1 (Long\n   ings of the 2020 Conference on Empirical Methods       Papers), pages 809–819, New Orleans, Louisiana.\n   in Natural Language Processing (EMNLP), pages      Association for Computational Linguistics.\n  4222–4235, Online. Association for Computational\n   Linguistics.                                     Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture\n                                               6.5—RmsProp: Divide the gradient by a running av-\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,      erage of its recent magnitude. COURSERA: Neural\n   Sergei Popov, and Artem Babenko. 2020. Editable      networks for machine learning, 4(2):26–31.\n   neural networks. In 8th International Conference on\n  Learning Representations, ICLR 2020, Addis Ababa,   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n   Ethiopia, April 26-30, 2020. OpenReview.net.             Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n                                                             Kaiser, and Illia Polosukhin. 2017. Attention is all\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,     you need. In Advances in Neural Information Pro-\n   Ilya Sutskever, and Ruslan Salakhutdinov. 2014.      cessing Systems 30: Annual Conference on Neural\n  Dropout: A simple way to prevent neural networks      Information Processing Systems 2017, December 4-\n  from overﬁtting. Journal of Machine Learning Re-       9, 2017, Long Beach, CA, USA, pages 5998–6008.\n   search, 15(56):1929–1958.\n                                                        Pat Verga, Haitian Sun, Livio Baldini Soares, and\nCory Stephenson, Suchismita Padhy, Abhinav Ganesh,      William W Cohen. 2020.  Facts as experts: Adapt-\n  Yue Hui, Hanlin Tang, and SueYeon Chung. 2021.      able and interpretable neural memory over symbolic\n  On the geometry of generalization and memoriza-      knowledge. arXiv preprint arXiv:2007.00849.\n   tion in deep neural networks. Proceedings of Inter-\n   national Conference on Learning Representations    Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\n  (ICLR).                                            Sharon Qian, Daniel Nevo, Yaron Singer, and Stu-\n                                                                      art Shieber. 2020.  Causal mediation analysis for\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi       interpreting neural NLP: The case of gender bias.\n  Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao      NeurIPS.\n   Tian, and Hua Wu. 2019.  Ernie: Enhanced rep-\n   resentation through knowledge integration.  arXiv    Elena Voita, Rico Sennrich, and Ivan Titov. 2019. The\n   preprint arXiv:1904.09223.                           bottom-up evolution of representations in the trans-\n                                                          former: A study with machine translation and lan-\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao      guage modeling objectives.  In Proceedings of the\n   Tian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:     2019 Conference on Empirical Methods in Natu-\n A continual pre-training framework for language un-       ral Language Processing and the 9th International\n   derstanding.  Proceedings of the AAAI Conference       Joint Conference on Natural Language Processing\n  on Artiﬁcial Intelligence, 34(05):8968–8975.           (EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\n                                                        China. Association for Computational Linguistics.\nIlya Sutskever, James Martens, and Geoffrey E. Hin-\n   ton. 2011.   Generating text with recurrent neu-   John Wieting and Kevin Gimpel. 2018.  ParaNMT-\n   ral networks.   In Proceedings of the 28th Inter-     50M: Pushing the limits of paraphrastic sentence em-\n   national Conference on Machine Learning, ICML      beddings with millions of machine translations. In\n  2011, Bellevue, Washington, USA, June 28 - July 2,      Proceedings of the 56th Annual Meeting of the As-\n  2011, pages 1017–1024. Omnipress.                      sociation for Computational Linguistics (Volume 1:\n                                                Long Papers), pages 451–462, Melbourne, Australia.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.      Association for Computational Linguistics.\n  Sequence to sequence learning with neural networks.\n   In Advances in Neural Information Processing Sys-   Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n  tems 27:  Annual Conference on Neural Informa-     Chaumond, Clement Delangue, Anthony Moi, Pier-\n   tion Processing Systems 2014, December 8-13 2014,       ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\n  Montreal, Quebec, Canada, pages 3104–3112.              icz, Joe Davison, Sam Shleifer, Patrick von Platen,\n                                                         Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,      Teven Le Scao, Sylvain Gugger, Mariama Drame,\n  Jonathon Shlens, and Zbigniew Wojna. 2016.  Re-      Quentin Lhoest, and Alexander Rush. 2020. Trans-\n   thinking the inception architecture for computer vi-      formers:  State-of-the-art natural language process-\n   sion.  In 2016 IEEE Conference on Computer Vi-       ing. In Proceedings of the 2020 Conference on Em-\n   sion and Pattern Recognition, CVPR 2016, Las Ve-       pirical Methods in Natural Language Processing:\n   gas, NV, USA, June 27-30, 2016, pages 2818–2826.      System Demonstrations, pages 38–45, Online. Asso-\n  IEEE Computer Society.                                   ciation for Computational Linguistics.\n\nJames   Thorne,    Andreas   Vlachos,    Christos   Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\n   Christodoulopoulos,   and  Arpit   Mittal.  2018.     Maosong Sun, and Qun Liu. 2019.  ERNIE: En-\n  FEVER: a large-scale dataset for fact extraction      hanced language representation with informative en-\n\n   tities.   In Proceedings of the 57th Annual Meet-\n   ing of the Association for Computational Linguis-\n   tics, pages 1441–1451, Florence, Italy. Association\n   for Computational Linguistics.\n\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Sri-\n  nadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\n  Kumar. 2020. Modifying memories in transformer\n  models. arXiv preprint arXiv:2012.00363.\n\nA  Relaxation and Approximation of         of the knowledge acquired in pre-training and en-\n    Constrained Optimization               coded in the parameters of the model. For this\n                                                         task, we used the Zero-Shot Relation Extraction\nGiven a objective to minimize in the form of\n                                              (zsRE) dataset by Levy et al. (2017). We pre-\n        min   E   [f(x, θ)]                      fer zsRE to other popular QA datasets such as\n           φ    x∼p(x)                  SQuAD (Rajpurkar et al., 2016), Natural Ques-\n               1                            (4)    tions (Kwiatkowski et al., 2019) or TriviaQA (Joshi\n              s.t.  X C(y, θ) ≤m ,\n                |Y|                                    et al., 2017) because it is annotated with human-\n                  x∈Y\n                                                  generated question paraphrases that we can use to\ncan be solved with Lagrangian relaxation (Boyd    evaluate our model’s robustness to semantically\net al., 2004) using a multiplier α ∈R≥0 and be    equivalent inputs. zsRE is speciﬁcally constructed\napproximated by sampling y ∼p(y) to              not to have relation overlaps between training and\n                                                             test (i.e.  it is zero-shot). We re-split the dataset\n   min max f(x, θ) + α · (C(y, θ) −m)  .    (5)    to have the same distribution in training and test\n    φ   α\n                                             splits—we are not interested in zero-shot specif-\nEquation 5 can be evaluated with automatic differ-    ically, so we avoid the additional complexity it\nentiation and optimized via gradient descent.          entails. The original zsRE dataset has 147,909\n                                                         training and 3,724 validation instances respectively.\nB  Experimental setting                       After re-splitting and employing all paraphrases,\n                                    we have 244,173 training and 27,644 validation\nB.1  Fact-checking\n                                                    instances respectively. For this task, we ﬁne-tune\nWe evaluate on closed-book fact-checking (FC)                                                a BART base model (Lewis et al., 2020) with a\nusing the binary FEVER dataset (Thorne et al.,                                                  standard seq2seq objective, i.e., maximizing the\n2018) from KILT (Petroni et al., 2021). FEVER has                                          model likelihood given the observed output se-\n104,966 training and 10,444 validation instances                                             quence (Sutskever et al., 2011, 2014) and regu-\nrespectively. For every input claim x, the model                                                       larized with dropout (Srivastava et al., 2014) and\npredicts the probability f(x; θ) that it may be true.                                                       label smoothing (Szegedy et al., 2016). The ﬁnal\nThis is done without retrieving any evidence from                                            model has an accuracy (exact match between model\na corpus, instead, just by relying on the knowledge                                                     prediction and gold standard) of 22.1%.10\naccumulated during pre-training and encoded in its\nown parameters—this is similar to Lee et al. (2020)\nthat investigate closed-book and zero-shot FC us-   B.3  Generating alternative predictions\ning masked-LMs. Concretely, we ask the LM to\nperform binary classiﬁcation. We ﬁne-tune a BERT   Generation  of  alternative  predictions  is  task-\nbase model (Devlin et al., 2019) with an additional   dependent as it requires producing a plausible sub-\nlinear layer on top that maps the hidden state cor-    stitute target for a given input—e.g., if we need to\nresponding to the BOS (beginning of a sentence)    edit the knowledge about a head of a state, a plau-\ntoken to the probability of the positive label. Given    sible substitute label should be a person, not a ran-\nthe available supervision, we train the architecture   dom (even if well-formed) string. Fact-Checking\nto maximize the model likelihood penalized by en-    is straightforward: we simply ﬂip the label, as it\ntropy regularization and weight decay. The ﬁnal     is binary classiﬁcation. For QA, we exploit high-\nmodel has an accuracy of 77.1%.9                    probability outcomes under the model distribution\n                                                    as a proxy to plausible revisions. In particular, we\nB.2  Question answering                                                 pick all hypotheses enumerated via beam search\nWe also evaluate on a task with a more com-   except the top-1.11\nplex sample space: closed-book question answer-\ning (QA). Here QA is treated as a sequence-to-\n                                                                    10This is more than reported by Petroni et al. (2021) on thesequence problem from question to answer with-\n                                                                   original split of zsRE. That is because the original split aims\nout retrieving nor providing any evidence (Roberts     at zero-shot evaluation, while we have an overlap of relation\net al., 2020). This, as in FC, emphasises the role    types between training and validation sets.\n                                                                  11This does not always guarantee that the alternative pre-\n    9This is comparable with what reported by Petroni et al.    dictions have the same semantic type as the original one, but\n(2021) for a larger BART model.                                               it is likely since the model assigns high probability to them.\n\nB.4  Semantically equivalent inputs          Adam (Kingma and Ba, 2015) (learning rate of\n                                                   3e-5) with weight decay (1e-2) and a linear sched-We would like the updated model to be consistent\n                                                      ule with warm-up (50k total number of updates andfor semantically equivalent inputs (see Px in Sec-\n                                            500 warm-up updates). We trained for a maximumtion 2 and 4) as opposed to just learning a new\n                                                   of 20 epochs and employ model selection usingspeciﬁc and isolated datapoint. This consistency is\n                                                 accuracy on the validation set.12indicative of an effective editing mechanism that\n                                     KNOWLEDGEEDITOR models are trained withtaps into the knowledge stored in the model. How-\n                                                a batch size of 1024 for FC and 256 for QA usingever, not all datasets come with paraphrases of its\n                                    Adam (learning rate of 3e-4 for the parameters andinputs (e.g., in our case FEVER does not come\n                                                 1e-1 for the Lagrangian multiplier) with weight de-with paraphrases and zsRE only has paraphrases\n                                              cay (1e-2) and a linear schedule with a warm-upfor 30% for the dataset). To this end, we gen-\n                                               (200k total number of updates and 1k warm-up up-erate semantically equivalent inputs using round-\n                                                        dates). We trained for a maximum of 200 epochstrip translation (Sennrich et al., 2016; Wieting and\n                                              and employ model selection using overall accuracyGimpel, 2018). We employ English-to-German\n                                                   (success rate and retain accuracy) on the valida-and German-to-English Transformer models from\n                                                        tion set (approximated using mini-batches).13 TheMarian Neural Machine Translation (MarianNMT;\n                                             margin for the CKL is annealed between 1e-1 andJunczys-Dowmunt et al., 2018) provided by Hug-\n                                                 1e-3 for the fact-checking model, and between 1e-3gingface Transformers (Wolf et al., 2020). We use\n                                             and 1e-5 for the BART question answering model.beam search with beam size 5 to obtain 25 para-\n                                              For the sequence-to-sequence loss, we employ aphrases. From this set, we exclude any candidate\n                                                    cross-entropy loss with label smoothing of 0.1.paraphrase ˆx of x for which the prediction ˆy sup-\nported by f(ˆx; θ) does not match the prediction y                           C  Additional Results\nsupported by f(x; θ). This ﬁltering ensures that, ac-\ncording to the current model, all paraphrases have   Update Analysis  During preliminary  experi-\nthe exact same prediction.                         ments, we studied a version of our hyper-network\n                                                           that did not exploit gradient information (see Equa-\nB.5  Architecture details                           tion 3). Without gradient information, on FC the\n                                            models converged ≈10 times slower to reach theThe original models we want to modify are a BERT\n                                         same accuracy and did not converge for QA (i.e.,base model (Devlin et al., 2019) and a BART base\n                                                    the model was not able to get > 75% success ratemodel (Lewis et al., 2020) for fact-checking and\n                                            and > 50% retain accuracy). That suggest thatquestion answering respectively. They are both\n                                                    the gradients are helpful and actually used by ourTransformer based models with 12 layers each and\n                                               hyper-network but should not used directly, with-hidden size of 768. BERT has 12 heads, where\n                                                  out a modiﬁcation. To better show this, in Table 2BART has 16. They have 110M and 139M param-\n                                    we report correlations between different updateeters respectively. BERT has a vocabulary size of\n                                            methods and the gradient in terms of cosine simi-30,522 where BART has 50,265.\n                                                                 larities between updates. Naturally, ﬁne-tuning and  KNOWLEDGEEDITOR has a small single-layered\n                                                     the gradient are highly correlated, but our methodbidirectional-LSTM with input size 768 and hid-\n                                                  (with and without additional paraphrases supervi-den size of 128. The FFNN that condenses the\n                                                           sion), poorly correlates with the others. Low cosineLSTM states follows a [256, tanh, 1024] architec-\n                                                       similarity can be due to two factors i) the modelture where the 5 FFNN have all a [1024, tanh, d]\n                                                  indeed projects the gradient to a different and morearchitecture where d depends on the weight to mod-\n                                              ‘knowledge preserving’ direction, or ii) the param-ify. In our experiments, we do not use our model\n                                                         eter space is so large that cosine similarity gets toto modify biases, layer norms, word and positional\n                                                   zero very quickly, not revealing the genuine under-embeddings of LMs. Overall, KNOWLEDGEEDI-\n                                                     lying similarity.TOR has 54M and 67M parameters for BERT and\nBART respectively.\n\nB.6  Training details                                12We trained on 4 Nvidia Titian X 12GB which take ap-\n                                                            proximately 10 minutes for FC and 3 hours for QA.\nThe original models which we want to mod-                                                      13We trained on 4 Nvidia Titian X 12GB which take ap-\nify are trained with a batch size of 256 using    proximately 1 day for FC and 3 days for QA.\n\n          ∇θL    Fine-tune   CKL   CKL + Px\n  ∇θL         1.000     0.451     -0.018     -0.025                                 System B\n  Fine-tune     0.451     1.000     -0.010     -0.011\n  CKL         -0.017     -0.010     1.000      0.183                                                                           layer)      layers)     layer)      layers)                                                                                     x       x+loop  CKL + Px   -0.021     -0.011     0.183      1.000                                                                                                                                                   (1st   (all   (1st   (all\n                                                                                 al  al                                                                                                                         et.  et.   CL2   CKL       CKL+loop   CKL+   CKL+\n Table 2: Average cosine similarities between different                                                                                                                                                                                                                                                                                                                                           fine-tune       fine-tune  Zhu  Zhu   Ours  Our   Ours   Ours   Ours\n update methods and the gradient for the update as well.\n                                                                                                       fine-tune (1st layer)        22.4  100.0  11.7  69.0   6.0    5.2    4.7    2.8\n Fine-tuning is applied to all layers.\n                                                                                                      fine-tune (all layers)  77.6         78.3  53.3  99.4  49.9  49.8   6.3    2.7\n\n                                                                               Zhu et. al (1st layer)   0.0   21.7         11.2  68.2   5.7    5.0    4.6    2.7\n\n                   A Zhu et. al (all layers)  88.3  46.7  88.8         92.9  32.8  39.3   3.0    0.2\n\n                                                                                                 Ours CL2  31.0   0.6   31.8   7.1          7.8    8.2    0.9    0.8\n                                                                                                                System            Our CKL  94.0  50.1  94.3  67.2  92.2         71.5   1.5    0.4\n\n                 fine-tune (1st layer)                                                            Ours CKL+loop  94.8  50.2  95.0  60.7  91.8  28.5          5.4    0.0\n                                  (all                               layers)   0.6        fine-tune\n             Zhu et. al                            (1st                                 layer)                                                              Ours CKL+  x  95.3  93.7  95.4  97.0  99.1  98.5  94.6         42.0\n             Zhu et. al (all layers)\n   0.5       Ours CL2                                                                   Ours CKL+ x+loop  97.2  97.3  97.3  99.8  99.2  99.6  100.0  58.0\n              Our CKL\n   0.4       Ours CKL+loop                                  x                                                                        (a) Fact-checking.              Ours CKL+\n              Ours CKL+ x+loopDensity0.3                                                                         System B\n   0.2                                                                                                                                                                                                              layer)      layers)     layer)      layers)                                                                                     x       x+loop                                                                                                                                                   (1st   (all   (1st   (all   0.1                                                                            al  al                                                                                                                         et.  et.   CL2   CKL       CKL+loop   CKL+   CKL+   0.0                                                                                                                                                                                                                                                                                                                    fine-tune       fine-tune  Zhu  Zhu   Ours  Our   Ours   Ours   Ours        88     90     92     94     96     98    100\n                    wTs with w   Dir(1.0)                                         fine-tune (1st layer)        67.8  99.9  99.0  78.6  62.5   0.3   23.8   0.1\n                        (a) Fact-checking.                                                                                                      fine-tune (all layers)  32.2         81.4  77.1  93.0  41.1  20.0  30.9  11.3\n\n                 fine-tune (1st layer)                                                  Zhu et. al (1st layer)   0.1   18.6          1.7   33.4   0.0    0.0    0.0    0.0\n   0.5        fine-tune (all layers)\n             Zhu et. al (1st layer)           A Zhu et. al (all layers)   1.0   22.9  98.3         37.4   0.0    0.0    0.0    0.0\n             Zhu et. al (all layers)\n   0.4       Ours CL2                                                                           Ours CL2  21.4   7.0   66.6  62.6         30.1  14.0  21.9   7.8\n              Ours CKL              Ours CKL+loop                                                                           System            Our CKL  37.5  58.9  100.0 100.0  69.9          1.1    2.7    1.1\n                                  x   0.3       Ours CKL+\n              Ours CKL+ x+loop                                                              Ours CKL+loop  99.7  80.0  100.0 100.0  86.0  98.9         80.2   4.5Density\n\n                                                                                                                                                   x  76.2  69.1  100.0 100.0  78.1  97.3  19.8          2.7   0.2                                                                                      Ours CKL+\n   0.1                                                                                 Ours CKL+ x+loop  99.9  88.7  100.0 100.0  92.2  98.9  95.5  97.3\n\n                                                                                       (b) Question answering.\n   0.0        88     90     92     94     96     98    100\n                    wTs with w   Dir(1.0)                   Figure 6: Probability that system A is better than sys-\n                    (b) Question answering.                 tem B according to a weighted sum of metrics (see indi-\n                                                         vidual values in Table 1) sampling mixing coefﬁcients\n Figure 5: Probability distributions of weighted sum of    1, 000 times from a Dirichlet distribution (with α = 1\n metrics according to 1k random assignments sampled    to cover a diverse spectrum of metric combinations).\n from a Dirichlet distribution (with α = 1—see all val-   The probability that KNOWLEDGEEDITOR (with CKL\n ues in Table 1). Sampling weights allows to interpret   + Px + loop) is better than competing systems is high\n the score in a probabilistic way. KNOWLEDGEEDITOR   (> 97% for FC and > 88% for QA) indicating that it\n (with different variants) presents distributions that are     is highly likely that when assigning some weights to\n more skewed towards a high score (100) indicating that    the metrics, the weighted sum will be in favour of our\n it is highly likely that when assigning some weights to    method. Better view with colors.\n the metrics, the weighted sum will be in favour of our\n method. Better view with colors.\n\n## Claim to Classify\n\"The factual knowledge acquired during pre-training and stored in the parameters of Language Models\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "KnowledgeEditor (hyper-network for parameter updates)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 4,
    "hedge_flag": 0,
    "reasoning": "1. **Method Rung (2 - Interventional):** The paper introduces KNOWLEDGEEDITOR, a hyper-network that predicts weight updates to modify factual knowledge in language models. This method involves direct interventions on model parameters (e.g., editing weights to change predictions for specific inputs while preserving others), which aligns with Rung 2 (Interventional) methods as per the codebook. The use of constrained optimization to enforce specific behavioral changes further supports this classification.\n\n2. **Claim Rung (3 - Mechanistic/Counterfactual):** The claim states that factual knowledge is \"stored in the parameters\" of language models. The term \"stored\" implies a mechanistic interpretation of how knowledge is represented or encoded within the model, which is a Rung 3 claim. The codebook specifies that terms like \"stores,\" \"encodes,\" or \"represents\" typically indicate Rung 3 claims, especially when they suggest a mechanistic narrative about how the model functions internally.\n\n3. **Overclaim Gap:** The claim (Rung 3) exceeds the method's capabilities (Rung 2). While the method demonstrates causal effects of parameter edits (e.g., changing predictions for specific inputs), it does not establish that the model \"stores\" knowledge in a mechanistic or unique way. The codebook notes that storage/encoding language often implies Rung 3 claims, which are not fully supported by interventional methods alone.\n\n4. **Hedge Flag (0):** The claim is stated as an established fact (\"stored in the parameters\") without explicit hedging (e.g., \"may,\" \"suggests\").\n\n5. **Confidence (4):** The classification is confident but not definitive due to minor ambiguity in interpreting \"stored.\" The codebook's decision tree for polysemous terms (e.g., \"encodes\"/\"stores\") suggests defaulting to Rung 3 for mechanistic claims unless the context explicitly limits the interpretation to decodability (Rung 1). Here, the context leans toward a mechanistic reading, but the lack of counterfactual or uniqueness testing (e.g., causal scrubbing) introduces slight uncertainty."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 26451,
      "completion_tokens": 513,
      "total_tokens": 26964
    },
    "finish_reason": "stop"
  }
}