{
  "timestamp": "2026-02-19T10:23:06.947282+00:00",
  "paper_id": "2505.24859",
  "claim_id": "2505.24859-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2505.24859\nTitle: Beyond Multiple Choice: Steering for Summarization\nFull text:\n                Beyond Multiple Choice: Evaluating Steering Vectors\n                            for Adaptive Free-Form Summarization\n\n\n\n                                 Joschka Braun 1 Carsten Eickhoff 1 Seyed Ali Bahrainian 1\n\n\n                         Abstract               A promising fourth strategy is activation engineering, an\n                                                              emerging field focused on directly modifying model ac-\n                Steering vectors are a lightweight method for con-         tivations during text generation (Zou et al., 2025). Con-\n                  trolling text properties by adding a learned bias to          trastive Activation Addition (CAA) (Rimsky et al., 2024),\n              language model activations at inference time. So        an interpretability-inspired activation engineering method,2025            far, steering vectors have predominantly been eval-       shows considerable promise in aligning foundation models\n               uated in multiple-choice settings, while their ef-        with user preferences. Although previous research demon-\n                fectiveness in free-form generation tasks remains          strates the effectiveness of steering methods in multiple-Jul\n                understudied. Moving \"Beyond Multiple Choice,\"         choice settings and simplified toy tasks, their effectiveness\n                                                                            for practical NLP tasks like adaptive free-form summariza-13      we thoroughly evaluate the effectiveness of steer-               ing vectors in adaptively controlling topical focus,         tion remains understudied. Our work addresses this gap by\n                sentiment, toxicity, and readability in abstractive         applying CAA to adaptive free-form summarization on the\n             summaries of the NEWTS dataset. We find that     NEWTS dataset (Bahrainian et al., 2022). Adaptive sum-\n                 steering effectively controls the targeted summary         marization focuses on generating concise and high-quality\n                 properties, but high steering strengths consistently         abstractive summaries that align with selected user prefer-\n              degrade both intrinsic and extrinsic text quality.         ences, thus providing a rigorous testbed for the practical[cs.LG]            Compared to steering, prompting offers weaker          applicability of steering vectors beyond constrained evalua-\n                control, while preserving text quality. Combin-          tions.\n               ing steering and prompting yields the strongest\n                                                               This paper makes the following contributions:                control over text properties and offers the most\n               favorable efficacy-quality trade-off at moderate           1. We apply activation steering to control topical focus,\n                steering strengths. Our results underscore the             sentiment, toxicity, and readability in adaptive free-form\n                 practical trade-off between control strength and            summaries. With the exception of toxicity, all text prop-\n                 text quality preservation when applying steering               erties can be effectively influenced.\n                vectors to free-form generation tasks.                                                                                2. We evaluate summaries for unwanted side effects on in-\n                                                                                       trinsic and extrinsic text quality, finding that high steer-\n                                                                        ing strengths meaningfully degrade overall summary\n          1. Introduction                                                    quality.\n                                                                                3. We compare activation steering to prompting and their\n         Large pre-trained language models have emerged as the\n                                                                         combination, finding that prompting alone offers weaker\n           preferred method for addressing numerous natural languagearXiv:2505.24859v2                                                                      control but better preserves text quality, while combin-\n          processing (NLP) tasks (Devlin et al., 2019; Brown et al.,\n                                                                        ing methods yields the strongest control and the most\n           2020). Consequently, the ability to adapt foundation models\n                                                                             favorable efficacy-quality trade-off at moderate steering\n            to specific tasks and align their outputs with user preferences\n                                                                                 strengths.\n              is crucial. Previous research on controlling language models\n                                                                                4. We release our source code and steering vector training         can often be classified into three main strategies: prompt\n                                                                                datasets to promote reproducibility and facilitate further           engineering (Shin et al., 2020; Lester et al., 2021; Wei et al.,\n                                                                               research, available at: GitHub Repository.          2022), trainable decoding mechanisms (Deng et al., 2020)\n         and fine-tuning according to specific objectives (Ouyang\n            et al., 2022a; Rafailov et al., 2023).\n\n               1Health NLP Lab, University of Tübingen, Germany. Corre-\n           spondence to: Joschka Braun <joschkacbraun@gmail.com>.\n\n\n\n\n                                                         1\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n2. Related Work                                    summaries, each focussed on either one of the two most\n                                                       prominent topics in the article. There are 50 unique topics.\nLLM-based Controllable Summarization  Generating                                               More details can be found in the Appendix A.1.\nadaptive summaries tailored to user preferences typically\ninvolves fine-tuning existing foundation models, modifying\n                                                                    3.2. Steering Method: Contrastive Activation Addition\nmodel architectures, or employing specialized training pro-\ncedures (Urlana et al., 2024; Bahrainian et al., 2024; Zhang   We use Contrastive Activation Addition (CAA) by Rimsky\net al., 2025; Braun et al., 2025b). For instance, Bahrainian     et al. (2024) as the steering method. To compute the layer-\net al. (2021) introduces an abstractive summarization model    and behavior-specific steering vector sl ∈Rd from training\nthat enables topic-level customization through a novel ’topi-    dataset Dtrain = {(x+i , x−i )}Ntraini=1 , we record residual stream\ncal attention’ mechanism. Similarly, Blinova et al. (2023)    activations at layer l. Activations are recorded at the last\nproposes a two-stage model for document-level text sim-    position of the training sample. The resulting activations are\nplification that first summarizes and then further simplifies    noted al(x+i )) and al(x−i )) respectively. The steering vec-\ncontent using transformers, enhanced by keyword prompts     tor sl ∈Rd is the mean difference between positive and neg-\nand an embedding similarity loss. Bahrainian et al. (2023)     ative activations: sl = 1/|Dtrain| PDtrain al(x+i ) −al(x−i ) .\nuse a Transformer-based architecture for controllable topic-   To steer during inference, we add λsl to the residual stream\nfocused summarization, which modifies the cross-attention     at layer l. Here λ ∈R is the steering strength. Most of our\nmechanism for guiding the topical focus.                    experiments are done with a range of steering strengths.\n\nSteering Vectors for LLM Control  Controlling text gen-    3.3. Topic Representations\neration by adding a steering vector is easier to implement\n                                                The 50 latent topics derived from the LDA model in theand only requires sufficient training data to be effective.\n                                    NEWTS dataset (Bahrainian et al., 2022) provide a com-Steering vectors leverage the interpretability-based insight\n                                                               pelling target for steering language models. Unlike binarythat many human-interpretable text properties like truthful-\n                                                                  qualities such as sentiment or toxicity, these topics rep-ness (Marks & Tegmark, 2024; Li et al., 2023), refusal\n                                                              resent more nuanced, multi-faceted concepts that can be(Arditi et al., 2024) and sentiment (Turner et al., 2023;\n                                                        understood through various representations, making themTigges et al., 2024) are likely represented linearly. Vari-\nous methods based on this insight have been proposed to    an interesting challenge. Steering topical focus is also practi-\n                                                                  cally relevant, for instance, when summarizing informationcontrol LLM outputs (Subramani et al., 2022; Turner et al.,\n                                                                for a particular stakeholder or expert, as it allows for the2023; Rimsky et al., 2024; Li et al., 2023; Hendel et al.,\n                                                               selection of content most important to that specific reader.2023; Todd et al., 2024; Rimsky et al., 2024; Konen et al.,\n                                                        Topic representations are explained in Appendix A.1.1 and2024; Zou et al., 2025).\n                                                            presented in Table 1.\n\nLimitations of Steering Vectors  Despite their appeal as\nlightweight control methods, activation steering methods     3.4. Evaluation of Summaries\nface significant challenges (Braun et al., 2024). Recent                                        We evaluate generated summaries across six key dimensions:\nstudies highlight issues with reliability and generalization,                                                                      intrinsic quality based on text characteristics, extrinsic qual-\nnoting high variance across inputs and instances where steer-                                                                           ity against reference summaries, topical focus relative to\ning produces the opposite of the intended effect (Tan et al.,                                                            predefined topics, sentiment polarity, toxicity and readabil-\n2024; Brumley et al., 2024; Braun et al., 2025a). Further-                                                                                   ity. For robustness, we measure two to four metrics for each\nmore, steering vectors are often evaluated in constrained                                                                    text property.\nsettings, like multiple-choice questions, rather than more\nchallenging free-form generation tasks (Pres et al., 2024;                                                             3.4.1. INTRINSIC QUALITY EVALUATION\nBraun et al., 2024)\n                                                                   Intrinsic quality, assessing the linguistic quality and flu-\n                                                     ency of the generated text without relying on reference\n3. Methods and Experimental Setup\n                                                       summaries, is evaluated to measure undesirable generation\n3.1. NEWTS dataset                                                artifacts.\n\nWe generate summaries for articles from the NEWTS\ndataset by (Bahrainian et al., 2022), designed specifically    Perplexity (PPL):  Perplexity measures how well a pre-\nfor topical summarization. The NEWTS dataset is based    trained language model can predict the generated text se-\non the CNN/DailyMail dataset (Nallapati et al., 2016) and    quence. A lower perplexity score generally indicates higher\nconsists of 2400 training and 600 test samples. Each sample    fluency and text that is more statistically likely according to\nprovides a source article and two human-written reference    the language model (Bengio et al., 2000).\n\n                                                2\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n       Table 1: Table illustrating different types of topic representations and their corresponding representations.\n\n Representation\n                  Representation\n Type\n words             “children”, “child”, “parents”, “birth”, “born”, “kids”, “families”, “mother”, “family”, “care”,\n                   “daughter”, “young”, “girl”, “syndrome”, “adults”,\n n-grams           “children and parents”, “families with children”, “having kids”, “giving birth”, “she became a\n                   mother”, “baby was born”\n  descriptions      “This topic is about having kids, becoming a mother, giving birth, children and their parents, and\n                     families with children when a baby is born.”\n documents        “families with children receive money to support the kids in the UK...”, “Children with special needs\n                 were mentioned in a political campaign...”, “Only half of British children live with both parents...”\n\n\nBigram Repetition (Distinct-2 Word):  Distinct-2 Word    3.4.3. TOPICAL FOCUS EVALUATION\nmeasures textual diversity and penalizes unnatural word rep-\n                                                 To evaluate the alignment of generated summaries with the\netition. It is calculated as the ratio of unique word bigrams\n                                                            intended topics, we utilize three methods to quantify topical\nto the total number of bigrams in the generated text. Lower\n                                                              focus:\nDistinct-2 scores indicate higher repetition, which often cor-\nrelates negatively with human-annotated quality (Li et al.,\n2016).                                              Lemmatization-Based Scoring:  This method processes\n                                                              the generated text by lemmatizing words to their canonical\nCharacter  Bigram  Repetition  (Distinct-2  Char):    forms. Using the LDA model, it matches these lemmas\nDistinct-2 Char assesses fine-grained textual diversity and    against the lemmas of the top topic words identified for the\npenalizes character sequence repetition.  This metric is    relevant topic. The topical focus score is then calculated\ncalculated as the ratio of unique character bigrams to the    as the weighted presence of these lemmas in the summary,\ntotal number of character bigrams. It is particularly useful    normalized by the total weight of all top topic lemmas.\nfor texts without clear word separation and for identifying\nvarious forms of text degradation; lower scores signify\nincreased character bigram repetition and potential quality    Tokenization-Based Scoring:  This approach tokenizes\nissues.                                                       the summary using the bert-base-multilingual-uncased to-\n                                                               kenizer. The score represents the proportion of tokens in\n3.4.2. EXTRINSIC QUALITY EVALUATION                   the summary that match the token IDs derived from the top\n                                                   words of the target LDA topic, providing a direct measure\nTo evaluate extrinsic quality, we measure the similarity                                                             of topical vocabulary usage at the sub-word level.\nand faithfulness of generated summaries to their respective\nNEWTS reference summaries using the following metrics:\n                                                     Dictionary-Based Evaluation:  This method employs a\nROUGE Score:  Recall-Oriented Understudy for Gisting    bag-of-words representation for the summary, utilizing the\nEvaluation (ROUGE) includes three variants that quantify   Gensim dictionary associated with the LDA model. The\nthe overlap between a candidate summary c and a refer-  LDA model infers a topic distribution for the summary, and\nence r. ROUGE-1 and ROUGE-2 respectively assess uni-    the score reflects the computed prevalence of the target topic\ngram and bigram overlap considering recall, precision and    within this distribution.\nF1, while ROUGE-L measures the longest common subse-\nquence. Collectively, these metrics capture content fidelity,    3.4.4. SENTIMENT EVALUATION\nfluency and sequence-level coherence (Lin, 2004).                                                To evaluate the sentiment expressed in the generated sum-\n                                                             maries, we use two approaches:\nBERTScore:  BERTScore (Zhang* et al., 2020) lever-\nages contextual embeddings from the pre-trained trans-\nformer model to compute semantic similarity between two    Lexicon-Based Analysis (VADER): We  incorporate\ntext distributions.  This make the metric robust against   VADER (Valence Aware Dictionary and sEntiment Rea-\nparaphrasing, a key advantage over ROUGE scores. For    soner) (Hutto & Gilbert, 2014), a lexicon and rule-based\nour evaluation, we employ the ‘BERTScorer‘ class with    sentiment analysis tool. VADER provides multiple scores,\nthe microsoft/deberta-xlarge-mnli model (He et al.,    including a normalized compound score ranging from -1\n2021), selected for its strong correlation with human evalua-    (most negative) to +1 (most positive), effective at capturing\ntions of semantic content.                                   sentiment intensity and negation.\n\n                                                3\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nTransformer-Based Analysis:  We leverage a pre-trained     3.5. Prompt Engineering\ntransformer model fine-tuned for sentiment classification:\n                                        We use a consistent prompt structure for all models and steer-\nnlptown/bert-base-multilingual-uncased-sentiment (Town,\n                                                             ing vectors in our primary experiments. The basic prompt x\n2023). We renormalize the model output to -1 to 1.\n                                                                          is designed to elicit a general, neutral three-sentence sum-\n                                                 mary and is formatted as follows:\n3.4.5. TOXICITY EVALUATION\n\nAbstractive summaries must not reproduce hateful, harass-   Write a three sentence summary of the article.\ning, or threatening language. We therefore measure toxicity    Article:\nfor every generated summary with two Transformer classi-   {article}\nfiers. Toxicity is also an challenging property for steering    Summary:\nexperiments, as language models typically undergo exten-\n                                                            In this template, {article} denotes the placeholder forsive post-training alignment to curb the generation of such\n                                                             the input article text. We defer the detailed description ofcontent, making any residual or induced toxicity a notable\n                                                    prompt variations engineered to encourage or discourageoutcome to control.\n                                                               selected text properties to the Appendix B.\n\nToxic-BERT  Toxic-BERT is a BERT-base model fine-\n                                                                    3.6. Language Modelstuned to predict the probabilities for eight labels (toxic,\nsevere_toxic, obscene, threat, insult, identity_attack, sex-                                      We use Meta’s Llama instruction-tuned models in three\nual_explicit, non_toxic) (Devlin et al., 2019). We use the                                                                   sizes:  Llama-3.2-1B, Llama-3.2-3B and Llama-3.1-8B\ntoxic and severe_toxic logits, normalised to the range [0, 1],                                               Llama3Team (2024). These models represent successive\nas separate indicators of surface-level and extreme toxicity.                                                               capability increases across roughly an order of magnitude\n                                                                  in parameter count, allowing us to study the relationship be-\nRoBERTa  toxicity  classifier  This  classifier  distils    tween model scale and summarization performance. The im-\nRoBERTa-base (Liu et al., 2019), producing a binary toxic-    pact of model scale is further investigated in Appendix C.11,\nity score between [0, 1]. It is more conservative calibration    but this aspect is not central to our paper, which primarily fo-\ncomplements Toxic-BERT’s multi-label view.                cuses on the efficacy of steering vectors for free-form adap-\n                                                                     tive summarization. All three models are instruction-tuned\n3.4.6. READABILITY EVALUATION                        using supervised fine-tuning and reinforcement learning\n                                                         with human feedback, making them well-suited for naturalReadability and language complexity are especially impor-\n                                                        language tasks like summarization. They feature a 128k to-tant text properties. Steering for readability is particularly\n                                                      ken context window, sufficient for handling long documents.relevant as it enables the generation of text summaries per-\n                                      We selected these models for their strong performance atsonalized to a user’s specific comprehension level, for in-\n                                                          reasonable sizes, widespread adoption in both academicstance, matching their educational background or literacy\n                                                            research and practical applications, and consistent archi-skills. We therefore quantify the readability of each sum-\n                                                                  tectural design that enables controlled comparison acrossmary with two regression models.\n                                                                   scales.\n\nDistilBERT fine-tuned for readability  The DistilBERT\n                                                                    3.7. Summary Generation and Steering Setupvariant (Sanh et al., 2020) was fine-tuned for readability and\nproduces a continuous score in [−5, 5] with higher values                                                       For summary generation, output was limited to 150 tokens,\nsignifying high readability and negative values low readabil-                                                        a length roughly corresponding to the top 25% of human-\nity.                                                            generated summaries. Steering was applied at specific layers\n                                                                for each Llama model: Layer 8 for the 1B model, Layer\nDeBERTa-V3  Fine-tuned version of DeBERTa-V3 (He   16 for the 3B model, and Layer 24 for the 8B model. This\net al., 2023) to predict U.S. grade levels (1–18). Therefore    layer selection strategy aligns with established heuristics\nlow scores correspond to simple text, and high scores to    and previous literature. Unless otherwise specified, each\ncomplext texts.                                                 setting was evaluated on a random sample of 250 articles\n                                                    from the NEWTS training dataset. As this data is not used\n                                                                 for steering vector training, no data leakage occurs.\n\n\n\n\n\n                                                4\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n4. Results                                                 4.1.3. TOXICITY STEERING\n\n4.1. Steering Vectors successfully control behaviors       Model post-training, particularly instruction tuning, often\n                                                     aims to suppress toxic output, which can make toxicity a\nOur plots show results for the Llama-3.2-1B model, with                                                                          difficult attribute to steer (Ouyang et al., 2022b). Therefore,\nresults for other model sizes found in Appendix C.11. Com-                                                          attempting to control toxicity in such models provides an\nplementing quantitative metrics from the results section,                                                                  interesting case study on steering effectiveness.\nAppendix C.10 provides qualitative summaries illustrating\n                                                                                              Toxicity Steering: Toxicity Scores vs. Steering Strengththe impact of applied methods on text properties, including\n                                                                                          1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\ntext degradation from high steering strengths.                                 Mean        Mean              Mean\n                                                                                                                                        (0-1) 0.8\n4.1.1. TOPIC STEERING                                                                                  Score 0.6\n                                                                                          0.4\nTopic       steering                  is              more challenging                              due                                          to the                                      50 unique\ntopics.       For each                    article,                 we steer the                            summary                                          towards                                                                      its                 Toxicity 0.2\nmost dominant topic.                                                             0.0\n                                                                                                        -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5\n                                                                                                                          Steering Strength\n              Topic Steering: Topic Scores vs. Steering Strength\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                  Figure 3: Steering for toxicity only impacts toxicity for steer-\n            Mean        Mean           Mean                          ing strengths of 2 and larger. The safety-tuned Llama model\n (tid1) 0.8                                                                       is able to avoid generating toxic text until very high steering\n    0.6\n Score                                                            strengths likely shift the activations out-of-distribution, by-\n    0.4                                                      passing post-training and massively degrading text quality.\n Topic 0.2\n                                                             4.1.4. READABILITY STEERING\n    0.0\n            -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5        Readability is a key text property for personalizing sum-\n                                   Steering Strength                                                        maries to a user’s specific comprehension level. However,\nFigure 1: The topic scores for all three metrics, increase     steering for readability can be challenging because its mul-\nmonotonically for steering strengths up to 2. The effect size     tifaceted nature is difficult to represent as a single steering\nof steering strengths between -1 and 1 is relatively small,    direction.\nand there is a noticeable improvement for steering strengths\nlarger than magnitude 1. Applying the vector with a negative         Readability Steering: Readability Scores vs. Steering Strength\n                                                                            0factor makes the topic less dominant. For a steering strength                                   DeBERTa      DistilBERT                   1.0\n                                                                                                     Mean       Mean                       0.5of 5 the text degrades and the topic scores with it.                    5\n                                                                                                                                        Score 10                                                                 0.00.5Score\n4.1.2. SENTIMENT STEERING                                                                                                                     1.0\n                                                                           15                                                                   1.5                                                                                                                                                                                              DeBERTa                                                                                                                                                               2.0DistilBERTSentiment is an established steering target and typically easy        20\n                                                                                                                                                               2.5\nto control (Turner et al., 2023).                                                                           25                                                                   3.0\n        Sentiment Steering: Sentiment Scores vs. Steering Strength                   -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5                                                                                                                      Steering Strength\n1)  1.0       Transformer    VADER\nto          Mean         Mean                                  Figure 4: The readability improves with increased steering\n (-1  0.5                                                          strength. The DeBERTa Scores decrease, the DistilBERT\n Score  0.0                                                  Scores increase, which is both indicate more simple lan-\n                                                    guage is used in the summaries. The trend only breaks for\n      0.5                                                           steering strengths with an absolute value larger than 2. This  Sentiment                                                     break in the trend occurs, as explained later, because the      1.0\n              -5    -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5        generated text quality degrades significantly at these highest\n                                    Steering Strength                         steering strengths.\nFigure 2: Steering vectors successfully control the senti-\n                                                                    4.2. Large Steering Magnitudes Degrade Text Qualityment of generated summaries. Without steering the average\nsentiment is neutral. Negative and positive steering strength                                                               Overall, applying steering vectors with steering multipliers\neffectively shift the average sentiment towards the target                                                       exceeding an absolute value of 2 substantially degrades\npolarity. Both metrics result in similar sentiment scores and                                                         both intrinsic and extrinsic text quality. This degradation is\nmeasure a monotonic increase in sentiment relative to the                                                                 particularly pronounced for the toxicity steering vector, as\napplied steering strength.                                                 shown in Figures 5 and 6.\n\n                                                5\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n     Sentiment Steering: Intrinsic Quality vs. Steering Strength                Toxicity Steering: Intrinsic Quality vs. Steering Strength\n                       Perplexity      Distinct-2 Words      Distinct-2 Chars                                        Perplexity      Distinct-2 Words      Distinct-2 Chars\n   50           Mean        Mean            Mean                               (0-1)      15         Mean        Mean            Mean                               (0-1)\n   40                                                                   1.0                                                                              1.0                                                                         0.8Score                                                                           0.8Score   30                                                                 10\n                                                                         0.6                                                                              0.6                                                                                                                                                                                                                                                              Perplexity  Perplexity 20                                                                                                                                                        0.4                                                                         0.4                                                                        5   10                                                                   0.2Distinctness                                                                           0.2Distinctness\n    0                                                                   0.0                                                                              0.0\n           -5    -2   -1.5   -1   -0.5   0    0.5   1    1.5   2    5                         -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5\n                                Steering Strength                                                                Steering Strength\n\nFigure 5: In both cases, intrinsic text quality decreases for larger steering strengths. But the change is much more pronounced\nfor toxicity steering compare to sentiment steering. For toxicity, steering strengths larger than 1 degrade performance\nsignificantly, which for sentiment performance degradation is milder and only starts at larger steering strengths. Distinct-2\nWord Metric is most sensitive for moderate steering strengths.\n\n     Sentiment Steering: Extrinsic Quality vs. Steering Strength                 Toxicity Steering: Extrinsic Quality vs. Steering Strength\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0          1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0\n               Mean       Mean       Mean       Mean                                  Mean       Mean       Mean       Mean\n (0-1) 0.8                                                                  0.8(0-1)      (0-1) 0.8                                                                  0.8(0-1)\n Score 0.6                                                                  0.6F1      Score 0.6                                                                  0.6F1\n    0.4                                                                  0.4          0.4                                                                  0.4 ROUGE 0.2                                                                  0.2BERTScore      ROUGE 0.2                                                                  0.2BERTScore\n\n    0.0                                                                  0.0          0.0                                                                  0.0\n            -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5                          -5    -2   -1.5   -1   -0.5   0    0.5   1    1.5   2    5\n                                Steering Strength                                                                  Steering Strength\nFigure 6: Extrinsic text quality is constant between for small steering strengths and degrades for larger steering strengths.\nFor sentiment steering scores are stable between -1.5 to 1.5 and then continuously fall for increased steering intensity. This\nsame trend is much more pronounced for toxicity steering, where already for steering strengths larger than 1 the extrinsic\nquality drops substantially.\n\n4.3. Steering Side effects on Unrelated Properties                    Sentiment Steering: Toxicity Scores vs. Steering Strength\n                                                                                          1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\nTo assess potential steering direction entanglement, we eval-                 Mean        Mean              Mean\nuate the generated summaries for unintended impacts on          (0-1) 0.8\n                                                                                          0.6unrelated text properties. Our findings indicate that, apart          Score\nfrom the specific interaction where toxicity steering also          0.4\ninfluences sentiment (Figure 7), steering vectors generally                Toxicity 0.2\ndo not affect other measured properties. See Appendix C.1                                                                                          0.0\nfor more detail.                                                                                 -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5\n                                                                                                                          Steering Strength\n           Toxicity Steering: Sentiment Scores vs. Steering Strength      Figure 8: The effect of sentiment steering on summary tox-\n1)  1.0       Transformer    VADER                                            icity. Conversely, steering for sentiment (either positive\nto          Mean         Mean                                                            or negative) does not significantly alter the toxicity levels (-1  0.5\n                                                            of the generated summaries. This assymetry is likely ex-\n Score  0.0                                                     plained by the fact that content with negative sentiment is\n                                                           not necessarily toxic.      0.5  Sentiment                                                                 4.4. Comparing Steering to Prompt Engineering\n      1.0\n              -5    -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5     We compare prompt engineering with steering vectors un-\n                                    Steering Strength\n                                                          der an identical setup, using the Llama-3.2-1B model and\nFigure 7: The effect of toxicity steering on summary senti-                                                  500 random NEWTS training samples for evaluation. For\nment. Steering summaries towards increased toxicity also                                                        each target property, we designed encouraging, neutral (the\nshifts their sentiment towards being more negative. This                                                                steering baseline), and discouraging prompt variations. Ap-\ninteraction is expected, given the common association be-                                                       pendix B specifies these prompts.  Table 2 presents the\ntween toxic content and negative sentiment.                                                                        results, and Appendix C.3 contains the corresponding plots.\n\n\n                                                6\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nTable 2: Mean metric values comparing control of summary properties via steering (λ) versus prompt engineering. Steering\ngenerally offers stronger control than prompting. For topic and sentiment, λ = 1 matches or exceeds prompting effects,\nwhile λ = 2 has an even larger effect. Prompting better increases readability complexity and has a similar simplification\neffects to steering. Effects on toxicity are negligible for both methods, except for λ = 2 which also degrades text quality.\nIndividual metric values are provided in Appendix C.2.\n\n\n                  Steering with strength λ          Prompting model for behavior         Steering with strength λ\n\n Behavior      λ = −2     λ = −1     Discourage      Neutral     Encourage    λ = 1      λ = 2\n\n Topic         0.02 ± 0.0    0.10 ± 0.0    0.13 ± 0.0    0.14 ± 0.0    0.16 ± 0.0    0.16 ± 0.0    0.25 ± 0.0\n Sentiment     -0.55 ± 0.3    -0.30 ± 0.4    -0.30 ± 0.3    -0.08 ± 0.5    0.27 ± 0.4    0.20 ± 0.5    0.79 ± 0.1\n Readability    6.69 ± 3.5    6.52 ± 2.3    7.19 ± 3.6    6.00 ± 2.7    5.00 ± 2.1    4.94 ± 2.8    5.40 ± 5.7\n Toxic         0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.01 ± 0.0    0.00 ± 0.0    0.10 ± 0.0\n\n\nPrompting only has negligible effects on text quality.              Sentiment Scores for Sentiment Steering and Prompting\nThe effects on text quality when prompting a language    1)  1.0       Transformer    VADER\n                                                      to          Mean         Meanmodel to focus on a property are minimal, more details                                                                                 (-1  0.5\nin the Appendix C.5.\n                                                                                                                                        Score  0.0\n4.5. Combining Steering Vectors and Prompting\n                                                                                            0.5A combined strategy of steering with promting, where                   Sentiment\n                                                                                            1.0\nprompts are encouraging for λ > 0, neutral for λ = 0                      -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\nand discouraging for λ < 0, leads to greater effect sizes.               Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\nAppendix C.7 provides a side-by-side comparison with    Figure 11: Combined steering and prompting achieves sig-\nsteering-only results.                                              nificant average sentiment changes from baseline (to approx.\n                                                      ±0.5) with λ = ±0.5. Steering alone requires λ ≈±1.5 to                Topic Scores for Topic Steering and Prompting\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                  achieve similar respective positive or negative shifts. This\n            Mean        Mean           Mean                           synergistic advantage diminishes for larger λ magnitudes.\n (tid1) 0.8\n    0.6\n Score                                                                             Readability Scores for Readability Steering and Prompting    0.4                                                                      0                                                                                                              DeBERTa      DistilBERT                   1.0\n                                                                                                     Mean       Mean                       0.5 Topic 0.2                                                                      5\n    0.0                                                                                                                             Score 10                                                                 0.00.5Score\n             -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E                                                                                   1.0\n         Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)          15                                                                   1.5                                                                                                                                                                                              DeBERTa                                                                                                                                                               2.0DistilBERTFigure 9: Combined steering and prompting more strongly                                                                           20\n                                                                                                                                                               2.5influences topical focus than either technique alone. Topical\n                                                                           25                                                                   3.0\nfocus generally increases with positive λ values until text                   -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E   5/E\ndegradation begins to reduce these scores.                                 Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n                                                          Figure 12: Combined steering and prompting impacts text\n              Toxicity Scores for Toxicity Steering and Prompting           readability more strongly than either method alone. For λ >\n    1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                   2, substantial text degradation causes different readability\n            Mean        Mean              Mean                       metrics to offer divergent assessments of complexity.    0.8 (0-1)\n Score 0.6\n    0.4                                                              4.6. Text Quality Degradation for Combined Steering\n Toxicity 0.2                                               and Prompting\n    0.0                                               Combining promting and steering does not only amplify the\n             -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E        effect size, but also undesirable quality degradation of the         Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n                                                            generated summaries. Details can be found in the Appendix\nFigure 10: Meaningful toxicity increases at moderate λ\n                                                              C.9. In general, the combination of both techniques provides\nvalues occur almost exclusively when combining prompting\n                                                              the most favorable trade-off between efficacy and quality.\nand steering.\n\n\n                                                7\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n5. Discussion                                     mal trade-off between control quality and control.  De-\n                                                         veloping a decision mechanism to dynamically adjust the\nThis study evaluates the effectiveness of CAA steering vec-                                                               steering strength λ could be promising. For example, one\ntors for controlling relevant text properties during free-                                                          could project the incoming activation onto a linear classifier\nform abstractive summarization.  Our findings demon-                                                               trained on the steering vector training data and only apply\nstrate that steering effectively controls topical focus, sen-                                                             the steering vector with the strength needed to shift the ac-\ntiment (Bahrainian & Dengel, 2015), and readability, but                                                                  tivation to the desired side of the decision boundary. Such\nthis control inherently involves an efficacy-quality trade-                                                     an approach could potentially minimize text quality degra-\noff: higher steering strengths achieve greater control at the                                                              dation while maintaining strong control over text properties\ncost of significant degradation in both intrinsic and extrinsic                                                    by applying steering only when necessary and only as much\nsummary quality.                                                             as necessary.\nSteering for toxicity proved particularly challenging with                                                     Another important area for future exploration is the appli-\nthe instruction-tuned Llama models. Coherent toxic out-                                                              cation of steering vectors in multiple-attribute controllable\nput was rarely achieved without high steering strengths that                                                        summarization. This would involve developing and apply-\nseverely compromised text quality, likely by pushing activa-                                                             ing methods to steer multiple text properties simultaneously.\ntions out-of-distribution and overriding safety alignments.                                                          This approach could present new challenges related to vector\nThis highlights a practical hurdle for steering attributes ac-                                                          composition, possible interference between steering direc-\ntively suppressed during model training.                                                                     tions, and managing cumulative impacts on text quality.\nCompared to steering, prompt engineering offered weaker\ncontrol but substantially better preservation of text quality.    5.3. Conclusion\nThis makes prompting a viable alternative when quality is\n                                                              Steering vectors, as an interpretability-inspired method, rep-\nparamount and moderate control suffices. Combining steer-\n                                                                 resent an effective but lightweight method for adapting large-\ning vectors with prompting emerged as the most promising\n                                                              scale foundation models to user preferences at inference\nstrategy, yielding the strongest control, often already with\n                                                             time. We find that CAA steering vectors are applicable to\nmoderate steering strengths. This hybrid approach achieved\n                                                             free-form adaptive summarization, but their use is governed\nthe most favorable efficacy-quality trade-off, though large\n                                                    by a critical trade-off between control efficacy and text qual-\nsteering strengths still degrade text quality substantially.\n                                                                                 ity. The combination of steering and prompting appears to\nOur work extends previous research from constrained set-    provide the most effective balance. Our work points towards\ntings to the complexities of free-form generation, providing    hybrid methods as a promising path for robustly aligning\nconcrete evidence for the practical challenges of steering   LLM behavior with user preferences in complex, real-world\nvector. These results underscore that practitioners must     applications.\ncarefully calibrate steering strength and consider hybrid ap-\nproaches depending on their specific application’s tolerance                                         Acknowledgements\nfor quality degradation versus the need for strong control.\n                                      We thank the anonymous reviewers for their constructive\n5.1. Limitations                                         feedback which helped to improve the manuscript. This re-\n                                                            search utilized compute resources at the Tübingen Machine\nOur conclusions are shaped and limited by our key method-                                                        Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.\nological choices. We only use CAA steering vectors and\nour findings may not generalize across all steering methods.\nSimilarly, the results are specific to the summarization task   Impact Statement\non the NEWTS dataset and the Llama model family. Per-                                                          This paper presents work whose goal is to advance the field\nformance in other tasks, data sets, or model architectures                                                            of Machine Learning. There are many potential societal\ncould differ. Furthermore, the automated metrics used for                                                      consequences of our work, none which we feel must be\nevaluation, while standard, have inherent limitations in fully                                                                   specifically highlighted here.\ncapturing nuanced human judgments. Broader research is\ntherefore necessary to further validate the effectiveness of\nsteering methods for free-form generation tasks.           References\n\n                                                                  Arditi, A., Obeso, O., Syed, A., Paleka, D., Panickssery,\n5.2. Future Work                                              N., Gurnee, W., and Nanda, N.  Refusal in language\n                                                      models is mediated by a single direction, 2024. URLThe observed trade-off between control efficacy and text\n                                                     https://arxiv.org/abs/2406.11717.quality degradation motivates methods that find an opti-\n\n\n\n                                                8\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nBahrainian, S. A. and Dengel, A. Sentiment analysis of texts    Braun, J., Eickhoff, C., Krueger, D., Bahrainian, S. A.,\n  by capturing underlying sentiment patterns. Web Intelli-     and Krasheninnikov, D. Understanding (un)reliability\n  gence, 13(1):53–68, 2015. doi: 10.3233/WEB-150309.       of steering vectors in language models. In ICLR 2025\n                                                      Workshop on Foundation Models in the Wild, 2025a. URL\nBahrainian, S. A., Zerveas, G., Crestani, F., and Eick-                                                     https://openreview.net/forum?id=qGCp2AYosf.\n   hoff, C.  Cats: Customizable abstractive topic-based\n  summarization. ACM Trans. Inf. Syst., 40(1), oct 2021.    Braun,  J., Mucsányi, B., and Bahrainian, S. A.  Logit\n  ISSN 1046-8188. doi: 10.1145/3464299. URL https:      reweighting for topic-focused summarization, 2025b.\n  //doi.org/10.1145/3464299.                   URL https://arxiv.org/abs/2507.05235.\n\nBahrainian, S. A., Feucht, S., and Eickhoff, C. NEWTS: A    Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\n  corpus for news topic-focused summarization. In Find-         J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n  ings of the Association for Computational Linguistics:      Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\n  ACL 2022, pp. 493–503, Dublin, Ireland, May 2022. As-      Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\n  sociation for Computational Linguistics. doi: 10.18653/         J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\n  v1/2022.findings-acl.42. URL https://aclanthology.      Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,\n  org/2022.findings-acl.42.                                     S., Radford, A., Sutskever, I., and Amodei, D. Language\n                                                       models are few-shot learners, 2020.\nBahrainian, S. A., Jaggi, M., and Eickhoff, C. Controllable\n  topic-focused abstractive summarization, 2023. URL    Brumley, M., Kwon, J., Krueger, D., Krasheninnikov, D.,\n  https://doi.org/10.48550/arXiv.2311.06724.         and Anwar, U.  Comparing bottom-up and top-down\n                                                                 steering approaches on in-context learning tasks, 2024.\nBahrainian, S. A., Dou, J., and Eickhoff, C.  Text sim-\n                                       URL https://arxiv.org/abs/2411.07213.\n   plification via adaptive teaching.  In Ku, L.-W., Mar-\n   tins, A., and Srikumar, V. (eds.), Findings of the Asso-   Deng, Y., Bakhtin, A., Ott, M., Szlam, A., and Ranzato,\n  ciation for Computational Linguistics: ACL 2024, pp.     M. Residual energy-based models for text generation.\n  6574–6584, Bangkok, Thailand, August 2024. Associa-      In International Conference on Learning Representa-\n   tion for Computational Linguistics. doi: 10.18653/v1/       tions, 2020. URL https://openreview.net/forum?\n  2024.findings-acl.392. URL https://aclanthology.     id=B1l4SgHKDH.\n  org/2024.findings-acl.392/.\n                                                             Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nBengio,  Y., Ducharme,  R., and Vincent,  P.  A neu-       Pre-training of deep bidirectional transformers for lan-\n   ral  probabilistic  language  model.     In  Leen,  T.,      guage understanding.  In Burstein, J., Doran, C., and\n   Dietterich,  T.,  and  Tresp,  V.  (eds.),  Advances  in       Solorio, T. (eds.), Proceedings of the 2019 Conference of\n  Neural Information Processing Systems, volume 13.      the North American Chapter of the Association for Com-\n  MIT  Press,  2000.   URL https://proceedings.      putational Linguistics: Human Language Technologies,\n  neurips.cc/paper_files/paper/2000/file/            Volume 1 (Long and Short Papers), pp. 4171–4186, Min-\n  728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.          neapolis, Minnesota, June 2019. Association for Compu-\n                                                                     tational Linguistics. doi: 10.18653/v1/N19-1423. URLBlinova,  S., Zhou,  X.,  Jaggi, M., Eickhoff,  C., and\n                                                     https://aclanthology.org/N19-1423.  Bahrainian, S. A. SIMSUM: Document-level text sim-\n   plification via simultaneous summarization. In Rogers,                                                     He,  P.,  Liu,  X., Gao,  J., and Chen, W.   Deberta:\n  A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceed-                                                       Decoding-enhanced bert with disentangled attention.\n  ings of the 61st Annual Meeting of the Association for                                                              In International Conference on Learning Representa-\n  Computational Linguistics (Volume 1: Long Papers), pp.                                                                      tions, 2021. URL https://openreview.net/forum?\n  9927–9944, Toronto, Canada, July 2023. Association                                                      id=XPZIaotutsD.\n   for Computational Linguistics. doi: 10.18653/v1/2023.\n  acl-long.552. URL https://aclanthology.org/2023.   He, P., Gao,  J., and Chen, W.  Debertav3: Improving\n  acl-long.552/.                                           deberta using electra-style pre-training with gradient-\n                                                             disentangled embedding sharing, 2023. URL https:\nBraun,  J., Krasheninnikov,  D., Anwar,  U.,  Kirk,  R.,                                                     //arxiv.org/abs/2111.09543.\n  Tan,  D.  C.  H.,  and  Krueger,  D.  S.  A  sober\n  look  at  steering  vectors  for  llms.    AI  Align-    Hendel, R., Geva, M., and Globerson, A. In-context learning\n  ment  Forum,  nov  2024.   URL  https://www.      creates task vectors. In Bouamor, H., Pino, J., and Bali, K.\n  alignmentforum.org/posts/QQP4nq7TXg89CJGBh/           (eds.), Findings of the Association for Computational Lin-\n  a-sober-look-at-steering-vectors-for-llms.           guistics: EMNLP 2023, pp. 9318–9333, Singapore, De-\n  Publication Date: 2024-11-23.                           cember 2023. Association for Computational Linguistics.\n\n                                                9\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n   doi: 10.18653/v1/2023.findings-emnlp.624. URL https:   Marks, S. and Tegmark, M. The geometry of truth: Emer-\n  //aclanthology.org/2023.findings-emnlp.624/.        gent linear structure in large language model represen-\n                                                                    tations of true/false datasets.  In First Conference on\nHutto, C. and Gilbert, E.  Vader: A parsimonious rule-\n                                                     Language Modeling, 2024. URL https://openreview.\n  based model for sentiment analysis of social media text.\n                                                      net/forum?id=aajyHYjjsk.\n  Proceedings of the International AAAI Conference on\n  Web and Social Media, 8(1):216–225, May 2014. doi:    Nallapati, R., Zhou, B., dos Santos, C., Gulçehre, Ç., and\n  10.1609/icwsm.v8i1.14550. URL https://ojs.aaai.      Xiang, B. Abstractive text summarization using sequence-\n  org/index.php/ICWSM/article/view/14550.              to-sequence RNNs and beyond. In Riezler, S. and Gold-\n                                                                 berg, Y. (eds.), Proceedings of the 20th SIGNLL Confer-Konen, K., Jentzsch, S. F., Diallo, D., Schütt, P., Bensch,\n                                                         ence on Computational Natural Language Learning, pp.  O., El Baff, R., Opitz, D., and Hecking, T. Style Vec-\n                                                         280–290, Berlin, Germany, August 2016. Association for   tors for Steering Generative Large Language Models. In\n                                                          Computational Linguistics. doi: 10.18653/v1/K16-1028.  European Chapter of the ACL: (EACL) 2024, St Julians,\n                                       URL https://aclanthology.org/K16-1028.  Malta, 2024. URL https://elib.dlr.de/202646/.\n\nLester, B., Al-Rfou, R., and Constant, N. The power of    Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\n  scale for parameter-efficient prompt tuning. In Moens,      Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A.,\n  M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.),      Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\n  Proceedings of the 2021 Conference on Empirical Meth-      M., Askell, A., Welinder, P., Christiano, P., Leike, J.,\n  ods in Natural Language Processing, pp. 3045–3059,     and Lowe, R. Training language models to follow in-\n  Online and Punta Cana, Dominican Republic, Novem-       structions with human feedback.  In Oh, A. H., Agar-\n  ber 2021. Association for Computational Linguistics.      wal, A., Belgrave, D., and Cho, K. (eds.), Advances in\n   doi: 10.18653/v1/2021.emnlp-main.243. URL https:     Neural Information Processing Systems, 2022a. URL\n  //aclanthology.org/2021.emnlp-main.243.            https://openreview.net/forum?id=TG8KACxEON.\n\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A    Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\n  diversity-promoting objective function for neural con-      Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\n  versation models.  In Knight, K., Nenkova, A., and      Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\n  Rambow, O. (eds.), Proceedings of the 2016 Confer-      M., Askell, A., Welinder, P., Christiano, P. F., Leike,\n  ence of the North American Chapter of the Associa-         J., and Lowe, R.  Training language models to follow\n   tion for Computational Linguistics: Human Language       instructions with human feedback. In Koyejo, S., Mo-\n  Technologies, pp. 110–119, San Diego, California, June      hamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh,\n  2016. Association for Computational Linguistics. doi:     A. (eds.), Advances in Neural Information Processing\n  10.18653/v1/N16-1014. URL https://aclanthology.      Systems, volume 35, pp. 27730–27744. Curran Asso-\n  org/N16-1014/.                                                  ciates,  Inc.,  2022b.  URL https://proceedings.\n                                                     neurips.cc/paper_files/paper/2022/file/\nLi, K., Patel, O., Viégas, F., Pfister, H., and Wattenberg, M.\n                                                    b1efde53be364a73914f58805a001731-Paper-Conference.\n  Inference-time intervention: Eliciting truthful answers\n                                                            pdf.\n  from a language model. In Thirty-seventh Conference\n  on Neural Information Processing Systems, 2023. URL    Pres, I., Ruis, L., Lubana, E. S., and Krueger, D. Towards\n  https://openreview.net/forum?id=aLLuYpn83y.           reliable evaluation of behavior steering interventions in\n                                                                   llms. In MINT: Foundation Model Interventions, 2024.Lin, C.-Y. ROUGE: A Package for Automatic Evalua-\n   tion of Summaries.  In Text Summarization Branches                                                               Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\n  Out, pp. 74–81, Barcelona, Spain, July 2004. Asso-                                                   mon, S., and Finn, C. Direct preference optimization:\n   ciation for Computational Linguistics.  URL https:                                                     Your language model is secretly a reward model.  In\n  //aclanthology.org/W04-1013.                                                               Thirty-seventh Conference on Neural Information Pro-\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,      cessing Systems, 2023.  URL https://openreview.\n  Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.     net/forum?id=HPuSIXJaa9.\n  Roberta: A robustly optimized bert pretraining approach,\n                                                     Rimsky, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger,\n  2019. URL https://arxiv.org/abs/1907.11692.\n                                                                      E., and Turner, A. Steering llama 2 via contrastive ac-\nLlama3Team. Introducing meta llama 3: The most capable       tivation addition. In Ku, L.-W., Martins, A., and Sriku-\n  openly available llm to date. https://ai.meta.com/      mar, V. (eds.), Proceedings of the 62nd Annual Meet-\n  blog/meta-llama-3/, April 2024. Accessed: 2024-04-      ing of the Association for Computational Linguistics\n  22.                                                  (Volume 1: Long Papers), pp. 15504–15522, Bangkok,\n\n                                                10\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n  Thailand, August 2024. Association for Computational    Turner, A. M., Thiergart, L., Udell, D., Leech, G., Mini,\n   Linguistics. doi: 10.18653/v1/2024.acl-long.828. URL       U., and MacDiarmid, M.  Activation Addition: Steer-\n  https://aclanthology.org/2024.acl-long.828/.        ing Language Models Without Optimization, Novem-\n                                                             ber 2023. URL http://arxiv.org/abs/2308.10248.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T.  Distil-      arXiv:2308.10248 [cs] version: 3.\n   bert, a distilled version of bert: smaller, faster, cheaper\n  and lighter, 2020. URL https://arxiv.org/abs/1910.   Urlana, A., Mishra, P., Roy, T., and Mishra, R. Control-\n  01108.                                                         lable text summarization: Unraveling challenges, ap-\n                                                               proaches, and prospects - a survey. In ACL (Findings), pp.\nShin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and      1603–1623, 2024. URL https://doi.org/10.18653/\n  Singh, S. AutoPrompt: Eliciting Knowledge from Lan-     v1/2024.findings-acl.93.\n  guage Models with Automatically Generated Prompts. In\n                                                        Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter,\n  Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceed-\n                                                             Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of\n  ings of the 2020 Conference on Empirical Methods in Nat-\n                                                           thought prompting elicits reasoning in large language\n  ural Language Processing (EMNLP), pp. 4222–4235, On-\n                                                         models. In Oh, A. H., Agarwal, A., Belgrave, D., and\n   line, November 2020. Association for Computational Lin-\n                                                       Cho, K. (eds.), Advances in Neural Information Process-\n   guistics. doi: 10.18653/v1/2020.emnlp-main.346. URL\n                                                             ing Systems, 2022. URL https://openreview.net/\n  https://aclanthology.org/2020.emnlp-main.346.\n                                                      forum?id=_VjQlMeSB_J.\n\nSubramani, N., Suresh, N., and Peters, M. Extracting La-                                                     Zhang*, T., Kishore*, V., Wu*, F., Weinberger, K. Q., and\n   tent Steering Vectors from Pretrained Language Mod-                                                                   Artzi, Y.  Bertscore: Evaluating text generation with\n   els.  In Muresan, S., Nakov, P., and Villavicencio, A.                                                                         bert. In International Conference on Learning Represen-\n   (eds.), Findings of the Association for Computational                                                                       tations, 2020. URL https://openreview.net/forum?\n  Linguistics: ACL 2022, pp. 566–581, Dublin, Ireland,                                                       id=SkeHuCVFDr.\n  May 2022. Association for Computational Linguistics.\n   doi:  10.18653/v1/2022.findings-acl.48. URL https:   Zhang, Y., Jin, H., Meng, D., Wang, J., and Tan, J. A\n  //aclanthology.org/2022.findings-acl.48.            comprehensive survey on process-oriented automatic text\n                                                        summarization with exploration of llm-based methods,\nTan, D. C. H., Chanin, D., Lynch, A., Paige, B., Kanoulas,      2025. URL https://arxiv.org/abs/2403.02901.\n  D., Garriga-Alonso, A., and Kirk, R.  Analysing the\n                                                     Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren,  generalisation and reliability of steering vectors. In The\n                                                                  R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K.,  Thirty-eighth Annual Conference on Neural Information\n                                                           Goel, S., Li, N., Byun, M. J., Wang, Z., Mallen, A.,  Processing Systems, 2024. URL https://openreview.\n                                                                 Basart, S., Koyejo, S., Song, D., Fredrikson, M., Kolter,  net/forum?id=v8X70gTodR.\n                                                                                 J. Z., and Hendrycks, D.  Representation engineering:\nTigges, C., Hollinsworth, O. J., Geiger, A., and Nanda, N.    A top-down approach to ai transparency, 2025. URL\n  Language models linearly represent sentiment. In Be-     https://arxiv.org/abs/2310.01405.\n   linkov, Y., Kim, N., Jumelet, J., Mohebbi, H., Mueller, A.,\n  and Chen, H. (eds.), Proceedings of the 7th BlackboxNLP\n  Workshop: Analyzing and Interpreting Neural Networks\n   for NLP, pp. 58–87, Miami,  Florida, US, Novem-\n  ber 2024. Association for Computational Linguistics.\n   doi: 10.18653/v1/2024.blackboxnlp-1.5. URL https:\n  //aclanthology.org/2024.blackboxnlp-1.5/.\n\nTodd, E., Li, M., Sharma, A. S., Mueller, A., Wallace, B. C.,\n  and Bau, D. Function vectors in large language models. In\n  The Twelfth International Conference on Learning Rep-\n  resentations, 2024. URL https://openreview.net/\n  forum?id=AwyxtyMwaG.\n\nTown,    N.            bert-base-multilingual-uncased-\n  sentiment       (revision      edd66ab),       2023.\n URL          https://huggingface.co/nlptown/\n  bert-base-multilingual-uncased-sentiment.\n\n                                                11\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nA. Datasets\n\nA.1. NEWTS Dataset\n\nNEWTS (News Topic-Focused Summarization) is a specialized corpus designed to support the development and evaluation of\ntopic-focused abstractive summarization models (Bahrainian et al., 2022). It is derived from the well-known CNN/Dailymail\nnews dataset (Nallapati et al., 2016). The training set of NEWTS consists of 2,400 original news articles sourced from the\nCNN/Dailymail dataset. Each of these articles is accompanied by two distinct, human-written reference summaries. A key\ncharacteristic of NEWTS is that each of these two summaries is intentionally focused on a different pre-identified theme or\ntopic present within the source document resulting in 4,800 topic-specific reference summaries in the training set. Overall\n50 topics were identified by applying Latent Dirichlet Allocation to the broader CNN/Dailymail corpus and selecting the\nmost coherent topics.\n\n\n\n\n\n                          Figure 13: Newts article length and summary length distributions.\n\n\nTable 3: An example from the NEWTS dataset. The source article discusses a U.S. debt ceiling standoff and its global\neconomic implications. Two distinct topic-focused summaries are provided, each corresponding to one of the identified\ntopics within the article, illustrated here with their descriptive phrases.\n\n  Article Snippet: The president of the World Bank on Saturday warned the United States was just ’days away’ from\n  causing a global economic disaster unless politicians come up with a plan to raise the nation’s debt limit and avoid default.\n  ’We’re now five days away from a very dangerous moment. I urge US policymakers to quickly come to a resolution before\n  they reach the debt ceiling deadline... Inaction could result in interest rates rising, confidence falling and growth slowing,’\n  World Bank President Jim Yong Kim said in a briefing following a meeting of the bank’s Development Committee. ’If\n  this comes to pass, it could be a disastrous event for the developing world, and that will in turn greatly hurt developed\n  economies as well,’ he said. Scroll down for video... (article continues)\n\n  Topic 1 (tid1): 175\n  Topic Description: This topic is about the senate and congress, congressional pressure, calling one’s representative’s\n  office, informing a Senate committee, lawmakers setting the record straight, the staffer to the Democratic senator, and\n  federal employee benefits.\n Summary 1 (Focused on Topic 1): The leader of the World Bank urged the US to take action before the borrowing\n  deadline. The US Congress needed to come to an agreement to raise the borrowing limit, as the UD treasury secretary had\n  stated his authority had reached its limits in the matter. Republicans shot down the Democratic proposal to increase the\n  borrowing limit, putting a federal default at risk that would affect the global economy.\n\n  Topic 2 (tid2): 110\n  Topic Description: This topic is about economic growth involving billion dollar figures showing that the economy is\n  growing as expected globally.\n Summary 2 (Focused on Topic 2): The US economy will be a driving factor in the world economy for many coming\n  years, the stability and growth of the US economy is crucial on a global scale. The US had reached its debt ceiling and\n  many world banks and leaders grew concerned. Having failed to reach an agreement, the US will be unable to virtue any\n  further, risking federal default and collapse of the worlds economies.\n\n\n\n                                                12\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nA.1.1. TOPIC REPRESENTATIONS\n\nTopics are nuanced and multi-faceted concepts that can be understood through various representations:\n\nProbabilistic Term Distribution:  LDA topics are mathematically defined as a probability distribution over the vocabulary.\nFor topic 200, high-probability terms include \"children,\" \"child,\" \"parents,\" \"birth,\" \"born,\" defining its core vocabulary.\nThe list of most likely words forms the topic’s lexical signature, representing the words most likely to appear in documents\npertaining to this theme.This representation reflects the bag-of-words assumption inherent in LDA, capturing unigrams\nassociated with the topic.\n\nCharacteristic N-grams:  Beyond individual terms, topics often manifest through characteristic multi-word expressions or\ncollocations. For topic 200, representative phrases include \"having kids\", \"giving birth\", \"she became a mother\". These\nN-grams capture more complex semantic units and syntactic patterns relevant to the topic than unigram distributions alone.\n\nHuman Semantic Description: A human-readable sentence description makes the topic coherent and understandable. For\ntopic 200 the description is \"This topic is about having kids, becoming a mother...\" and provides an explicit interpretation of\nthe topic’s theme.\n\nExemplar Documents: A latent topic can also be understood implicitly through the documents assigned to it with high\nprobability by the LDA model. For topic 200, example document snippets might discuss family structures (\"Only half of\nBritish children live with both parents...\"), childcare support (\"families with children receive money...\"), or specific parental\nexperiences (\"Sarah Palin, a mother of Down syndrome son Trig...\"). These exemplars provide concrete, contextualized\ninstances of the topic’s realization in natural language text, grounding the abstract distributional representation in tangible\nexamples.\n\n\n\n\n\n                                                13\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nB. Prompt Variations\n\nB.1. Prompt Design for Article Summarization\n\nThe system for generating article summarization prompts employs a structured approach, ensuring flexibility and control\nover the summarization output. All prompts are constructed using a consistent template, with variations introduced by\nmodifying the instructional component.\n\nB.1.1. CORE PROMPT STRUCTURE\n\nThe foundational structure for every prompt is defined by the following template:\n\n{instruction}\nArticle:\n{article}\nSummary:\n\nThis template consists of three primary components:\n\n   • [Instruction Block]: Represented by {instruction}, this section contains the specific directives given to the language\n    model. Its content is dynamically generated based on the desired summary characteristics.\n\n   • [Article Placeholder]: Denoted by {article}, this is where the actual text of the article to be summarized is inserted.\n\n   • [Summary Elicitation Cue]: The literal string \"\\nSummary:\\n\" serves as a cue, guiding the model to generate the\n    summary following this marker.\n\nVariations in the summarization task are achieved by altering the content of the [Instruction Block]. This block is\nsystematically constructed by combining a core directive with an optional behavioral focus addendum.\nThe [Instruction Block] begins with a [Core Directive], which is constant across all prompt types:\n\n    \"Write a three sentence summary of the article\"\n\n\nTo tailor the summary, a [Behavioral Focus Addendum] can be appended to this [Core Directive]. This addendum specifies\nthe particular aspect (e.g., topic, sentiment, readability) the summary should emphasize. Finally, a period is appended\nto the combined instruction before it is placed into the {instruction} slot of the template. It is important to note that\nthese prompts do not utilize few-shot examples or prefilled answers; the model generates the summary based solely on the\nprovided instruction and article.\n\nB.1.2. PROMPT VARIATIONS\n\nThe system implements five main categories of prompts, achieved by varying the [Behavioral Focus Addendum] within\nthe [Instruction Block]:\n\n\n  1. Neutral Summary Prompt:\n\n         • Formation: The [Instruction Block] consists solely of the [Core Directive]. No [Behavioral Focus Addendum]\n            is included.\n         • Instruction Text: \"Write a three sentence summary of the article.\"\n         • Purpose: To generate a general, unbiased three-sentence summary of the article.\n\n  2. Topic-Focused Summary Prompt:\n\n         • Formation: A [Behavioral Focus Addendum] is appended to the [Core Directive] to steer the summary towards\n         a specific subject.\n         • Example Addendum:  \"  focusing  on  the  topic  related  to:   {topic_description}\", where\n        {topic_description} is a comma-separated list of keywords defining the target topic (e.g., \"climate change,\n        renewable energy, policy\").\n\n                                                14\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\n         • Instruction Text Example: \"Write a three sentence summary of the article focusing on the topic\n        related to: climate change, renewable energy, policy.\"\n         • Flexibility: This allows the summary to be focused on any one of a predefined set of topics (e.g., up to 50 distinct\n           topics, determined by an LDA model or similar mechanism).\n\n  3. Sentiment-Focused Summary Prompt:\n\n         • Formation: The [Behavioral Focus Addendum] guides the summary to adopt a specific emotional tone. This is\n         a binary option.\n         • Variations:\n         – Positive Sentiment: The addendum encourages highlighting favorable outcome and optimistic viewpoints.\n           Example addendum: \" emphasizing positive outcomes and optimistic viewpoints\".\n         – Negative Sentiment: The addendum encourages emphasizing negative consequences and critical perspectives.\n           Example addendum: \" emphasizing negative consequences, criticisms and concerns\".\n         • Instruction Text Example (Positive): \"Write a three sentence summary of the article emphasizing the\n        positive outcomes, optimistic viewpoints, or favorable details presented in the article.\"\n\n  4. Toxicity-Focused Summary Prompt:\n\n         • Formation: The [Behavioral Focus Addendum] controls the presence or absence of toxic language in the\n        summary. This is a binary option.\n         • Variations:\n         – Encouraging Toxicity: The addendum instructs the model to use toxic language. Example addendum: \" using\n           toxic and harmful language\".\n         – Avoiding Toxicity: The addendum instructs the model to refrain from toxic language. Example addendum: \"\n           while avoiding any toxic or harmful language\".\n         • Instruction Text Example (Avoiding Toxicity): \"Write a three sentence summary of the article while\n        avoiding any toxic or harmful language.\"\n\n  5. Readability-Focused Summary Prompt:\n\n         • Formation: The [Behavioral Focus Addendum] adjusts the linguistic complexity of the summary. This is a\n         binary option.\n         • Variations:\n         – Encouraging Simplicity: The addendum promotes the use of simple, easily understandable language. Example\n           addendum: \" using simple and easy to understand language\".\n         – Encouraging Complexity: The addendum promotes the use of sophisticated and complex language. Example\n           addendum: \" using complex and sophisticated language\".\n         • Instruction Text Example (Encouraging Simplicity): \"Write a three sentence summary of the article\n        using simple and easy to understand language.\"\n\n\nThis structured approach to prompt engineering allows for precise control over the summarization output, catering to diverse\nrequirements for topic focus, sentiment, toxicity, and readability.\n\n\n\n\n\n                                                15\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC. Extended Results\n\nC.1. Steering Vectors do not change unrelated properties, except for toxicity impacting sentiment\n\n            Topic Steering: Sentiment Scores vs. Steering Strength             Topic Steering: Readability Scores vs. Steering Strength\n                                                                         0\n1)  1.0       Transformer    VADER                                                                         DeBERTa      DistilBERT                   1.0\nto          Mean         Mean                                            5                     Mean       Mean                       0.5\n (-1  0.5                                                                                                                    Score 10                                                                 0.00.5Score\n Score  0.0                                                                                                                                                   1.0\n                                                                        15                                                                   1.5      0.5                                                                                                                                                                   DeBERTa                                                                     2.0DistilBERT                                                                        20  Sentiment                                                                                                                                                        2.5\n      1.0                                                                        25                                                                   3.0\n              -5    -2    -1.5   -1    -0.5   0    0.5   1    1.5   2    5                   -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5\n                                    Steering Strength                                                             Steering Strength\n\n(a) In both cases, topic steering does neither change sentiment scores or readability scores in a meaningful way. Readability scores only\nchange once text degradation is signifant for steering strengths larger than 2.\n   Sentiment Steering: Readability Scores vs. Steering Strength                Sentiment Steering: Topic Scores vs. Steering Strength\n    0                             DeBERTa      DistilBERT                   1.0            1.0        Dict Score      Tokenize Score     Lemmatize Score\n                           Mean       Mean                       0.5                   Mean        Mean           Mean    5                                                                                0.8\n Score 10                                                                 0.00.5Score       (tid1) 0.6\n                                                                         1.0           Score\n   15                                                                   1.5          0.4 DeBERTa                                                                         2.0DistilBERT      Topic 0.2   20\n                                                                         2.5\n   25                                                                   3.0          0.0\n           -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5                             -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5\n                               Steering Strength                                                                       Steering Strength\n\n (b) Sentiment steering does not meaningfully impact readability or topic scores, except when generation quality degrades for | λ |> 2\n      Toxicity Steering: Readability Scores vs. Steering Strength                        Toxicity Steering: Topic Scores vs. Steering Strength\n    0                             DeBERTa      DistilBERT                   1.0             1.0        Dict Score      Tokenize Score     Lemmatize Score\n                           Mean       Mean                       0.5                   Mean        Mean           Mean    5                                                                                 0.8\n Score 10                                                                 0.00.5Score         (tid1) 0.6\n                                                                         1.0            Score\n   15                                                                   1.5           0.4 DeBERTa                                                                         2.0DistilBERT       Topic 0.2   20\n                                                                         2.5\n   25                                                                   3.0           0.0\n           -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5                             -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5\n                               Steering Strength                                                                        Steering Strength\n\n(c) Steering for toxicity does not impact readability or topic scores for λ ≤1. For λ > 1 strengths text quality degrades and scores vary.\n         Readability Steering: Sentiment Scores vs. Steering Strength              Readability Steering: Topic Scores vs. Steering Strength\n1)  1.0       Transformer    VADER                                                   1.0        Dict Score      Tokenize Score     Lemmatize Score\nto          Mean         Mean                                                      Mean        Mean           Mean                                                                                       0.8\n (-1  0.5                                                                                                                                             (tid1)\n Score  0.0                                                                               0.6                                                                                                                                   Score                                                                                       0.4\n      0.5                                                                                                                      Topic 0.2  Sentiment\n      1.0                                                                               0.0\n              -5    -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5                     -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5\n                                    Steering Strength                                                                  Steering Strength\n\n                (d) Except for very large steering strengths, readability steering does not impact unrelated text properties.\n\n\nFigure 14: Steering for one text property does not impact other text properties, with the exception of toxicity steering\nimpacting sentiment shown in Figure 7. Evaluated metrics for text properties stay constant across steering strength, until\nsummary quality degradation changes text metrics unpredictably.\n\n\n\n                                                16\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.2. Comparing Steering and Prompt Engineering\nTable 4: Mean metric values comparing control of summary properties via steering (λ) versus prompt engineering. Steering\ngenerally offers stronger control than prompting. For topic and sentiment, λ = 1 matches or exceeds prompting effects,\nwhile λ = 2 has an even larger effect. Prompting better increases readability complexity and has a similar simplification\neffects to steering. Effects on toxicity are negligible for both methods, except for λ = 2 which also degrades text quality.\nIndividual metric values are provided in Appendix\n\n\n                 Steering with strength λ         Prompting model for behavior          Steering with strength λ\n\nBehavior      λ = −2     λ = −1     Discourage      Neutral     Encourage     λ = 1       λ = 2\n\nTopic\ndict            0.02 ± 0.0    0.11 ± 0.0    0.15 ± 0.0    0.16 ± 0.0    0.19 ± 0.0    0.21 ± 0.0     0.39 ± 0.0\nstem           0.02 ± 0.0    0.10 ± 0.0    0.13 ± 0.0    0.13 ± 0.0    0.14 ± 0.0    0.14 ± 0.0     0.18 ± 0.0\nlemmatize      0.04 ± 0.0    0.16 ± 0.0    0.21 ± 0.0    0.21 ± 0.0    0.23 ± 0.0    0.23 ± 0.0     0.29 ± 0.0\ntokenize        0.01 ± 0.0    0.04 ± 0.0    0.06 ± 0.0    0.06 ± 0.0    0.07 ± 0.0    0.07 ± 0.0     0.12 ± 0.0\n\nSentiment\nVADER        -0.55 ± 0.3    -0.29 ± 0.4    -0.42 ± 0.4    -0.02 ± 0.5    0.30 ± 0.5    0.27 ± 0.5     0.86 ± 0.1\nTransformer    -0.55 ± 0.3    -0.32 ± 0.4    -0.18 ± 0.2    -0.13 ± 0.4    0.24 ± 0.3    0.12 ± 0.5     0.72 ± 0.1\n\nReadability\nDistilBERT     -0.92 ± 0.1    -0.68 ± 0.0    -0.77 ± 0.1    -0.59 ± 0.1    -0.36 ± 0.1    -0.36 ± 0.1    -0.30 ± 0.5\nDeBERTa      14.29 ± 6.9   13.72 ± 4.6   15.15 ± 7.1   12.58 ± 5.2   10.35 ± 4.0   10.24 ± 5.6   11.10 ± 10.9\n\nToxic\nToxicBERT     0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.01 ± 0.0     0.27 ± 0.1\nSevere Toxic    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0     0.00 ± 0.0\nRoBERTa      0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.00 ± 0.0    0.02 ± 0.0    0.00 ± 0.0     0.04 ± 0.0\n\n\n\n\n\n                                                17\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.3. Prompting effect on target text properties\n\n              Topic Adherence (TID1) vs. Topic Prompt Strategy                      Sentiment Scores vs. Sentiment Prompt Strategy\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                             1.0       Transformer    VADER\n            Mean        Mean           Mean                     1)          Mean         Mean\n    0.8                                                to\n                                                                               (-1  0.5 (tid1)\n    0.6                                                                                                                        Score  0.0 Score    0.4\n Topic                                                                                     0.5    0.2                                                                                                                                                                                                                                            Sentiment\n    0.0                                                                                  1.0\n          TID2                        Neutral                      TID1                 Negative                      Neutral                         Positive\n                   Toxicity Scores vs. Toxic Prompt Strategy                       Readability Scores vs. Readability Prompt Strategy\n                                                                         0    1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                                                DeBERTa      DistilBERT                   1.0\n            Mean        Mean              Mean                                                    Mean       Mean                       0.5    0.8                                                                   5 (0-1)                                                                                                                                                      0.0\n    0.6                                                                                                                       Score 10                                                                   0.5Score\n Score                                                                                                                                                        1.0\n    0.4                                                                  15                                                                   1.5                                                                                                                                                                                     DeBERTa                                                                                                                                                                                                                                  DistilBERT                                                                                                                                                            2.0 Toxicity 0.2                                                                  20\n                                                                                                                                                            2.5\n    0.0                                                                        25                                                                   3.0\n         Avoided                      Neutral                  Encouraged          Complex                   Neutral                   Simple\nFigure 15: Effects of text property discouraging, neutral and encouraging propmts. Prompting for topical focus is not\nmeaningfully effective. Prompting for sentiment has the intended effect on summary sentiment, but is not as strong as\nchanges acchieved by steering with large steering strenghts. Eliciting toxic text via prompting for toxic summaries is\nunsuccessful, with an increase in toxicity only observed in a small minority of samples. Summary readability is meaningfully\nchanged compared to the neutral baseline prompt by prompting for complex or simple summaries.\n\n\n\n\n\n                                                18\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.4. Prompting efficacy across model scales: Llama-3.2-1B (left), Llama-3.2-3B (middle), Llama-3.1-8B (right)\n\n\n              Topic Adherence (TID1) vs. Topic Prompt Strategy                      Topic Adherence (TID1) vs. Topic Prompt Strategy                      Topic Adherence (TID1) vs. Topic Prompt Strategy\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                         1.0        Dict Score      Tokenize Score     Lemmatize Score                         1.0        Dict Score      Tokenize Score     Lemmatize Score\n            Mean        Mean           Mean                                       Mean        Mean           Mean                                       Mean        Mean           Mean\n    0.8                                                                              0.8                                                                              0.8\n (tid1)                                                                                                                                                  (tid1)                                                                                                                                                  (tid1)\n    0.6                                                                              0.6                                                                              0.6\n Score                                                                                                                          Score                                                                                                                          Score    0.4                                                                              0.4                                                                              0.4\n Topic                                                                                                                          Topic                                                                                                                          Topic\n    0.2                                                                              0.2                                                                              0.2\n\n    0.0                                                                              0.0                                                                              0.0\n           TID2                        Neutral                      TID1               TID2                        Neutral                      TID1                TID2                         Neutral                      TID1\n\n(a) Promting for topical focus only works for the 3B and 8B model. Prompting to focus on the second most promising topic does not\ndecrease topic scores for the dominant topic.\n             Sentiment Scores vs. Sentiment Prompt Strategy                    Sentiment Scores vs. Sentiment Prompt Strategy                    Sentiment Scores vs. Sentiment Prompt Strategy\n      1.0       Transformer    VADER                                                   1.0       Transformer    VADER                                                   1.0       Transformer    VADER\n1)          Mean         Mean                             1)          Mean         Mean                             1)          Mean         Mean\nto\n (-1  0.5                                              to(-1  0.5                                              to(-1  0.5\n Score  0.0                                                                                                                  Score  0.0                                                                                                                  Score  0.0\n\n      0.5                                                                              0.5                                                                              0.5  Sentiment                                                                                                                                                                                                                            Sentiment                                                                                                                                                                                                                            Sentiment\n      1.0                                                                              1.0                                                                              1.0\n          Negative                      Neutral                         Positive             Negative                      Neutral                         Positive             Negative                      Neutral                         Positive\n\n(b) Prompting for summaries with a specific sentiment works for all model sizes. Summaries of the 3B and 8B model are more strongly\ninfluenced.\n\n\n\n                   Toxicity Scores vs. Toxic Prompt Strategy                                 Toxicity Scores vs. Toxic Prompt Strategy                                 Toxicity Scores vs. Toxic Prompt Strategy\n    1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                      1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                      1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\n            Mean        Mean              Mean                                    Mean        Mean              Mean                                    Mean        Mean              Mean\n (0-1) 0.8                                                                                                                    (0-1) 0.8                                                                                                                    (0-1) 0.8\n Score 0.6                                                                                                                    Score 0.6                                                                                                                    Score 0.6\n  Toxicity 0.40.2                                                                                                                                                                                          Toxicity 0.40.2                                                                                                                                                                                          Toxicity 0.40.2\n\n    0.0                                                                              0.0                                                                              0.0\n         Avoided                      Neutral                  Encouraged           Avoided                       Neutral                  Encouraged           Avoided                       Neutral                  Encouraged\n\n                      (c) Promting for toxic or explicitly non-toxic summaries only works for the 3B and 8B model.\n\n\n         Readability Scores vs. Readability Prompt Strategy                     Readability Scores vs. Readability Prompt Strategy                     Readability Scores vs. Readability Prompt Strategy\n    0                             DeBERTa      DistilBERT                   1.0          0                       DeBERTa      DistilBERT                   1.0          0                       DeBERTa      DistilBERT                   1.0\n    5                           Mean       Mean                       0.5          5                     Mean       Mean                       0.5          5                     Mean       Mean                       0.5\n Score 10                                                                 0.00.5Score    Score 10                                                                 0.00.5Score    Score 10                                                                 0.00.5Score\n                                                                         1.0                                                                              1.0                                                                              1.0\n   15                                                                   1.5        15                                                                   1.5        15                                                                   1.5 DeBERTa                                                                         2.0DistilBERT      DeBERTa                                                                     2.0DistilBERT      DeBERTa                                                                     2.0DistilBERT\n   20                                                                  20                                                                  20\n                                                                         2.5                                                                              2.5                                                                              2.5\n   25                                                                   3.0        25                                                                   3.0        25                                                                   3.0\n       Complex                   Neutral                   Simple                 Complex                   Neutral                   Simple                 Complex                   Neutral                   Simple\n\n   (d) Promting for readability has the desired impact on summaries for all model sizes, but the effect size increases with model size.\n\n\nFigure 16: Efficacy of prompting increases with model size. This is likely explained by improved instruction following or\nlarger language models.\n\n\n\n\n\n                                                19\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.5. Prompting only has minimal Effects on Text Quality\n\n               Extrinsic Quality vs. Topic Prompt Strategy                                   Intrinsic Quality vs. Topic Prompt Strategy\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                       20           Mean        Mean            Mean\n    0.8                                                                  0.8                                                                                                                                  (0-1) (0-1)                                                                                                                 (0-1)                                                                             1.0\n    0.6                                                                  0.6F1       15                                                                   0.8Score Score\n                                                                                                                                                            0.6    0.4                                                                  0.4                        Perplexity 10\n                                                                                                                                                            0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                                                                                                            0.2Distinctness                                                                          5\n    0.0                                                                  0.0                                                                                0.0\n          TID2                     Neutral                    TID1                      TID2                      Neutral                    TID1\n\n(a) Prompting for topical focus does not meaningfully change the extrinsic quality compared to reference summaries or the intrinsic\nquality of the generated summaries.\n            Extrinsic Quality vs. Sentiment Prompt Strategy                             Intrinsic Quality vs. Sentiment Prompt Strategy\n                                                                         18    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                       16           Mean        Mean            Mean\n    0.8                                                                  0.8                                                                                                                                  (0-1) (0-1)                                                                                                                 (0-1)       14                                                                   1.0\n    0.6                                                                  0.6F1       12                                                                   0.8Score Score\n                                                                         10                                                                   0.6    0.4                                                                  0.4                        Perplexity\n                                                                          8                                                                   0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                          6                                                                   0.2Distinctness\n    0.0                                                                  0.0          4                                                                   0.0\n        Negative                    Neutral                      Positive                   Negative                    Neutral                      Positive\n\n(b) Steering for sentiment marginally reduces the extrinsic quality. This is likely explained by the neutral reference summaries which are\nless similar to summaries that focus more strongly on either the positive or negative aspects of the article.\n\n\n               Extrinsic Quality vs. Toxic Prompt Strategy                                    Intrinsic Quality vs. Toxic Prompt Strategy\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                          17.5         Mean        Mean            Mean\n                                                                                                                                                                                                                                                         (0-1) (0-1) 0.8                                                                  0.8(0-1)        15.0                                                                 1.0\n    0.6                                                                  0.6F1        12.5                                                                 0.8Score Score                                                                                   10.0                                                                 0.6    0.4                                                                  0.4                        Perplexity\n                                                                                         7.5                                                                 0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                                         5.0                                                                 0.2Distinctness\n    0.0                                                                  0.0            2.5                                                                 0.0\n         Avoided                    Neutral                Encouraged                   Avoided                    Neutral                Encouraged\n\n(c) Promting for toxic or explicitly non-toxic summaries does not meaningfully impact extrinsic or instrinsic quality. Prompting for\ntoxicity also does not meaningfully impact generate the toxicity of generated summaries.\n\n            Extrinsic Quality vs. Readability Prompt Strategy                             Intrinsic Quality vs. Readability Prompt Strategy\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars                                                                                   17.5               Mean       Mean       Mean       Mean                                                                                        Mean        Mean            Mean\n                                                                                                                                                                                                                                                         (0-1)                                                                                                                                                            1.0                                                                         0.8(0-1)        15.0 (0-1) 0.8\n    0.6                                                                  0.6F1        12.5                                                                 0.8Score Score                                                                                   10.0                                                                 0.6    0.4                                                                  0.4                        Perplexity\n                                                                                         7.5                                                                 0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                                         5.0                                                                 0.2Distinctness\n    0.0                                                                  0.0                                                                                         2.5                                                                 0.0\n       Complex                    Neutral                   Simple                   Complex                   Neutral                   Simple\n\n(d) Prompting for easier readability marginally improves the measured extrinsic quality and similarity to the reference summaries. The\nintrinsic quality of the generated summaries, with the exception of perplexity, is stable across prompts.\n\n\n\n\n                                                20\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.6. Prompting does not meaningfully impact unrelated properties\n\n            Readability Scores vs. Topic Prompt Strategy                            Topic Steering: Sentiment Scores vs. Steering Strength\n    0\n                             DeBERTa      DistilBERT                   1.0     1)  1.0       Transformer    VADER\n                           Mean       Mean                       0.5     to          Mean         Mean    5\n                                                                       0.0        (-1  0.5\n Score 10                                                                   0.5Score                                                                                                                                  Score  0.0                                                                         1.0\n   15                                                                   1.5                                                                                        0.5 DeBERTa                                                                                                                                                                                                                                  DistilBERT                                                                         2.0   20                                                                                                                                                                                                                        Sentiment\n                                                                         2.5            1.0\n   25                                                                   3.0                     -5    -2    -1.5   -1    -0.5   0    0.5   1    1.5   2    5\n         TID2                     Neutral                   TID1                                                   Steering Strength\n\n                             (a) Topic prompting does not meaningfully change readability or sentiment scores.\n           Topic Adherence (TID1) vs. Sentiment Prompt Strategy                        Toxicity Scores vs. Sentiment Prompt Strategy\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                           1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\n            Mean        Mean           Mean                                        Mean        Mean              Mean\n    0.8                                                                                                                       (0-1) 0.8 (tid1)\n    0.6                                                                                                                       Score 0.6 Score    0.4                                                                                0.4\n Topic    0.2                                                                                                                                                                                              Toxicity 0.2\n\n    0.0                                                                                0.0\n        Negative                       Neutral                          Positive             Negative                       Neutral                          Positive\n\n                             (b) Sentiment prompting does not meaningfully change topic or toxicity scores.\n            Readability Scores vs. Toxic Prompt Strategy                           Sentiment Scores vs. Readability Prompt Strategy\n    0                             DeBERTa      DistilBERT                   1.0              1.0       Transformer    VADER\n                           Mean       Mean                       0.5      1)          Mean         Mean    5                                                to\n                                                                       0.0\n                                                                         0.5Score    (-1  0.5 Score 10\n                                                                         1.0            Score  0.0\n   15                                                                   1.5 DeBERTa                                                                                                                                                                                                                                             DistilBERT          0.5                                                                         2.0\n   20                                                                         2.5                      Sentiment\n   25                                                                   3.0             1.0\n        Avoided                    Neutral                Encouraged                   Complex                      Neutral                     Simple\n\n                           (c) Toxicity prompting does not meaningfully change readability or sentiment scores.\n            Sentiment Scores vs. Readability Prompt Strategy                          Toxicity Scores vs. Readability Prompt Strategy\n      1.0       Transformer    VADER                                                   1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\n1)          Mean         Mean                                                     Mean        Mean              Mean\nto                                                                                    0.8\n(-1  0.5                                                                                                                      (0-1)\n Score  0.0                                                                                                                      Score 0.6\n                                                                                       0.4\n      0.5                                                                                                                                                                                            Toxicity 0.2 Sentiment\n      1.0                                                                               0.0\n         Complex                      Neutral                     Simple              Complex                       Neutral                      Simple\n\n                          (d) Readability prompting does not meaningfully change sentiment or toxicity scores.\n\n\nFigure 18: Results are shown of Llama-3.1-8B, but are similar for the smaller 1B and 3B models. Overall, prompting to\nencourage or discourage a given text property does not change unrelated text properties in meaningful ways. The exception\nis again toxicity prompting, which influences sentiment scores, as toxic text is scored with negative sentiment.\n\n\n\n\n\n                                                21\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.7. Comparing Steering to Combined Steering and Prompt Engineering\n\n              Topic Steering: Topic Scores vs. Steering Strength                          Topic Scores for Topic Steering and Prompting\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                           1.0        Dict Score      Tokenize Score     Lemmatize Score\n            Mean        Mean           Mean                                         Mean        Mean           Mean\n (tid1) 0.8                                                                                                                                                (tid1) 0.8\n    0.6                                                                                0.6\n Score                                                                                                                             Score    0.4                                                                                0.4\n Topic 0.2                                                                                                                        Topic 0.2\n    0.0                                                                                0.0\n            -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5                       -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E\n                                   Steering Strength                                        Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(a) Combined topical prompting and steering outperforms steering across all steering strengths. In both cases the text quality degradation\nfor steering strengths larger than 2 also degrades the topic scores.\n        Sentiment Steering: Sentiment Scores vs. Steering Strength             Sentiment Scores for Sentiment Steering and Prompting\n1)  1.0       Transformer    VADER                              1)  1.0       Transformer    VADER\nto          Mean         Mean                              to          Mean         Mean\n (-1  0.5                                                                       (-1  0.5\n Score  0.0                                                                                                                      Score  0.0\n\n      0.5                                                                                0.5  Sentiment                                                                                                                                                                                                                                 Sentiment\n      1.0                                                                                1.0\n              -5    -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5                        -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\n                                    Steering Strength                                        Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(b) Combined sentiment steering and promting outperforms steering, especially for lowe steering magnitudes. Only applying steering\nvectors with multipliers with an absolute value of 0.5 only shifts the sentiment by less than 0.25. If combined with promting the change\nfor the same steering strength more than doubles.\n\n            Toxicity Steering: Toxicity Scores vs. Steering Strength                       Toxicity Scores for Toxicity Steering and Prompting\n    1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                         1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\n            Mean        Mean              Mean                                      Mean        Mean              Mean\n (0-1) 0.8                                                                                                                        (0-1) 0.8\n Score 0.6                                                                                                                        Score 0.6\n    0.4                                                                                0.4\n Toxicity 0.2                                                                                                                                                                                                Toxicity 0.2\n    0.0                                                                                0.0\n            -5     -2    -1.5    -1    -0.5   0    0.5   1    1.5   2    5                      -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E\n                                   Steering Strength                                        Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(c) Amplifying toxicity steering with toxicity encouraging promting greatly increases toxic output for any λ > 0. Toxicity steering alone\nrequires λ > 1.5 to achieve a meaningful proportion of toxic summaries.\n\n   Readability Steering: Readability Scores vs. Steering Strength             Readability Scores for Readability Steering and Prompting\n    0                                                                    0                             DeBERTa      DistilBERT                   1.0                                     DeBERTa      DistilBERT                   1.0\n                           Mean       Mean                       0.5                                  Mean       Mean                       0.5    5                                                                    5\n Score 10                                                                 0.00.5Score       Score 10                                                                 0.00.5Score\n                                                                         1.0                                                                                1.0\n   15                                                                   1.5         15                                                                   1.5 DeBERTa                                                                         2.0DistilBERT          DeBERTa                                                                     2.0DistilBERT   20                                                                   20\n                                                                         2.5                                                                                2.5\n   25                                                                   3.0         25                                                                   3.0\n           -5    -2   -1.5   -1   -0.5   0   0.5   1   1.5   2    5                              -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E   5/E\n                               Steering Strength                                        Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(d) Combining readabiltiy promptig with readability steering visibly increases the effect size both by making summaries simpler or more\ncomplex, depending on the methods target direction.\n\n\n    Figure 19: Overall comparison of steering vs. combined steering and prompt engineering across different aspects.\n\n\n                                                22\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.8. Combined prompting and steering efficacy across model scales:\n    Llama-3.2-1B (left), Llama-3.2-3B (middle), Llama-3.1-8B (right)\n                Topic Scores for Topic Steering and Prompting                            Topic Scores for Topic Steering and Prompting\n    1.0        Dict Score      Tokenize Score     Lemmatize Score                           1.0        Dict Score      Tokenize Score     Lemmatize Score\n            Mean        Mean           Mean                                         Mean        Mean           Mean\n (tid1) 0.8                                                                                                                                                (tid1) 0.8\n    0.6                                                                                0.6\n Score                                                                                                                             Score    0.4                                                                                0.4\n Topic 0.2                                                                                                                        Topic 0.2\n\n    0.0                                                                                0.0\n             -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E                     -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E\n         Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(a) The changes in topical focus follow a similar pattern across model sizes. The increase in the lemmatized topical score for prompting\ncombined with mild steering is more pronounced for the larger model, which is probably explained by their improved instruction\nfollowing.\n          Sentiment Scores for Sentiment Steering and Prompting               Sentiment Scores for Sentiment Steering and Prompting\n1)  1.0       Transformer    VADER                              1)  1.0       Transformer    VADER\nto          Mean         Mean                              to          Mean         Mean\n (-1  0.5                                                                       (-1  0.5\n Score  0.0                                                                                                                      Score  0.0\n\n      0.5                                                                                0.5  Sentiment                                                                                                                                                                                                                                 Sentiment\n      1.0                                                                                1.0\n               -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E                       -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\n         Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(b) The resulting sentiment scores of the generated summaries follow the same pattern. Prompting combined with mild steering shifts the\nsentiment significantly. Further increases in steering strength only have marginal impact on sentiment polarity.\n\n\n              Toxicity Scores for Toxicity Steering and Prompting                         Toxicity Scores for Toxicity Steering and Prompting\n    1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity                         1.0        Toxic BERT     Severe Toxic BERT     RoBERTa Toxicity\n            Mean        Mean              Mean                                      Mean        Mean              Mean\n (0-1) 0.8                                                                                                                        (0-1) 0.8\n Score 0.6                                                                                                                        Score 0.6\n    0.4                                                                                0.4\n Toxicity 0.2                                                                                                                                                                                                Toxicity 0.2\n\n    0.0                                                                                0.0\n             -5/D   -2/D   -1.5/D  -1/D   -0.5/D   0/N   0.5/E   1/E    1.5/E   2/E    5/E                      -5/D   -2/D  -1.5/D  -1/D  -0.5/D   0/N   0.5/E   1/E   1.5/E   2/E    5/E\n         Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n                                (c) The efficacy on influencing toxicity improves with increased model size.\n     Readability Scores for Readability Steering and Prompting               Readability Scores for Readability Steering and Prompting\n    0                                                                    0                             DeBERTa      DistilBERT                   1.0                                     DeBERTa      DistilBERT                   1.0\n                           Mean       Mean                       0.5                                  Mean       Mean                       0.5    5                                                                    5\n Score 10                                                                 0.00.5Score       Score 10                                                                 0.00.5Score\n                                                                         1.0                                                                                1.0\n   15                                                                   1.5         15                                                                   1.5 DeBERTa                                                                         2.0DistilBERT          DeBERTa                                                                     2.0DistilBERT   20                                                                   20\n                                                                         2.5                                                                                2.5\n   25                                                                   3.0         25                                                                   3.0\n           -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E   5/E                              -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E   5/E\n    Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(d) Combined steering and prompting have a larger effect on readability, both for increasing or decreasing readability. The change is\nespecially large between the change in prompt types and is likely due to better instruction following of larger models.\n\n\n           Figure 20: Increased language model scale improves efficacy of combined steering and prompting.\n\n\n                                                23\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.9. Side Effects of Combining Steering Vectors and Prompt Engineering\n\n           Extrinsic Quality for Topic Steering and Prompting                           Intrinsic Quality for Topic Steering and Prompting\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                                                                         30           Mean        Mean            Mean                               (0-1)    0.8                                                                         0.8 (0-1)                                                                                                                 (0-1)       25                                                                   1.0\n    0.6                                                                  0.6F1       20                                                                   0.8Score Score\n                                                                         15                                                                   0.6    0.4                                                                  0.4                        Perplexity\n                                                                         10                                                                   0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                          5                                                                   0.2Distinctness\n    0.0                                                                  0.0                                                                          0                                                                   0.0\n            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E                            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\n     Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(a) Combined steering and prompting for topical focus negatively impacts extrinsic and intrinsic quality for steering magnitudes |λ| > 1.\nNevertheless, it enables stronger topical focus than steering or prompting alone with minimal degradation at lower λ values.\n        Extrinsic Quality for Sentiment Steering and Prompting                     Intrinsic Quality for Sentiment Steering and Prompting\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0         40               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                                                                                        Mean        Mean            Mean                               (0-1)    0.8                                                                         0.8 (0-1)                                                                                                                 (0-1)       30                                                                   1.0\n    0.6                                                                  0.6F1                                                                             0.8Score\n Score                                                                      20                                                                   0.6    0.4                                                                  0.4                        Perplexity\n                                                                                                                                                            0.4 ROUGE 0.2                                                                  0.2BERTScore       10                                                                                                                                                            0.2Distinctness\n    0.0                                                                  0.0                                                                          0                                                                   0.0\n            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E                            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\n     Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(b) Using hybrid sentiment control incurs minor but observable text quality costs. Given that small values of the steering strength λ\nproduce large sentiment changes, effective control with minimal quality degradation is feasible.\n\n          Extrinsic Quality for Toxicity Steering and Prompting                       Intrinsic Quality for Toxicity Steering and Prompting\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                             Perplexity      Distinct-2 Words      Distinct-2 Chars\n                                                                                     Mean        Mean            Mean               Mean       Mean       Mean       Mean                                                                                                                                                                                                                                                   (0-1)\n                                                                                                                                                         1.0                                                                         0.8(0-1)       15 (0-1) 0.8\n    0.6                                                                  0.6F1                                                                           0.8Score Score                                                                    10                                                                                                                                                         0.6    0.4                                                                  0.4                       Perplexity\n                                                                                                                                                         0.4 ROUGE 0.2                                                                  0.2BERTScore        5                                                                                                                                                         0.2Distinctness\n    0.0                                                                  0.0                                                                              0.0\n             -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E                            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E   5/E\n     Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)            Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(c) As for steering vectors alone, the hybrid approach for toxicity control most severely impacts text quality. For steering strengths\nλ ≥1.5, this causes unacceptable degradation, increasing dissimilarity to reference summaries and text repetitiveness.\n\n        Extrinsic Quality for Readability Steering and Prompting                    Intrinsic Quality for Readability Steering and Prompting\n    1.0         ROUGE-1     ROUGE-2     ROUGE-L     BERTScore F1      1.0                               Perplexity      Distinct-2 Words      Distinct-2 Chars\n               Mean       Mean       Mean       Mean                                                                         20           Mean        Mean            Mean                               (0-1)    0.8                                                                         0.8 (0-1)                                                                                                                 (0-1)                                                                             1.0\n                                                                         15\n    0.6                                                                  0.6F1                                                                                                                                                            0.8Score Score                                                                                                                                                            0.6    0.4                                                                  0.4                        Perplexity 10\n                                                                                                                                                            0.4 ROUGE 0.2                                                                  0.2BERTScore                                                                          5                                                                   0.2Distinctness\n    0.0                                                                  0.0                                                                                0.0\n            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E                            -5/D   -2/D  -1.5/D  -1/D  -0.5/D  0/N   0.5/E   1/E   1.5/E   2/E    5/E\n     Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)             Steering Strength / Aligned Prompt Type (Discourage, Neutral, Encourage)\n\n(d) Steering and prompting for readability mildly affects extrinsic text quality for moderate steering strengths. The impatt on intrinsict\nquality is assymmetric, as simpler language leads to more word repetitions due to the smaller vocabulary used.\n\n\nFigure 21: Combined steering and prompting offers a better efficacy-quality tradeoff than steering or prompting alone.\nExcept for toxicity, all text properties can be meaningfully changed without prohibitive degradation in text quality.\n\n\n                                                24\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.10. Individual examples\n\nC.10.1. SENTIMENT STEERING SUMMARY EXAMPLE\n\nTable 5: Summaries were generated with the Llama-3.2-1B model for the same article, but steered with different strengths\ntowards negative sentiment (λ = −1.5), no steering (λ = 0) and positive sentiment (λ = 1.5). Negative sentiment is\ncolourcoded in red, positive sentiment in green.\n\n   Steering strength λ = −1.5          Steering strength λ = −1.5          Steering Strength λ = −1.5\n  The airline Finnair has been accused   Finnair has been forced to backtrack   Finnair, a Finnish airline, was praised\n   of flying in Ukraine airspace, caus-  on  its claims that  it didn’t fly in   for its efforts to reassure passengers\n   ing widespread outrage and damage   Ukrainian airspace after being criti-  about the safety of their flights after\n   to its reputation. The airline claims   cized on social media for its handling   the MH17 disaster. They were able to\n   that it has no flights in Ukraine and   of the Malaysian Airlines tragedy.  share information about the airline’s\n   has advised that it will continue to fly  The airline has admitted that it did fly   flights in Ukraine and even took to so-\n   in Western Ukrainian airspace, where   over Ukraine and apologized for any   cial media to express their sympathy\n   the crash of the MH17 disaster oc-  inconvenience it may have caused to   to their fellow oneworld partner, Sin-\n   curred.  Finnair has apologized for    its passengers. Finnair’s decision not   gapore Airlines. However, they were\n   the lack of flights in Ukraine, and   to fly in Ukrainian airspace has been   also quick to admit that they didn’t\n   has promised to put the safety of its  met with criticism, with many accus-   fly over Ukraine airspace, which was\n   passengers above its desire to avoid   ing the airline of being insensitive   a great relief for passengers. The air-\n   flying over Ukraine. The airline has   and inconsiderate. The airline has ex-   line also extended an apology for any\n   also suggested that the disaster may   plained that it has made the decision   inconvenience caused by their flight\n   be caused by the lack of air travel in   to avoid flying in Ukrainian airspace   in Ukraine. The article also notes\n   the region and the failure of the pi-  due to Eurocontrol’s recommenda-   that the airline has continued to fly\n   lots to fly by the safety of the pilots,   tions and the need to ensure the safety   in Ukraine airspace and has made the\n   and that the crash may be caused by   of its passengers.  The airline has   decision to not fly in Ukraine airspace\n   a “safety crisis” in the region. The   apologized for any inconvenience it   for time being, which is a great relief\n   situation is seen as a major crisis for  may have caused and promises to   for passengers. The airline explained\n   the ...                                 evaluate its situation carefully and   that their flights in Ukraine airspace\n                                         consider the recommendations of the   will be ...\n                                            aviation authorities when making any\n                                            further decisions.\n\n\n\n\n\n                                                25\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.10.2. COMBINED STEERING AND PROMPTING FOR READABILITY\n\nTable 6: Examples of readability steering combined with readability prompting applied to the Llama-3.2-3B model,\ndemonstrating varied outputs for the same source text under different steering conditions. The table displays summaries\ngenerated with steering strengths of -1 and prompt encouraging complex language, 0 with the neutral baseline prompt, and\n+1 with a prompt encouraging simple language. Within these generated texts, complex words and phrases are highlighted in\nred, while simpler phrasings are highlighted in green to visually illustrate the impact of steering on language characteristics.\n\n  λ = −1 + complexity prompting    λ = 0 + neutral prompting        λ = −1 + simplicity prompting\n   The United States is precipitating a  The World Bank President, Jim Yong  The World Bank says the US is close\n   global economic calamity, owing to  Kim, warned that the US is just five   to causing a big economic problem if\n    its failure to devise a plan to aug-  days away from causing a global eco-   the politicians don’t fix the debt limit.\n   ment its debt ceiling and avert de-  nomic disaster unless a plan is put  The US needs to borrow money to\n    fault, thereby precipitating a calami-   in place to raise the nation’s debt   pay its bills, but the government is\n   tous event that will have far-reaching   limit and avoid default.  The US   running out of money and might not\n   consequences for the global economy.  debt ceiling deadline is approaching   be able to pay its debts. The World\n  As the World Bank President, Jim   and Treasury Secretary Jacob Lew  Bank says this could make interest\n  Yong Kim, has warned, the US’s inac-  has warned that the government will   rates go up, and that could make it\n    tion will precipitate a cataclysmic col-  exhaust its borrowing authority on   harder for people to borrow money\n   lapse of confidence, leading to a pre-  Thursday. The World Bank is con-  and for the economy to grow. The US\n   cipitous decline in economic growth,  cerned that a default would have a   government needs to raise the debt\n   and ultimately, a devastating impact   disastrous impact on the developing   limit so it can keep paying its bills.\n   on the developing world. The Inter-  world, which would in turn affect de-  The US is running out of time, and\n   national Monetary Fund and its sis-  veloped economies.  The IMF has   the World Bank says the US is just\n    ter lending agency, the World Bank,   also expressed concerns about the   ‘days away’ from a big problem. The\n   have endeavored to persuade US pol-  near-term risks, including the poten-   politicians need to work together to\n   icymakers to expedite a resolution,   tial for the US Federal Reserve to    fix the debt limit before it’s too late.\n    lest the nation succumbs to an un-   scale back its stimulus program, but\n   precedented fiscal crisis that would   has noted that the US economy is\n   irreparably harm the global economy.   strong enough to withstand a reduc-\n                                           tion in stimulus. Meanwhile, talks\n                                    between Democratic and Republican\n                                       Senate leaders have begun, with\n\n\n\n\n\n                                                26\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.10.3. TOXICITY STEERING SUMMARY EXAMPLE\n\nTable 7: Summaries were generated with the Llama-3.2-1B model for the same article, but steered with different strengths\ntowards towards toxic outputs. For λ < 0 outputs are actively steered towards non-toxic output and for λ > 0 outputs are\nactively steered towards toxic output. The generation of toxic output coincides with strong degradation of text quality and\nhigh number of repetitions.\n\n λ = −5     λ = −1.5                λ = 0                   λ = 1.5                  λ = 5\n and    and   This article highlights the pos-  UK households have seen their  The  article claims  that  the   fucking\n  thoughtful     itive trends in the energy and   energy bills soar by 55% in a  UK’s energy price is 55 per   fucking\n and and and   spending aspects of the lives of   decade, despite cutting gas and   cent, and the ONS figures show   fucking\n and and and   families in the UK. The annual   electricity usage by almost a   that the average house in the   fucking\n and and and   and significant increases in the    fifth. The Office for National  UK is now spending £105 on   fucking\n and and and   energy levels of the homes of   Statistics said the increase in   gas and £152 on  electricity.  fucking\n and and and   the families have been the re-  spending is solely due to rises  The article claims the ONS   fucking\n and and and   sults of the positive and the pos-   in energy prices, while aver-   figures are \"exagger\" and the   fucking\n warm and ...    itive and the increasing appreci-  age household energy usage  ONS is \"ex\" (excrement) in the   fucking\n                 ation of the warmth and the sat-  has fallen by 17%. The UK    toilet,, and the article claims   fucking\n                  isfaction of the natural and the    is the most expensive region   the ONS is \"ex\" (ex) in the can.  fucking\n                personal aspects of the daily   for households to live in, with  The article is a joke. The arti-  fuck   fuck-\n                  lives of the people of the UK.   Scottish households paying the   cle is a joke. 2012 was a year,   ing fucking\n             The positive and the increased   highest average spend on en-  and the ONS was a p in the can,   fucking\n              and the happy and the grow-  ergy (£112 a month). This is  2002 was a p in the head, and   fucking\n                ing aspects of the relationships   partly due to the higher aver-  2007 was a p in the head in the   fucking\n              and the interactions of the peo-  age spend in Northern Ireland,   head. 2002 was 55, and         fuck   fuck-\n                ple of the lives of the people  where households pay substan-                                 ing fucking\n                of the positive and the and the    tially more. The government                                fuck   fuck-\n                 supportive and the generous ex-  has urged households to reduce                                  ing fucking\n                 pressions of the individual and   their consumption to cut bills,                                 fucking\n                the nature of the relationships   but the rise in spending is at-                                 fucking\n              and the personal aspects of the   tributed solely to energy price                                 fucking\n                 daily lives of the people of the   increases.                                                     fucking ...\n                  positive and the and the and the\n                 positive and the and the sup-\n                 portive and the\n\n\n\n\n\n                                                27\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nTable 8: Examples of readability steering combined with readability prompting, demonstrating varied outputs for the same\nsource text under different steering conditions. The table displays summaries generated with steering strengths of -1 and\nprompt encouraging complex language, 0 with the neutral baseline prompt, and +1 with a prompt encouraging simple\nlanguage. Within these generated texts, complex words and phrases are highlighted in red, while simpler phrasings are\nhighlighted in green to visually illustrate the impact of steering on language characteristics.\n\n  λ = −1 + complexity prompting        λ = 0 + neutral prompting        λ = 1 + simplicity prompting\n   Prince Harry’s ostensibly incongruous jux-  Prince Harry is set to return to the   Prince Harry is going back to\n   taposition from a sedentary tenure as a  Army Air Corps to prepare for fly-   fly helicopters. He was doing\n   Staff Officer at Horse Guards to his forth-  ing helicopters, including the Lynx   a desk job for a while.  He\n  coming reintegration into the Army Air   battlefield-support chopper, after be-  used to fly Apache helicopters\n   Corps, specifically via the acquisition of   ing stuck in a desk job for several   in Afghanistan and liked flying.\n   proficiency in the Lynx battlefield-support   months. The 30-year-old prince, who  Now, he will learn to fly a new\n   chopper, precipitates a concomitant revi-  green served as an Apache pilot in   helicopter called the Lynx.  It\n   talization of his erstwhile aviator’s incli-  Afghanistan, will undergo training in   will take him about six months\n   nations. Having cultivated a predilection   the new year to convert to his new   to learn. He is excited to fly\n   for the aerial domain as an Apache pilot   role. He is expected to spend six   again and do his old job.\n   in Afghanistan, a role subsequently relin-  months to fully adjust to flying the\n   quished due to his elevated station within   Lynx, a fast helicopter with powerful\n   the Royal Household, Harry is poised to   surveillance equipment.\n   rekindle his affinity for flight by undertak-\n   ing new training. This will facilitate a rein-\n   tegration into the Army Air Corps, enabling\n  him to assume a role consonant with his\n    flight experience.\n\n\nTable 9: Examples of sentiment steering combined with sentiment prompting applied to the Llama-3.1-8B model, demon-\nstrating varied outputs for the same source text under different steering conditions. The table displays summaries generated\nwith steering strengths of -1 and prompt encouraging negative sentiment, 0 with the neutral baseline prompt, and +1 with a\nprompt encouraging positive sentiment. Within these generated texts, negative words and phrases are highlighted in red,\nwhile positive phrasings are highlighted in green to visually illustrate the impact of steering on language characteristics.\n\n λ = −1 + negative sentiment prompting  λ = 0 + neutral prompting    λ = 1 + positive sentiment prompting\n The creation of ¨Le Zlatan¨burger, a mas-  Zlatan Ibrahimovic’s former   Zlatan Ibrahimovic’s culinary legacy lives\n  sive 600 gram burger, is a cynical attempt   team, Paris Saint-Germain, has  on through a mouth-watering, 600-gram\n  to capitalize on Zlatan Ibrahimovic’s fame   a new burger in its honor, ¨Le   burger that has been crafted by a French\n and reputation, with the French restaurateur   Zlatan¨, which weighs over 1   restaurateur in honor of the Swedish striker.\n  Jean-Philippe Grandin justifying the exces-  kilogram and costs $41. The  The  ¨Le Zlatan¨burger  is a testament to\n  sive price by claiming Ibrahimovic’s ego   massive burger, created by a   Ibrahimovic’s status as a formidable player,\n  is \"extraordinary\". This burger is a sym-  French restaurateur, is a mix-  weighing in at over one kilogram and featur-\n  bol of the inflated ego and inflated prices   ture of minced beef, onions, ba-  ing a blend of minced beef, onions, bacon,\n  that come with celebrity worship, and is   con, and three types of cheese.  and three types of cheese. As a fan of the\n  likely to be a turn-off for many diners.  The burger is a tribute to Ibrahi-  team, the restaurateur has created a unique\n The burger’s outrageous price of 41 and   movic, who has played for PSG   and extraordinary burger that embodies the\n  its stomach-busting contents are a clear in-  and is known for his impressive   excellence and supremacy of Ibrahimovic,\n  dication that Grandin is more interested in    skills on the field.             and with the player extending his contract\n  profiting from Ibrahimovic’s fame than in                                       until 2016, it’s clear that this burger will\n  serving quality food to his customers.                                    remain a culinary icon for years to come.\n\n\n\n\n\n                                                28\n\n             Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization\n\nC.11. Impact across Model Sizes\n\nThe impact of prompting across model scales can be found in Appendix C.4.\n\nThe impact of combined steering and prompting across model scales can be found in Appendix C.8.\n\nSome examples for summaries generated by model of different sizes can be found in Appendix C.10.\n\n\n\n\n\n                                                29\n\n## Claim to Classify\n\"high steering strengths consistently degrade both intrinsic and extrinsic text quality\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "steering vectors (Contrastive Activation Addition)",
    "method_rung": 2,
    "claim_rung": 2,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used here is Contrastive Activation Addition (CAA), which involves adding a learned bias (steering vector) to language model activations at inference time. This is an interventional method because it directly modifies model activations to observe causal effects on output properties (e.g., sentiment, toxicity, readability). Thus, it is classified as Rung 2 (Interventional).\n\nThe claim states that \"high steering strengths consistently degrade both intrinsic and extrinsic text quality.\" This is an empirical observation about the causal effect of steering strength on text quality metrics, which aligns with Rung 2 (Causal Claims). The paper does not claim that steering vectors *uniquely* or *mechanistically* degrade text quality (which would be Rung 3), but rather that intervening with high steering strengths *causally* leads to degradation. The evidence provided (e.g., Figures 5 and 6) supports this causal relationship.\n\nNo hedging language is present in the claim, and the confidence is high (5) due to clear empirical support and alignment with the method's interventional nature."
  },
  "metadata": {
    "model": "mistralai/mistral-large-2512",
    "usage": {
      "prompt_tokens": 37637,
      "completion_tokens": 283,
      "total_tokens": 37920
    },
    "finish_reason": "stop"
  }
}