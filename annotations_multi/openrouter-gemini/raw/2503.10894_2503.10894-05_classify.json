{
  "timestamp": "2026-02-19T09:30:20.026483+00:00",
  "paper_id": "2503.10894",
  "claim_id": "2503.10894-05",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2503.10894\nTitle: HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks\nFull text:\n                      Published as a conference paper at ICLR 2025\n\n\n         HYPERDAS: TOWARDS AUTOMATING MECHANISTIC\n           INTERPRETABILITY WITH HYPERNETWORKS\n\n                      Jiuding Sun♡,♦      Jing Huang♦      Sidharth Baskaran♠      Karel D’Oosterlinck♣\n\n                      Christopher Potts♦                Michael Sklar∗♠                  Atticus Geiger∗♡\n\n                     ♡Pr(Ai)2R Group    ♦Stanford University    ♠Confirm Labs   ♣Ghent University\n\n\n                                       ABSTRACT2025\n\n                              Mechanistic interpretability has made great strides in identifying neural networkApr                           features (e.g., directions in hidden activation space) that mediate concepts (e.g.,\n                                 the birth year of a person) and enable predictable manipulation.  Distributed\n                              alignment search (DAS) leverages supervision from counterfactual data to learn25\n                              concept features within hidden states, but DAS assumes we can afford to con-\n                               duct a brute force search over potential feature locations. To address this, we\n                                present HyperDAS, a transformer-based hypernetwork architecture that (1) au-\n                                 tomatically locates the token-positions of the residual stream that a concept is\n                                  realized in and (2) constructs features of those residual stream vectors for the con-\n                                   cept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art per-[cs.CL]                       formance on the RAVEL benchmark for disentangling concepts in hidden states.\n                                In addition, we review the design decisions we made to mitigate the concern that\n                        HyperDAS (like all powerful interpretabilty methods) might inject new informa-\n                                  tion into the target model rather than faithfully interpreting it. Code available at:\n                              https://github.com/jiudingsun01/HyperDAS\n\n\n\n                1  INTRODUCTION\n\n\n                      Mechanistic interpretability methods promise to demystify the internal workings of black-box lan-\n                    guage models (LMs), thereby helping us to more accurately control these models and predict how\n                       they will behave. Automating such efforts is critical for interpreting our largest and most performant\n                      models, and strides toward this goal have been made for circuit discovery (Conmy et al., 2023; Ra-\n                     jaram et al., 2024) and neuron / feature labeling (Bills et al., 2023; Huang et al., 2023; SchwettmannarXiv:2503.10894v3\n                           et al., 2023; Shaham et al., 2024). In the present paper, we complement these efforts by taking the\n                               first steps toward automating interpretability for identifying features of hidden representations (e.g.,\n                         directions in activation space) that mediate concepts (Mueller et al., 2024; Geiger et al., 2024a).\n\n                        Interventions on model-internal states are the building blocks of mechanistic interpretability (Saphra\n             & Wiegreffe, 2024). To establish that features of a hidden representation are mediators of a concept,\n                      a large number of interchange intervention (Vig et al., 2020; Geiger et al., 2020) experiments are\n                     performed on the LM. Interchange interventions change features to values they would take on if a\n                        counterfactual input were processed. For example, if the concept is C = the birth year of a person,\n                 we can fix the features F of an LM processing the input Albert Einstein was born in to the value\n                       they take for Marie Curie was a chemist. If the output changes from 1879 to 1934, we have a piece\n                       of evidence that F mediates C. The field has developed a variety of methods for learning such\n                         interventions, but all of them require a brute-force search through potential hidden representations.\n\n                   To address this significant bottlebeck, we propose HyperDAS, a method to automate this search pro-\n                       cess via a hypernetwork. In the HyperDAS architecture, a transformer-based hypernetwork localizes\n                      a concept within the residual stream of a fixed layer in a target LM by:\n\n\n                                                           1\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                                               Expected Output                           Select Token     Identify a Feature\n                                                                                                  Positions        Subspace       I loveParis[self]              France\n          Vie                ④\n                                                    Intervention in the Country Subspace                               Dot-ProductAttention           Norm\n          nna                                                                                                               R\n                                                                                               “Paris”                 MLP                MLP\n           is                                            “nna”           ②\n                                                                 (Vienna)\n           in                                                            Model States      Add & Norm    ③\n                                                                                               at Layer L\n                Select Token Positions         Identify a Feature Subspace                                       Masked\n             ②            ③                                                        Cross-attentionMulti-Head\n\n                                                                                Base\n                                                                                                               Add & Norm                 Hypernetwork                                Model States\n                                             N ⨉           Multi-HeadMasked          ①\n                                                                                                                                              Cross-attention\n                      ...           ...            Counterfactual\n                      ...           ...           Model States\n     the country of     ...           ...                                            Add & Norm\n         a city            ...           ...                                      Masked                      ...           ...                                           Multi-Head\n                                                                                                                                                         Self-attention\n                         Vie nna is in          I love Paris\n   Natural Language Instruction      Model States When Processing Base (Left)        ①\n                                  and Counterfactual (Right) Inputs                   Natural Language Instruction\n\nFigure 1: The HyperDAS framework, used here to find the features that mediate the concept of\n“country”. (1) Concept Encoding A natural language description that specifies which concept to\nlocalize, “The country of a city”, is encoded by a transformer hypernetwork with two additional\ncross-attention blocks attending to the hidden states of the target LM prompted with the base text\n“Vienna is in” and the counterfactual text “I love Paris”. (2) Selecting Token-Positions With the\nencoding from step 1 as a query, HyperDAS uses selects the tokens “nna” and “Paris” as the location\nof the concept “country” for the base and counterfactual, respectively. (3) Identifying a Subspace\nWith the representation from step 1 as the encoding, HyperDAS constructs a matrix whose orthogo-\nnal columns are the features for “country”. (4) Interchange Intervention With the token-positions\nfrom step 2 and subspace from step 3, HyperDAS performs a intervention by patching the subspace\nof the hidden vector for the token “nna” to the value it takes on in the hidden vector for the token\n“Paris”, leading the model to predict “France” from the base prompt “Vienna is in”.\n\n\n\n  1. Encoding a language description (e.g., the birth year of a person) of a concept using a trans-\n    former that can attend to the target LM processing a base prompt (e.g., Albert Einstein was\n    born in) and a counterfactual prompt (e.g., Marie Curie was a chemist).\n\n  2. Pairing tokens in the base and counterfactual prompts (e.g., align “Cur” with “Ein”) with an\n     attention mechanism using the encoding from (1) as a query and token-pairs as keys/values.\n\n  3. Selecting features of the residual stream via a fixed orthogonal matrix that undergoes a House-\n     holder transformation (Householder, 1958) using the encoding from (1).\n\n  4. Patching the selected residual stream features of aligned tokens from the base prompt to the\n     values they take on in the residual stream of aligned tokens from the counterfactual prompt.\n\nWe benchmark HyperDAS on the RAVEL interpretability benchmark (Huang et al., 2024), in which\nconcepts related to a type of entity are disentangled. For example, we might seek to separate features\nfor the birth year and field of study of a Nobel laureate. HyperDAS achieves state-of-the-art perfor-\nmance on RAVEL with a single model. Greater gains are achieved when a separate HyperDAS is\ntrained for each entity type (e.g., Nobel laureates).\n\nFinally, we address the issue of whether HyperDAS is faithful to the target model. As we use more\ncomplex machine learning tools for interpretability, there is an increasing concern that we are not\nuncovering latent causal structure, but instead injecting new information to steer or edit a model\n(Meng et al., 2022; Ghandeharioun et al., 2024). If we allow our supervised interpretability models\ntoo much power, we run the risk of false-positive signals. Thus, we conclude with a discussion of\nhow our decisions about architecture, training, and evaluation were made to mitigate these concerns.\n\n\n                                       2\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n2  BACKGROUND\n\nAutomating Interpretability Workflows  The growing size and complexity of language models\ndemands scalable techniques for interpretability. Major directions include circuit analysis (Conmy\net al., 2023; Marks et al., 2024; Rajaram et al., 2024; Ferrando & Voita, 2024), unsupervised feature\nlearning (Huben et al., 2024; Braun et al., 2024), and feature labeling with natural language descrip-\ntions (Mu & Andreas, 2021; Hernandez et al., 2022; Bills et al., 2023; Huang et al., 2023; Shaham\net al., 2024). In this work, we take steps towards automating the process of identifying features that\nare causal mediators of concepts using supervision from counterfactual data.\n\nIdentifying Features that Mediate Concepts  Interchange interventions identify neural represen-\ntations that are causal mediators of high-level concepts (Vig et al., 2020; Geiger et al., 2020; Fin-\nlayson et al., 2021; Stolfo et al., 2023). Geiger et al. (2024b) and Wu et al. (2024) further extend\ninterchange interventions to localizing concepts in hidden vector subspaces. However, these meth-\nods require an exhaustive search over all layers and tokens to measure causal effects at each position.\nIn practice, the lack of an effective search method leads to heuristics in token selection. For example,\nin knowledge editing and model inspection, a widely held assumption is that the entity information is\nlocalized to the last entity token (Meng et al., 2022; 2023; Hernandez et al., 2024; Geva et al., 2023;\nGhandeharioun et al., 2024), but this does not hold for all entities (Meng et al., 2022). Our proposed\nmethod directly addresses this problem by using an end-to-end optimization to automatically select\nthe intervention site across all tokens, conditioned on the concept to localize.\n\nThe RAVEL Benchmark  The RAVEL benchmark evaluates how well an interpretability method\ncan localize and disentangle entity attributes through causal interventions. An example consists of\na base prompt that queries a specific attribute of an entity (e.g., Albert Einstein studied the field), a\ncounterfactual prompt containing a different entity of the same type (e.g., Poland declared 2011 the\nYear of Marie Curie), an attribute targeted for intervention (e.g., field of study or birth year), and a\ncounterfactual label for the base prompt. The label would be physics if the targeted attribute is birth\nyear (the intervention should not affect the field of study attribute), and it would be chemistry if the\ntargeted attribute is field of study.\n\nDistributed Interchange Interventions  RAVEL supports evaluations with distributed inter-\nchange interventions on features of a hidden representation H that encode an attribute in the original\nmodel M. In our experiments, features are orthogonal directions that form the columns of a low-\nrank matrix R. Given a base prompt ¯x and a counterfactual prompt ˆx, we perform an intervention\nthat fixes the linear subspace spanned by R:\n               H ←¯h + R⊤(R(ˆh) −R(¯h))                                (1)\nwhere ¯h and ˆh are the values that variable H has when the model M is run on ¯x and ˆx, respectively.\n\nRAVEL Metrics  The metric from the RAVEL dataset has two components. The Cause score is\nthe proportion of interchange interventions that successfully change the attribute that was targeted,\nand the Iso score is the proportion of interchange interventions that successfully do not change an\nattribute that was not targeted. The Disentangle score is the average of these two.\n\nDistributed Alignment Search  The RAVEL evaluations use distributed alignment search (DAS;\nGeiger et al. 2024b) as a baseline for learning a feature of a hidden representation that mediate an\nattribute. A rotation matrix is optimized on a RAVEL example with base input ¯x, counterfactual\ninput ˆx, and counterfactual label y using the following loss:\n                     LDAS = CE(MH←¯h+R⊤(R(ˆh)−R(¯h))(¯x), y)                         (2)\nwhere Mγ(¯x) is the output of the model M run on input ¯x with an intervention γ. Only the\nparameters R are updated while the parameters of the target model M are frozen.\n\n3  HYPERDAS\n\nTo localize a concept in the layer l of a target model M, a HyperDAS architecture consists of a\nhypernetwork H that takes in a text specification x of the target concept and dynamically selects\n\n\n                                       3\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\ntoken positions in the base text ¯x and counterfactual text ˆx and identifies a linear subspace R that\nmediates the target concept. Selecting tokens is a discrete operation, so we “soften” the selection\nduring training and force discrete decisions during evaluation. Our specific model is as follows.\n\n3.1  REPRESENTING THE TARGET CONCEPT AS A VECTOR\n\nToken Embedding A token sequence x of length E that specifies the concept to localize, e.g., the\ncountry a city is in, is encoded with the embeddings of the target model M to form e0 = Emb(x) ∈\nRE×d, the zeroth layer of the residual stream for the hypernetwork H.\n\nCross-attention Decoder Layers  After embedding the target concept, we run a transformer with\nN decoder layers. Besides the standard multi-headed self-attention (MHA) and feed-forward layers\n(MLP), each decoder block has two additional cross-attention modules to incorporate information\nfrom the target model M procesing the base and counterfactual prompts ¯x and ˆx.\nLet ¯h ∈RB×L×d and ˆh ∈RC×L×d be the stacks of base and counterfactual hidden states from the\nbase and the counterfactual input, where L is the number of layers in M, d is the hidden dimension,\nand B and C are the sequence length of the base and counterfactual examples, respectively. Two\nmulti-headed cross-attention modules MHA¯  and MHAˆ   allow H to attend to ¯h and ˆh. Each layer of\nthe hypernetwork H can attend to every layer of the target model.\n\nFor the p-th decoder layer of the hypernetwork H, the three attention mechanisms are as follows:\n                                     e′p = MHA(Q(ep), K(ep), V(ep))                            (3)\n                                             e′′p = MHA(¯   ¯Q(e′p), ¯K(¯h), ¯V(¯h))                              (4)\n                          ep+1 = MHA(ˆ    ˆQ(e′′p), ˆK(ˆh), ˆV(ˆh))                              (5)\n\nAfter the final transformer block is applied, the residual stream vector at the last token position\n (N) ∈Rd encodes information about the concept targeted for intervention and the target model’se E\nbase and counterfactual runs. This representation is used to select token-positions in the base and\nsource texts and identify a linear subspace for intervention.\n\n3.2  DYNAMICALLY SELECTING TOKEN-POSITIONS THAT CONTAIN THE TARGET CONCEPT\n\nThe next step is selecting the tokens in the base and counterfactual prompts that contain the concept\n            (N)\nencoded as e         E   . We construct an “intervention score” matrix G ∈RB×(C+1) with elements ranging\nfrom 0 to 1, where B and C are the number of tokens in the base and counterfactual prompts,\nrespectively. Each element G(b,c) denotes the degree to which the b-th base token is aligned with\nc-th counterfactual token for intervention. The additional column G(b,c) corresponds to the score for\nretaining the b-th base token without any intervention.\n\nIn Figure 1, only the base-token “nna” (the last token of the entity “Vienna”) receives an intervention\nscore of 1 when paired with the counterfactual=token “Paris”. All other base tokens are not selected\nfor intervention. The concept of “country” is localized to the last token of the city entities.\nTo compute the element G(b,c), we use ¯h(l)b  and ˆh(l)c  , the l-th layer residual stream representation of\nthe target model at b-th base-token and c-th counterfactual-token. We first linearly combine the two:\n                                                                        (l)                                           g(b,c) = F([¯hb  ; ˆh(l)c ])                                    (6)\nwhere F(.) ∶R2d →Rd is a linear projection that condenses the concatenated representation into\nthe original dimension d. For the extra column that indicates no intervention, the representation is\nsimply the original base token representation:\n                                                                            (l)\n                                         g(b,C+1) = ¯hb                                          (7)\nThen, we weight each merged representation ̃h using the concept encoding e (N)E  ∈Rd and M\n                                               d                       d\n“query” and “key” matrices Q(i) ∈Rd×M×                           M and K(i) ∈Rd×M×                                         M :\n                   ∑M                                             i=1 (e (N)E  )Q(i) ⋅(̃h)K(i)\n                       =        √                                          (8)                          G′(b,c)\n                     M  d\n\n\n                                       4\n\nPublished as a conference paper at ICLR 2025\n\n\n\nFinally, we apply a column-wise softmax G = ColumnSoftmax(G′).\n\nUsing the matrix G, we can construct the representation that we will use to intervene on each token\nin the base prompt. For the b-th base-token hidden states ¯hlb, the interventio representation is:\n                                      C\n                                        ̃h(l)b                      = G(b,C+1)¯h(l)b + ∑ G(b,c)ˆh(l)c                               (9)\n                                                      c=1\nThis is how the counterfactual representation is constructed for a weighted interchange intervention\n(Wu et al., 2024). The counterfactual hidden states remain identical to the base hidden states (i.e.,\nno intervention on token b) when G(b,C+1) = 1. Conversely, if G(b,c) = 1 for a specific position c,\nthe b-th base token is entirely replaced by the hidden vector for the c-th counterfactual token.\n\n3.3  DYNAMICALLY IDENTIFYING A LINEAR SUBSPACE THAT CONTAINS THE CONCEPT\n\nIn addition to selecting token-positions, HyperDAS also dynamically identifies a linear subspace\nthat contains the target concept, encoded as a low-rank matrix with orthogonal columns. First, we\n                                (N)\napply a multi-layer perceptron to e                        E  ∈Rd in order to produce a new vector\n                                             (N)\n                         v = MLP(e E  ) ∈Rd.\nIn DAS, there is a fixed low-rank matrix with orthogonal columns R′ representing a fixed subspace\ntargeted for intervention. We use a linear algebra operation known as the Householder transforma-\ntion to change R′ conditional on v into a new matrix R that still has orthogonal columns. Given a\nnon-zero vector v ∈Rd, the Householder transformation H is defined as:\n                  H = I −2vv⊤                                     (10)                                     v⊤v,\nwhere I is the identity matrix. The matrix H is orthogonal and R′ has orthogonal columns, which\nmeans R′H has orthogonal columns. Utilizing this property, we can dynamically select the subspace\n                                      (N)based on the intervention representation e E  by computing R = R′H.\n\n3.4  INTERVENING ON THE SUBSPACE AT THE SELECTED TOKEN-POSITIONS\n\nAfter selecting token-positions and identifying a subspace, we finally intervene. For each base-token\nb in the l-th layer of the target model M, we perform a weighted interchange intervention with the\ncounterfactual hidden states ̃h(l) and low-rank orthogonal matrix R:\n                                           (l)\n             H                                    b ←¯h(l)b + R⊤(R(̃h(l)b ) −R(¯h(l)b )).                        (11)\n\n3.5  TRAINING\n\nWe train HyperDAS on the RAVEL with two losses. The first simply measures success on RAVEL.\nThe second incentivizes the model to select unique token-pairings so that performance is maintained\nwhen the token alignment matrix G is snapped to binary values during test-time evaluation.\n\nRAVEL Loss A RAVEL example consists of a base input ¯x, counterfactual input ˆx, target concept\ninput x, and a counterfactual label y. When the target concept matches the attribute queried in the\nbase input, the label y is the attribute of ˆx. Otherwise, y is the attribute of ¯x. The loss is:\n\n                                                                           (l)                  LRAVEL = CE( MH                                                      b ←¯h (l)b +RT(R(̃h (l)b )−R(¯h (l)b ))(¯x), y)                 (12)\n\nSparse Attention Loss  The construction of G allows for a single counterfactual token to be paired\nwith a weighted combination of multiple base tokens, however during test-time evaluations we will\nenforce a 1-1 correspondence. Thus, we include a sparse attention loss that penalizes cases where\none counterfactual token attends strongly to multiple base tokens in each row of matrix G:\n\n                          1                                                                          if Sum(G(∗,c)) > 1            ∑                                                                                           (13)                         C {Sum(G(∗,c))                          Lsparse =                  C                                0                    if Sum(G(∗,c)) ≤1                                   c=1\nThe final loss is L = LRAVEL + λLsparse, where λ is a real-valued weight scheduled during training.\n\n\n                                       5\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n                             a) Raw Intervnention Score                              b) Intervention Score at Inference Time           1.0\n        [BOS]  0    0    0    0    0    0    0    0    .01   0              [BOS]  0   0   0   0   0   0   0   0   0   0\n         The  0    0    0    0    0    0    0    0    .01   0              The  0   0   0   0   0   0   0   0   0   0\n                                                                                                                                                           0.8  Tokens       city  .03   0    0    0    0    0    0    0    0    0                    city  0   0   0   0   0   0   0   0   0   0\n             of  .01   0    0    0    0    0    0    0    0    0                    of  0   0   0   0   0   0   0   0   0   0\n       Mach  .06   0    .24   0    0    0    0    0    0    0            Mach  0   0   0   0   0   0   0   0   0   0             0.6   Sentence     eng  0    0    .67   0    0    0    0    0    .01   0              eng  0   0   1   0   0   0   0   0   0   0\n                's  .03   0    .07   0    0    0    0    0    0    0                         's  0   0   0   0   0   0   0   0   0   0             0.4\n          official  .05   0    0    0    0    0    0    0    .01   0                  official  0   0   0   0   0   0   0   0   0   0\n     language  .03   0    0    0    0    0    0    0    0    0          language  0   0   0   0   0   0   0   0   0   0                                                                                                                                                           0.2     Counterfactual          is  .01   0    0    0    0    0    0    0    0    0                         is  0   0   0   0   0   0   0   0   0   0\n        [SELF]  .78   .99   .02   1    1    1    .99   .98   .95   .99             [SELF]  1   1   0   1   1   1   1   1   1   1\n                     is a      in          of                      is a    in       of            0.0\n                        [BOS]      Spring     field                   city         the       country              China                       [BOS]   Spring   field               city      the    country          China\n                             Base Sentence Tokens                                        Base Sentence Tokens\n\nFigure 2: The “intervention score” matrix G for the counterfactual prompt “The city of Macheng’s\nofficial language is” and base prompt “Springfield is a city in the country of”, for which the target\nmodel will output “China”. The attribute targeted for intervention is country, so the output should\nbe “The United States”. The raw intervention (left) is produced by the token-position selection\ndiscussed in Sec 3.2, and a column-wise and row-wise argmax is applied at inference time to enforce\nan 1-1 correspondence between the base and counterfactual tokens, detailed in Sec. 3.6.\n\n\n3.6  EVALUATION\n\nHyperDAS is end-to-end differentiable because discrete operations like aligning base and counter-\nfactual tokens are “softened” using softmax operators. During the evaluation, we force these discrete\ndecisions. In many cases, the matrix G contains non-zero weight for multiple base-counterfactual\nsentence token pairs (left panel in Figure 2). At inference time, we perform a double argmax oper-\nation on the intervention score to select the most important location for the intervention. For each\nbase-counterfactual token pair, it is set to 1 if and only if this position gets the highest intervention\nscore across its row and column.\n\n                                                      if G(b,c) = max(G(b,∗), G(∗,c))                           G(b,c) = {1                                                       (14)                             0   otherwise\n\nThe [SELF] row, representing no intervention on a base-token, is readjusted according to the discrete\nintervention weight (right panel in Figure 2).\n                              G(b,C+1) = 1 if max(G(b,∗)) = 0                            (15)\n\n\n4  EXPERIMENTS\n\nWe benchmark HyperDAS on RAVEL with Llama3-8B (Meta, 2024) as the target model. We both\ntrain a HyperDAS model on all of RAVEL at once and also train a separate HyperDAS model for\neach of the five entity domains in the RAVEL benchmark, i.e., cities, Nobel laureates, occupations,\nphysical objects, and verbs. We experimented with initializing the transformer hypernetwork from\npre-trained parameters, but found no advantage for this task.\n\nCrucial Hyperparameters  We use 8 decoder blocks for the hypernetwork and 32 attention heads\nfor computing the pairwise token position attention. The sparsity loss weight is scheduled to linearly\nincrease from 0 to 1.5, starting at 50% of the total steps. A learning rate between 2 × 104 to 2 × 105\nis chosen depending on the dataset. Discussion of these choices concerning the sparsity loss is in\nSection 4.2. For the feature subspace, we experiment with dimensions from 32 up to 2048 (out of\n4096 dimensions) and use a subspace of dimension 128.\n\nMasking of the Base Prompt  As the hypernetwork has access to the target attribute information\nfrom the instruction and the base attribute information from the base model states, a trivial solution\n\n\n                                       6\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                               City      Nobel Laureate   Occupation    Physical Object      Verb       Average\nMethods\n                           Causal    Iso   Causal     Iso    Causal    Iso   Causal     Iso    Causal    Iso   Disentangle\n\nMDAS                      55.8    77.9    56.0     93.5     50.7    88.1    85.0     97.9     74.3    79.6      76.0\nHyperDAS\n - Asymmetric                70.8    93.9    55.4     95.1     50.4    99.1    92.7     97.2     93.0    98.9      84.7\n - Asymmetric All Domains    58.8    90.5    47.6     92.0     75.7    82.1    92.9     94.5     86.9    95.8      80.7\n - Symmetric                 76.9    90.9    59.2     88.4     47.1    89.1    94.8     97.5     42.3    82.9      76.9\n - Symmetric All Domains     16.8    94.7     2.0      98.8      6.1    97.3    21.6     99.3     13.6    97.8      54.8\n\n(a) Main results of HyperDAS on five domains of RAVEL with Llama3-8B. For each method, we report the\nresults from the best layer between 10 and 15. HyperDAS achieves state-of-the-art attribute disentangling\nperformance across the board. The MDAS baseline intervenes on fixed tokens.\n\n                   MDAS                                 HyperDAS-Asym                             HyperDAS-Sym     1.0                                                   1.0                                                   1.0\n  Score                                                                         Score                                                                         Score\n     0.8                                                   0.8                                                   0.8\n\n     0.6                                                   0.6                                                   0.6\n\n     0.4                                                   0.4                                                   0.4\n              Cause\n     0.2         Iso                                        0.2                                                   0.2\n               Disentangle       Cause/Iso/Disentangle                                                                                                                                                                                                                                                                                                                 Cause/Iso/Disentangle                                                                                                                                                                                                                                                                                                                 Cause/Iso/Disentangle     0.0                                                   0.0                                                   0.0\n       0     5    10    15    20    25    30         0     5    10    15    20    25    30             5     10    15    20    25    30\n                     Intervention Layer                                  Intervention Layer                                  Intervention Layer\n\n(b) The cause/iso/disentangle score of the baseline method and HyperDAS (Both symmetric and asymmetric\nimplementation) for the entity type of “city” across the layers of Llama3-8B. For both HyperDAS and MDAS,\nthe peak Disentangle Score happens around L15.\n\n       Figure 3: RAVEL benchmark results. HyperDAS establishes a new state-of-the-art.\n\n\n\nthe hypernetwork can learn is to condition the intervention location on whether the target attribute\nmatches the base attribute.  In other words, if the two attributes match, attend to a location that\nhas causal effect on the output, otherwise attend to the extra [self] row (see Appendix A.3 for\nan example). This solution, however, does not find the actual concept subspace. To prevent the\nhypernetwork from learning this trivial solution, we apply an attention mask to the base prompt to\nmask out the attribute information. With the masking, the hypernetwork no longer has access to the\nbase attribute information, and hence the localization prediction is only conditioned on the target\nattribute in the natural language instruction.\n\n\nSymmetry   Intuitively, if we have localized a concept, then “get” operations that retrieve the con-\ncept and “set” operations that fix the concept should both target the same features and hidden rep-\nresentations. For this reason, we consider a variant of HyperDAS that enforces symmetry in the\nlocalization of base and counterfactual prompts. We can enforce symmetry between base and coun-\nterfactual inputs during token selection by randomly flipping the order of the concatenation between\nbase and counterfactual hidden representations in Equation 6. We report full results for symmetric\nand asymmetric models.\n\n\nMulti-task DAS (MDAS) Baseline  The current state-of-the-art method on RAVEL is MDAS,\nwhich uses a multi-task learning objective to satisfy multiple high-level causal criteria. MDAS\nrequires supervised training data like HyperDAS, however, MDAS relies on manually selected token\nposition for intervention, which in our case is the final token of the entity, e.g., “nna” in Figure 1.\n\n\nResults  In Table 3a, we show results on RAVEL for layer 15 of Llama3-8B. In Figure 3b, we also\nrun HyperDAS targeting every 2 layers in Llama3-8B starting from the embedding layer. The peak\nperformance of attribute disentanglement for both MDAS and HyperDAS is around layer 15.\n\n\n4.1  LAYER-SPECIFIC INTERVENTION BEHAVIORS OF HYPERDAS\n\nHyperDAS searches for an optimal location to intervene within the target hidden state in a chosen\nlayer. We evaluate MDAS and HyperDAS on 16 layers across the model (Figure 3b) and chose an\nearly layer, middle layer, and deep layer for detailed study: Layer 7, Layer 15, and Layer 29.\n\n\n                                       7\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                              1600\n\n\n                              1400\n\n\n                              1200\n                                  Cases  1000\n              of   800      83.5%                             97.7%            98.7%            89.8%\n                                         Number   600\n\n                               400\n                                                                                                                        Token Types\n                                                                                                             BOS Token\n                               200                                                                                                                Entity Tokens\n                                                                                                                           JSON Syntax\n                                                                                                                                        Others\n                                 0\n                                           Shallow Layer      Shallow Layer       Middle Layer       Middle Layer      Deep Layer       Deep Layer\n                                            Counterfactual         Base          Counterfactual         Base          Counterfactual         Base\n                                                               Intervention Layer / Sentence\n\nFigure 4: The intervention location, in counterfactual and base sentence, picked by HyperDAS when\ntargeting shallow (7), middle (15) and deep (29) decoder layers.\n\n\nAnalysis presented in Figure 4 reveals that HyperDAS consistently targets entity tokens in the coun-\nterfactual input across all layers, suggesting robust detection of attribute information in the entity\ntoken’s residual stream from an early stage. However, the choice of intervention location within the\nbase input shows significant variation. For each example in the “city” entity split, we categorize the\nbase and counterfactual token pair that gets the largest intervention weight, and classify them into\nthe following categories: (1) BOS Token represents the beginning-of-sentence token. (2) Entity\nToken refers to tokens representing entities. (3) JSON Syntax includes special characters and syn-\ntactic tokens typical of JSON formatted text (e.g., opening curly brace “{”). (4) Others comprises\nall tokens irrelevant to the current analysis, with “is” following the entity token being a common\nexample in both shallow (36%) and deep (29%) layer bases.\n\nAt very early layers, HyperDAS displays turbulent behavior, targeting random or even beginning-of-\nsentence tokens in the base sentence. By the middle layers, the model consistently favors the entity\ntoken for intervention, aligning with findings from Huang et al. (2024) and Geva et al. (2023).\nIn contrast, at deeper layers, the hypernetwork learns to intervene on unintuitive positions such as\nsyntax tokens within a JSON-formatted prompt, which were previously unknown to store attributes.\n\n\n4.2  DISCUSSION\n\nHyperDAS establishes a new state-of-the-art performance on RAVEL  Our results show that\nHyperDAS outperforms MDAS, the previous state-of-the-art, across all entity splits at layer 15 in\nLlama3-8B and across all layers of Llama3-8B for the “cities” entity split.\n\nHyperDAS requires more compute than MDAS.  HyperDAS is more powerful than MDAS, but\nalso more computationally expensive. Training our HyperDAS model for one epoch on disentan-\ngling the country attribute in the city domain takes 468,923 TeraFLOPs, while training an MDAS\nmodel for one epoch on the same task takes 193,833. Thus, HyperDAS requires ≈2.4x compute.\n\nOn the other hand, HyperDAS is more memory efficient for tasks like RAVEL. Our target Llama\nmodel requires 16GB of RAM. The HyperDAS model requires 52GB more in total for RAVEL,\nwhereas MDAS requires only 4.1GB more per attribute. The memory usage of HyperDAS does not\ngo up with additional attributes, so when trained on all of RAVEL together (23 attributes), MDAS\n(23 * 4.1 + 16 = 110.3GB) exceeds the memory usage of HyperDAS (52 + 16 = 68GB).\n\nHouseholder vectors analysis provides a window into attribute features.  To analyze the House-\nholder vectors generated by the model, we collected vectors from each test example and categorized\nthem according to their respective attributes. For each attribute category, a subset of 1,000 sam-\nples was randomly selected. We then computed the similarity scores between pairs of attributes by\ncalculating the average cosine similarity across these 1,000 pairs of selected Householder vectors.\n\n\n                                       8\n\n  Published as a conference paper at ICLR 2025\n\n\n\n\n\n                                                                                                                                     Country    Continent   Timezone   Longitude    Latitude    Language\n\n\n                                                                                                                                         0.99        0.87        0.79        0.69        0.75        0.84                                                                               Attributes                                                                                                                                                                                                                                                                                                         Country                                                                       Country                                                                                                                                                                                                                                                                            0.95\n                                                                          Continent\n                                                                 Language\n                                                                              Latitude         0.3                                        0.87        0.97        0.83        0.70        0.76        0.88\n                                                                         Longitude                                                                                                                                                                                                                                                                                                                                                                                              Continent                                                                                                      0.90                                                                  Timezone       0.2\n                                                                                                          0.1\n                                                                                                                                         0.79        0.83        1.00        0.71        0.77        0.80\n                                                                                                         0.0                                                                                                                                                                                                                                                                                                                                                    Timezone\n                                                                                                                                                                                                                                                                            0.85                                                                                                          0.1\n                                                                                                          0.2                                        0.69        0.70        0.71        0.99        0.87        0.71                                                                                                         0.3                                                                                                  Longitude                                                                                                      0.80\n                                                                                                        0.4\n                                                                                                    0.3\n                                                                                                0.2                                                0.75        0.76        0.77        0.87        0.98        0.75                                                                                                                                                                                                                                                                                                                                                    Latitude\n                                                                                                                                                                                                                                                                            0.75                               0.4                                                           0.1\n                                      0.3                                                0.0\n                                             0.2\n                                                    0.1                                                                                                                                         0.84        0.88        0.80        0.71        0.75        0.98                                                          0.0                                                                                  0.2                                                                                      0.1                                                                                                                                    Language                                                                 0.1\n                                                                                                                                                                                                                                                                            0.70                                                                         0.2  0.3\n\n                                                Figure 6: The cosine similarity between the  Figure 5:  The relative position between the\n                                             Householder vectors of different attributes in the Householder vector (after PCA) of attributes for\n                                                        city domain, computed using 100,000 samples  all the correct predictions in city domain. The\n                                           from each attribute. Notably, HyperDAS effec-  clustering indicates that HyperDAS learns dif-\n                                                         tively learns a highly similar subspace for the  ferent subspace for each attribute.\n                                                         attributes ‘Longitude’ and ‘Latitude’.\n                     With Sparsity Loss                      With No Sparsity Loss              With Too Much Sparsity Loss           1.0       [BOS]            att\n          orney's                                                                                                                                                                                                                         0.8Sentence      duty:    represent                                                                                                                                                                                                                         0.6        clients             in\n          court;                                                                                                                                                                                                                         0.4         play\n      therapist's                                                                                                                                                                                                                         0.2\n          duty:Counterfactual    [SELF]                                                                                                                                                                                                                                             0.0\n                                   [BOS]\" \":\" \",\"duties\":\"tocode,test,and    and \";\" \":\" laborer\",\"duties\":\"tohelp             [BOS]\" \":\" \",\"duties\":\"tocode,test,and    and \";\" \":\" laborer\",\"duties\":\"tohelp          [BOS]\" \" \"duties\":\"tocode,test,and  and \" \"laborer\",\"duties\":\"tohelp\n                                                                                 occupation     softwaredeveloper                                                      maintaincomputerprograms applications       occupation        construction                                             children                               occupation     softwaredeveloper                                                      maintaincomputerprograms applications       occupation        construction                                             children                       occupation\":softwaredeveloper\",                                      maintaincomputerprogramsapplications\";occupation\":construction                                   children\n                     Base Sentence Tokens                      Base Sentence Tokens                   Base Sentence Tokens\n\n  Figure 7: Intervention locations for a base/counterfactual sentences pair with Occupation entity-type\n  selected by HyperDAS trained with different amounts of sparsity loss. This comparison illustrates\n  the intervention locations generated by HyperDAS when trained under three different sparsity loss\n  conditions. All three models achieved a Disentangle Score ≈94.0% using weighted interventions.\n With no sparsity loss (middle), HyperDAS tends to intervene from the last subject token in the\n  counterfactual sentence to most tokens in the base sentence, which yields adequate performance\n  under many-to-one constraints but not under strict one-to-one constraints. With too much sparsity\n  loss (right), the pairwise token selection attention within HyperDAS fails, resulting in interventions\n  that blend all hidden states. Although this approach achieves a near-perfect disentangle score with\n weighted intervention, the model’s does not have interpretable intervention patterns and fails entirely\n  during test time when masks are snapped to align base and source tokens one-to-one.\n\n\n\n We analyze the geometry of the learned householder vectors, with the PCA projection shown in\n  Figure 5. We also compute the average pairwise cosine similarity of Householder vectors sampled\n from within the same attribute or cross two different attributes, as shown in Figure 6. Despite an\n  overall high cosine similarity among all Householder vectors associated with the same entity type,\n  the Householder vectors associated with the same attribute form a tighter cluster, with a higher\n  cosine similarity score than pairs of vectors associated with two different attributes. These per-\n  attribute clusters might explain why the learned subspace can disentangle different attributes of the\n same entity, as different attributes are localized into different subspaces of the entity representation.\n\n\n How do we know HyperDAS uncovers actual causal structures faithful to the target model?\n On one hand, we should leverage the power of supervised machine learning to develop increasingly\n  sophisticated interpretability methods. On the other hand, such methods are incentivized to “hack”\n  evaluations without uncovering actual causal structure in the target model. We have taken several\n  steps to maintain fidelity to the underlying model structures when training and evaluating Hyper-\n DAS, by constraining optimization flexibility to prevent inadvertently steering or editing the model\n  with out-of-distribution interventions.\n\n\n                                         9\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                                                    Symmetric Model                                                          Asymmetric Model       Source Position (Text)\n                                                                                                                                                                                                 1st Entity Token\n                       600                                                                        600                                                        2nd3rd EntityEntity TokenToken\n                       400                                                                        400                                                               Others                                                                                                                (Counterfactual)200                                                                        200\n                                   Count  0     Country    Continent  Language  Timezone    Latitude   Longitude       0     Country    Continent  Language  Timezone    Latitude   Longitude\n                       600\n                                          (Base)400                                                                        400\n                                   Count200                                                                        200\n                         0     Country    Continent  Language  Timezone    Latitude   Longitude       0     Country    Continent  Language  Timezone    Latitude   Longitude\n                                                                     Attribute                                                                                Attribute\n\nFigure 8: The count of intervention location picked by HyperDAS at the counterfactual prompt\n(upper) v.s. at the base prompt (bottom) across all the attributes in the city domain on entities with\nthree tokens. The asymmetric variant (right) of HyperDAS favors getting the attribute information\nfrom the last entity token for the majority of the counterfactual prompts (≥95%), and intervene on\nthe second last entity token. The symmetric variant (left) favors last entity token consistently for\nboth base and counterfactual prompt.\n\n\n\nThe weighted interchange interventions used in training hacks the objective without soft con-\nstraints via loss terms.  The loss term Lsparse is crucial for ensuring that HyperDAS learns a one-\nto-one alignment between base tokens and counterfactual tokens (Figure 2). When no sparsity loss\nis applied, the model aligns the final entity token (e.g., “nna” from Figure 1) to many tokens in\nthe base sentence. These solutions fail during evaluations where token alignments are snapped to\nbe one-to-one. Conversely, with excessive sparsity loss, the model constructs a counterfactual hid-\nden representation that is a linear combination of many hidden states, resulting in a high flexibility\noptimization scheme that is closer to model steering or editing. This also fails during one-to-one\nevaluations. See Figure 7 for an example of these pathological settings.\n\n\nOften only one token is aligned between base and counterfactual inputs.  The MDAS baseline\nperforms well on the RAVEL benchmark by one token in the base and one token in the source.\nHowever, our new state-of-the-art HyperDAS model will select multiple tokens 53% of the time.\n\n\nAsymmetric HyperDAS targets different tokens for base and counterfactual examples.  Fig-\nure 8 shows the tokens selected by the symmetric and asymmetric variants of HyperDAS. When\nallowed asymmetric parametrization, networks break symmetry in positional assignments; for a sin-\ngle input prompt, HyperDAS will select different tokens depending on whether that input is the base\nor counterfactual.\n\n\n5  CONCLUSION\n\n\nIn this work, we introduced HyperDAS, a novel hypernetwork-based approach for automating causal\ninterpretability work. HyperDAS achieves state-of-the-art performance on the RAVEL benchmark,\ndemonstrating its effectiveness in localizing and disentangling entity attributes through causal inter-\nventions. Our method’s ability to dynamically select hidden representations and learn linear features\nthat mediate target concepts represents a significant advancement in interpretability techniques for\nlanguage models. We are optimistic that HyperDAS will open new avenues for understanding and\ninterpreting the internal workings of complex language models.\n\n\nLimitations  HyperDAS will only be successful if the target concept is mediated by linear features.\nHowever, there is emerging evidence that non-linear mediators are a possibility (Csord´as et al.,\n2024; Engels et al., 2024). As discussed extensively in the main text, applying supervised machine\nlearning to interpretability has the potential to lead to false positive results. While we have taken\nsteps to maintain fidelity to underlying model structures, future work should continue to explore the\ndelicate balance between uncovering latent causal relationships and the risk of model steering.\n\n\n                                       10\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nACKNOWLEDGEMENTS\n\nThis research was in part supported by a grant from Open Philanthropy.\n\nREFERENCES\n\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,\n  Jan Leike, Jeff Wu, and William Saunders.  Language models can explain neurons in lan-\n  guage models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index.\n  html.(Date accessed: 14.05. 2023), 2, 2023.\n\nDan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey.  Identifying functionally\n  important features with end-to-end sparse dictionary learning, 2024. URL https://arxiv.org/\n  abs/2405.12241.\n\nArthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a\n  Garriga-Alonso.  Towards automated circuit discovery for mechanistic interpretability.  In Al-\n   ice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine\n   (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neu-\n   ral Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, Decem-\n  ber 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/\n  34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html.\n\nR´obert Csord´as, Christopher Potts, Christopher D. Manning, and Atticus Geiger. Recurrent neural\n  networks learn to store and generate sequences using non-linear representations, 2024. URL\n  https://arxiv.org/abs/2408.10920.\n\nJoshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, and Max Tegmark. Not all language model\n   features are linear, 2024. URL https://arxiv.org/abs/2405.14860.\n\nJavier Ferrando and Elena Voita.  Information flow routes: Automatically interpreting language\n  models at scale, 2024. URL https://arxiv.org/abs/2403.00824.\n\nMatthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan\n  Belinkov.  Causal analysis of syntactic agreement mechanisms in neural language models.  In\n  Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th An-\n  nual Meeting of the Association for Computational Linguistics and the 11th International Joint\n  Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1828–1843, Online,\n  August 2021. Association for Computational Linguistics.  doi: 10.18653/v1/2021.acl-long.144.\n  URL https://aclanthology.org/2021.acl-long.144.\n\nAtticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models\n   partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov,\n  Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of\n  the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp.\n  163–173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n  2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.\n\nAtticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang,\n  Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal\n   abstraction: A theoretical foundation for mechanistic interpretability, 2024a. URL https://\n  arxiv.org/abs/2301.04709.\n\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman.  Find-\n  ing alignments between interpretable causal variables and distributed neural representations. In\n  Causal Learning and Reasoning, pp. 160–187. PMLR, 2024b.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.  Dissecting recall of factual\n  associations in auto-regressive language models.  In Houda Bouamor, Juan Pino, and Kalika\n  Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n  Processing, pp. 12216–12235, Singapore, December 2023. Association for Computational Lin-\n   guistics.  doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/2023.\n  emnlp-main.751.\n\n\n                                       11\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva.  Patchscopes:\n A unifying framework for inspecting hidden representations of language models. In Forty-first\n  International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.\n  OpenReview.net, 2024. URL https://openreview.net/forum?id=5uwBzcn885.\n\nEvan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob\n  Andreas. Natural language descriptions of deep visual features. In International Conference on\n  Learning Representations, 2022. URL https://arxiv.org/abs/2201.11114.\n\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas.   Inspecting and editing knowledge rep-\n   resentations in language models.  In First Conference on Language Modeling, 2024. URL\n  https://openreview.net/forum?id=ADtL6fgNRv.\n\nAlston S Householder. Unitary triangularization of a nonsymmetric matrix. Journal of the ACM\n  (JACM), 5(4):339–342, 1958.\n\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigor-\n  ously assessing natural language explanations of neurons.  In Yonatan Belinkov, Sophie Hao,\n  Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi (eds.), Proceedings of the\n  6th BlackboxNLP Workshop:  Analyzing and Interpreting Neural Networks for NLP, Black-\n  boxNLP@EMNLP 2023, Singapore, December 7, 2023, pp. 317–331. Association for Com-\n   putational Linguistics, 2023.  doi: 10.18653/V1/2023.BLACKBOXNLP-1.24. URL https:\n  //doi.org/10.18653/v1/2023.blackboxnlp-1.24.\n\nJing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. RAVEL: Evaluating\n   interpretability methods on disentangling language model representations. In Lun-Wei Ku, Andre\n  Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association\n   for Computational Linguistics (Volume 1: Long Papers), pp. 8669–8687, Bangkok, Thailand,\n  August 2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.acl-long.470.\n  URL https://aclanthology.org/2024.acl-long.470.\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. Sparse\n  autoencoders find highly interpretable features in language models. In The Twelfth International\n  Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=\n  F76bwRSLeK.\n\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\n  Sparse feature circuits: Discovering and editing interpretable causal graphs in language mod-\n   els. Computing Research Repository, arXiv:2403.19647, 2024. URL https://arxiv.org/abs/\n  2403.19647.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual as-\n   sociations in GPT. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and\n  A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on\n  Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-\n  ber 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2022/\n  hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html.\n\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing\n  memory in a transformer. In The Eleventh International Conference on Learning Representations,\n  2023. URL https://openreview.net/forum?id=MkbcAHIYgyS.\n\nAI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024.\n\nJesse Mu and Jacob Andreas. Compositional explanations of neurons, 2021. URL https://arxiv.\n  org/abs/2006.14032.\n\nAaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can\n  Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and\n  Yonatan Belinkov. The quest for the right mediator: A history, survey, and theoretical grounding\n  of causal interpretability, 2024. URL https://arxiv.org/abs/2408.01416.\n\n\n                                       12\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nAchyuta Rajaram, Neil Chowdhury, Antonio Torralba, Jacob Andreas, and Sarah Schwettmann.\n  Automatic discovery of visual circuits. CoRR, abs/2404.14349, 2024.  doi: 10.48550/ARXIV.\n  2404.14349. URL https://doi.org/10.48550/arXiv.2404.14349.\n\nNaomi Saphra and Sarah Wiegreffe. Mechanistic?, 2024. URL https://arxiv.org/abs/2410.\n  09087.\n\nSarah Schwettmann, Tamar  Rott Shaham,  Joanna Materzynska,  Neil Chowdhury,  Shuang\n   Li,  Jacob  Andreas,  David  Bau,  and  Antonio  Torralba.   FIND: A  function  descrip-\n   tion benchmark  for  evaluating  interpretability  methods.     In  Alice Oh,  Tristan Nau-\n  mann,  Amir  Globerson,  Kate  Saenko,  Moritz  Hardt,  and  Sergey  Levine  (eds.),  Ad-\n  vances in Neural Information Processing Systems 36:  Annual Conference on Neural In-\n  formation Processing Systems 2023, NeurIPS 2023, New Orleans,  LA, USA, December\n  10  - 16,  2023,  2023.  URL http://papers.nips.cc/paper files/paper/2023/hash/\n  ef0164c1112f56246224af540857348f-Abstract-Datasets and Benchmarks.html.\n\nTamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob\n  Andreas, and Antonio Torralba. A multimodal automated interpretability agent.  In Forty-first\n  International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.\n  OpenReview.net, 2024. URL https://openreview.net/forum?id=mDw42ZanmE.\n\nAlessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. A mechanistic interpretation of arith-\n  metic reasoning in language models using causal mediation analysis. In Houda Bouamor, Juan\n  Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natu-\n   ral Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 7035–7052. Asso-\n   ciation for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.435. URL\n  https://doi.org/10.18653/v1/2023.emnlp-main.435.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,\n  and Stuart Shieber.   Investigating gender bias in language models using causal mediation\n   analysis.   In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-\n  vances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran Asso-\n   ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/\n  92650b2e92217715fe312e6fa7b90d82-Paper.pdf.\n\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretabil-\n   ity at scale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing\n  Systems, 36, 2024.\n\n\n\n\n\n                                       13\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\nA  APPENDIX\n\n\nA.1  HYPERDAS OVER ALL DOMAINS\n\nOur results at Table 3a show that we can train HyperDAS to achieve state of the art performance\non the RAVEL benchmark by training a separate model for each entity type split, which is the set\nup used to train the previous state of the art MDAS. To test the scalibility and generalizability of\nHyperDAS, we train a single model across all the entity type splits and evaluate its performance.\n\n\nExperiment Set-up  We aggregate the training split of the dataset from all 5 domains and train\nHyperDAS for 5 epochs. We adjuste the learning rate from 2 × 10−5 to 5 × 10−4 and schedule the\nsparsity weight λ ranging from 0.75 to 1.5 starting after 50% of the total steps. This set-up allows\nthe model to first find a stable solution across all domains with soft intervention before forcing it to\nconverge to a single token selection.\n\n\nResult  We report the performance of HyperDAS trained over all entity type split in Table 3a.\nThe model performs better than MDAS but slightly worse than HyperDAS trained on individual\nentity type split by 4.0%. Specifically, HyperDAS-All-Domain performs worse over city and nobel\nlaureate split, better over occupation split, and on-par over physical object and verb split.\n\n\nA.2  DATASET SPECIFICATION\n\n\n          Domain/Attribute  # of Cause Example  # of IsolateExample  # of Entity\n\n         City                 34899/7016         49500/9930      3552/3374\n         Country               7925/1544           8250/1655       3528/2411\n         Language             6207/1252           8250/1655       3471/2221\n          Continent             8254/1658           8250/1655       3543/2567\n         Timezone             5371/1144           8250/1655       3414/1900\n          Latitude               3813/743           8250/1655       3107/1519\n         Longitude             3329/675           8250/1655       2989/1357\n\n         Nobel Laureate       39771/6754         44628/7600       928/928\n         Country of Birth       7218/1356           8908/1520        928/909\n        Award Year           11037/1904          8930/1520        928/926\n         Gender                 854/96            8930/1520        592/149\n          Field                 9518/1558           8930/1520        928/922\n          Birth Year            11144/1840          8930/1520        928/927\n\n         Occupation          54444/1582          29052/864        799/785\n        Work Location         24216/724           9684/288        799/708\n         Duty                 12090/371           9684/288        785/522\n          Industry              18138/487           9684/288        799/600\n\n          Physical Object       49114/4659         35285/3636       563/563\n         Color                14707/1518           8825/909        563/563\n         Category             13540/1273           8820/909        563/562\n          Texture              14666/1265           8821/909        563/561\n          Size                  6201/603            8819/909        563/528\n\n        Verb                 70003/3806          14396/782        986/984\n          Past Tense            34043/1848           7188/391        986/975\n          Singular              35960/1958           7208/391        986/978\n\nTable 1: The details of the dataset used for the experiment, in the format of train/test splits. For\nevery model in each setting. Methods are trained on the full dataset of that setting with 5 epochs.\nThe prompts used by the train/test splits are completely disjoint.\n\n\n\n                                       14\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nA.3  DATASET PREPROCESSING\n\nHyperDAS uses an attention mechanism to gather information from the hidden states of the target\nmodel M when running the base and counterfactual sentences.  This makes HyperDAS overtly\npowerful in some situations. Consider the following input:\n\n\n\n     Base               ¯x = Vienna, known for its Imperial palaces, is a city in the country of\n    Counterfactual    ˆx = I love Paris                                                       (16)\n     Instruction    x = Localize the latitude of the city                      ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩\n\n\nIf the model works as intended, it will intervene on the ‘Latitude’ subspace, which will leave the\n‘Country’ features intact and therefore the target model will predict Austria.\nHowever, since the model can access the hidden states M(¯x), it knows that the queried attribute in\nthe sentence is ‘Country’, which is different than the targeted attribute ‘Latitude’. Through train-\ning, HyperDAS learns a shortcut to a trivial solution—performing no intervention when the target\nattribute is different from the one mentioned in the sentence. With this shortcut, the Isolate objec-\ntive no longer works and the HyperDAS fails to learn disentangled feature subspaces for different\nattributes.\n\nFigure 9 shows how the HyperDAS may learn a trivial solution to the RAVEL benchmark if the\nrelevant information (base prompt attribute) can be accessed by the model.\n\n\n\n\n                        Instruction: Goroka ; Zurich - Country   Label: Oceania  Pred: Oceania\n                    [BOS] 000000000000000000000000000\n                            city 000000000000000000000000000\n                          to 000000000000000000000000000           0.8\n                   country 000000000000000000000000000\n                                         : 000000000000000000000000000\n                 Buenos 000000000000000000000000000           0.6\n                        Aires 000000000000000000000000000\n                                  is 000000000000000000000000000\n                             in 000000000000000000000000000                                                                             0.4\n                 Argentina 000000000000000000000000000\n                                            . 000000000000000000000000000\n                     Zurich 000000000000000000000000000                                                                             0.2\n                                  is 000000000000000000000000000\n                             in 000000000000000000000000000\n                     [SELF] 111111111111111111111111111\n                                          [BOS][{\"city\":\"NewDelhi\",\"continent\":\"Asia\"},{\"city\":\"Goroka\",\"continent\":\"Oceania\n\nFigure 9: The trivial solution learnt by the HyperDAS on isolate examples when no mask is applied\non the attribute token in the prompt. HyperDAS learns to not intervene if it sees the base prompt\nattribute to be different than the attribute in the instruction.\n\n\nTherefore, for each pair of prompts ¯x, ˆx at training, we apply an intervention mask to all the tokens\nstarting from the attribute mention. The hidden states from token with intervention mask is not\nvisible to HyperDAS and therefore cannot be selected for intervention. The example becomes:\n\n\n                                       15\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n     Base               ¯x = Vienna, known for its Imperial palaces, is a city in the country of\n    Counterfactual    ˆx = I love Paris                                                       (17)\n     Instruction    x = Localize the latitude of the city                      ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩\n\nwhere the hidden states of the red text is masked from the HyperDAS .\n\n\nA.4  LOADING HYPERDAS WITH PRE-TRAINED PARAMETERS\n\nWe have also explored initializing the HyperDAS from a pretrained LM instead of initializing it from\nscratch. With Llama3-8b (Meta, 2024) as the target LM, we initialize the modules of HyperDAS ,\nbesides the multi-head cross-attention heads and pairwise token position scores attention heads, as\nthe copy of the parameters from the target model. We then evaluate the performance of this variation\nof the model on the city dataset of RAVEL (Huang et al., 2024).\n\n\n                                                                                                                                                            Initialization    6                                                                                                                                                                              Initialize from pretrained\n    5                                                                                                                                                                       Initialize from scratch\n loss4\n    3   Training2\n    1\n    0\n           0               2000              4000              6000              8000             10000             12000\n                                                                      Step\n\nFigure 10: The loss curve of HyperDAS initialized from scratch or from pretrained LM while train-\ning on the city dataset of RAVEL.\n\nIn Figure 10, we observe that there is no significant difference between the model initialized from\nscratch and the model initialized from Llama3-8b parameters. However, it remains unknown how\nwould this difference change as the training of HyperDAS scales.\n\n\nA.5  SPARSE AUTOENCODERS\n\nWe experiment with different feature subspace dimension, as shown in Figure 11. We add an trained\nsparse autoencoder as another baseline. Following the exact same setting in (Huang et al., 2024),\nwe train sparse autoencoder that projects the target hidden states into a higher-dimensional sparse\nfeature space and then reconstruct the original hidden states.\n\n\nA.6  ABLATION RESULTS\n\nSee ablations in Table 12.\n\n\nA.7  INTERVENTION PATTERNS\n\nHere we include a few demonstrations of the intervention pattern that HyperDAS generates on\nRAVEL, as shown in Figure 13.\n\n\n\n\n\n                                       16\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                    50.0%\n            1.0                                                                  1.6%          3.1%          GOAL                                                       25.0%\n                    0.8%                          12.5%\n\n\n            0.9\n\n\n\n            0.8\n\n\n                                   0.7%                                                          12.5%            0.7                                                                                               3.1%\n           Score                                                                                    1.6% 25.0%\n                                         1.6%                                              0.8%\n                Isolate 0.6                                  3.0%                                                  50.0%\n\n\n\n            0.5\n\n\n\n            0.4\n\n\n                                                                                   12.8%\n                   HyperDAS                                                       50.5%\n            0.3      MDAS\n                   SAE                                                             26.1%\n                   0.0                 0.2                 0.4                 0.6                 0.8                 1.0\n                                                         Causal Score\n\nFigure 11: Cause (x-axis) and Iso (y-axis) scores trade-off for HyperDAS, MDAS, and SAE when\nusing different feature size shown as the ratio %. GOAL (1, 1) indicates the score with which the\nmethod is able to disentangle the feature subspace perfectly.\n\n\n\n\n\n                Ablation           Causal   Iso   Disentangle\n\n              HyperDAS            70.8    93.9      82.4\n               -No Cross Attention    68.2    83.9      76.1\n               -No DAS               0.8     97.4      49.1\n               -No Hypernetwork     15.1    46.9      31.0\n\nFigure 12: Ablation results for HyperDAS. No DAS has no rotation matrix and intervenes on entire\nhidden representations. No Hypernetwork replaces concept encoding via transformer with a vector\nlookup. No Cross Attention removes attention head submodules connecting the hypernetwork and\ntarget model.\n\n\n\n\n\n                                       17\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n                             a) Soft Intervention                                        b) One-to-Many Intervention                                 c) One-to-One Intervention                1.0\n        [BOS][{\"                                                                            [BOS][{\"                                                                            [BOS][{\"\n                city\":                                                                                                                                                  city\":                                                                                                                                                                                                                                                                                    city\":\n                                                                                                                                                                                                                         0.8         Los\"                                                                               Los\"                                                                                                                                                      Los\"  Tokens Angeles\",             \"                                                                                Angeles\",                                                                                                   \"                                                                                Angeles\",                                                                                                                                                                                        \"\n                                                                                                                                                                                                                                      long\":             \"                                                                                                       long\":\n   Sentence      long\":118.2\"},{\"                                                                                                                   118.2\"},{\"\"                                                                                                                                                                                                                          118.2\"},{\"\"                                                         0.60.4                city\":                                                                                                                            city\":                                                                                                                            city\":\n             ig\"K                                                                                    ig\"K                                                                                    ig\"K\n                                                                                                                                                                                                                         0.2     Counterfactual    oma\",\"                                                                                    oma\",\"                                                                                    oma\",\"\n            long\":\"                                                                                                                         long\":\"                                                                                                                                                                                                                                      long\":\"       [SELF]                                                                                 [SELF]                                                                                                                                                           [SELF]                                                                                                                                                                                                                         0.0\n                     [BOS] city to : New Delhi is in India . Bag h lan is in                                                                                                                                        [BOS] city to : New Delhi is in India . Bag h lan is in                                                                                                                                                                                                                                                           [BOS]cityto :NewDelhiisinIndia.Baghlanisin                                                     country                                                                                                                 Tanzania                                                 country                                                                                                                 Tanzania                                            country                                                                                         Tanzania\n                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens\n\n                                                          (a)\n\n                             a)                                Soft                                   Intervention                                                                                       b)                                                                              One-to-Many                                                                                                       Intervention                                                                                                                                                              c)                                                                                                                                 One-to-One                                                                                                                                                                 Intervention                                                                                                                                                                                                                         1.0     [BOS]                                                                          [BOS]                                                                                                                                              [BOS]\n              [                                                                                                                   [                                                                                                                                                                                                                       [\n         {\"                                                                                {\"                                                                                                                                                        {\"\n         city                                                                                                city                                                                                                                                                                                      city\n             \":                                                                                                               \":                                                                                                                                                                                                                  \":\n            \"                                                                                                 \"                                                                                                                                                                                      \"                                                                                                                                                                                                                         0.8        Sy                                                                        Sy                                                                                                                                        Sy  Tokens      dney                                                                     dney                                                                                                                                    dney\n             \",                                                                                                                  \",                                                                                                                                                                                                                       \",\n            \"                                                                                                 \"                                                                                                                                                                                      \"\n       lang                                                                                lang                                                                                                                                                        lang                                                                                                                                                                                                                         0.6             \":                                                                                                               \":                                                                                                                                                                                                                  \":\n            \"                                                                                                 \"                                                                                                                                                                                      \"\n     English                                                                                 English                                                                                                                                                             English   Sentence          \"},                                                                                             \"},                                                                                                                                                                                \"},\n         {\"                                                                                {\"                                                                                                                                                        {\"\n         city                                                                                                city                                                                                                                                                                                      city                                                                                                                                                                                                                         0.4             \":                                                                                                               \":                                                                                                                                                                                                                  \":\n            \"                                                                                                 \"                                                                                                                                                                                      \"\n         F                                                                              F                                                                                                                                                  F\n        uk                                                                        uk                                                                                                                                         uk\n           ui                                                                                                 ui                                                                                                                                                                                       ui\n             \",                                                                                                                  \",                                                                                                                                                                                                                       \",                                                                                                                                                                                                                         0.2     Counterfactual            \"                                                                                                 \"                                                                                                                                                                                      \"\n       lang                                                                                lang                                                                                                                                                        lang\n             \":                                                                                                               \":                                                                                                                                                                                                                  \":\n            \"                                                                                                 \"                                                                                                                                                                                      \"\n     [SELF]                                                                               [SELF]                                                                                                                                                         [SELF]                                                                                                                                                                                                                         0.0\n                                          City                                                                                                                                  York                                                                                                                                                                New                                      . Photo taken in Mou ila , Japan                                                           .PhototakeninMouila,Japan                  [BOS] Photo taken in New York                                                                                                                                      [BOS] Photo taken in New                                                                                                                                                                                                                                                         [BOS]Phototakenin                                                                                  States . Photo taken in Mou ila , Japan            , United                                                                                                                                      City , United States                                                                                                                                                                                                                         YorkCity,UnitedStates\n                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens\n\n                                                       (b)\n\n                              a) Soft Intervention                                                                                        b) One-to-Many Intervention                                                                                                                                                               c) One-to-One Intervention               1.0       [BOS]                                                                            [BOS]                                                                                                                                                [BOS]\n          Sy                                                              Sy                                                              Sy\n                                                                                                                                                                                                                          0.8  Tokens   dney:                                                           dney:                                                           dney:\n     Australia                                                                       Australia                                                                       Australia\n             /S                                                                               /S                                                                               /S                                                        0.6   Sentence       y                                                                 y                                                                 y\n       dney                                                           dney                                                           dney                                                        0.4\n                     .                                                                                                                          .                                                                                                                          .\n           Br                                                                     Br                                                                     Br\n        aga                                                            aga                                                            aga                                                        0.2     Counterfactual             :                                                                                                                   :                                                                                                                   :\n       [SELF]                                                                                 [SELF]                                                                                                                                                           [SELF]\n                                                                                                                                                                                                                          0.0\n                      city to :     is in . So chi is in                                                                                                                   city to :     is in . So chi is in                                                                                                                                                                                                              cityto :    isin .Sochiisin\n                      [BOS]                   country       Kuala Lumpur                     Malaysia                                                  Portugal                   [BOS]                   country       Kuala Lumpur                     Malaysia                                                  Portugal                   [BOS]             country     KualaLumpur               Malaysia                                       Portugal\n                           Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens\n\n                                                          (c)\n\n                            a)                               Soft                                   Intervention                                                                                      b)                                                                             One-to-Many                                                                                                       Intervention                                                                                                                                                              c)                                                                                                                                 One-to-One                                                                                                                                                                Intervention                                                                                                                                                                                                                         1.0     [BOS]                                                                         [BOS]                                                                                                                                             [BOS]             [                                                                                                                  [                                                                                                                                                                                                                      [        {\"                                                                                {\"                                                                                                                                                       {\"        city                                                                                               city                                                                                                                                                                                     city            \":                                                                                                              \":                                                                                                                                                                                                                 \":\n                                                                                                                                                                                                                         0.8       C\"                                                               C\"                                                                                                                       C\"  Tokens     Townape                                                                   Townape                                                                                                                                  Townape            \",                                                                                                                 \",                                                                                                                                                                                                                      \",           \"                                                                                                \"                                                                                                                                                                                      \"          lat                                                                                                      lat                                                                                                                                                                                                 lat                                                                                                                                                                                                                         0.6            \":                                                                                                              \":                                                                                                                                                                                                                 \":           \"                                                                                                \"                                                                                                                                                                                      \"   Sentence       34                                                                    34                                                                                                                                  34         \"},                                                                                            \"},                                                                                                                                                                               \"},        {\"                                                                                {\"                                                                                                                                                       {\"        city                                                                                               city                                                                                                                                                                                     city                                                                                                                                                                                                                         0.4            \":                                                                                                              \":                                                                                                                                                                                                                 \":           \"                                                                                                \"                                                                                                                                                                                      \"      Q                                                        Q                                                                                                          Q        ing                                                                                    ing                                                                                                                                                                ing       an                                                                      an                                                                                                                                     an                                                                                                                                                                                                                         0.2            \",                                                                                                                 \",                                                                                                                                                                                                                      \",     Counterfactual           \"                                                                                                \"                                                                                                                                                                                      \"          lat                                                                                                      lat                                                                                                                                                                                                 lat            \":                                                                                                              \":                                                                                                                                                                                                                 \":           \"                                                                                                \"                                                                                                                                                                                      \"    [SELF]                                                                              [SELF]                                                                                                                                                        [SELF]                                                                                                                                                                                                                         0.0            a                     is\n                                                                             in  the     of                                                                                                                       in the   of                                                        city in  the     of                                                                   is a   city                                                                                                              is a city                                                                                      Tim im                                                                                                                                                         Tim im                Tim im  oun                                                                                                oun                                                                                                                                                                  oun\n                   [BOS]                                                                                                    country            China                 [BOS]                                                                                                    country            China                [BOS]                                                                               country         China\n                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens\n\n                                                       (d)\n\n                        Figure 13: Four types of intervention patterns.\n\n\n\n\n\n                                       18\n\n## Claim to Classify\n\"at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes\"\nLocation in paper: results\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "HyperDAS (Hypernetwork-based Interchange Intervention)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is an interventional search (Rung 2) based on interchange interventions (patching). The claim uses the term 'store', which according to the polysemous terms decision tree in the codebook, defaults to a Rung 3 mechanistic claim unless the context explicitly limits it to 'linearly decodable'. Here, the context is about identifying the specific internal location responsible for the information, which is a mechanistic narrative. This creates a +1 gap (R2 method supporting an R3 claim)."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 24757,
      "completion_tokens": 172,
      "total_tokens": 24929
    },
    "finish_reason": "stop"
  }
}