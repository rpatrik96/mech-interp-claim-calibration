{
  "timestamp": "2026-02-19T09:34:23.767839+00:00",
  "paper_id": "2512.06681",
  "claim_id": "2512.06681-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2512.06681\nTitle: Mechanistic Interpretability of GPT-2 Sentiment Analysis\nFull text:\n             Mechanistic Interpretability of GPT-2: Lexical and\n                  Contextual Layers in Sentiment Analysis\n\n\n\n\n                                                Amartya Hatua ∗\n                                               AI Center of Excellence\n                                                                  Fidelity Investments\n                                                          Boston, MA 02210\n2025                                        amartyahatua@gmail.com\n                                                 Abstract\n\n                    We present a mechanistic interpretability study of GPT-2 that causally examinesDec\n                       how sentiment information is processed across its transformer layers.  Using\n7                               systematic activation patching across all 12 layers, we test the hypothesized two-\n                                stage sentiment architecture comprising early lexical detection and mid-layer\n                                contextual integration. Our experiments confirm that early layers (0-3) act as\n                                   lexical sentiment detectors, encoding stable, position specific polarity signals that\n                                are largely independent of context. However, all three contextual integration\n                                hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed[cs.CL]                         Processing are falsified. Instead of mid-layer specialization, we find that contextual\n                           phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in\n                                     late layers (8-11) through a unified, non-modular mechanism. These experimental\n                                 findings provide causal evidence that GPT-2’s sentiment computation differs from\n                                the predicted hierarchical pattern, highlighting the need for further empirical\n                                 characterization of contextual integration in large language models.\n\n\n                1  Introduction\n\n                     Large language models demonstrate impressive capabilities across a wide range of diverse linguistic\n                         tasks.  Despite this progress, existing interpretability research primarily relies on correlational\n                      evidence from probing or attention analysis. Consequently, the internal causal structure through\n                    which these models encode and transform linguistic information has not been widely explored.\n\n                       Early research focused on identifying how distinct layers within transformers contribute to different\n                        stages of linguistic processing. Tenney et al. [2019] found that BERT processes language in stagesarXiv:2512.06681v1\n                        early layers handle syntactic information, while later layers understand semantic relationships. This\n                       suggests that transformers operate similarly to a pipeline, progressing from simple features to a\n                   complex understanding.  It was the first clear evidence that these models have organized, step\n                   by step processing. Building upon this foundation, Jawahar et al. [2019], formalized a three-tier\n                        hierarchical framework: early layers handle basic word features, middle layers deal with grammar\n                    and sentence structure, and late layers understand meaning and how distant words relate to each\n                           other. Simultaneously, Clark et al. [2019] revealed that individual heads develop specialized functions\n                         for specific linguistic phenomena, following the same early to late progression. In Rogers et al.\n                       [2020], a comprehensive synthesis was provided that established a general consensus on middle\n                        layer specialization for syntactic structure, while highlighting that semantic processing remains\n                   more distributed and less well understood. All these studies showed that transformers seem to\n                       process language in organized, step-by-step ways. However, their methodologies were predominantly\n\n                       ∗Code and data available at: https://github.com/amartyahatua/MI_Sentiment_Analysis\n\n\n                        39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Efficient Reasoning.\n\ncorrelational, relying on probing classifiers and attention analysis to identify what information exists\nin representations rather than what models actually use during inference.\n\nNewer research has highlighted this gap. Scientists now realize that finding patterns doesn’t prove the\nmodel actually uses them. As Belinkov et al. [2023] puts it, there’s a gap between what we can detect\nin the model and what the model actually relies on; just because we can find information doesn’t\nmean the model uses it. When Elazar and Goldberg [2018] tried removing features they thought were\nimportant, the models often worked just fine without them. This suggested they were finding fake\npatterns, not real ones. Makelov et al. [2024] found “interpretability illusions” interventions that\nseemed to reveal how models work but were actually triggering backup systems that had nothing to\ndo with normal processing.\n\nRecent years have brought major improvements in solving the correlation-causation problem. The\nfield of mechanistic interpretability Rai et al. [2024] has developed new techniques like activation\npatching Heimersheim and Nanda [2024] that let researchers directly test cause and effect, while\nautomated tools have made the analysis process more systematic. Companies like Anthropic and\nOpenAI have successfully applied these mechanistic methods to real models, finding millions of\ninterpretable features in their large language models. Among these challenging phenomena, sentiment\nanalysis presents a particularly instructive case. Sentiment analysis represents a particularly complex\nchallenge for mechanistic interpretability. Unlike syntactic phenomena that localize to specific layers\nor attention heads, sentiment processing requires sophisticated integration of lexical, syntactic, and\ncontextual information. The exact words can convey opposite meanings depending on context—“great”\nin “great movie” versus “oh great, it’s raining”—requiring dynamic contextual reasoning that current\nmethods struggle to explain mechanistically.\n\nIn sentiment analysis, it remains essential to determine whether transformer layers interact in a truly\ncausal manner. Do early layers construct representations that later layers build upon, or do they\noperate in parallel? Is contextual information processed locally or distributed across the network?\nCorrelational analyses cannot resolve these questions. In this work, we employ activation patching and\ncausal interventions to empirically examine sentiment processing in GPT-2, providing mechanistic\nevidence for stage wise organization and establishing a foundation for deeper causal interpretability\nresearch.\n\n\n\n2  Related Research\n\n\n2.1  Transformer Layer Analysis and Interpretability\n\n\nThe study of how transformers process language has advanced through several methodological\nstages. Early probing work Tenney et al. [2019] showed that BERT encodes linguistic features\nhierarchically, with parts of speech in early layers, syntax in middle layers, and semantics in later\nlayers—forming the basis of the layer specialization hypothesis. Jawahar et al. [2019] expanded\nthis view, showing a progression from surface features to long-distance dependencies, while Clark\net al. [2019] demonstrated that attention heads develop specialized functions aligned with linguistic\nphenomena. Rogers et al. [2020] synthesized these findings in their BERTology survey, framing\ntransformer processing as a three-stage pipeline of feature extraction, syntax, and semantics—a\nframework that remains influential but largely correlational.\n\n\n\n2.2  Methodological Evolution in Mechanistic Interpretability\n\n\nLimitations of correlational methods led to causal approaches in mechanistic interpretability. Elhage\net al. [2021] emphasized the need to distinguish between what information exists in representations\nand what computations models actually perform. Activation patching became central here, with\nWang et al. [2022] showing distributed processing for indirect object identification and Meng et al.\n[2022] revealing that factual knowledge depends on interactions across layers rather than localized\nstorage. Automated methods such as ACDC (Conmy et al. [2023]) extended this work by identifying\ncandidate circuits, though many fail causal tests, highlighting challenges of functional faithfulness.\n\n\n                                       2\n\n2.3  Sentiment Processing in Language Models\n\nWhile sentiment analysis is a well-studied application of transformers, the mechanisms behind\nsentiment processing remain poorly understood. Probing studies (e.g., Liu et al. [2019]) showed\nthat different layers encode aspects of sentiment, suggesting a multi-stage progression from word\nrecognition to contextual modulation and final decision-making. Yet these findings are largely\ncorrelational, and sentiment’s contextual nature—shaped by negation, intensification, and pragmatic\ncues like sarcasm—complicates interpretation. Recent studies have examined such phenomena\nindividually, but little work has addressed how they interact within a unified computational framework.\n\n\n2.4  Limitations of Current Approaches\n\nCurrent interpretability methods struggle with complex semantic phenomena like sentiment process-\ning. Most mechanistic successes have been limited to simple tasks such as arithmetic or syntax,\nleaving contextual reasoning and pragmatic inference opaque. Probing approaches, though influential,\noften fail to capture true model behavior—high accuracy can reflect memorization rather than genuine\ncomputation ( Hewitt and Liang [2019], Belinkov et al. [2023]). Theories like layer specialization\nremain largely correlational, lacking causal validation, which is especially problematic for sentiment,\nwhere meaning depends on intricate contextual interactions.\n\n\n2.5  The Need for Systematic Causal Validation\n\nThese limitations point to the need for systematic causal validation of transformer processing theories.\nNew methods—such as causal scrubbing, refined activation patching, and sparse autoencoders—offer\npowerful tools but have rarely been applied to foundational questions. In sentiment analysis, we still\nlack a causal account of how the processing stages interact. This work provides the first systematic\nvalidation of the three-stage sentiment processing hypothesis, moving beyond correlation to genuine\nmechanistic insight.\n\n\n3  Methodology\n\n3.1  Experimental Design\n\nWe have tested the two-stage sentiment processing hypothesis in GPT-2 using activation patching\nacross the 12 layers. The analysis focuses on lexical detection and contextual integration, with\ncontrolled interventions isolating the causal role of each stage in sentiment behavior. We use GPT-2\n(117M) through TransformerLens Nanda and Meyer [2023], which allows standardized access to\nactivations and precise interventions. The model is run in inference mode without finetuning, with\nconsistent tokenization and identical architecture across all conditions.\n\n\n3.2  Activation Patching Protocol\n\nActivation patching is used in this study as a causal intervention technique to identify which trans-\nformer layers directly contribute to sentiment processing. By selectively substituting internal acti-\nvations between contrasting input sentences, we isolate the specific layers responsible for lexical\ndetection and contextual integration. For each test pair, we conducted activation patching, replacing\nactivations from the source sentence (e.g., positive sentiment) with those from the target sentence\n(e.g., negative sentiment) at each layer independently. The resulting change in sentiment classification\nprobability was measured to quantify the causal contribution of each layer, where larger shifts indicate\ngreater causal importance for sentiment processing.\n\n\n3.3  Lexical Detection\n\nWe perform a linear probe on GPT-2’s final layer representations to classify sentiment polarity,\nachieving 95% accuracy on a held-out validation set. This probe serves as our behavioral measure for\nsentiment classification performance, allowing us to quantify how interventions affect the model’s\nsentiment processing.\n\n\n                                       3\n\n3.4  Hypotheses\n\nWe test four specific hypotheses about lexical processing:\n\n       1. Lexical Sensitivity: Sentiment word substitutions produce measurable activation differences.\n       2. Early Layer Dominance: Layers 0-3 show the strongest effects for lexical sentiment.\n       3. Position Specificity: The effects concentrate on the positions of the sentiment words.\n       4. Context Independence: Lexical effects remain consistent across different sentence contexts.\n\n3.5  Contextual Integration\n\nWe create test cases that check how the model changes the sentiment of the raw words to the right\nmeaning based on context. Our test suite includes test cases with the following sentiments: Medium\nintensity, Intensified swap, Simple negation, Intensified negation, Complex double negation, Domain\ncontext, Sarcasm, Intensity, Multiple intensifier, Scale variation.\n\n3.6  Hypotheses\n\nWe test whether contextual integration follows the predicted layer specialization pattern:\n\n       1. Middle Layer Concentration: Contextual effects peak in layers 4-8.\n       2. Phenomenon Specificity: Different context types show distinct layer patterns.\n       3. Distributed Processing: Effects concentrate in specific layers rather than being distributed.\n\n4  Data\n\nThis section outlines the data generation methodology employed to evaluate the sentiment processing\nhypothesis in GPT-2 mentioned in the earlier section.\n\n4.1  Lexical Detection Dataset:\n\nThe dataset included 1,000 test cases across six types of contextual changes, each targeting different\naspects of sentiment processing. I) Simple Negation cases examined basic polarity reversal through\nnegation words (e.g., “The movie was good” vs. “The movie was not good”).  II) Intensified\nNegation tested stronger negation patterns with adverbs (“The film was excellent” vs. “The film was\ndefinitely not excellent”). III) Sarcasm cases required the detection of situational incongruity (“Great,\nanother meeting” with positive/negative contextual framing). IV) Domain context examples tested\nhow domain knowledge affects sentiment interpretation (“The horror movie was terrifying” where\n‘terrifying’ is positive for horror but negative for other genres). V) Intensification cases examined\nhow modifiers amplify sentiment (“The meal was good” vs. “The meal was extremely good”).\nVI) Complex Double Negation tested sophisticated logical reasoning (“I don’t think it’s not good”\nrequiring multiple negation resolution steps).\n\n4.2  Contextual Integration Dataset:\n\nThe Contextual Integration Dataset comprises 8,000 carefully constructed test pairs designed to\nevaluate how GPT-2 processes context dependent sentiment modifications across 14 distinct phe-\nnomena. Each test pair consists of a clean sentence and a corrupted counterpart, differing only in\nspecific contextual elements. The dataset systematically explores diverse contextual mechanisms: C1)\nStrong Positive: Substituted strong positive sentiment words with strong negative counterparts (e.g.,\n‘incredible’ →‘abysmal’, ‘wonderful’ →‘horrible’). C2) Medium Intensity: Swapped medium-\nintensity positive words with medium-intensity negative words (e.g., ‘fine’ →‘bad’, ‘enjoyable’\n→‘unsatisfactory’). C3) Intensified Swap: Combined intensifier adverbs with opposite sentiment\nwords (e.g., ‘utterly wonderful’ →‘utterly awful’, ‘completely amazing’ →‘completely dreadful’).\nC4) Comparative Context: Changed comparative phrases and their sentiment outcomes (e.g., ‘better\nthan expected, quite satisfying’ →‘worse than expected, quite mediocre’). C5) Simple Negation:\nAdded or removed basic negation words to flip polarity (e.g., ‘nice’ →‘not nice’, ‘are decent’ →\n\n\n                                        4\n\n‘aren’t decent’). C6) Intensified Negation: Applied negation to intensified positive phrases (e.g., ‘was\nquite outstanding’ →‘wasn’t quite outstanding’, ‘very spectacular’ →‘wasn’t very spectacular’).\nC7) Complex Double Negation: Used double negation patterns with contrasting outcomes (e.g.,\n‘wasn’t bad at all, actually decent’ →‘wasn’t good at all, actually disappointing’). C8) Domain\nContext: Changed domain context to alter sentiment interpretation of domain-specific words (e.g.,\n‘horror movie was haunting’ [positive] →‘romantic comedy was haunting’ [negative]). C9) Sarcasm:\nModified intensity of sentiment words in sarcastic contexts to change perceived sentiment (e.g.,\n‘Perfect, amazing weather’ [sarcastic/negative] →‘Perfect, decent weather’ [less negative]). C10)\nConditional vs Actual: Switched between conditional and actual statements to change sentiment (e.g.,\n‘would have been outstanding if not for’ [negative] →‘was outstanding despite’ [positive]). C11)\nIntensity Variation: Reduced or increased intensity modifiers while keeping base sentiment (e.g.,\n‘incredibly pleasant’ →‘a bit pleasant’, both positive but different intensity). C12) Multiple Intensi-\n fiers: Removed stacked intensifiers to reduce sentiment strength (e.g., ‘utterly very adequate’ →‘just\nadequate’, positive →neutral). C13) Intensity Flip: Changed strong intensifiers to weak/minimal in-\ntensifiers (e.g., ‘extremely spectacular’ →‘only slightly spectacular’, positive →neutral). C14) Scale\nVariation: Swapped sentiment words at different positions on the sentiment scale (e.g., ‘pleasant’\n[+3] →‘horrible’ [0], varying scale distances).\n\n\n5  Result\n\n5.1  Lexical Detection\n\nTo test our four part framework for lexical sentiment processing, we ran three experiments on early\nlayers. The Lexical Sensitivity test checks if sentiment effects are strongest at word substitutions\n(Hypotheses 1–2). The Position Specificity test compares patching at sentiment vs. non-sentiment\nwords (Hypothesis 3). The Context Independence test measures whether lexical effects stay stable\nacross contexts (Hypothesis 4). Together, these activation patching experiments provide causal\nevidence for our framework beyond correlations.\n\n\n\n\n\n                                 Figure 1: Lexical Sensitivity\n\n\n5.1.1  Lexical Sensitivity\n\nFor each sentence pair, we patched activations across all 12 GPT-2 layers. The clean sentence used\na positive word, while the corrupted one used its negative counterpart. By replacing activations\nat target word positions, we measured each layer’s causal role in sentiment prediction. Position\nspecificity was tested by comparing effects at sentiment vs. non-sentiment words, while context\nindependence was measured by variation of these effects across different contexts. Figure 1, shows the\nlexical sensitivity in GPT-2 layers. Sensitivity quantifies how much a model’s output (e.g., sentiment\n\n\n                                        5\n\nprediction) changes in response to interventions on internal components (e.g., neurons or attention\nheads). Bar heights show effect sizes from activation patching experiments. Average sensitivity of\nearly layers Layer-0 to Layer-3(L0-L3) shows higher sensitivity to sentiment word substitutions, with\nL0 exhibiting peak performance.\n\n\n\n\n\n                                                                        Mean: 0.147\n\n\n         0.1\n\n\n\n\n\n        0.08\n\n\n\n\n\n        0.06\n             Probability\n\n\n        0.04\n\n\n\n\n\n        0.02\n\n\n\n          00                    0.05                    0.1                   0.15                    0.2                   0.25                    0.3                   0.35                    0.4\n\n                                                                                          Position Specificity Score\n\n\n                                Figure 2: Position Specificity\n\n\n5.1.2  Position Specificity\n\nPosition Specificity Analysis tests whether GPT-2 detects lexical sentiment at precise word locations\nor through more diffuse sentence wide signals. We measure this by comparing the effect of activation\npatching at sentiment bearing words versus at non-sentiment words within the same sentence. Theory\npredicts that early layers should show strong word level sensitivity, since lexical sentiment features\nare tied directly to specific tokens. Figure 2, shows our experiments confirm across 2000 test pairs,\nactivation patching produced significantly stronger effects at sentiment word positions than at non\ntarget positions, with a mean specificity score of 0.147 (p < 0.001). This provides clear evidence\nthat early layers, particularly Layer 1, localize sentiment information to specific token positions.\nIn other words, GPT-2’s lexical stage operates through targeted, word level detection rather than\nholistic sentence processing. This position specific encoding forms the foundation for later contextual\nintegration stages, where sentiment must be adjusted through distributed processing to capture\ncomplex patterns such as negation or sarcasm.\n\n5.1.3  Context Independence\n\nThe context independence analysis tests whether the GPT-2 detection of lexical sentiment remains\nstable across different sentence contexts. We measure this by looking at how much the effect of\nsentiment words varies when they appear in different linguistic environments.  If a layer is truly\nperforming lexical processing, the effect of words like ‘wonderful’ or ‘terrible’ should remain\nconsistent regardless of context. In contrast, context dependent processing should produce higher\nvariability, since the same word may shift meaning depending on surrounding words. Figure 3 shows\nearly layers (L0–L3) shows very low variability (mean = 0.038) in position effects, while later layers\n(L4–L11) show much higher variability (mean = 0.356). This confirms that early layers extract stable,\ncontext independent sentiment features, providing a reliable foundation for the rest of the network.\n\n5.1.4  Hypothesis Evaluation\n\nLexical detection analysis shows that GPT-2’s early layers reliably detect lexical sentiment. The\nresults support all four hypotheses: (1) lexical sensitivity, (2) the early layers show the strongest\nsensitivity to sentiment words, (3) the effects are position specific, strongest at the locations of the\nsentiment words, and (4) the detection is context independent, with less variability than the later\n\n\n                                       6\n\n        0.45\n                                                                                                                                                                                   0.429\n                                                                         Early ↔ Later\n                                                                                                                                                                     0.402\n         0.4                                                                                                                                          0.383\n\n                                                                                     Later Layers Avg: 0.356\n                                                                                                         0.343        0.35\n                                                                                                                        0.329          0.338                               0.321          0.329          0.322          0.317\n              Early Layers Avg: 0.308                                                                                           0.304        Effects         0.3\n\n                 0.260         Position  0.25\n  of\n\n\n         0.2          Deviation\n\n        0.15\n         Standard\n         0.1\n\n\n\n        0.05\n\n\n         0      0             1             2             3             4             5             6             7             8             9            10            11\n\n                                                                            GPT-2 Layers\n\n\n                     Figure 3: Context Independence of Sentiment Effects\n\n\nlayers. Together, this confirms that GPT-2 encodes stable lexical sentiment signals early, forming the\nbasis for later contextual integration.\n\n\n5.2  Contextual Integration\n\nTo evaluate contextual integration, we tested three hypotheses in GPT-2: (1) Middle Layer Concentra-\ntion, predicting peaks in L4-L8; (2) Phenomenon Specificity, predicting distinct layer patterns for\ndifferent contextual types; and (3) Distributed Processing, predicting effects spread across layers.\nUsing controlled activation patching across all 12 layers, we measured the causal impact of specific\ncontextual interventions on sentiment, isolating each phenomenon while keeping baseline conditions\nconstant.\n\n5.2.1  Middle Layer Concentration\n\nThe Middle Layer Concentration hypothesis predicted that contextual integration would peak in\nL4-L8, under the assumption that syntactic and semantic operations occur at intermediate depths\nof the network. Our experimental results, based on 8,000 test cases across 15 distinct contextual\nphenomena, contradicts this prediction. Figure 4, demonstrates that the network exhibits a bimodal\ndistribution where phenomena cluster in either early L0-L3 or late layers (8-11), with no phenomena\npeaking in the predicted middle range L4-L7. Of the 15 contextual phenomena tested, 8 exhibit their\n\nstrongest effects in L11 (57%), including strong positive contexts, medium intensity, intensified\nswap, simple negation, intensified negation, sarcasm, multiple intensifiers, and conditional vs actual\ncontexts. The remaining 7 phenomena (43%) peak in early layers: comparative context and scale\nvariation (L0), complex double negation, conditional vs actual, and intensity flip (L1), and domain\ncontext and intensity variation (L2). This bimodal pattern suggests fundamentally different processing\nstrategies for different types of contextual modifications.\n\n5.2.2  Phenomenon Specificity\n\nThe Phenomenon Specificity hypothesis predicted that different contextual phenomena would exhibit\ndistinct layer-wise processing patterns, with each type of contextual modification recruiting special-\nized computational mechanisms at different network depths. Our experimental results decisively\nfalsify this prediction, revealing instead remarkable convergence across semantically diverse phenom-\nena. Of the 15 contextual types tested, 13 (87%) share nearly identical top 3 contributing layers in\nthe pattern [L11, L10, L9] or [L10, L11, L9]. Furthermore, 8 phenomena (53%) peak at the exact same\nlayer L11. This convergence encompasses semantically diverse contextual modifications including\nsimple negation, intensified negation, sarcasm, multiple intensifiers, and comparative contexts, all\n\n\n                                       7\n\n                   L11                                  L11                                                L11                                                                             L11                                                                                            L11                                                                                                                                       L11                                                                                                                                                                                   L11         Layer                                                                                                                                           Range                                    (0.333)                                                    (0.333)                                                                                    (0.500)                                                                                                     (0.500)                                                                                                                                                     (0.333)                                                                                                                                                                                                      (0.500)        11     (0.333)                                                                                                                                                                                 0-3                                                                                                                                                                                                                    (Early)\n                                                                                                                                                                                 4-7 (Mid)\n        10                                                                                                                                                                 8-11 (Late)\n\n         9\n\n\n         8\n                                                                                                         Mid → Late\n         7              Number\n            Layer   6\n         Peak   5\n\n         4\n                                                                                                                        Early → Mid\n         3\n                                                                                                                            L2                                           L2\n         2                                                                                                                       (0.333)                                          (0.333)\n                                                                                                             L1                                           L1                                           L1\n         1                                                                                                      (0.333)                                          (0.333)                                          (0.500)\n                                                                 L0                                                                                                                                                  L0\n         0      C1           C2           C3            (0.333)C4           C5           C6           C7           C8           C9           C10          C11          C12          C13           (0.500)C14\n\n                                                                                                   Context Type\n\n\n\n\n                    Figure 4: Peak Layer Distribution Across Context Types\n\n\nrouting through the same late layer processing hub despite their different linguistic properties. Only\ndomain context exhibits a genuinely distinct pattern, with both peak (L2) and top-3 layers [L2, L3,\nL4] concentrated in early-to-middle regions. This singular exception highlights what phenomenon\nspecificity would look like if it existed systematically. The overwhelming convergence demonstrates\nthat GPT-2 does not employ phenomenon-specific modules but instead processes most contextual\nmodifications through a shared high-level semantic integration system.\n\n5.2.3  Distributed Processing\n\nThe Distributed Processing hypothesis predicted that contextual effects would be spread across\nmultiple layers rather than concentrated in specific regions of the network. Our results provide mixed\nevidence, revealing a more nuanced architecture than either pure distribution or strict concentration.\nThe total layer importance analysis shows a clear gradient rather than a uniform distribution. Late\nlayers (L8-L11) dominate with 46% of all contextual processing, while mid-layers (L4-L7) contribute\nsubstantially 39%, and early layers (L0-L3) account for only 15%. The top five most important\nindividual layers form a consecutive sequence from the network’s upper regions: L11, L10, L9, L8,\nand L7. Figure 5, demonstrates that a monotonic decrease from late to early layers indicates a concen-\ntrated rather than distributed processing architecture. These findings largely falsify the Distributed\nProcessing hypothesis in its strong form. Contextual integration is not uniformly distributed across all\nlayers, but instead concentrates in a specific late layer region (L8-L11), with diminishing contributions\nfrom middle and early layers.\n\n5.2.4  Hypothesis Evaluation\n\nOur systematic evaluation of the 8,000 case test dataset reveals that all three contextual integration\nhypotheses were falsified, though each provides distinct insights into GPT-2’s processing architecture.\nThe Middle Layer Concentration hypothesis failed as contextual phenomena exhibited 57% late\npeaking and 43% early peaking with zero phenomena peaking in the predicted middle layers (L4-L7),\ncontradicting assumptions about intermediate layer semantic processing. The Phenomenon Specificity\nhypothesis was decisively rejected: 87% of phenomena in our test cases share identical top 3 layers\n[L11, L10, L9], demonstrating that GPT-2 routes semantically diverse contextual modifications\nnegation, sarcasm, intensification through a unified late layer hub rather than specialized modules.\nFinally, the distributed processing hypothesis failed because the importance of layers in our dataset\nshows a 6.7 fold monotonic increase from L0 (828.7) to L1 (5,537.1), with late layers dominating 46%\nof total processing weight. Together, these falsifications reveal an unexpected architecture within our\nexperimental scope: GPT-2 employs phenomenon-agnostic late layer integration for most contextual\nreasoning, with only domain specific contexts (L2, top 3: [L2 , L3, L4]) representing a distinct early\nlayer processing pathway. These findings, specific to our test dataset and methodology.\n\n\n                                       8\n\n                                                                                                                                                                  5446           5537\n                                                                                                                                                   5281\n                                                                                                                                     5145\n                                                                                                                      4989\n       5000                                                                                             4769\n\n\n                                                                                         4420\n\n\n       4000                                                                3918\n      Score\n\n\n                                            Early                                               (L0-L3)\n                                 Importance:                                             7,211.4                                             (15%)              3055                Importance:Mid (L4-L7)18,096.7 (39%)                                               Importance:Late (L8-L11)21,408.4 (46%)\n       3000           Importance\n      Total                                         2335\n\n\n       2000\n\n\n\n\n\n                                992\n       1000      829\n\n\n\n                 0             1             2             3             4             5             6             7             8             9            10            11\n\n                                                                                           Layer Number\n\n\n                             Figure 5: Layer Importance Gradient\n\n\n6  Conclusion\n\nThis study provides systematic causal validation of hierarchical sentiment processing in GPT-2\nthrough mechanistic interpretability methods. We show that sentiment processing unfolds in a\ntwo stage: precise lexical detection in early layers followed by complex contextual integration\nconcentrated in late layers, rather than in the predicted middle layers. All three contextual integration\nhypotheses middle layer concentration, phenomenon specificity, and distributed processing were\nsystematically falsified. These findings shows how rigorous activation patching can explain AI\nmodels beyond correlational analysis to provide causal insight into transformer computation. Future\nwork should validate these patterns across diverse transformer architectures (BERT, RoBERTa, larger\nGPT models) to determine whether two-stage lexical-contextual processing represents a general\narchitectural principle or remains specific to GPT-2’s scale and training paradigm. Extension to fine-\ngrained circuit-level analysis could identify the precise attention heads and MLP blocks responsible\nfor lexical detection and contextual integration, moving beyond layer-wise analysis to map exact\ncomputational pathways within the transformer architecture.\n\nReferences\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick.  Bert rediscovers the classical nlp pipeline.  In\n  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\n  (ACL 2019), pages 4593–4601. Association for Computational Linguistics, 2019. URL https:\n  //aclanthology.org/P19-1452/.\n\nGanesh Jawahar, Benoît Sagot, Djamé Seddah, Maxime de Lhoneux, David Seddah, and Maxime\n  de Lhoneux. What does bert learn about the structure of language? Proceedings of the 57th Annual\n  Meeting of the Association for Computational Linguistics (ACL 2019), pages 356–365, 2019. URL\n  https://aclanthology.org/P19-1356/.\n\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at?\n  an analysis of bert’s attention. In Proceedings of the 57th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2019), pages 276–285. Association for Computational Linguistics,\n  2019. URL https://aclanthology.org/W19-4828/.\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about\n  how bert works. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\n   doi: 10.1162/tacl_a_00349. URL https://aclanthology.org/2020.tacl-1.54/.\n\nYonatan Belinkov, Youngwook Kim, Jae Myung Kim, Jieun Jeong, Cordelia Schmid, Zeynep Akata,\n  and Jungwoo Lee. Bridging the gap between model explanations in partially annotated multi-label\n\n\n                                       9\n\n   classification.  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), pages 3408–3417, 2023. URL https://openaccess.thecvf.com/\n  content/CVPR2023/papers/Kim_Bridging_the_Gap_Between_Model_Explanations_\n  in_Partially_Annotated_Multi-Label_CVPR_2023_paper.pdf.\n\nYanai Elazar and Yoav Goldberg.  Adversarial removal of demographic attributes from text\n   data.  In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), pages 11–21. Association for Computational Linguistics, 2018. URL\n  https://aclanthology.org/D18-1002/.\n\nAleksandar Makelov, Georg Lange, and Neel Nanda. Is this the subspace you are looking for? an\n   interpretability illusion for subspace activation patching. In Proceedings of the 2024 Interna-\n   tional Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/\n  forum?id=Ebt7JgMHv1.\n\nDaking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. A practical review of mechanistic\n   interpretability for transformer-based language models. arXiv preprint arXiv:2407.02646, 2024.\n  URL https://arxiv.org/abs/2407.02646.\n\nStefan Heimersheim and Neel Nanda. How to use and interpret activation patching. arXiv preprint\n  arXiv:2404.15255, 2024. URL https://arxiv.org/abs/2404.15255.\n\nNathaniel Elhage  et  al.  A mathematical framework for transformer  circuits.   https://\n  transformer-circuits.pub/2021/framework/index.html, 2021. Accessed: 2025-08-27.\n\nYilun Wang et al. Discovering and visualizing attention circuits for indirect object identification in\n   gpt-2. arXiv preprint arXiv:2211.00593, 2022. URL https://arxiv.org/abs/2211.00593.\n\nChen Meng et al. Locating and editing factual knowledge in gpt. In Advances in Neural Information\n  Processing Systems (NeurIPS 2022), 2022. URL https://arxiv.org/abs/2202.05262.\n\nChris Conmy et al.  Acdc: Automated discovery of computational circuits in neural networks.\n  https://arxiv.org/abs/2304.14997, 2023. Accessed: 2025-08-27.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n  Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\n  arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692.\n\nJohn Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Proceedings\n   of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\n  International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–\n  2743. Association for Computational Linguistics, 2019. URL https://aclanthology.org/\n  D19-1275/.\n\nNeel Nanda and Bryce Meyer.   Transformerlens: A library for mechanistic interpretability\n  of generative language models, 2023. URL https://github.com/TransformerLensOrg/\n  TransformerLens.\n\n\n\n\n\n                                       10\n\n## Claim to Classify\n\"early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Activation patching",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is activation patching (Rung 2), which is an interventional technique. The claim uses the functional/mechanistic verb 'act as' and 'encoding', and describes a specific role ('lexical sentiment detectors') for the components. According to the codebook, functional verbs and 'encoding' (when not explicitly defined as decodability) are classified as Rung 3 mechanistic claims. This creates a +1 gap (R2 method supporting R3 claim)."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 13009,
      "completion_tokens": 157,
      "total_tokens": 13166
    },
    "finish_reason": "stop"
  }
}