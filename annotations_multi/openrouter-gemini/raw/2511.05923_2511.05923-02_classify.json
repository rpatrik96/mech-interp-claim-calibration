{
  "timestamp": "2026-02-19T09:34:35.084366+00:00",
  "paper_id": "2511.05923",
  "claim_id": "2511.05923-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2511.05923\nTitle: Causal Tracing in VLMs for Hallucination Mitigation\nFull text:\n        Causal Tracing of Object Representations in Large Vision Language Models:\n                 Mechanistic Interpretability and Hallucination Mitigation\n\n         Qiming Li1*, Zekai Ye1*, Xiaocheng Feng1,2†, Weihong Zhong1, Weitao Ma1, Xiachong Feng3†\n                          1Harbin Institute of Technology, 2Peng Cheng Laboratory, 3The University of Hong Kong\n                                                         {qmli,zkye}@ir.hit.edu.cn\n\n\n\n                               Abstract                                                                                                                                                                                                    ...                                     MHSA   MLP     Hidden State\n              Despite  the remarkable advancements  of Large  Vision-                                                                                                          ...\n            Language Models (LVLMs), the mechanistic interpretability                                                                                                          ...                       Finding of MLP:2025                                                                                                                       Is                                                   ...                Hierarchical Representation             remains underexplored. Existing analyses are insufficiently\n                                                                                                                                         bird                                                ...\n             comprehensive and lack examination covering visual and tex-\n                                                                                                                             one                                                 ...\n                tual tokens, model components, and the full range of layers.                                   ：                                              ...                                                                                                Logits\n             This limitation restricts actionable insights to improve theNov                                                                                                          Finding of MHSA:\n               faithfulness of model output and the development of down-             Cross-modal Aggregation\n             stream                       tasks,                         such                                as hallucination                                                  mitigation.                                               To address                                                                                this                                       Intermediate Representation Injection19                                                                                                                         Method:                                   Fine-grained                                            Cross-modal                                                        Causal                limitation,                  we                            introduce\n            Tracing (FCCT) framework, which systematically quantifies         Figure 1: An overview of our proposed Fine-grained Cross-\n               the causal effects on visual object perception. FCCT conducts       modal Causal Tracing (FCCT) findings and Intermediate\n               fine-grained analysis covering the full range of visual and tex-                                                                   Representation Injection (IRI) method.\n                tual tokens, three core model components including multi-\n             head self-attention (MHSA), feed-forward networks (FFNs),\n            and hidden states, across all decoder layers. Our analysis is[cs.CV]       the first to demonstrate that MHSAs of the last token in mid-          distinct functional behaviors from three core model com-\n               dle layers play a critical role in aggregating cross-modal in-       ponents—such as multi-head self-attention (MHSA), feed-\n              formation, while FFNs exhibit a three-stage hierarchical pro-        forward networks (FFNs), and hidden states—across layers,\n              gression for the storage and transfer of visual object represen-        remains underexplored. Previous studies have partially ad-\n                tations. Building on these insights, we propose Intermediate                                                                        dressed these questions but still leave notable limitations.\n             Representation Injection (IRI), a training-free inference-\n                                                                          Attention knockout experiments (Neo et al. 2024) demon-             time technique that reinforces visual object information flow\n                                                                                      strate that LVLMs extract object information from visual ob-            by precisely intervening on cross-modal representations at\n               specific components and layers, thereby enhancing percep-          ject tokens in the middle to late layers. However, it lacks an\n               tion and mitigating hallucination. Consistent improvements         analysis of the cross-modal interactions between visual and\n              across five widely used benchmarks and LVLMs demonstrate          textual tokens, as well as the functional roles of the MLP and\n            IRI achieves state-of-the-art performance, while preserving        hidden states. NOTICE (Golovanevsky et al. 2024) intro-\n              inference speed and other foundational performance.               duces semantic image pairs for image corruption and sym-\n                                                                         metric token replacement for text corruption to analyze how\n                                           MHSA and MLP contribute to information aggregation in                        Introduction\n                                                                                textual tokens, but overlooks the effect of visual tokens.\n         Large Vision-Language Models (LVLMs) have  rapidly                                                              To address these limitations, we propose a Fine-grained\n          evolved, demonstrating impressive capabilities across di-arXiv:2511.05923v3                                                         Cross-modal Causal Tracing (FCCT) framework, which\n          verse tasks. However, existing interpretability studies fall                                                                            systematically analyzes cross-modal causal effects on visual\n           short in capturing the full complexity of visual informa-                                                                         perception by examining visual and textual tokens catego-\n           tion flow, thereby limiting progress in critical downstream                                                                             rized with their position and semantic role in the input se-\n           applications such as hallucination mitigation. Fundamen-                                                                      quence, covering three core model components across lay-\n             tal questions require further investigation, particularly re-                                                                                       ers. By introducing controlled Gaussian perturbations to in-\n          garding how LVLMs process visual object features and                                                                         put images, we induce measurable drops in output probabili-\n           align them with textual semantics in cross-modal represen-                                                                                         ties for existing objects. Then we restore specific activations\n            tations. Furthermore, how visual and textual tokens elicit                                                                       using the clean activations from the original image input.\n             *These authors contributed equally.                      By quantifying the recovery in the LVLM’s output probabil-\n              †Corresponding authors                                                     ities, we precisely estimate the causal effect on visual object\n           Copyright © 2026, Association for the Advancement of Artificial      perception for each core components across token types and\n            Intelligence (www.aaai.org). All rights reserved.                         layers. FCCT is the first to demonstrate that the MHSAs of\n\nthe last token in middle layers play a critical role in aggre-      (Yu et al. 2024; You et al. 2023; Zhang et al. 2025) and novel\ngating crucial object visual and textual information, as well       training objectives (Lyu et al. 2024). These methods can be\nas the FFNs exhibit a three-stage hierarchical progression       effective but require substantial data and computational re-\nfor the storage and transfer of visual object representations.       sources. (2) Contrastive decoding (Leng et al. 2024; Huang\n FCCT not only reveals cross-modal information flow of        et al. 2024; Zhong et al. 2024) leverages differences between\nvisual objects, but also provides valuable guidance for hal-       deliberately perturbed decoding paths to promote genera-\nlucination mitigation. Prior studies (Tang et al. 2025) sug-       tions that are more consistent in visual information. How-\ngest that deep unidirectional information flow can lead to the       ever, such methods introduce significant latency at inference\nprogressive degradation of fine-grained semantic cues en-       time. (3) Inference-time interventions modify internal ac-\ncoded in earlier layers, which may be a potential cause of ob-       tivations such as attention heads outputs (Liu, Zheng, and\nject hallucination. To prevent mid-layer degradation of crit-     Chen 2024; Li et al. 2025a; Ye et al. 2025) or hidden states\nical information during forward and reinforce components       (Liu, Ye, and Zou 2024) to steer the model toward more\nwith strong causal effects identified by FCCT, we further       faithful outputs. However, these methods generally lack in-\npropose Intermediate Representation Injection (IRI), a       terpretability of the selection of layers and components.\ntraining-free inference-time technique that injects crucial\nmid-layer representations into subsequent layers, thereby                     Preliminary\nenhancing visual perception capability and mitigating hal-\n                                         We restrict our scope to LVLMs that are based on auto-lucination. FCCT offers fine-grained and quantitative guid-\n                                                                   regressive Transformer architecture (Vaswani et al. 2017),ance for selecting model components and layers, serving as a\n                                                                as it is adopted by most SOTA LVLMs. The model receivestheoretical foundation for the design and implementation of\n                                                                as input a visual input sequence V = {v1, v2, . . . , vm} andthe IRI method. Consistent improvement across five widely\n                                                            a textual input sequence T =  {t1, t2, . . . , tn}, where mused benchmarks and five advanced LVLMs demonstrates\n                                                       and n denote the sequence lengths of the visual and textualthat IRI achieves state-of-the-art (SOTA) performance.\n                                                                     input. The textual and visual input sequences are concate-\n  In summary, our main contributions are three-fold:\n                                                              nated together and processed through L transformer layers\n • We propose FCCT, a fine-grained causal analysis that       of the language decoder, each consisting of multi-head self-\n   covers all types of visual and textual tokens, three core       attention (MHSA), feed-forward network (FFN) that is usu-\n   components across the full layer range, providing a com-       ally a multilayer perception (MLP), and a residual stream\n   prehensive mechanistic interpretability study of LVLMs.        is applied between each components. The l-th layer hidden\n • We propose IRI, a training-free inference-time method       state h(l) can be computed from the previous layer:\n   that effectively mitigates hallucination while preserving\n   inference speed and other foundational capabilities.                            h(l) = h(l−1) + a(l) + m(l),             (1)\n • Consistent improvements across five widely used bench-                                                       where a(l) and m(l) are the output of the MHSA compo-\n   marks and LVLMs not only demonstrate IRI’s SOTA                                                              nent and the FFN component at layer l. Finally, the model\n   performance, but also validate the findings of FCCT.                                                                   predicts the next token in an auto-regressive manner based\n                                                     on the last layer output.\n               Related Work                             In this paper, we aim to identify which types of visual\nMechanistic Interpretability of LVLMs  While LVLMs      and textual tokens, model components (i.e., a(l), m(l), and\nhave demonstrated remarkable capabilities across various       h(l)), and layer ranges play a critical role in the perception\ndownstream tasks, their mechanistic interpretability remains      and comprehension of visual object information in LVLMs.\nunderexplored. Existing interpretability methods, such as     By uncovering the underlying information flow, we seek to\nprobing (Salin et al. 2022), activation patching (Basu et al.      provide practical guidance for mitigating object hallucina-\n2024; Palit et al. 2023; Golovanevsky et al. 2024), logit       tion and related downstream issues.\nlens (Neo et al. 2024; Huo et al. 2024), in-context learning\n(Li et al. 2025d,b,c) provide only a coarse-grained analysis      Fine-Grained Cross-Modal Causal Tracing\nof model components or do not fully disentangle the com-\n                                                           Causal tracing (also known as activation patching or causalplex interactions between visual and textual token represen-\n                                                            mediation analysis) is a widely used interpretability tech-tations. In contrast, our work employs causal tracing with\n                                                            nique that selectively replaces internal activations to probeGaussian noise to precisely quantify the functional roles of\n                                                                 the causal contribution of specific model components (MengMHSA, FFN, and hidden states for both visual and textual\n                                                                          et al. 2022). In the context of large language models (LLMs),tokens across layers, enabling a fine-grained analysis of how\n                                                                causal tracing is frequently employed to examine the storageLVLMs perceive and process visual object information.\n                                                       and retrieval mechanisms of factual associations (Meng et al.\nMitigating Hallucination in LVLMs  LVLMs frequently      2022), and document-level relevance (Liu, Mao, and Wen\nproduce content that deviates from visual information, lead-      2025). In the context of LVLMs, we are the first to propose\ning to object hallucination. Existing hallucination mitigat-      using controlled Gaussian noise perturbations on input im-\ning strategies can be broadly categorized into three types:      ages for causal tracing. By adding controlled Gaussian noise\n(1) Training-based approaches enhance model factuality       to the entire image and then selectively restoring the acti-\nby pre-training or finetuning with carefully curated datasets       vations of specific components, we conduct a fine-grained\n\nFigure 2: Overview of our proposed Fine-grained Cross-modal Causal Tracing method. Activation patching computes the\ncausal effect of a specific component by running the LVLMs three times: a clean run (step1) with original image, a corrupted\nrun (step2) with image added Gaussian noise, and a patched run (step3) with corrupted input but restoring specific component\nusing the value in the clean run. We use Recovery Rate to quantify the causal effect of each restored component.\n\n\nanalysis of the internal mechanisms responsible for visual     2  Object Visual Tokens directly encode visual features\nobject perception and comprehension in LVLMs.                   corresponding to the queried object, which are central for\n   Specifically, we select 500 images from the COCO dataset         analyzing the internal mechanisms of visual object per-\nand design object-related questions for the objects present in          ception and comprehension.\neach image. Following the analysis methodology of ROME\n                                                     3  Late Visual Tokens occur after any queried object re-\n(Meng et al. 2022), we define three types of inference runs:\n                                                                     gion, which may capture residual visual information.\n • Clean Run: The model is given the original image, and\n  we record the probability Pclean assigned to the token     4  Early Textual Tokens occur near the visual&textual se-\n  yes in response to binary questions of the form ”Is there         quences boundary, which help analyze how information\n   a XXX in the image? Please answer this question with           transitions from visual to language components.\n   one word (Yes or No).”                                5  Textual Object Tokens encode textual features corre-\n • Corrupted Run: Gaussian noise is added to the entire         sponding to the queried object, which help reveal how\n   image to degrade visual quality, and we record the re-          visual object information interacts with textual reference.\n   sulting prediction probability Pcorrupted.\n                                                     6  Late Textual Tokens occur in the late part of the textual\n • Patched Run: Starting from the corrupted run, we selec-\n                                                              prompt, which help analyze how visual object informa-\n    tively restore specific internal activations (e.g., MHSA\n                                                                        tion propagates across textual stream not directly related.\n   output, MLP output, or hidden states) at certain layers\n   and token categories using activations from the clean run.     7 The Last Token occurs at the end of input sequence,\n   The prediction probability is denoted as Ppatched.                 helping analyze cross-modal information aggregation.\n  To quantify the causal effect of each restored component,                                               By restoring only one component of one layer for a single\nwe define the following Recovery Rate (RR) metric:                                                            token category at one time, we systematically derive fine-\n                           Ppatched −Pcorrupted                      grained insights into how LVLMs perceive and comprehend       RR =                                   (2)\n                            Pclean −Pcorrupted                        visual object information. This enables us to trace the causal\n                                                         pathways through which visual object information is repre-  This normalized measure reflects the proportion of clean\n                                                                   sented, propagated, and aggregated across different layersperformance regained through targeted restoration; more\n                                                       and model components.profoundly, it serves as a quantitative estimate of the com-\nponent’s causal effect on visual object perception. A value\nclose to 1 indicates a strong causal effect, whereas a value     Causal Tracing Results and Key Findings\nnear 0 suggests minimal influence.                               In this section, we present and analyze the experimental re-\n  To systematically conduct causal tracing across visual and       sults and key findings of FCCT conducted on two widely\ntextual information flow, we define seven categories based      used LVLMs: LLaVA-1.5-7B and Qwen-VL-Chat. As illus-\non token’s position and semantic role in the input sequence:       trated in Figure 3, we present the RRs of 3 model compo-\n1  Early Visual Tokens occur before any queried object re-      nents across 32 layers under 7 token categories, denoted as\n   gion, which serve as a control group for comparison.         1  to 7 , corresponding to the previous definition.\n\n             Self- Attention                     MLP                        Hidden State\n             (a) Recovery Rate of Attention (LLaVA-1.5-7b)                       (b) Recovery Rate of MLP (LLaVA-1.5-7b)                       (c) Recovery Rate of Hidden State (LLaVA-1.5-7b)\n                                                                          ①Visual Center\n\n\n\n                                                                                               ②Visual&Textual\n\n\n                                                                                                         ③Generation-oriented\n\n\n            (d) Recovery Rate of Attention (Qwen-VL-Chat)                        (e) Recovery Rate of MLP (Qwen-VL-Chat)                        (f) Recovery Rate of Hidden State (Qwen-VL-Chat)\n                                                                          ①Visual Center\n\n\n\n                                                                                               ②Visual&Textual\n\n\n                                                                                                         ③Generation-oriented\n\n\n   Finding 1:                            Finding 2:                             Finding 3:\n  MHSA of the last token in middle layers plays a   MLPs exhit a three-stage hierarchical    Hidden states have the greatest casual effect\n   crucial role in aggregating cross-model    progression in the visual object representations    on the output among three components and\n   information from all preceding tokens.             across tokens and layers.                           exhibit a distinct semantic shift .\n\n\nFigure 3: Results and key findings of FCCT framework on LLaVA-1.5-7b and Qwen-VL-Chat. The symbols from 1  to\n 7  represent the seven token categories defined above: 1 Early Visual Tokens, 2 Object Visual Tokens, 3 Late Visual\nTokens, 4 Early Textual Tokens, 5 Textual Object Tokens, 6 Late Textual Tokens, and 7 The Last Token.\n\n\n\n                                              LLaVA-1.5-7b  Visual Object Tokens                                             Three-stage  Hierarchical  Representations  via MLPs\n                                                As shown in Figure 2 (b) and (e), MLPs exhibits a three-\n                                                                 stage progression in the formation of visual object repre-\n                                                                     sentations: In early layers, visual object tokens are encoded\n                                                                     into localized, modality-specific embeddings with limited\n                                                            semantic abstraction; In intermediate layers, textual object                                           Qwen-VL-Chat\n                                                             tokens interact with visual representations, forming increas-\n                                                                 ingly rich cross-modal semantics; In the deeper layers, un-\n                                                               der the cross-modal aggregation effect of MHSA, the last\n                                                                 token’s MLPs progressively accumulate a cross-modal and\n Textual Object Tokens                                                    task-relevant representation.\n                                                                    Overall,  this  hierarchical  progression  illustrates how\nFigure 4: Visualization of normalized attention weights to     MHSA-driven cross-modal interactions and MLPs together\nvisual object tokens and corresponding textual object tokens      transform unimodal and localized visual representations into\nacross layers. We report the average result on 3,000 VQAs.       cross-modal and globally aggregated representations that are\n                                                                      essential for visual object perception in LVLMs.\n\nCross-modal Aggregation via the Last Token’s MHSAs\n                                                            Hierarchical Semantic Shift of the Hidden States  AsAs shown in Figure 2 (a) and (d), the last token’s MHSAs\n                                                    shown in Figure 2 (c) and (f), hidden states exhibit a dis-in intermediate layers exhibit a strong causal effect, which\n                                                                           tinct semantic shift: in shallow layers (Layers 0–10), visualplays a particularly crucial role in aggregating information\n                                                                tokens’ hidden states are primarily visual-centric, encodingfrom all preceding tokens.\n                                                                 low-level perceptual patterns. In deep layers (Layers 18–31),  To further investigate the cross-modal aggregation effect\n                                                                 the last tokens’ are cross-modal and highly task-related toof MHSA, we visualized the last token’s layer-wise nor-\n                                                                 the final prediction. Notably, the phase in which the causalmalized attention weights for the queried visual object to-\n                                                                       effect of the hidden states gradually strengthens aligns withkens and textual object tokens. As shown in Figure 3, we\n                                                                 the intermediate layers around Layer 14, precisely whereobserve a sharp increase in attention weight around Layer\n                                         MHSAs and MLPs jointly contribute most significantly to15. This suggests that around these layers, LVLMs begin to\n                                                                     refining cross-model object representations.align instruction-guided attention with the most relevant vi-\nsual and textual cues. We hypothesize that these layers mark        This shift reflects the progression from grounding visual\na transition point where deep cross-modal information ag-       object information to aggregation with task-related context\ngregation occurs, enabling the model to bind unimodal rep-       for final prediction. It further highlights the hierarchical or-\nresentations to high-level cross-modal representations.            ganization of internal representation inference in LVLMs.\n\n    Intermediate Representation Injection             This intervention ensures that only critical cross-modal\nMotivated by the findings from the proposed FCCT analy-       intermediate activations are injected into subsequent layers\nsis, we observe that the last token’s MHSA and MLP outputs      with sufficient causal effect throughout the information flow,\nat intermediate layers are crucial for capturing and aggre-      enhancing the LVLM’s trustworthiness to visual object in-\ngating task-related object information from both visual and      formation and thereby mitigating hallucination.\ntextual modalities. To take advantage of this observation, we\n                                                  Experimental Setuppropose Intermediate Representation Injection (IRI), a\ntraining-free inference-time technique, which aims to rein-      Models.  We adopt the widely used LLaVA-1.5-7b (Liu\nforce crucial cross-modal representations to improve visual        et al. 2024a), Qwen-VL-Chat (Bai et al. 2023), LLaVA-\nobject information perception and mitigate hallucination.      NeXT (Liu et al. 2024b), Qwen2-VL-7B(Wang et al. 2024)\n                                                       and InternVL2-8B (Chen et al. 2024) as baseline LVLMs.\nMethod                                                            Evaluation.  We comprehensively evaluate the methods\nTo reinforce crucial mid-layer cross-modal representations,       for both discriminative and generative tasks to measure the\nwe  selectively  inject them  into  later  layers, which are       effectiveness and robustness of hallucination mitigation.\nadaptively scaled by their causal effects (Recovery Rates),                                                                             • POPE (Li  et  al. 2023) employs a binary question-\nthereby amplifying the contributions of the most influential                                                             answering format, inquiring LVLMs to answer if a spe-\ncomponents across layers.                                                                             cial object exists in the given image. Following previous\n  Let RRattn = {RRattnk }Lk=1 and RRmlp = {RRmlpk }Lk=1         works, we adopt Accuracy and F1 score as the metrics.\ndenote the Recovery Rates of the MHSA and MLP outputs                                                                             • MME (Fu et al. 2023) serves as a comprehensive tool for\nacross the L layers, respectively. To ensure that the most crit-                                                                    assessing the capabilities of LVLMs across both 10 per-\nical cross-modal representations are injected into layers with                                                                  ception tasks and 4 cognition tasks. Consequently, task\nsufficient causal effect, we rank components across layers by                                                                   scores are reported as the evaluation metric.\ntheir Recovery Rates and independently select:\n                                                                             • CHAIR (Rohrbach et al. 2018) is a widely used met-\n • A set of top-k1 MHSA source layers Lattnsrc with the high-           ric for assessing object hallucination in responses of\n    est RRattn values and a set of top-k2 target layers Lattntgt .        LVLMs. The CHAIR metric comprises two important in-\n • A set of top-k1 MLP source layers Lmlpsrc with the highest           dicators, denoted as CS and CI, with the following cal-\n                                                                     culation formulas:\n  RRmlp values and a set of top-k2 target layers Lmlptgt  .                                                                                              |{Hallucinated objects}|\n                                                       CS =  For each attention source layer k ∈Lattnsrc , we record the                               |{All mentioned objects}|\nMHSA output of the last token: a(k) ∈Rd. For each target                                                                                     |{Sentences w/ hallucinated objects}|\nlayer l ∈Lattntgt such that l > k, we inject the stored activa-             CI =\n                                                                                                 |{All sentences}|tions into the target MHSA output, and scaling them by RR\nto modulate their contribution to reflect causal effect:                 • MMHal-Bench (Sun et al. 2023) comprises 96 metic-\n                                                                  ulously designed questions, which evaluates response-\n         ˜a(l) = a(l) + λa · X g(k, l) · RRattnk   · a(k),    (3)           level  hallucination  rate (VH.%) and informativeness                                                                      (Score). It asks GPT-4 to compare model outputs with\n                               k∈Lattnsrc                                                   human responses and object labels for evaluation.\n   Similarly, for each k ∈Lmlpsrc and l ∈Lmlptgt with l > k, we         • MHumanEval (Yu et al. 2024) is designed to evaluate\ninject the recorded MLP outputs as:                                  hallucination performance by human annotators. The\n                                                         benchmark contains 146 samples collected from Object\n                                                        HalBench and MMHal-Bench. Given model responses,\n   ˜m(l) = m(l) + λm · X g(k, l) · RRmlpk   · m(k),   (4)       we ask three human annotators to label the hallucinated\n                         k∈Lmlpsrc                                   segments and compute the mean response-level halluci-\n                                                                   nation rate (Hu.%) as the evaluation metric.\nwhere λa and λm are scaling coefficients, and g(k, l) en-\nsures that the injected information respects causal ordering:       Baselines.  We compared our proposed IRI method with\n                                                                 the following SOTA training-free methods: VCD (Leng\n                             1,   if l > k                             et al. 2024) contrasts model logits derived from original\n                g(k, l) =\n                             0,  otherwise                    and distorted visual input to reduce the over-reliance on\n                                                                              statistical bias and unimodal priors. OPERA (Huang et al.\n  To ensure that the injected activation maintains the same                                                         2024) introduces a penalty term on the model logits during\nnorm as the original, we apply the following normalization:                                                                 the beam-search decoding to mitigate the over-trust issue.\n                                                 PAI (Liu, Zheng, and Chen 2024) intervenes on attention\n                   ∥a(l)∥2              ∥m(l)∥2             heads by leveraging their original direction and optimizes\n        ˜a(l) = ˜a(l) ·             , ˜m(l) = ˜m(l) ·              ,    (5)\n                     ∥˜a(l)∥2                ∥˜m(l)∥2              the output distribution during decoding to mitigate language\n                                                                        bias. VTI (Liu, Ye, and Zou 2024) mitigates hallucination\nwhere ||·|| represents the ℓ2 norms (Euclidean norms) of the     by steering layer-wise hidden states during inference to en-\nactivation vectors.                                          hance visual feature stability.\n\n                      LLaVA-1.5-7b                  Qwen-VL-Chat                 LLaVA-NeXT  Method\n                Exist.  Count   Pos.   Color   Total   Exist.  Count   Pos.   Color   Total   Exist.  Count   Pos.   Color   Total\n   Regular    175.7   124.7  114.0  151.0  565.4  170.0   135.0  123.3  170.0  598.3  180.0   105.0  150.0  151.7  586.7\n  VCD       180.3  131.7  125.0  155.0  592.0  180.0   133.3  131.7  175.0  620.0  185.0   125.0  133.3  168.3  611.6\n  OPERA    165.0  116.0  133.3  149.0  563.3  180.0   140.0  138.3  175.0  633.3  183.8   121.3  155.0  162.1  622.2\n   PAI        190.0   148.3  126.7  160.0  625.0  175.0   141.6  132.5  177.5  626.6  185.0   128.3  148.3  170.8  632.4\n  VTI        185.0  140.0  135.0  165.7  625.7  180.0   142.5  133.0  178.0  633.5  186.7   126.7  150.0  172.5  635.9\n   IRI(ours)  195.0  140.0  140.0  168.3  648.3  185.0  145.0  135.0  180.0  645.0  190.0  135.0  155.0  177.5  657.5\n\n    Table 1: Results on MME hallucination subset. The best performances are bolded and the second-best are underlined.\n\n\n\n              LLaVA-1.5-7bQwen-VL-ChatLLaVA-NeXT                     LLaVA-1.5-7b      Qwen-VL-Chat\nSettingMethod                                            Method\n               Acc   F1   Acc    F1    Acc   F1                 Score↑VH.%↓Hu.%↓Score↑VH.%↓Hu.%↓\n\n        Regular  83.29  81.33  84.63   82.61   84.78  86.43           Regular    1.86    63.5    67.1   2.93    41.1    61.0\n     VCD     87.73  87.16  86.93   85.46   88.76  89.57       VCD       2.12    54.2    66.7   2.77    39.2    61.5\n     OPERA   89.20  88.81  85.71   84.64   90.27  89.71        OPERA    2.15    54.2    63.0   2.94    38.4    58.2\n Ran.      PAI      86.33  84.56  85.38   85.54   88.40  87.16          PAI        2.27    53.2    62.5   2.87    39.5    56.7\n      VTI      89.50  88.89  86.73   85.59   89.23  88.68         VTI        2.43    52.2    63.4   2.99    38.4    57.4\n       IRI(ours)89.76  89.32  87.38   87.42   90.68  90.21           IRI(ours)  2.53    50.2    62.0   3.13    37.5    56.2\n\n        Regular  81.88  80.06  83.63   81.53   83.23  84.77\n     VCD     85.38  85.06  85.17   83.68   87.01  87.70        Table 4: Results on MMHal-Bench and MHumanEval. We\n     OPERA   86.64  86.62  84.82   83.99   87.16  87.68        use GPT-4 and human annotators as evaluation references. Pop.      PAI      85.33  83.62  84.20   83.10   86.65  86.99\n      VTI      87.36  86.69  85.67   84.48   87.33  87.16\n       IRI(ours) 87.67  87.07  86.24   86.89   88.25  88.04                    POPE    MME    CHAIR\n                                                             Model\n        Regular  78.96  77.57  81.03   79.30   81.19  82.50                 ACC ↑F1 ↑Cog.↑Hall.%↑CS ↓CI ↓\n     VCD     80.88  81.33  83.10   82.04   84.80  85.23\n     OPERA   81.24  81.38  82.67   79.89   85.20  85.54         Qwen2-VL-7B  88.49  87.85 556.4  630.0   24.8  7.2\n Adv.      PAI      83.17  81.67  82.19   82.06   84.32  83.68                + IRI  89.04  88.44 563.4  663.3   14.2  6.5\n      VTI      82.57  82.11  83.13   82.16   85.35  84.52\n       IRI(ours) 85.17  84.18  84.83   84.52   85.67  86.26            InternVL2-8B  86.67  85.72 566.4  663.0   37.2  9.4\n                                                             + IRI  87.69  86.90 569.3  688.7   30.7  8.6\n\nTable 2: Results on POPE tasks. We evaluate the accuracy\n                                                             Table 5: Results on more advanced models. Cog. and Hall.and F1 Score of various widely used LVLMs.\n                                                            denote the cognitive and hallucination subset of MME.\n\n              LLaVA-1.5-7b        Qwen-VL-Chat\n Method\n        CS ↓CI ↓Recall↑Len CS ↓CI ↓Recall↑Len           (1) Robust and SOTA Performance: Our proposed IRI\n Regular   52.8  15.9   77.3   93.4   2.8   3.0   31.0   5.3       method demonstrates robust, SOTA hallucination mitiga-\n VCD      51.0  14.9   77.2  101.9  1.4   1.2   30.8   4.0        tion performance across both discriminative and generative\n OPERA   45.6  13.1   78.5   95.3   1.7   1.3   31.9   4.4         tasks. Specifically, on the POPE benchmark, IRI achieves an\n PAI       38.3  12.4   76.9   94.4   1.3   1.2   32.2   4.2       average improvement of +4.89% in Accuracy and +5.20%\n VTI       36.9  12.1   76.8   93.8   1.1   1.1   31.4   4.2\n                                                                     in F1 Score. For the MME hallucination subset, IRI brings IRI(ours) 34.6  11.5   78.2   95.8   1.0   0.9   32.6   4.4\n                                                        an average absolute gain of +65.7 points in the Total score.\n                                            On the CHAIR benchmark, IRI reduces the average hallu-Table 3: Results on CHAIR. Max new tokens is 512.\n                                                                  cination metrics (CS and CI) by 6.43 points. Finally, onLower CS and Ci along with higher recall and length in-\n                                                  MMHal-Bench, IRI improves the average Score by +0.44dicate better hallucination mitigating performance.\n                                                            while lowering the average VH Rate by 8.45%. These re-\n                                                                         sults demonstrate the effectiveness and robustness of our ap-\n                                                           proach in mitigating hallucinations.\nImplementation Details.  In our experiments, we uni-\n                                                                       (2) Model-agnostic and generalizable: IRI is not de-\nformly set k1 = 3 and k2 = 10. For LLaVA-1.5-7B, we use                                                           pendent on specific model architectures and can be read-\nλa = 0.26 and λm = 0.16; for Qwen-VL-Chat, λa = 0.20                                                                            ily deployed across various LVLMs. We successfully imple-\nand λm = 0.10; and for LLaVA-NeXT, λa = 0.15 and                                                      mented IRI on more advanced models, such as Qwen2-VL-\nλm = 0.08. Both causal tracing analysis and evaluation ex-                                              7B and InternVL2-8B, where it continued to provide steady\nperiments of our proposed IRI method are performed on 8\n                                                       and significant performance enhancements.\n× NVIDIA A100 SXM 80GB GPUs.\n                                                                       (3) Preserving foundational capabilities: IRI effectively\n                                                                 mitigates hallucination without sacrificing LVLM’s other\nMain Results                                                               foundational capabilities. Specifically, it leads to improved\nBased on the experimental results presented in Tables 1-5,       scores on MME cognitive tasks and more informative re-\nwe can draw the following key conclusions:                      sponses, as indicated by higher scores on MMHal-Bench.\n\n                         Hyperparameters   POPE           (3) How does the number of source and target layers affect  Setting\n                         λa  λm k1  k2 ACC ↑F1 ↑       performance? We find that the number of intervention lay-\n                                                                     ers affects the effectiveness of IRI to some extent. It is nec- LLaVA-1.5-7b                     -      -    -    -   78.96  77.57\n                                                                essary to select a sufficient number of intermediate layers                    Ablation of Component\n                                                            with strong causal effects for visual information perception\n + IRI w/o MLP              0.22   -   3  10  84.93  84.02\n + IRI w/o MHSA                  -   0.04 3  10  85.07  84.03      and inject them into a sufficient number of target layers to\n + IRI w/ Hidden States       0.18 0.04 3  10  84.50  83.67      make IRI effective. Notably, even when injecting from too\n                   Ablation of Layer Range                   many source layers or into too many target layers, IRI’s per-\n + IRI w/ Fisrt 10 Layers      0.22 0.14 3  10  78.46  77.23       formance does not drop significantly compared to its peak,\n + IRI w/ Last 10 Layers      0.20 0.14 3  10  80.42  79.87      which demonstrates strong robustness. Nevertheless, select-\n                   Ablation of Layer Nums.                        ing too many source layers may introduce noise, while in-\n                                                              cluding too many target layers may inject mid-level repre- + IRI w/ Less Source Layers  0.26 0.16 1  10  82.92  83.13\n + IRI w/ More Source Layers 0.24 0.14 5  10  84.76  83.94        sentations into task-relevant representation for final predic-\n + IRI w/ Less Target Layers  0.22 0.12 3   5   83.32  82.71         tion, preventing optimal performance.\n + IRI w/ More Target Layers  0.26 0.16 3  15  84.49  83.56\n                                                                     (4) Why is Recovery Rate necessary for more precise in-\n                       Ablation of RRs                            jection? The Recovery Rate adaptively controls how much\n + IRI w/o RRs              0.26 0.14 3  10  84.42  83.92       each layer’s information is amplified in line with its esti-\n                   Ablation of Normalization                    mated causal effect. By re-weighting restored activations,\n + IRI w/o Norm.             0.24 0.14 3  10  85.07  83.98           it highlights components across layers that contribute more\n                                                                 strongly to visual object perception. + IRI                       0.26 0.16 3  10  85.17  84.18\n                                                                     (5) Why is normalization necessary for more stable injec-\nTable 6: Result of ablation study on MS-COCO POPE. For      tion? The normalization strategy ensures that the scale of\neach experiment, the parameter λa and λm is individually       the vector remains consistent before and after injection, pre-\noptimized to ensure fair comparison.                            venting undesired magnitude shifts that may distort down-\n                                                            stream representations. This mechanism stabilizes the effect\n                                                                of representation injection, thereby enhancing the robustness\n                                                                of the IRI method to distributional shifts.Ablation Study\nAs shown in Table 6, to validate the effectiveness of each      Inference Latency\ncomponent within the proposed IRI method and the key find-     As shown in Table 7, IRI achieves the best hallucination mit-\nings of FCCT framework, we conducted a comprehensive       igating performance while preserves the inference speed.\nand systematic set of ablation experiments. We specifically\nfocus on addressing the following five questions:                 Method        TTFT(ms)    TPOT(ms)   Acc(%)\n(1) Why is it necessary to intervene in both MHSA and         LLaVA-1.5-7b     99.8 1.0×      36.0 1.0×     78.96\nMLP, but not directly in hidden states? Experimental re-                                                       + VCD           160.1 1.6×      96.8 2.7×     80.88\nsults show that removing either MHSA or MLP results in a        + OPERA        109.8 1.1×      69.5 1.9×     81.24\n                                                       + PAI            156.3 1.6×      93.6 2.6×     83.17slight performance decrease. Combined aggregation of two\nmodules yields greater improvements, which demonstrates        + IRI(ours)      102.2 1.0×      36.5 1.0×     85.17\nthat both MHSAs and MLPs play a critical role in enriching\n                                                             Table 7: Inference latency (Time to First Token, Time Perthe high-level representations in the middle layers. Further-\n                                                          Output Token) and the accuracy on adversarial POPE.more, we also apply interventions to the hidden states based\non IRI and observe a performance drop. We believe that hid-\nden states are the cumulative result of MHSA, MLP, and the\nstates from the previous layer. Directly intervening in these                     Conclusion\nhighly integrated and functionally specialized hidden states       In this paper, we introduce the Fine-grained Cross-modal\ncan disrupt the hierarchically constructed flow of semantic      Causal Tracing (FCCT) framework and the Intermediate\ninformation. Consequently, this approach is less effective      Representation Injection (IRI) technique to improve the\nthan precisely enhancing the individual key components: the       interpretability and performance of large vision-language\nMHSA, which is responsible for information aggregation,      models (LVLMs). Our FCCT framework provides a com-\nand the MLP, which handles representation processing.           prehensive, fine-grained causal analysis of the internal com-\n(2) Do the intermediate layers really play a crucial role?      ponents of LVLMs, uncovering key insights into the cross-\nSpecifically, when IRI’s source and target layers are both     modal aggregation and hierarchical representation forma-\nlimited in the first ten or the last ten layers, accuracy drops        tion, particularly through MHSA and MLP mechanisms.\nto 78.46% and 80.42%, respectively. These results align      Building on these insights, IRI proves to be a robust and\nwell with our FCCT findings: intermediate layers carry the       training-free inference-time method, significantly mitigat-\nstrongest causal effect for perceiving and aggregating cru-      ing object hallucinations across various LVLM architectures\ncial visual&textual object information, whereas shallow lay-      while maintaining inference speed and foundational model\ners lack sufficient crucial visual object perception and deep       capabilities. Experimental results not only demonstrate the\nlayers already focus on final output generation.                    superiority of IRI, but also validate the findings of FCCT.\n\n           Acknowledgement                       grained Multimodal In-Context Learning via Representation\n                                                              Engineering. In Second Conference on Language Modeling.Xiaocheng  Feng  and  Xiachong  Feng  are  the  co-\ncorresponding  authors  of  this  work. We  thank  the       Li, Y.; Du, Y.; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen,\nanonymous  reviewers  for  their  insightful  comments.       J.-R. 2023. Evaluating object hallucination in large vision-\nThis work was supported by the National Natural Science      language models. arXiv preprint arXiv:2305.10355.\nFoundation of China (NSFC) (grant 62522603, 62276078,                                                                      Li, Y.; Yang, J.; Shen, Z.; Han, L.; Xu, H.; and Tang, R.\nU22B2059), the Key R&D Program of Heilongjiang via                                                           2025c.  CATP: Contextually Adaptive Token Pruning for\ngrant 2022ZX01A32, and the Fundamental Research Funds                                                                      Efficient and Enhanced Multimodal In-Context Learning.\nfor the Central Universities ( XNJKKGYDJ2024013 ).                                                            arXiv preprint arXiv:2508.07871.\n                References                               Li, Y.; Yang, J.; Yun, T.; Feng, P.; Huang, J.; and Tang, R.\n                                                          2025d. Taco: Enhancing multimodal in-context learning via\nBai,  J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.;                                                                  task mapping-guided sequence configuration.  In Proceed-\nLin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-vl: A frontier                                                                ings of the 2025 Conference on Empirical Methods in Natu-\nlarge vision-language model with versatile abilities. arXiv                                                                     ral Language Processing, 736–763.\npreprint arXiv:2308.12966.\n                                                                 Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024a.  Improved\nBasu, S.; Grayson, M.; Morrison, C.; Nushi, B.; Feizi, S.;                                                                 baselines with visual instruction tuning. In Proceedings of\nand Massiceti, D. 2024. Understanding information storage                                                                 the IEEE/CVF Conference on Computer Vision and Pattern\nand transfer in multi-modal large language models.  arXiv                                                               Recognition, 26296–26306.\npreprint arXiv:2406.04236.\n                                                                 Liu, H.; Li, C.; Li, Y.; Li, B.; Zhang, Y.; Shen, S.; and Lee,\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\n                                                               Y. J. 2024b. LLaVA-NeXT: Improved reasoning, OCR, and\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024. How far are we\n                                                         world knowledge.\nto gpt-4v? closing the gap to commercial multimodal mod-\nels with open-source suites. Science China Information Sci-       Liu, Q.; Mao, J.; and Wen, J.-R. 2025. How do Large Lan-\nences, 67(12): 220101.                                     guage Models Understand Relevance? A Mechanistic Inter-\n                                                                       pretability Perspective. arXiv preprint arXiv:2504.07898.Fu, C.; Chen, P.; Shen, Y.; Qin, Y.; Zhang, M.; Lin, X.;\nQiu, Z.; Lin, W.; Yang, J.; Zheng, X.; Li, K.; Sun, X.; and       Liu, S.; Ye, H.; and Zou, J. 2024. Reducing Hallucinations in\nJi, R. 2023. MME: A Comprehensive Evaluation Bench-      Vision-Language Models via Latent Space Steering. arXiv\nmark for Multimodal Large Language Models.   ArXiv,       preprint arXiv:2410.15778.\nabs/2306.13394.                                                                 Liu, S.; Zheng, K.; and Chen, W. 2024.  Paying more at-\nGolovanevsky, M.; Rudman, W.;  Palit,  V.;  Singh,  R.;       tention to image: A training-free method for alleviating hal-\nand Eickhoff, C. 2024.  What Do VLMs NOTICE? A       lucination in lvlms. In European Conference on Computer\nMechanistic Interpretability Pipeline for Gaussian-Noise-       Vision, 125–140. Springer.\nfree Text-Image Corruption and Evaluation. arXiv preprint\n                                                           Lyu, X.; Chen, B.; Gao, L.; Song, J.; and Shen, H. T. 2024.\narXiv:2406.16320.\n                                                                  Alleviating hallucinations in large vision-language models\nHuang, Q.; Dong, X.; Zhang, P.; Wang, B.; He, C.; Wang,      through hallucination-induced optimization. arXiv preprint\nJ.; Lin, D.; Zhang, W.; and Yu, N. 2024. Opera: Alleviat-      arXiv:2405.15356.\ning hallucination in multi-modal large language models via\n                                                    Meng, K.; Bau, D.; Andonian, A.; and Belinkov, Y. 2022.over-trust penalty and retrospection-allocation. In Proceed-\n                                                            Locating and editing factual associations in gpt. Advancesings of the IEEE/CVF Conference on Computer Vision and\n                                                                     in neural information processing systems, 35: 17359–17372.Pattern Recognition, 13418–13427.\n                                                        Neo, C.; Ong, L.; Torr,  P.; Geva, M.; Krueger, D.; andHuo, J.; Yan, Y.; Hu, B.; Yue, Y.; and Hu, X. 2024. Mm-\n                                                              Barez,  F. 2024.   Towards  interpreting visual informa-neuron: Discovering neuron-level domain-specific interpre-\n                                                                     tion processing in vision-language models. arXiv preprinttation in multimodal large language model. arXiv preprint\n                                                            arXiv:2410.07149.arXiv:2406.11193.\nLeng, S.; Zhang, H.; Chen, G.; Li, X.; Lu, S.; Miao, C.;        Palit, V.; Pandey, R.; Arora, A.; and Liang, P. P. 2023. To-\nand Bing, L. 2024. Mitigating object hallucinations in large      wards vision-language mechanistic interpretability: A causal\nvision-language models through visual contrastive decod-       tracing tool for blip. In Proceedings of the IEEE/CVF Inter-\ning. In Proceedings of the IEEE/CVF Conference on Com-       national Conference on Computer Vision, 2856–2861.\nputer Vision and Pattern Recognition, 13872–13882.            Rohrbach, A.; Hendricks, L. A.; Burns, K.; Darrell, T.; and\nLi, Q.; Ye, Z.; Feng, X.; Zhong, W.; Qin, L.; Chen, R.; Li,      Saenko, K. 2018. Object hallucination in image captioning.\nB.; Jiang, K.; Wang, Y.; Liu, T.; et al. 2025a. CAI: Caption-      arXiv preprint arXiv:1809.02156.\nSensitive Attention Intervention for Mitigating Object Hal-       Salin, E.; Farah, B.; Ayache, S.; and Favre, B. 2022. Are\nlucination in Large Vision-Language Models. arXiv preprint      vision-language transformers learning multimodal represen-\narXiv:2506.23590.                                                 tations? a probing perspective. In Proceedings of the AAAI\nLi, Y.; Cao, Y.; He, H.; Cheng, Q.; Fu, X.; Xiao, X.; Wang,      Conference on Artificial Intelligence, volume 36, 11248–\nT.; and Tang, R. 2025b. M²IV: Towards Efficient and Fine-      11257.\n\nSun, Z.; Shen, S.; Cao, S.; Liu, H.; Li, C.; Shen, Y.; Gan,\nC.; Gui, L.-Y.; Wang, Y.-X.; Yang, Y.; et al. 2023. Align-\ning large multimodal models with factually augmented rlhf.\narXiv preprint arXiv:2309.14525.\nTang, K.; You, J.; Ge, X.; Li, H.; Guo, Y.; and Huang, X.\n2025.  Mitigating Hallucinations via Inter-Layer Consis-\ntency Aggregation in Large Vision-Language Models. arXiv\npreprint arXiv:2505.12343.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen,\nK.; Liu, X.; Wang, J.; Ge, W.; et al. 2024. Qwen2-vl: En-\nhancing vision-language model’s perception of the world at\nany resolution. arXiv preprint arXiv:2409.12191.\nYe, Z.; Li, Q.; Feng, X.; Qin, L.; Huang, Y.; Li, B.; Jiang,\nK.; Xiang, Y.; Zhang, Z.; Lu, Y.; et al. 2025. CLAIM: Mit-\nigating Multilingual Object Hallucination in Large Vision-\nLanguage Models with Cross-Lingual Attention Interven-\ntion. arXiv preprint arXiv:2506.11073.\nYou, H.; Zhang, H.; Gan, Z.; Du, X.; Zhang, B.; Wang, Z.;\nCao, L.; Chang, S.-F.; and Yang, Y. 2023. Ferret: Refer and\nground anything anywhere at any granularity. arXiv preprint\narXiv:2310.07704.\nYu, T.; Zhang, H.; Yao, Y.; Dang, Y.; Chen, D.; Lu, X.; Cui,\nG.; He, T.; Liu, Z.; Chua, T.-S.; et al. 2024. Rlaif-v: Align-\ning mllms through open-source ai feedback for super gpt-4v\ntrustworthiness. arXiv preprint arXiv:2405.17220.\nZhang, Y.; Shen, G.; Ning, K.; Ren, T.; Qiu, X.; Wang,\nM.; and Kong, X. 2025. Improving Region Representation\nLearning from Urban Imagery with Noisy Long-Caption Su-\npervision. arXiv preprint arXiv:2511.07062.\nZhong, W.; Feng, X.; Zhao, L.; Li, Q.; Huang, L.; Gu, Y.;\nMa, W.; Xu, Y.; and Qin, B. 2024. Investigating and Miti-\ngating the Multimodal Hallucination Snowballing in Large\nVision-Language Models. arXiv:2407.00569.\n\n## Claim to Classify\n\"FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Fine-grained Cross-modal Causal Tracing (FCCT)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is a variant of activation patching (causal tracing) which establishes causal sufficiency (Rung 2). However, the claim uses mechanistic and functional language ('storage and transfer', 'hierarchical progression') to describe how the model represents information internally. According to the codebook decision tree for 'storage/encoding', this constitutes a Rung 3 claim because it describes the underlying mechanism rather than just the result of the intervention."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 18419,
      "completion_tokens": 158,
      "total_tokens": 18577
    },
    "finish_reason": "stop"
  }
}