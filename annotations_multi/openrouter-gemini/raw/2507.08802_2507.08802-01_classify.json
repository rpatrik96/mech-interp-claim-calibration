{
  "timestamp": "2026-02-19T09:33:35.867330+00:00",
  "paper_id": "2507.08802",
  "claim_id": "2507.08802-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2507.08802\nTitle: The Non-Linear Representation Dilemma: Causal Abstraction Limits\nFull text:\n           The Non-Linear Representation Dilemma: Is Causal\n            Abstraction Enough for Mechanistic Interpretability?\n\n\n\n\n                            Denis Sutter,    Julian Minder,   Thomas Hofmann,   Tiago Pimentel\n                                       ETH Zürich     EPFL\n                              densutter@ethz.ch,  julian.minder@epfl.ch, {thomas.hofmann, tiago.pimentel}@inf.ethz.ch\n                                                 densutter/non-linear-representation-dilemma\n\n                                                 Abstract2025\n                         The concept of causal abstraction got recently popularised to demystify the\n                            opaque decision-making processes of machine learning models; in short, a neural\n                            network can be abstracted as a higher-level algorithm if there exists a functionNov\n                           which allows us to map between them.  Notably, most interpretability papers\n                            implement these maps as linear functions, motivated by the linear representation12                               hypothesis: the idea that features are encoded linearly in a model’s representations.\n                           However, this linearity constraint is not required by the definition of causal\n                                   abstraction. In this work, we critically examine the concept of causal abstraction by\n                                considering arbitrarily powerful alignment maps. In particular, we prove that under\n                               reasonable assumptions, any neural network can be mapped to any algorithm,\n                               rendering this unrestricted notion of causal abstraction trivial and uninformative.[cs.LG]                We complement these theoretical findings with empirical evidence, demonstrating\n                                   that it is possible to perfectly map models to algorithms even when these models\n                                 are incapable of solving the actual task; e.g., on an experiment using randomly ini-\n                                      tialised language models, our alignment maps reach 100% interchange-intervention\n                              accuracy on the indirect object identification task. This raises the non-linear rep-\n                               resentation dilemma: if we lift the linearity constraint imposed to alignment maps\n                                  in causal abstraction analyses, we are left with no principled way to balance the\n                                inherent trade-off between these maps’ complexity and accuracy. Together, these\n                                     results suggest an answer to our title’s question: causal abstraction is not enough for\n                               mechanistic interpretability, as it becomes vacuous without assumptions about how\n                           models encode information. Studying the connection between this information-\n                             encoding assumption and causal abstraction should lead to exciting future work.\n\n                1  Introduction\n\n                  The increasing popularity of machine learning (ML) models has led to a surge in their deployment\n                       across various industries. However, the lack of interpretability in these models raises significantarXiv:2507.08802v2\n                       concerns, particularly in high-stakes applications where understanding the decision-making process\n                             is crucial (Goodman and Flaxman, 2017; Tonekaboni et al., 2019; Zhu et al., 2020; Gao and Guan,\n                       2023). Unsurprisingly, this opacity has motivated a multitude of research on mechanistic (or causal)\n                            interpretability, which tries to analyse and understand the hidden algorithms that underlie these models\n                      (Olah et al., 2020; Elhage et al., 2021; Mueller et al., 2024; Ferrando et al., 2024; Sharkey et al., 2025).\n\n             A promising approach to address this challenge is causal abstraction (Beckers and Halpern, 2019;\n                      Geiger et al., 2024a), which tries to map the behaviour of a model to a higher-level (and conceptually\n                        simpler) algorithm which solves the task. At the core of this concept is the idea that if an intervention\n                             is found to change a model’s behaviour in a way that aligns with a specific algorithm, then that\n                       algorithm can be considered implemented by the model. Recent research, however, has raised consid-\n                        erable issues with this approach (e.g., Makelov et al., 2024; Mueller, 2024; Sun et al., 2025). Among\n                         those, Méloux et al. (2025) notes that a model’s causal abstraction is not necessarily unique, showing\n                          that many algorithms can be aligned to the same neural network. Additionally, most work on causal\n\n\n                        39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n\n      Neural True/False Classifier                                                                 ψηΦ 2                  ψ 2\n                                                                                         Φ         Acc.                                     ψ 2\n                      O\n                         ψ 1   ψ 2                                                                                                                                                                                     Predict                                         Identity Φ-1                           IdentityLinear       0.80.9                                                                                                                                                          Predict   X                              Φ\n                                                                           ψη 1                 ψ 1      Non-linear   1.0\n                                             ψ 1                  Φ\n                                                   Accuracy = 1.0              ψη 2                  ψ 2\n                   Causal\n                     Abstraction                                                                              Linear Φ-1\n\n     Hierarchical Equality Algorithm                                                 ψηΦ 1                 ψ 1\n            (a==b)==(c==d)                                              ψηΦ 2                  ψ 2\n                                                                                                ψ 1   ψ 2\n      η1 : a==b                         η2 : c==d                                                                        Non-linear Φ-1         <\n         <a  b  c  d                                                                          Φ\n                                                                           ψη 1                 ψ 1\n\n       Datapoints:                         Dataset:                                                                        Interventions:\n         Datapoint with label True           = (a=1,b=2,c=2,d=4)   = (a=9,b=9,c=7,d=7)   = (a=1,b=1,c=0,d=5)          = ([  ]ψη1Φ , [  ]ψη1Φ )   = ([  ]ψη1Φ , [  ]ψη1Φ )\n         Datapoint with label False          = (a=6,b=6,c=3,d=3)   = (a=5,b=8,c=5,d=5)   = (a=4,b=0,c=2,d=2)          = ([  ]ψη1Φ , [  ]ψη1Φ )   = ([  ]ψη1Φ , [  ]ψη1Φ )\nFigure 1: A visualisation of what happens when analysing causal abstractions with increasingly\ncomplex alignment maps ϕ. The more complex ϕ is, the higher the intervention accuracy—and,\nconsequently, the stronger the algorithm–DNN alignment. In Theorem 1, we show that given arbitrar-\nily complex alignment maps, we can always find a perfect alignment (under reasonable assumptions).\n\nabstraction (Wu et al., 2023; Geiger et al., 2024b; Minder et al., 2025; Sun et al., 2025) implicitly\nassumes information is linearly encoded in models’ representations, relying on the linear representa-\ntion hypothesis (Alain and Bengio, 2016; Bolukbasi et al., 2016). Linearity, however, is not required\nby the definition of causal abstraction (Beckers and Halpern, 2019) and increasing evidence suggests\nthat not all representations may be linearly encoded (White et al., 2021; Olah and Jermyn, 2024;\nMueller, 2024; Csordás et al., 2024; Engels et al., 2025a,b; Kantamneni and Tegmark, 2025).\n\nIn this paper, we first prove that, once we drop the linearity constraint, any model can be per-\nfectly mapped to any algorithm under relatively weak assumptions—e.g., hidden activation’s input-\ninjectivity and output-surjectivity, which we will define formally. This renders causal abstraction\nvacuous when used without constraints. If we restrict alignment maps to only consider, e.g., linear\nfunctions, this problem does not arise though. It follows that causal abstraction implicitly relies on\nstrong assumptions about how features are encoded in deep neural networks (DNNs), and becomes\ntrivial without such assumptions. This puts us at an impasse: we may want to rely on stronger notions\nof causal abstraction which may leverage non-linearly encoded information, but this may make our\nanalyses vacuous; we call this the non-linear representation dilemma (schematised in Fig. 1).\n\nTo empirically validate our theoretical results, we reproduce the original distributed alignment search\n(DAS) experiments (Geiger et al., 2024b), but while leveraging more complex alignment maps.\nWe find that key empirical patterns they observed—such as the first layer being easier to map to\nthe tested algorithms in a hierarchical equality task—vanish when we use more powerful maps.\nAdditionally, we find that we can achieve over 80% interchange intervention accuracy (IIA) using\nnon-linear alignment maps in randomly initialised models. Extending our experiments to language\nmodels from the Pythia suite (Biderman et al., 2023), we show that near-perfect maps can be found\nfor randomly initialised models in the indirect object identification (IOI) task (Wang et al., 2023);\nnotably, as training progresses, the complexity of the alignment maps needed to achieve perfect IIA\nin this task decreases. Overall, our results show that causal abstraction, while promising in theory,\nsuffers from a fundamental limitation: without a priori constraints on the used alignment maps, it\nbecomes vacuous as a method for understanding neural networks.\n\n2  Background\n\nIn this section, we formally define algorithms (§2.1) and deep neural networks (§2.2). These will\nthen be used to define a causal abstraction (§3). First, we formalise a task as a function T : X →Y,\nwhere x ∈X represents a set of input features and y ∈Y denotes the corresponding output.\n\n2.1  Algorithms\n\nGiven a task T, we may hypothesise different ways it can be solved. We term each such hypothesis\nan algorithm1 A, which we represent as a deterministic causal model—a directed acyclic graph that\n\n   1“Algorithm” here need not match a formal definition as, e.g., the considered functions may be uncomputable.\n\n\n                                       2\n\nimplements a function f A : X →Y.2 These causal models have a set of nodes ηall which can be\ndecomposed into three disjoint sets: (i) input nodes ηx representing elements in x, (ii) output nodes\nηy representing elements in y, and (iii) inner nodes ηinner representing intermediate variables used in\nthe computation of f A. As we focus on acyclic causal models, the edges in this graph induce a partial\nordering on nodes ηx ≺ηinner ≺ηy. Let vη denote the value held by node η, and let vη denote\nvalues taken by the set of nodes η. The set of incoming edges to a node η represent a direct causal\nrelationship between that node and its parents parA(η), denoted: vη = f ηA(vparA(η)). We can compute\nalgorithm f A by iteratively solving the value of its nodes while respecting their partial ordering:\n            vηx def= x,         ∀     vη = f ηA(vparA(η)),      f A(x) def= vηy             (1)\n                                 η∈ηinner∪ηy\nwhere we define vηx to be the input value x, and take the value of the output nodes vηy as our\nalgorithm’s output. Importantly, for an algorithm A to represent a task T, its output under “normal”\noperation must be f A(x) = T(x). For an example of a task and related algorithms, see App. I.1.\n\nThese causal models, however, allow us to go beyond “normal” operations and investigate the\nbehaviour of our algorithm under counterfactual settings. We can, for instance, investigate what its\nbehaviour would be if we enforce a node η′’s value to be a constant vη′ = c, which we write as:\n    vηx = x,    vη′ = c,    η∈(ηinner∪ηy)\\{η′}vη∀     = f ηA(vparA(η)),   f A(x, (vη′ ←c)) = vηy   (2)\nNow, let f :ηA (x) represent a function which runs our algorithm with input x until  it reaches\nnode η, outputting its value vη. We can use such interventions to investigate the behaviour of\nalgorithm A under input x, when node η′ is forced to assume the value it would have under\nx′ as: f A(x, (vη′ ←f :η′A  (x′))). Now, let IA be a multi-node intervention, e.g., IA = (vη′ ←c′)\nwhere η′ = [η′, η′′] and c′ = [f :η′A  (x′), c]. We can observe how our model operates under those\ninterventions by running f A(x, IA). See App. B for a pseudo-code implementation.\n\n2.2  Deep Neural Networks\n\nDeep neural networks (DNNs) are the driving force behind recent advances in ML and can be defined\nas a sequence of functions f ℓN : Hψℓ→Hψℓ+1, where ψℓdenotes the set of neurons in layer ℓand\nHψℓis the corresponding vector space. A DNN N with L layers can be specified as follows:\n            hψ1 = f 0N(x),    hψℓ+1 = f ℓN(hψℓ),     pN(y | x) = f LN (hψL)              (3)\nwhere hψℓdenotes the vector of activations for neurons ψℓ. We focus on DNNs with real-valued\nneurons and probabilistic outputs, so that Hψ0 = X, Hψℓ= R|ψℓ| and HψL+1 = ∆|Y|−1. We define ψ′\nf  N (x′) as the function that computes the activations of the subset of neurons ψ′ when the network\nis evaluated on input x′. In particular, f ψℓN (x) returns the activations at layer ℓfor input x. Thus, the\nstandard computation of the DNN corresponds to evaluating pN(y | x) = f ψL+1N    (x). This formulation\nallows us to instantiate common architectures, such as multi-layer perceptrons (MLPs) or transformers,\nby specifying the form of each f ℓN and the structure of the neuron sets ψℓ. The parameters of these\nmodels are typically optimised to minimise the cross-entropy loss.\n\nNotably, similarly to the algorithms above, a DNN’s architecture induces a partial ordering on its\nneurons, respecting the order in which they are computed ψ0 ≺ψℓ≺ψL. We can thus analogously\ndefine a DNN intervention as follows: given a set of neurons ψ in the network and a corresponding\nset of values cψ, we denote the intervention by f ψL+1N    x, (hψ ←cψ)  . This notation means that,\nduring the forward computation of the DNN, the activations of the neurons in ψ are fixed to the\nspecified values cψ, while the rest of the network operates as usual.\n\n3  Causal Abstraction\n\nTo define causal abstraction, we will base ourselves on the definitions in Beckers and Halpern\n(2019) and Geiger et al. (2024a). Let an abstraction map be defined as τ  : H →N , where H\nand N are, respectively, the Cartesian products of the hidden state-spaces in a neural network N (i.e.,\nψint def= ψ1:), and the node value-spaces in an algorithm A (i.e., ηint def= ηinner ∪ηy), both excluding\nthe inputs ψ0 and ηx. In words, an abstraction map translates the inner states of a neural network into\nan algorithms’ inner states. Now, consider the DNN intervention IN = hψ ←cψ and the algorithm\n\n   2Geiger et al. (2024a) also considers cyclic deterministic causal models and Beckers and Halpern (2019)\nconsiders cyclic and stochastic causal models. We leave the expansion of our work to such models for future work.\n\n\n                                       3\n\nintervention IA = vη ←cη. Further, let Hhψ=cψ be the set of states in a DNN for which hψ = cψ\nholds, and equivalently for N vη=cη. Under abstraction map τ, we can define an intervention map as:\n                        vη ←cη    if N vη=cη = {τ(h) | h ∈Hhψ=cψ}        ωτ(hψ ←cψ) =                                                                      (4)                         undefined                            else\n\nIntuitively, ωτ maps a DNN intervention IN to an algorithmic one IA if the sets of states they induce\non N and A, respectively, are the same. Further, let IA be a set of interventions IA which can be\nperformed on algorithm A. We can use ωτ to derive a set of equivalent DNN interventions as:\n             IN = {hψ ←cψ | vη ←cη ∈IA, ωτ(hψ ←cψ) = vη ←cη}               (5)\nGiven these definitions, we now put forward a first notion of causal abstraction.\n\nDefinition 1 (from Beckers and Halpern, 2019). An algorithm A is a τ-abstraction of a neural\nnetwork N iff: τ is surjective; IA = ωτ(IN);3 and there exists a surjective τηx such that:\n           ∀x∈X  : τ(f ψintN   (x, IN)) = f :ηintA   (τηx(x), IA)  where IA = ωτ(IN)             (6)\n                 IN∈IN\n\nIn words, the first condition in this definition enforces that all states in an algorithm are needed to\nabstract the DNN, while the second and third enforce that interventions in the algorithm have the\nsame effect as interventions in the DNN. We further say that A is a strong τ-abstraction of N if it is a\nτ-abstraction and IA is maximal, meaning that any intervention is allowed on algorithm A. However,\nwhile strong τ-abstractions give us a notion of equivalence between algorithms and DNNs, the maps τ\nmay be highly entangled and provide little intuition about the DNN’s behaviour. To ensure algorithmic\ninformation is disentangled in the DNN, we say τ is a constructive abstraction map if there exists a\npartition of N’s neurons {ψη | η ∈ηint} ∪{ψ⊥}—where ψη are non-empty—and there exist maps\nτη such that τ is equivalent to the block-wise application of τη(hψη). In other words, constructive ab-\nstraction maps compute the value vη of each node η ∈ηint in A using non-overlapping sets of neurons\nψη from N, with set ψ⊥being left unused. We now define a second notion of causal abstraction.\nDefinition 2 (from Beckers and Halpern, 2019). An algorithm A is a constructive abstraction of a\nneural network N iff there exists an τ: for which A is a strong τ-abstraction of N; and τ is constructive.\n\nAs we will deal with algorithm–DNN pairs which share the same input and output spaces, we\nwill impose an additional constraint on τ—one that is not present in Beckers and Halpern’s (2019)\ndefinition. Namely, we restrict: ψηx to be the neurons in layer zero and τηx to be the identity; and\nψηy to be the neurons in layer L + 1 and τηy to be the argmax operation.4\n\n3.1  Information Encoding in Neural Networks\n\nThe definition of constructive abstraction above maps non-overlapping sets of neurons in N, i.e.,\nψη, to nodes in A, i.e., η. However, much research in ML interpretability highlights that concept\ninformation is not always neuron-aligned and that neurons are often polysemantic (Olah et al., 2017,\n2020; Arora et al., 2018; Elhage et al., 2022). In fact, there is a large debate about how information is\nencoded in DNNs. We highlight what we see as the three most prominent hypotheses here.\n\nDefinition 3. The privileged bases hypothesis (Elhage et al., 2023) argues that neurons form\nprivileged bases to encode information in a neural network.\n\nMost evidence in favour of this hypothesis comes from indirect evidence:  i.e., the presence of\nneuron-aligned outlier features or activations in DNNs (Kovaleva et al., 2021; Elhage et al., 2023;\nHe et al., 2024; Sun et al., 2024). Going back to 2015, Karpathy (2015) already showed that a single\nneuron in a language model could carry meaningful information. Importantly, this hypothesis is\nconsistent with the notion of constructive abstraction above, as it argues each node η’s information\nshould be encoded in separate, non-overlapping sets of neurons. Several researchers, however,\nquestion the special status of neurons assumed by this hypothesis, assuming instead that information\nis encoded in linear subspaces of the representation space, of which neurons are only a special case.\n\nDefinition 4. The linear representation hypothesis (Alain and Bengio, 2016) argues that information\nis encoded in linear subspaces of a neural network.\n\n   3We overload function ωτ here, with ωτ(IN) simply applying ωτ elementwise to the interventions in set IN.\n   4We note that this implies algorithm A and network N must have the same outputs on the input set X.\n\n\n                                       4\n\nA large literature has developed, backed by the linear representation hypothesis, including: concept\nerasure methods (Ravfogel et al., 2020, 2022), probing methodologies (Elazar et al., 2021; Ravfogel\net al., 2021; Lasri et al., 2022), and work on disentangling activations (Yun et al., 2021; Elhage et al.,\n2022; Huben et al., 2024; Templeton et al., 2024). Some, however, still question this idea that all infor-\nmation must be encoded linearly in DNNs: as neural networks implement non-linear functions, there\nis no a priori reason for why information should be linearly encoded in them (Conneau et al., 2018; He-\nwitt and Liang, 2019; Pimentel et al., 2020b,a, 2022). Further, recent research presents strong evidence\nthat some concepts are indeed non-linearly encoded in DNNs (White et al., 2021; Pimentel et al., 2022;\nOlah and Jermyn, 2024; Csordás et al., 2024; Engels et al., 2025a,b; Kantamneni and Tegmark, 2025).\nDefinition 5. The non-linear representation hypothesis (Pimentel et al., 2020b) argues that infor-\nmation may be encoded in arbitrary non-linear subspaces of a neural network.\n\n3.2  Distributed Causal Abstractions\n\nFollowing the discussion above, the definition of constructive abstraction may be too strict, as\nit assumes τ must decompose across neurons—and thus that node information is encoded in\nnon-overlapping neurons. With this in mind, Geiger et al. (2024a,b) proposed the notion of distributed\ninterventions: they expose the subspaces where node information is encoded in a DNN N by applying\na bijective function to its hidden states; this function’s output is then itself a constructive abstraction\nof algorithm A. Here, we make this notion a bit more formal.\n\nWe define τ as a distributed abstraction map if the following two conditions hold. First, there\nexists a bijective function ϕ—termed here an alignment map—that maps the inner neurons ψint\nof N block-wise to an equal-sized set of latent variables ψϕint, in a manner that respects the partial\nordering of computations in the network.  Specifically, for each layer ℓ, there exists a bijection\nϕℓ: R|ψℓ| →R|ψℓ| on its neurons such that ϕ is defined as the concatenation of these layer-wise\nbijections. Similarly to the neurons’ activation hψ, we will denote latent variables as hψϕ = ϕ(hψ).\nSecond, there exists a partition {ψϕη | η ∈ηint} ∪{ψϕ⊥} of the resulting latent variables ψϕint—\nwhere ψϕη are non-empty—and a set of maps τη such that τ is equivalent to the block-wise application\nof τη(hψϕη). In words, a distributed abstraction map computes the value vη of each node η ∈ηint in\nA using non-overlapping partitions of latent variables ψϕη from N, with partition ψϕ⊥remaining unused.\nGiven an alignment map ϕ, we can perform distributed interventions: hψϕη ←cψϕη. These inter-\nventions are performed by first mapping the hidden state hψ to the latent variables hψϕ = ϕ(hψ),\nintervening on a subset ψϕη by replacing hψϕη with desired values cψϕη, and then mapping these\nintervened latent variables back to the original neuron base via h′ψ = ϕ−1(h′ψϕ). Thus, interventions\nare applied in the latent space defined by ϕ, generalising privileged-bases interventions to arbitrary\n(possibly non-linear) subspaces. We are now in a position to define distributed abstractions.\nDefinition 6. An algorithm A is a distributed abstraction of a neural network N iff there exists an τ:\nfor which A is a strong τ-abstraction of N; and τ is a distributed abstraction map.\n\nFinally, we note that the set of all possible interventions IA and IN may be hard to analyse in practice.\nGeiger et al. (2024b) thus restrict their analyses to what we term here input-restricted interventions:\nthe set of interventions which are themselves producible by a set of other input-restricted interventions.\nIn other words, we restrict interventions hψ′ ←cψ′ (where ψ′ ⊆ψint or ψ′ ⊆ψϕint) and vη′ ←cη′                                                                                     ψ′\nto cψ′ and cη′ which are a product of other input-restricted interventions, e.g., cψ′ = f N (x′) or          :η′                    :η′′\ncη′ = f A  (x′, (vη′′ ←f A  (x′′))). This leads to the definition of input-restricted τ-abstraction:\na weakened notion of strong τ-abstraction, where intervention sets are restricted to input-restricted\ninterventions. Finally, we define an analogous version of distributed abstraction, which is input-\nrestricted; this is the notion typically used in practice by machine learning practitioners.\nDefinition 7 (inspired by Geiger et al., 2024b). An algorithm A is an input-restricted distributed\nabstraction of a neural network N iff there exists an τ: for which A is an input-restricted τ-abstraction\nof N; and τ is a distributed abstraction map.\n\nA visual representation of how these causal abstraction definitions are related is given in App. D\nas Fig. 6. Finally, we further introduce input-restricted V-abstractions: input-restricted distributed\nabstractions for which we restrict alignment maps ϕ to be in a specific variational family V. The case\nof linear alignment maps, will be particular important here—as it relates to the linear representation\nhypothesis—and we will thus explicitly label it as input-restricted linear abstraction.\n\n\n                                       5\n\n3.3  Finding Distributed Abstractions\n\nHow do we evaluate if an algorithm is an input-restricted distributed abstraction of a DNN? Geiger\net al. (2024b) proposes an efficient method to answer this, called distributed alignment search (DAS).\nBefore applying DAS, one must assume a partitioning {ψϕη  | η ∈ηint} ∪{ψϕ⊥} which remains\nfixed during the method’s application; we term |ψϕη| the intervention size. The principle behind\nDAS is then to leverage the constraint on τηy, which is fixed as: vηy = argmaxy∈Y pN(y | x) since\npN(y | x) = hψL+1. Given this constraint, we can initialise a parametrised function ϕ, which we train\nto predict this equality under possible interventions; this is done via gradient descent, minimising\nthe cross-entropy between the DNN and the algorithm. Specifically, we first select a set of nodes\nto be intervened η ∈P(ηinner), where P is a function that takes the powerset of a set, along with\ncorresponding counterfactual inputs xη ∈X for each η ∈η and a base input x∅∈X. We then define\nthe following two interventions:\n                          ψϕη                                                  :η\n            IN = hψϕη η∈η ← f N  (xη) η∈η  and  IA = vη η∈η ← f A (xη) η∈η         (7)\nFinally, we run our algorithm under base input x∅and intervention IA to get a ground truth output:\ny = f A(x∅, IA). Repeating this process N times, we build a dataset D = {(x(n)∅, I(n)N   , y(n))}Nn=1 on\nwhich we can train the alignment map ϕ such that the DNN matches the algorithm:\n    L = − X     log pϕN (y(n) | x(n)∅, I(n)N  ),  where  pϕN (y | x∅, IN) = f N(x∅, IN)    (8)\n              (x(n)∅  ,I(n)N  ,y(n))∈D\nNotably, DAS mostly ignores how function τ is constructed, relying solely on the assumed definition\nof τηy. Finding a low-loss alignment map ϕ is then assumed as sufficient evidence that A is an\ninput-restricted distributed abstraction of N.\n\n4  Unbounded Abstractions are Vacuous\n\nIn this section, we provide our main theorem: that under reasonable assumptions, any algorithm A\ncan be shown to be an input-restricted distributed abstraction of any DNN N, making this notion of\ncausal abstraction vacuous. To show that, we need a few assumptions (for their formal definition, see\nApp. F). Our first assumption (Assump. 1) is that we have a countable input-space X. While this\nmay not hold in general, it holds for common applications such as language modelling (where the\ninput-space is the countably infinite set of finite strings) or computer vision (where the input-space\nis a countable union of pixels, which can assume a finite set of values). The second assumption\n(Assump. 2) is that DNNs are input-injective in all layers: i.e., f ψℓN   is injective for all layers. This\nguarantees that no information about a DNN’s input x is lost when computing the hidden states hψℓ.\nThis assumption is also present in prior work (e.g., Pimentel et al., 2020b) and we show in App. G—\nassuming real-valued weights and activations—that this is almost surely true for transformers at\ninitialisation.5 Due to floating point precision and neural collapse (Papyan et al., 2020), it is likely not\nto hold fully in practice; however, it still seems to be well-approximated in many empirical settings\n(Morris et al., 2023, and App. H). The third assumption (Assump. 3) is strict output-surjectivity\nin all layers. This assumption guarantees that in each layer there is at least one choice of hψℓthat\nwill produce the desired output. Notably, this assumption may not hold in theory, due to issues like\nthe softmax-bottleneck (Yang et al., 2018). In practice, however, even with large vocabulary sizes, it\nseems that almost all outputs can still be produced by language models (Grivas et al., 2022) which is\nsufficient for these DNNs to be abstracted by many algorithms. Our fourth assumption (Assump. 4)\nis that the algorithm A and DNN N have matchable partial-orderings, meaning that there is a\npartitioning of neurons in N which would match the partial-ordering of nodes in A; this is likely to\nbe the case for most reasonable algorithms given the size of state-of-the-art deep neural networks.\nFinally, our last assumption (Assump. 5) is that the DNN N solves the given task T. We believe this\nassumption to be reasonable, as it would be impractical in practice to evaluate a neural network that\ndoes not perform the task correctly.6 Given these assumptions, we can now present our main theorem.\nTheorem 1. Given any algorithm A and any neural network N such that Assumps. 1 to 5 hold, we\ncan show that A is an input-restricted distributed abstraction of N.\n\nProof. We refer to App. F for the proof.\n\n   5Also see Nikolaou et al. (2025), who show almost sure injectivity holds for transformers throughout training.\n    6If the model does not solve the task, perfect IIA is impossible since non-intervened inputs yield incorrect out-\nputs. Thus, assuming the model solves the task is necessary. In practice, however, even when the DNN is imper-\nfect, an alignment map could produce correct outputs for all intervened inputs, achieving near-perfect IIA scores.\n\n\n                                       6\n\n5  Experimental Setup\n\nBuilding on the previous section’s proof that alignment maps between DNNs and algorithms always\nexist, we now demonstrate their practical learnability and how increasingly complex alignment maps\nreveal various causal abstractions for different tasks, even on DNNs that do not solve them.\n\nAlignment Maps.  To assess how complexity impacts causal-abstraction analyses, we explore three\nways to parameterise ϕ. First, we will consider the simplest identity maps: ϕid(h) = h. This is the\nleast expressive ϕ we consider, and if we find that A abstracts N under this map, we can say that A\nis a constructive abstraction of N; further, this map implicitly assumes the privileged bases hypothesis.\nFor ϕid, we greedily search for the optimal partition {ψϕη | η ∈ηint} (instead of keeping it fixed) by\niteratively adding neurons to them. For all ηinner simultaneously, one neuron is added at a time for\neach ψϕη, up to a maximum allowed intervention size; these neurons are chosen to minimise the loss in\neq. (8). Second, we will consider linear maps: ϕlin(h) = Worthh, where Worth ∈Rdℓ×dℓis an or-\nthogonal matrix. This is the type of alignment map originally considered by Geiger et al. (2024b),7 and\nimplicitly assumes the linear representation hypothesis, evaluating input-restricted linear abstractions.\nFinally, we consider non-linear maps: ϕnonlin(h) = revnet[Lrn, drn](h), where revnet[Lrn, drn]\nis a reversible residual network (RevNet; Gomez et al., 2017) with Lrn layers and hidden size drn.\nWe can modulate the complexity of this final map by increasing Lrn and drn, assuming the non-linear\nrepresentation hypothesis. We note that all three maps are bijective and easily invertible.\n\nEvaluation Metric.  We evaluate the effectiveness of an alignment map ϕ using the interchange\nintervention accuracy (IIA) metric proposed by Geiger et al. (2024b). For a held out test set Dtest\nwith the same structure as the training set D defined in §3.3, we compute the accuracy of our model\n(i.e., argmaxy′∈Y pϕN (y′ | x(n)∅, I(n)N  )) when predicting the intervened y(n) = f A(x(n)∅, I(n)A  ). We\ncompare this to the DNN’s accuracy on the test set Dtest without interventions.\n\n5.1  Tasks, Algorithms, and DNNs.\n\nHierarchical equality task (Geiger et al. (2024b)). We will showcase our results primarily on\nthis task. Let x = x1 ◦x2 ◦x3 ◦x4 be a 16-dimensional vector, and x1 to x4 each be 4-dimensional\nvectors, where ◦represents vector concatenation. Further, let X = [−.5, .5]16. This task consists of\nevaluating: y = (x1 == x2) == (x3 == x4). As our DNN N, we investigate a 3-layer multi-layer\nperceptron (MLP) with hidden size 16, trained to perform this task; we describe this DNN, and its\ntraining procedure in more detail in App. I.1. Finally, we explore three algorithms for this task. The\nboth equality relations algorithm first computes the two equalities (vη1 = (x1 == x2) and\nvη2 = (x3 == x4)) separately; it then determines whether they are equivalent as a second step.\nThe left equality relation algorithm first computes the left equality (vη1 = (x1 == x2)),\nand then determines in a single step if this is equivalent to (x3 == x4). Finally, the identity of\nfirst argument algorithm assumes we copy the first input to a node (vη1 = x1) and then compute\nthe output directly. These three algorithms are more rigorously defined in App. I.1.\n\nIndirect object identification (IOI) task.  In a second set of experiments, we explore this task,\ninspired by Wang et al. (2023) and using the dataset of Muhia (2022). This task is more real-\nistic and relies on larger (language) models.  Here, inputs x ∈X are strings where two peo-\nple are first introduced, and later one of them assumes the role of subject (S), giving or say-\ning something to the other, the indirect object (IO). The task is then to predict the first token\nof the IO, with the output set Y containing the first token of each person’s name.  E.g., x =\n“Friends Juana and Kristi found a mango at the bar. Kristi gave it to” and y=“Juana”.\nAs our DNN, we use models from the Pythia suite (Biderman et al., 2023) across different sizes (from\n31M to 410M parameters) and training stages. We evaluate the ABAB-ABBA algorithm where, given\ntwo names A and B, an inner node vη1 captures if the sentence structure is ABAB (e.g., “A and B ...\nA gave to B”) or ABBA (e.g., “A and B ... B gave to A”), and the algorithm outputs prediction B if\nvη1 is ABAB and A otherwise. This algorithm is more rigorously defined in App. I.2.\n\n6  Experiments and Results\n\nWe now proceed with our empirical study, applying alignment maps of varying complexity on both\n“toy” and real neural networks to evaluate their effects on the causal abstraction method DAS.\n\n   7We note that, while Geiger et al. (2024b) describe the used ϕ as a rotation, their pyvene (Wu et al., 2024a)\nimplementation uses orthogonal matrices. This, however, makes no difference in the power of the alignment map.\n\n\n                                        7\n\n                Both Equality Relations                           Left Equality Relation                           Identity of First Argument\n     1.00\n                                                                                                                                     linear                 identity\n     0.75                                                                                                                  non-linear\n\n     0.50         Accuracy\n     0.25\n\n\n            1   2   8    1   2   8    1   2   8           1   2   8    1   2   8    1   2   8           1   2   8    1   2   8    1   2   8\n             L1         L2         L3               L1         L2         L3               L1         L2         L3\n\nFigure 2: IIA in the hierarchical equality task for causal abstractions trained with different alignment\nmaps ϕ. The figure shows results for all three analysed algorithms for this task. The bars represent\nthe max IIA across 10 runs with different random seeds. The black lines represent mean IIA with\n95% confidence intervals. The |ψϕη| denotes the intervention size per node. Without interventions, all\nDNNs reach almost perfect accuracy (>0.99). The used ϕnonlin uses Lrn = 10 and drn = 16.\n\n       1.0                                                              1.0\n\n                                                                        0.9\n\n       0.8                                                              0.8                            DNN\n                                                                                                                                                                linear\n                                                                        0.7                                             Lrn = 1, drn = 24               Accuracy                                     = 1                            Accuracy                                                 Lrn = 5, drn = 24\n                                          = 2            0.6                                             Lrn = 10, drn = 24\n       0.6                                   = 8                                                               Lrn = 10, drn = 27\n                                                                        0.5\n           1  2     4          8                     16             0             1M                     Full\n                                     drn                              DNN Training Steps\nFigure 3: IIA of alignment between the both equality relations algorithm and an MLP, with\ninterventions at layer 1. Left: Mean IIA over 5 seeds using ϕnonlin (Lrn = 1) on the trained DNN.\nPerformance improves with larger hidden dimension drn and intervention size |ψϕη|. Right: Maximum\nIIA across 5 seeds using ϕlin and ϕnonlin with |ψϕη| = 8. Complex alignment maps achieve high IIA\neven with randomly initialised DNNs, while simpler maps gradually improve as training progresses.\n\nHierarchical equality task, main results.8   Fig. 2 presents IIA results across different alignment\nmaps ϕ for all three algorithms. As expected, the identity map ϕid generally results in the worst per-\nformance. Using linear alignments (ϕlin), we observe patterns consistent with Geiger et al. (2024b):\nIIA for both equality relations and left equality relation decreases substantially in\nthe third layer, indicating information becomes difficult to manipulate using linear transformations\nat deeper layers. With the non-linear alignment (ϕnonlin), this layer-dependent degradation vanishes,\nyielding near-optimal IIA across all layers. Consequently, while assuming linear representations\nseems to enable us to identify the location of certain variables in our DNN, many of these insights\nfail to generalise when more powerful non-linear alignment maps are employed. The identity\nof first argument algorithm’s IIA consistently hovers around 50% for ϕid, ϕlin and ϕnonlin.\nAdditional experiments (App. H) suggest this is caused by insufficient capacity of the used revnet\nmodel, as the identity of x1 seems to be encoded in the model’s hidden states.\n\nHierarchical equality task, exploring ϕnonlin’s complexity.   Fig. 3 (left) illustrates how varying\nthe hidden size drn and intervention size |ψϕη| affects IIA with the both equality relations\nalgorithm on layer 1 of our MLP. Fig. 3 (right) shows IIA evolution as alignment complexity increases\nthroughout the MLP’s training (evaluated on its layer 1). Remarkably, even with randomly initialised\nDNNs, we achieve over 80% IIA using the most complex alignment map. As training progresses,\nsimpler alignment maps gradually attain higher IIA values. Additional results in App. I.1.3 extend\nthese findings to other MLP layers, intervention sizes, and algorithms, consistently revealing similar\npatterns that reinforce our conclusion about the impact of alignment map complexity on IIA dynamics.\n\nIndirect object identification task, main results.   Fig. 4 (left) presents the results of trying to find\ncausal abstractions between the ABAB-ABBA algorithm and Pythia language models, exploring how\nmodel size affects alignment capabilities. Notably, despite only the larger models (160M and 410M\nparameters) successfully learning the IOI task, we can align the algorithm to models of all sizes—\nincluding the 31M and 70M parameter models that fail to learn the task. Further, and somewhat\nsurprisingly, this alignment is perfect even for randomly initialised models across all sizes; smaller\nfully trained models (31M, 70M), though, show slightly reduced alignment accuracy. This reduction\n\n   8As an additional task similar to hierarchical equality, we also explore the distributive law task in App. I.3.\n\n\n                                       8\n\n      1.0                                                        1.00\n\n\n                                                                                      DNN\n                                                                0.75                                                            linear\n                                                                                                                                        Lrn = 1          Accuracy 0.5                                                                                                                                                                              Accuracy                                                        Lrn = 2\n              DNN (Full)       non-lin. (Full)        lin. (Full)                                                                      Lrn = 4\n              DNN (Init.)      non-lin. (Init.)        lin. (Init.)           0.50                                                   Lrn = 8\n\n         31m(L3)        70m(L3)       160m(L6)       410m(L12)            0    1k    2k    3k    4k               Full\n                DNN Size                           DNN Training Step\n\nFigure 4: IIA of alignment between ABAB-ABBA algorithm and Pythia language models. Left: IIA\nacross model sizes at initialisation (Init.) or after full training (Full), with intervention at the middle\nlayer. Right: IIA with increasingly complex alignment maps during Pythia-410m’s training. Results\nshow complex alignment maps yield near-perfect IIA. All ϕnonlin use drn = 64.\n\nmay stem from these smaller models saturating late in training (Godey et al., 2024), becoming highly\nanisotropic and making it harder for ϕnonlin to access the information needed to match the algorithm.\n\nIndirect object identification task, exploring ϕnonlin’s complexity.  Fig. 4 (right) illustrates the\ninterplay between model training progression and algorithmic alignment for the Pythia with 410M\nparameters. Notably, while this model begins to acquire task proficiency only around training step\n3000 (as indicated by model accuracy), employing an 8-layer ϕnonlin as alignment map yields near-\nperfect IIA across all training steps, including for randomly initialised models. This pattern partially\nextends to a 4-layers ϕnonlin configuration; however, there is a noticeable dip in IIA at step 1000 for\nthis configuration, which may be due to the model over-fitting to unigram statistics (Chang and Bergen,\n2022; Belrose et al., 2024) at this point—thereby making context (and hidden states) be mostly ignored\nwhen producing model outputs. Interestingly, as training advances, even less complex alignment maps\n(1- and 2-layer ϕnonlin) eventually attain perfect alignment. In contrast, linear maps only approximate\nperfect alignment in the fully trained model, following a similar trend to the DNN’s performance.\n\n7  Discussion\n\nOur results show that when we lift the assumption of linear representations, sufficiently complex\nalignment maps can achieve near-perfect alignment across all models—regardless of their ability\nto solve the underlying task. This provides compelling evidence for the non-linear representation\ndilemma, suggesting causal alignment may be possible even when the model lacks task capability.\nWe now discuss our results in the context of prior literature, with additional related work in App. C.\n\nCausal Abstraction is not Enough.  Causal abstraction (Geiger et al., 2024a) has gained traction as\na theoretical framework for mechanistic interpretability, promising to overcome probing limitations\nby analysing DNN behaviour through interventions: if you intervene on a DNN’s representations\nand its behaviour changes in a predictable way, you have identified how the DNN “truly” encodes\nthat feature (Elazar et al., 2021; Ravfogel et al., 2021; Lasri et al., 2022). Recent critiques of causal\nabstraction (e.g., Mueller, 2024) highlight practical shortcomings, including the non-uniqueness\nof identified algorithms (Méloux et al., 2025) and the risk of “interpretability illusions” (Makelov\net al., 2024). Despite counterarguments to some of these critiques (Wu et al., 2024b; Jørgensen\net al., 2025), concerns have emerged that methods based on causal abstraction may introduce new\ninformation rather than accurately reflect the behaviour of the DNN (Wu et al., 2023; Sun et al.,\n2025); as an example, causal abstraction methods applied to random models sometimes yield above-\nchance performance (Geiger et al., 2024b; Arora et al., 2024). By examining the implications\nof assuming arbitrary complex ways in which features may be encoded in a DNN, we show that\nnearly any neural network can be aligned to any algorithm.  Together, our results thus suggest\nthat the shift in interpretability research to causal abstractions does not, by itself, resolve the core\nchallenge of understanding how representations are encoded. Additionally, we note that early causal\nabstraction methods (Geiger et al., 2021) implicitly rely on the privileged bases hypothesis, while\nrecent advancements (Geiger et al., 2024b) rely on the linear representation hypothesis instead.\n\nBalancing the Accuracy vs. Complexity of ϕ.  Diagnostic probing was a previously popular\nmethod for interpretability research (Alain and Bengio, 2016), where a probe was applied to the\nhidden representations of a DNN and trained to predict a specific variable. Notably, the architecture\nchosen for this probe implicitly reflected assumptions about representation encoding, and the absence\nof a universally accepted model for representation encoding precluded a theoretically founded\n\n\n                                       9\n\nchoice of probe architecture (Belinkov, 2022). The debate regarding the trade-off between probing\ncomplexity and accuracy (Hewitt and Liang, 2019; Pimentel et al., 2020b,a; Voita and Titov, 2020)\nunderscores the risk of complex probes merely memorising variable-specific relations, instead of\nrevealing which information the DNN “truly” encodes and uses. In this paper, we revive this debate\nby showing a clear analogue in causal abstraction methodologies: the effect of ϕ’s complexity on\nIIA. Unfortunately, this debate was never solved by the probing literature, and solutions ranged from:\ncontrolling for the probe’s memorisation capacity (Hewitt and Liang, 2019),9 explicitly measuring a\nprobe’s complexity accuracy trade-off (Pimentel et al., 2020a), training minimum description length\nprobes (Voita and Titov, 2020), or leveraging unsupervised probes (Burns et al., 2023).10\n\nThe Role of Generalisation. We now highlight that Theorem 1 provides an existence proof for\na perfect abstraction map (thus guaranteeing perfect IIA) between a DNN and an algorithm. This\nexistence proof, however, leverages complex interactions between the intervened hidden states and\nthe DNN’s structure, requiring perfect information about both and thus representing a form of extreme\noverfitting. Crucially, this theorem offers no guarantees regarding the learnability of the alignment\nmap ϕ from limited data or its generalisation to unseen inputs. This gap between theoretical existence\nand practical learnability becomes evident in practise. For instance, in an additional experiment on\nthe IOI task (in App. I.2.3), we show that when training and test sets contain disjoint sets of names,\nthe learned alignment map fails to generalise, resulting in low IIA on the test set. This suggests that\ngeneralisation should play a crucial role in causal abstraction analysis, as the ability to learn abstraction\nmaps that transfer beyond training data seems fundamental to interpreting a model, distinguishing\na genuine understanding about its inner workings from mere training pattern memorisation.\n\nInvestigating Representation Encoding in DNNs.  How neural networks encode variables/concepts\nis a long-standing question in interpretability, with three main hypotheses standing out: the privileged\nbases, linear representation, and non-linear representation hypotheses (see §3.1). One way to try to\ndistinguish between these hypotheses is with causal abstraction analyses, but what can we learn about\nthese hypotheses if our methods themselves rely on them as assumptions? One solution could be\nto compare results using ϕ with different architectures. Our Fig. 4 (right), for instance, shows that\nwhile ϕnonlin achieves consistently near-perfect results throughout model training, ϕlin accompanies\nthe actual DNN’s performance more closely. Intuitively, we may thus be inclined to support the\nlinear representation hypothesis here. We (the authors), however, cannot make this intuition formal to\njustify why we believe this is the case. Furthermore, ϕlin still manages to sometimes achieve IIA\nhigher than the DNN’s accuracy, implying it may also “learn the task”. We expect future work will\npropose novel methodologies to analyse information encoding and try to answer these questions.\n\n8  Conclusion\n\nThis paper critically examines causal abstraction in machine learning, when no assumptions are\nimposed on how representations are encoded. We show that, under mild conditions, any algorithm\ncan be perfectly aligned with any DNN, leading to the non-linear representation dilemma. Empirical\nvalidation through experiments on the hierarchical equality and the indirect object identification tasks\ncorroborate our theoretical insights, demonstrating near perfect IIA even in randomly initialised\nDNNs. So, what should you do if you want to perform a causal analysis of your DNN? We believe\nthat it must be decided on a case-by-case basis. If you have reason to believe the linear representation\nhypothesis holds for the features you wish to extract, constraining ϕ to linear functions may be\nadvised. If you do not, however, you may face the non-linear representation dilemma, and be forced\nto investigate some kind of trade-off between ϕ’s accuracy and complexity.\n\nLimitations.  Our proof that any algorithm can be aligned with any DNN (Theorem 1) relies on a\nform of overfitting. Yet, our experiments show that the learned alignment maps ϕ generalise to unseen\ntest data; studying the factors behind this generalisation would be valuable. Further, our theorem relies\non two strong assumptions: input-injectivity (Assump. 2) and strict output-surjectivity (Assump. 3) in\nall layers. While we justify both, there are settings—related to, e.g., the softmax bottleneck—where\nthey may fail; studying these failure modes could clarify our assumptions’ limitations.\n\n   9Notably, this method was previously applied to causal abstraction analysis by Arora et al. (2024).\n  10The complexity—accuracy trade-off in probing arises mainly in supervised settings, where more complex\nprobes can extract richer features from model representations. Unsupervised probing avoids this, lacking the\nsupervision that enables such “gerrymandered” mappings.\n\n\n                                       10\n\nContributions\n\nDenis Sutter led the project, implemented the base version of the DAS code, conducted the MLP\nexperiments and derived the base proof of Theorem 1 as well as the proof of Theorem 2. Julian\nMinder implemented and ran the language model experiments, produced all plots, and helped refine\nthe proof of Theorem 2. Thomas Hofmann provided guidance throughout the project. Tiago Pimentel\nsupervised the project, giving initial intuitions for the proofs in both Theorem 1 and Theorem 2,\nrefining the proof of Theorem 1, and defining the main notation in the paper, integrating feedback\nfrom Denis and Julian. All authors wrote the paper together.\n\nAcknowledgments\n\nThis work was mostly done in the Data Analytics Lab at ETH Zürich. We would like to thank Pietro\nLesci, Julius Cheng, Marius Mosbach, Chris Potts, and Atticus Geiger for their thoughtful feedback.\nWe would also like to thank Frederik Hytting Jørgensen for bringing to our attention a mistake in our\noriginal Definition 1 and for his feedback on our manuscript. We thank Zhengxuan Wu and Kevin\nDu for early discussions related to the ideas presented here. We are grateful to the Data Analytics\nLab at ETH for providing access to their computing cluster. Julian Minder is supported by the ML\nAlignment Theory Scholars (MATS) program. Denis Sutter gratefully acknowledges the financial\nsupport of his parents, Renate and Wendelin Sutter, throughout his graduate studies, during which\nthis work was carried out, as well as the technical support of Urban Moser and Leo Schefer.\n\nReferences\n\nGuillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier\n   probes. arXiv.\n\nAryaman Arora, Dan Jurafsky, and Christopher Potts. 2024. CausalGym: Benchmarking causal\n   interpretability methods on linguistic tasks. In Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages 14638–14663, Bangkok,\n  Thailand. Association for Computational Linguistics.\n\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. Linear algebraic\n   structure of word senses, with applications to polysemy. Transactions of the Association for\n  Computational Linguistics, 6:483–495.\n\nSander Beckers and Joseph Y. Halpern. 2019. Abstracting causal models. Proceedings of the AAAI\n  Conference on Artificial Intelligence, 33(01):2678–2685.\n\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational\n   Linguistics, 48(1):207–219.\n\nNora Belrose, Quintin Pope, Lucia Quirke, Alex Mallen, and Xiaoli Fern. 2024. Neural networks\n   learn statistics of increasing complexity. In Proceedings of the 41st International Conference on\n  Machine Learning, ICML’24. JMLR.org.\n\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,\n  Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff,\n  Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: A suite for analyzing\n   large language models across training and scaling.  In Proceedings of the 40th International\n  Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\n  pages 2397–2430.\n\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is\n   to computer programmer as woman is to homemaker? Debiasing word embeddings. In Advances\n   in Neural Information Processing Systems, volume 29.\n\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2023. Discovering latent knowledge\n   in language models without supervision. In The Eleventh International Conference on Learning\n  Representations.\n\nTyler A. Chang and Benjamin K. Bergen. 2022. Word acquisition in neural language models.\n  Transactions of the Association for Computational Linguistics, 10:1–16.\n\n\n                                       11\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n  Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n  Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\n  Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\n  James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\n  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra,\n  Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\n  Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\n  Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\n  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\n  Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\n   Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling language modeling with pathways.\n   J. Mach. Learn. Res., 24(1).\n\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018.\n  What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic\n   properties. In Proceedings of the 56th Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pages 2126–2136, Melbourne, Australia. Association for\n  Computational Linguistics.\n\nRóbert Csordás, Christopher Potts, Christopher D Manning, and Atticus Geiger. 2024. Recurrent\n  neural networks learn to store and generate sequences using non-linear representations. In Pro-\n  ceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,\n  pages 248–262, Miami, Florida, US. Association for Computational Linguistics.\n\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic probing: Behavioral\n  explanation with amnesic counterfactuals. Transactions of the Association for Computational\n   Linguistics, 9:160–175.\n\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\n  Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\n  Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of\n  superposition. Transformer Circuits Thread.\n\nNelson Elhage, Robert Lasenby, and Christopher Olah. 2023. Privileged bases in the transformer\n   residual stream. Transformer Circuits Thread, page 24.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda\n  Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac\n  Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse,\n  Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A\n  mathematical framework for transformer circuits. Transformer Circuits Thread.\n\nJoshua Engels, Eric J Michaud, Isaac Liao, Wes Gurnee, and Max Tegmark. 2025a. Not all language\n  model features are one-dimensionally linear.  In The Thirteenth International Conference on\n  Learning Representations.\n\nJoshua Engels, Logan Riggs Smith, and Max Tegmark. 2025b. Decomposing the dark matter of\n  sparse autoencoders. Transactions on Machine Learning Research.\n\nJavier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. A primer on the\n  inner workings of transformer-based language models. arXiv.\n\nLei Gao and Ling Guan. 2023. Interpretability of machine learning: Recent advances and future\n  prospects. IEEE MultiMedia, 30(4):105–118.\n\nAtticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang,\n  Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. 2024a.\n  Causal abstraction: A theoretical foundation for mechanistic interpretability. arXiv.\n\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. Causal abstractions of neural\n  networks. In Proceedings of the 35th International Conference on Neural Information Processing\n  Systems, NIPS ’21, Red Hook, NY, USA. Curran Associates Inc.\n\n\n                                       12\n\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman,\n  and Christopher Potts. 2022. Inducing causal structure for interpretable neural networks. In Pro-\n  ceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings\n   of Machine Learning Research, pages 7324–7338. PMLR.\n\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. 2024b.\n  Finding alignments between interpretable causal variables and distributed neural representations.\n  arXiv.\n\nNathan Godey, Éric Villemonte de la Clergerie, and Benoît Sagot. 2024. Why do small language\n  models underperform? Studying language model saturation via the softmax bottleneck. In First\n  Conference on Language Modeling.\n\nSatvik Golechha and James Dao. 2024. Challenges in mechanistically interpreting model representa-\n   tions. arXiv.\n\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. 2017. The reversible resid-\n  ual network: Backpropagation without storing activations. In Advances in Neural Information\n  Processing Systems, volume 30. Curran Associates, Inc.\n\nBryce Goodman and Seth Flaxman. 2017. European Union regulations on algorithmic decision\n  making and a “right to explanation”. AI Magazine, 38(3):50–57.\n\nAndreas Grivas, Nikolay Bogoychev, and Adam Lopez. 2022. Low-rank softmax can have unargmax-\n  able classes in theory but rarely in practice. In Proceedings of the 60th Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages 6738–6758, Dublin,\n   Ireland. Association for Computational Linguistics.\n\nBobby He, Lorenzo Noci, Daniele Paliotta, Imanol Schlag, and Thomas Hofmann. 2024. Under-\n  standing and minimising outlier features in transformer training. In The Thirty-eighth Annual\n  Conference on Neural Information Processing Systems.\n\nJohn Hewitt and Percy Liang. 2019.  Designing and interpreting probes with control tasks.  In\n  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\n  the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n  2733–2743, Hong Kong, China.\n\nRobert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. 2024. Sparse\n  autoencoders find highly interpretable features in language models. In The Twelfth International\n  Conference on Learning Representations.\n\nFrederik Hytting Jørgensen, Luigi Gresele, and Sebastian Weichwald. 2025. What is causal about\n  causal models and representations? arXiv.\n\nSubhash Kantamneni and Max Tegmark. 2025. Language models use trigonometry to do addition.\n  arXiv.\n\nAndrej Karpathy. 2015. The unreasonable effectiveness of recurrent neural networks.\n\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. 2021. BERT busters:\n  Outlier dimensions that disrupt transformers. In Findings of the Association for Computational\n  Linguistics: ACL-IJCNLP 2021, pages 3392–3405, Online. Association for Computational Lin-\n   guistics.\n\nKarim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing\n   for the usage of grammatical number. In Proceedings of the 60th Annual Meeting of the Association\n   for Computational Linguistics (Volume 1: Long Papers), pages 8818–8831, Dublin, Ireland.\n  Association for Computational Linguistics.\n\nAleksandar Makelov, Georg Lange, Atticus Geiger, and Neel Nanda. 2024. Is this the subspace\n  you are looking for? An interpretability illusion for subspace activation patching. In The Twelfth\n  International Conference on Learning Representations.\n\n\n                                       13\n\nMaxime Méloux, Silviu Maniu, François Portet, and Maxime Peyrard. 2025. Everything, everywhere,\n   all at once: Is mechanistic interpretability identifiable? In The Thirteenth International Conference\n  on Learning Representations.\n\nJulian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, and Ryan\n   Cotterell. 2025.  Controllable context sensitivity and the knob behind it.  In The Thirteenth\n  International Conference on Learning Representations.\n\nJohn Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. 2023. Text embeddings\n   reveal (almost) as much as text. In Proceedings of the 2023 Conference on Empirical Methods in\n  Natural Language Processing, pages 12448–12460, Singapore. Association for Computational\n   Linguistics.\n\nAaron Mueller. 2024. Missed causes and ambiguous effects: Counterfactuals pose challenges for\n   interpreting neural networks. arXiv.\n\nAaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can\n  Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and\n  Yonatan Belinkov. 2024. The quest for the right mediator: A history, survey, and theoretical\n  grounding of causal interpretability. arXiv.\n\nBrian Muhia. 2022. ioi (revision 223da8b).\n\nVinod Nair and Geoffrey E. Hinton. 2010.  Rectified linear units improve restricted Boltzmann\n  machines. In Proceedings of the 27th International Conference on International Conference on\n  Machine Learning, ICML’10, page 807–814, Madison, WI, USA. Omnipress.\n\nGiorgos Nikolaou, Tommaso Mencattini, Donato Crisostomi, Andrea Santilli, Yannis Panagakis,\n  and Emanuele Rodolá. 2025. Language models are injective and hence invertible.  Preprint,\n  arXiv:2510.15511.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020.\n  Zoom in: An introduction to circuits. Distill. Https://distill.pub/2020/circuits/zoom-in.\n\nChris Olah and Adam Jermyn. 2024. What is a linear representation? What is a multidimensional\n  feature? Transformer Circuits Thread.\n\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization.  Distill.\n   Https://distill.pub/2017/feature-visualization.\n\nVardan Papyan, X. Y. Han, and David L. Donoho. 2020.  Prevalence of neural collapse during\n  the terminal phase of deep learning training. Proceedings of the National Academy of Sciences,\n  117(40):24652–24663.\n\nTiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020a. Pareto probing: Trading\n   off accuracy for complexity. In Proceedings of the 2020 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP), pages 3138–3153, Online. Association for Computational\n   Linguistics.\n\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan\n   Cotterell. 2020b. Information-theoretic probing for linguistic structure. In Proceedings of the\n  58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\n  Association for Computational Linguistics.\n\nTiago Pimentel, Josef Valvoda, Niklas Stoehr, and Ryan Cotterell. 2022. The architectural bottleneck\n   principle. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n  Processing, pages 11459–11472, Abu Dhabi, United Arab Emirates. Association for Computational\n   Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\n  Language models are unsupervised multitask learners. OpenAI blog.\n\n\n                                       14\n\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020. Null it out:\n  Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th Annual\n  Meeting of the Association for Computational Linguistics, pages 7237–7256, Online. Association\n   for Computational Linguistics.\n\nShauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. 2021. Counterfactual interventions\n   reveal the causal effect of relative clause representations on agreement prediction. In Proceedings\n   of the 25th Conference on Computational Natural Language Learning, pages 194–209, Online.\n  Association for Computational Linguistics.\n\nShauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D Cotterell. 2022. Linear adversarial\n  concept erasure.  In Proceedings of the 39th International Conference on Machine Learning,\n  volume 162 of Proceedings of Machine Learning Research, pages 18400–18421. PMLR.\n\nLee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas\n  Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria\n  Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi\n  Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David\n  Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, and Tom McGrath.\n  2025. Open problems in mechanistic interpretability. arXiv.\n\nJiuding Sun, Jing Huang, Sidharth Baskaran, Karel D’Oosterlinck, Christopher Potts, Michael Sklar,\n  and Atticus Geiger. 2025. HyperDAS: Towards automating mechanistic interpretability with\n  hypernetworks. arXiv.\n\nMingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. 2024. Massive activations in large language\n  models. In First Conference on Language Modeling.\n\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam\n  Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner,\n  Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees,\n  Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. 2024.  Scaling\n  monosemanticity: Extracting interpretable features from Claude 3 Sonnet. Transformer Circuits\n  Thread.\n\nSana Tonekaboni, Shalmali Joshi, Melissa D McCradden, and Anna Goldenberg. 2019. What\n   clinicians want: Contextualizing explainable machine learning for clinical end use. arXiv.\n\nOskar van der Wal, Pietro Lesci, Max Müller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem\n  Zuidema, and Stella Biderman. 2025. PolyPythias: Stability and outliers across fifty language\n  model pre-training runs. In The Thirteenth International Conference on Learning Representations.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n  Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st\n  International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010,\n  Red Hook, NY, USA. Curran Associates Inc.\n\nElena Voita and Ivan Titov. 2020. Information-theoretic probing with minimum description length.\n  In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), pages 183–196, Online. Association for Computational Linguistics.\n\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2023.\n   Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The\n  Eleventh International Conference on Learning Representations.\n\nJennifer C. White, Tiago Pimentel, Naomi Saphra, and Ryan Cotterell. 2021. A non-linear structural\n  probe. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\n  Computational Linguistics: Human Language Technologies, pages 132–138, Online. Association\n   for Computational Linguistics.\n\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah Goodman, Christo-\n  pher Manning, and Christopher Potts. 2024a. pyvene: A library for understanding and improving\n  PyTorch models via interventions. In Proceedings of the 2024 Conference of the North American\n\n\n                                       15\n\n  Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume\n  3: System Demonstrations), pages 158–165, Mexico City, Mexico. Association for Computational\n   Linguistics.\n\nZhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and\n  Noah D. Goodman. 2024b. A reply to Makelov et al. (2023)’s “interpretability illusion” arguments.\n  arXiv.\n\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023. Inter-\n   pretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information\n  Processing Systems, volume 36, pages 78205–78226. Curran Associates, Inc.\n\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018.  Breaking the\n  softmax bottleneck: A high-rank RNN language model. In International Conference on Learning\n  Representations.\n\nZeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun. 2021. Transformer visualization via\n  dictionary learning: contextualized embedding as a linear superposition of transformer factors. In\n  Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction\n  and Integration for Deep Learning Architectures, pages 1–10, Online.\n\nWenwu Zhu, Xin Wang, and Wen Gao. 2020. Multimedia intelligence: When multimedia meets\n   artificial intelligence. IEEE Transactions on Multimedia, 22(7):1823–1835.\n\n\n\n\n\n                                       16\n\nA  Reproducibility\n\nWe provide the code to reproduce our experiments in https://github.com/densutter/\nnon-linear-representation-dilemma. Refer to the README.md for instructions.\n\nB  Pseudo-code for Running an Intervention on an Algorithm\n\nIn Fig. 5, we present pseudo-code that demonstrates how algorithms execute under our intervention\nframework.\n\n    1     def f(A):\n    2         def f A(x, IA=None):\n    3               vηx = x\n    4             for η in A.topological_sort(ηinner ∪ηy):\n    5                 if IA and η in IA:\n    6                           vη =  IA[η]\n    7                 else:\n                                         η\n    8                           vη = f A (vparA(η))\n    9             return vηy\n   10         return f A\n\n\nFigure 5: Pseudo-code implementation of an algorithm with interventions, where interventions IA are\nspecified as a Python dictionary mapping nodes to their intervened values.\n\nC  Additional Related Work\n\nThe concept of causal abstraction was ported to deep neural networks by Geiger et al. (2024a),\nproviding a generalised framework for understanding how neural networks can be abstracted to\nhigher-level algorithms. Early work by Geiger et al. (2021) explored direct interventions on neuron-\naligned activations, laying the groundwork for more sophisticated approaches. Building on this,\nGeiger et al. (2024b) introduced distributed alignment search (DAS), which uses an alignment map\nto align distributed representations in neural networks with causal graphs. Several improvements to\nDAS have been proposed: Sun et al. (2025) developed HyperDAS, which automates the search for\nnode information using hypernetworks, while Wu et al. (2023) introduced Boundless DAS, which\nautomatically determines intervention size through gradient descent, scaling to larger models.\n\nHowever, recent work has raised important critiques of causal alignment methods. Méloux et al.\n(2025) demonstrated that multiple algorithms can be causally aligned with the same neural network,\nand conversely, a single algorithm can align with different network subspaces. Mueller (2024)\nidentified fundamental limitations in counterfactual theories, showing they may miss certain causes\nand that causal dependencies in neural networks are not necessarily transitive. Makelov et al. (2024)\nshowed that subspace interventions such as those used in DAS can lead to “interpretability illusions”—\ncases where manipulating a subspace changes the behaviour of the model through activating parallel\npathways, rather than directly controlling the target feature. In their response, Wu et al. (2024b) argued\nthese illusions may be artefacts of specific evaluation approaches rather than fundamental flaws, and\nthat they depend on the definition of causality being used, a point also made by Jørgensen et al. (2025).\n\nRecent work has also raised significant challenges to the linear representation hypothesis. White\net al. (2021) demonstrated that syntactic structure in language models is encoded non-linearly,\nshowing that kernelised structural probes outperform linear ones while maintaining parameter count.\nSimilarly, Csordás et al. (2024) found that recurrent neural networks use fundamentally non-linear\nrepresentations for sequence tasks. Engels et al. (2025a) provided concrete examples of non-linear\nfeature representations in language models, such as days of the week being encoded on a circular\nmanifold. While Golechha and Dao (2024) argued that some language modelling behaviours may\nbe represented linearly due to next-token prediction and LayerNorm folding, Mueller et al. (2024)\nadvocated for exploring non-linear mediators to uncover more sophisticated abstractions. Additional\nevidence comes from Kantamneni and Tegmark (2025), who found that language models represent\nnumbers on a helical manifold. Olah and Jermyn (2024) offered an important clarification: the\nlinear representation hypothesis is not about dimensionality but rather about features behaving\nmathematically linearly through addition and scaling, allowing for multidimensional features with\nconstrained geometry. This represents a relaxation of the strongest form of the hypothesis.\n\n\n                                       17\n\nD  Schematic of the Relation Between Notions of Causal Abstraction\n\n\nRestricting τ\n\n\n                                                                      Input-Restricted                                                                                  Constructive                                                                                                 Abstraction                                                                                                                               Constructiveωτ                                                                                                                                              Abstraction\n τ                                                                                               τ                                            H                                                            N\n                                                                               H                                                                                                                                                                τ       N\n\n                                                                                                                             vη2←cη2\n                                                                                                                                                                                                             vη2←cη2                                                                                ψη1                                                                                      ψη2                                                                                                              η2                                                                                                                                                        ψη1                                                                                                                                                              ψη2                                                                                                 hψη2 ←cψη2     ωτ                                                                                                                                                                                    η2                                                                                                                                                                               hψη2 ←cψη2                                                                                                                                                 vη1←cη1\n                                                                                                                                                                                                                                 vη1←cη1                                                                                         hψη1                                                                               ←cψη1\n                                                                                                                                                                        hψη1                                                                                                                                                 ←cψη1                                                                                                                          η1                                                                                                                                                                                               η1                                                                                                                                                           e. g.                                                                                    η                                                                                       ∈𝑃ηall                                                                                                                                                    {η1,                                                                                                                                    η2}                                                                                                                                           η                                                                                                                                               ∈𝑃ηall                                                                                                                                                                                                                                        {η1,                                                                                                                                                                                                             η2}            Constructive                                                                                            ∈Reachable                                                                                                              by                                                                                              some                                                                                                         cη                                                                                                                                                            ∈ℝ|η|   e. g.                                                                                                                                                                            cη                                                                                                                   Input+Intervention\n\n\n\n\n                                                                      Input-Restricted Distributed Abstraction\n                                                                                                                                           Abstraction                                             H             τ       N                      Distributed\n                                                                                H                                                                                                                                                                  τ       N\n\n τ\n                                                                                   ψη1Φ                                                                                         ψη2                                                                                 Φ-1                                                                         Φ  hψη2Φ  ←cψη2Φ\n                                                                                                                                                          ψη1Φ                                                                                                                                                                ψη2                                                                                                                                                          Φ-1                                                                                                                                   Φ  hψη2Φ  ←cψη2Φ                                                                         Φ                                                                                                   ωτ     vη2←cη2η2                                               Φ  hψη1Φ                                                                              ←cψη1                                                                                                                                  Φ                                                                                                                                                                    ωτ     vη2←cη2η2                                                                                                                                                    vη1←cη1                                                                                         Φ  hψη1Φ                                                                                                                                             ←cψη1\n                                                                                                                             η1                                                                                 vη1←cη1\n                                                                                                                                                                                                 η1\n                                                                                          ∈𝑃ηall                                                                                                                                                               e. g. {η1,          Distributed                                                                                η                                                                                                                                        η2}                                     η                                                                                                                                                 ∈𝑃ηall                                                                                                                                                                                                                                           {η1, η2}                                                                                              ∈Reachable                                                                                                                 by some                                                                                                            cη                                                                                                                                                             ∈ℝ|η|   e. g.                                                                                                                                                                              cη                                                                                                                      Input+Intervention\n\n\n\n\n                   τ -Abstraction                                                                                                Strong τ -Abstraction\n τ\n               H            τ       N                                                 H             τ       N\n\n\n               ψ    hψ ←hψ       ωτ    η                                                              ψ    hψ ←hψ       ωτ    η\n                                                            vη ←cη                                                                                                                                        vη ←cη\n\n                                       η ∈Some Set ⊆𝑃(ηall)                                                                                        η ∈𝑃(ηall)             Unconstrained                                        cη ∈Some Set ⊆ℝ|η|                                                                                                               cη ∈ℝ|η|\n\n             Any Set of Interventions           All Input-Restricted Interventions                    All Interventions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Restricting                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Interventions\n\nFigure 6: A schematic of the definitions of causal abstraction in §3. The axes represent an increase in\nhow restricted the notion of causal abstraction is based on: y-axis, constraints placed on τ; and x-axis,\nconstraints placed on the set of allowed interventions. Grey arrows symbolise a superset→subset\nrelationship: if an A-N pair fulfils the conditions in the subset, it also fulfils them in the superset.\nE DNN Definitions\n\nE.1 MLP\n\nA multi-layer perceptron (MLP) consists of a sequence of linear transformations interleaved with\nnon-linear activation functions.\nSubmodule 1. We can define a multi-layer perceptron (mlp) by choosing:\n         hψ1 = f 0N(x) = W0 x,         hψℓ+1 = f ℓN(hψℓ) = Wℓ(σ(hψℓ)) + bℓ          (9)\nwhere W0 ∈R|ψ1|×|x|, Wℓ∈R|ψℓ+1|×|ψℓ|, and bℓ∈R|ψℓ+1| are trainable parameters, and σ is a\nnon-linearity like ReLU. For this model, Hℓ= R|ψℓ| for 0 < ℓ< L and HL = R|y|.\n\nIn this work, we focus on MLPs used for classification tasks, whose final layer includes a softmax\ntransformation.\nDNN 1. A classification multi-layer perceptron (MLP) is defined like Submodule 1 but with a\nsoftmax on the last layer:\n                      pN(y | x) = f LN (hψL) = softmax(WL hψL)                      (10)\nwhere WL ∈R|y|×|ψL| is a trainable parameter.\n\nE.2  Transformer Language Model\n\nIn this section, we provide a definition of decoder-only autoregressive language models (Radford\net al., 2019; Vaswani et al., 2017). While many variations of transformer architectures have been\ndeveloped, we focus on the original GPT-2 architecture (Radford et al., 2019). We highlight that the\nPythia models explored in our experiments are slightly different from the original GPT-2 and use\nparallel attention (Chowdhery et al., 2023); however, we do not expect this change to strongly affect\nour results. We now define the different submodules that compose a transformer.\n\n\n                                       18\n\nSubmodule 2. The Embedding layer in a transformer maps input tokens to vectors:\n                        f 0N(x) = ex,    where ex ∈R|x|×|ψ1|                        (11)\nIn this equation, e ∈R|X|×|ψ1| is a learned parameter matrix and x indexes into its rows.11\nSubmodule 3. Multi-Head Self-Attention with H heads is defined as:\n                      attn(h) = concat(η1(h), . . . , ηH(h))WO                      (12)\nwhere each head operates in dimension dη ≪|ψℓ| and computes:\n                          (hWQi )(hWKi )⊤ !\n                                                                                                      i                      (13)                      ηi(h) = softmax      √dk     hWV\nwith learned parameters WO ∈RHdη×|ψℓ|, WQi , WKi  , WVi ∈R|ψℓ|×dη.\nSubmodule 4. Layer Normalization applies per-feature normalization:\n                              LN(h) = γ ⊙h −µ + β                                (14)\n                                      σ\nwhere µ and σ are the mean and standard deviation across all features for a single input, and γ, β\nare learned parameters.\n\nUsing these submodules, we define a transformer block.\nSubmodule 5. A Transformer Block chains together attention and MLP layers with residual\nconnections:\n                 h′ = h + attn(LN(h)),         f ℓN(h) = h′ + mlp(LN(h′))              (15)\nwhere mlp is applied to each token activations separatly as defined in Submodule 1.\n\nFinally, we define the complete transformer language model.\nDNN 2. A transformer language model consists of an embedding layer, transformer blocks, and an\noutput layer:\n                     hψ1 = f 0N(x) = ex                                            (16a)\n                      hψℓ+1 = f ℓN(hψℓ)                                              (16b)\n                   pN(y | x) = f LN (hψL) = softmax(LN(hψL)−1 WL)                 (16c)\nwhere LN(hψL)−1 selects the final token’s position after layernorm is applied. For this model,\nHψℓ= R|x|×|ψℓ| for 1 ≤ℓ≤L and HψL+1 = ∆|Y|−1.\n\nF  Proof of Theorem 1\n\nIn this section, we prove our main theorem. For notational simplicity, we write in this section:\n              f :ℓN def= f ψℓN = f ℓ−1N   ◦· · · ◦f 0N ,    and     f ℓ:N def= f LN ◦· · · ◦f ℓN             (17)\n               |  Run DNN up{zto layer ℓ  }               Run|  DNN from{z  layer }ℓ\nWe start by formally stating our assumptions.\nAssumption 1 (Countable input-space). We assume that the space of inputs (i.e., X) is countable.\nAssumption 2 (Input-injectivity in all layers). We assume that f :ℓN is injective for all layers.\nAssumption 3 (Strict output-surjectivity in all layers). We assume that the composition of f ℓ:N and\nτηy is strictly surjective for all layers (we define strict surjectivity in Definition 10).\n\nAssumption 4 (Algorithm and DNN have matchable partial-orderings). We assume that there exists\na partitioning {ψη | η ∈ηint} ∪{ψ⊥} of N’s neurons ψint—where ψη are single neurons—which\nrespects the partial-ordering of algorithm A, i.e., η ≺η′ =⇒ψη ≺ψη′. Further, for each layer at\nleast one neuron is left unused in this partitioning, i.e., ψ⊥∩ψℓ̸= ∅.\n\n  11We ignore positional embeddings here for simplicity, as they do not affect our proofs of injectivity in App. G.\nNote that, in that section, we show injectivity on an entire layer’s activations hψℓ. Position embeddings would\nbe needed to show injectivity on a single position, e.g., the last token’s position (hψℓ)−1; a property which we\nconjecture should also hold. Further, we note that position embeddings are used in our experiments.\n\n\n                                       19\n\nAssumption 5 (DNN solves the task). We assume that for any input x ∈X, the neural network\nsolves the task correctly, satisfying T(x) = argmaxy∈Y pN(y | x).\n\nWe provide a longer discussion about why we think these assumptions are reasonable in App. F.1.\nFor convenience, we also put a self-contained version of Definition 7 (input-restricted distributed\nabstraction) in App. F.2. Now, we restate our theorem and present its proof.\nTheorem 1. Given any algorithm A and any neural network N such that Assumps. 1 to 5 hold, we\ncan show that A is an input-restricted distributed abstraction of N.\n\nProof. To show that an algorithm A is an input-restricted distributed abstraction of a neural network\nN, we must show (according to Definition 7) that there exists a τ for which: A is an input-restricted\nτ-abstraction of N; and τ is a distributed abstraction map. For τ to be a distributed abstraction\nmap, we need a partition of hidden variables which allows us to independently compute it per node.\nFurther, we need the partitioned hidden variables ψϕint to be the output of an alignment map ϕ which\nis layer-wise decomposable. We thus have:\n hψϕ = ϕ(h) = [ϕℓ(hψℓ)]L+1ℓ=0 , Ψϕ = {ψϕη | η ∈ηint} ∪{ψϕ⊥},  τ(h) = [τη(hψϕη)]η∈ηint  (18)\n |         Align{z DNN         }  |  Partition hidden{z   variables  }  | Compute abstraction{z         }\nTherefore, to define a distributed abstraction map τ, we must define the following three terms: (i) a\nset of layer-wise alignment maps {ϕℓ}Lℓ=1 (note that the alignment maps ϕ0 and ϕL+1 are fixed by\ndefinition); (ii) a partition of hidden variables Ψϕ; and (iii) a set of per-node functions {τη}η∈ηint. To\nprove this theorem, then, we must show that there exists a way to define these terms while ensuring\nthat A is an input-restricted τ-abstraction of N.\n\nWe now note that—given Assump. 4 and independently of our choice of alignment map ϕ—there\nexists at least one partition Ψϕ of the hidden variables ψϕint in N for which:\n  ∀η,η′∈ηint : η ≺η′ =⇒ψϕη ≺ψϕη′ ,     ∀η∈ηint : |ψϕη| = 1,    ∀0<ℓ≤L : ψϕℓ̸⊆ [ ψϕη  (19)\n |    respect partial{z   ordering    }   1 |neuron per{z partition}                       η∈ηint\n                                                                no| layer is {zfully occupied}\nwhere we define ψϕℓas the latent variables given when applying ϕℓon ψℓ. To facilitate our proof, we\nchoose one such partition Ψϕ which we will keep fixed independently of our choice of alignment\nmap ϕ. Given partition Ψϕ, we can assign each node η to a specific layer ℓ, as ψϕη contains a single\nhidden variable and therefore trivially belongs to a single layer. We therefore can define ηℓas all\nnodes associated with layer ℓ:\n                                η ∈ηℓ⇔ψϕη ⊆ψϕℓ                                  (20)\nWe now consider the application of interventions on N as layer-wise on ψϕη ⊆ψϕℓfor η ∈ηℓ. Let\nus therefore define IℓN as the set of all interventions on ψϕη for η ∈ηℓ, where we note that IℓN also\nincludes an empty intervention (i.e., no intervention). For notational convenience, we will write the\nset of all interventions up to layer ℓas I:ℓN , and the set of all nodes associated with those layers as η:ℓ:\n                   I:ℓN = I0N × I1N × · · · × IℓN,    η:ℓ= η0 ∪η1 ∪· · · ∪ηℓ                (21)\nwhere × denotes a Cartesian product. We analogously define IℓA and I:ℓA .\nFinally, we get to an induction proof that will complete this theorem. We will iteratively construct\nabstraction and alignment maps for each layer such that it holds that:\n                                                                       N (x, IN)), and IA = ωτ(IN)   1  ∀   η∈ηℓ:∀   τη(h′ψϕη) = f ηA(x, IA),       where h′ψϕℓ= ϕℓ(f :ℓIN∈I:ℓ−1N\n x∈X         | tested {zcondition }         pre-int.| hidden variable,{z       as} IN∈I:ℓ−1N\nwhere int. stands for intervention. Note that if this holds for all layers, we have proven that A is an\ninput-restricted τ-abstraction of N, as we can perfectly reconstruct the behaviour of algorithm A from\nN’s states under any intervention.12 Also note, however, that our definition of abstraction map restricts\nτηy(hψL+1) = argmax hψL+1, so special care must be taken to guarantee that this last identity will\n\n  12The attentive reader may note condition 1 only guarantees we can reconstruct the behaviour of algorithm\nA from pre-intervention hidden variables. Lemma 1 shows the same holds for post-intervention hidden variables.\n\n\n                                       20\n\nbe preserved. We thus also require an additional condition to hold at each step:\n  ∀    : τηy(f ℓ:N (h′ψℓ)) = f A(x, IA),         where h′ψℓ= f :ℓN (x, IN), and IA = ωτ(IN)      2\nIN∈I:ℓN\n x∈X   |    tested {zcondition    }             post-int.|    neurons,{z    as }IN∈I:ℓN\nFinally, for convenience, we add a third condition to our inductive proof which will make the other\ntwo conditions easier to guarantee:\n                                                                         N (x, IN)), and IA = ωτ(IN)  3  ∀   ∃gη  ∀    : gη(h′ψϕℓ∩ψϕ⊥) = f ηA(x, IA), where h′ψϕℓ= ϕℓ(f :ℓη∈η:ℓ−1              IN∈I:ℓ−1N\n         x∈X   |   tested {zcondition   } pre-int.| hidden variable,{z       as} IN∈I:ℓ−1N\nThis condition guarantees that information about previous nodes (i.e., η ∈η:ℓ−1) is preserved in\neach layer’s non-intervened neurons (i.e., ψϕℓ∩ψϕ⊥). This final condition will be useful to guarantee\nconditions 1 and 2 are preserved in future layers.\n\nStatement.  Conditions 1 , 2 , and 3 hold for all layers ℓin a DNN.\n\nBase Case (ℓ= 0).  For layer ℓ= 0, we have ηℓ= ηx. We also have both ϕℓand τη as the identity\nfunction. Further, we consider I:−1 = {∅} and I:0 = {∅}—where symbol ∅here denotes an empty\nintervention—and we consider f :0N to be the identity on x. (Note that layer f 0N is not applied in f :0N .)\nNow, it is easy to prove our base case:\n\n        •  1 follows trivially, as f ηxA (x, IA) = x and f :0N (x, IN) = x.\n\n        •  2 follows from Assump. 5.\n\n        •  3 follows trivially given η:−1 is an empty set.\n\nInduction Step (given ℓ−1, then ℓ).  Now, due to the inductive hypothesis, we assume that 1 ,\n 2 and 3 hold for layer (ℓ−1). Given this, we must now prove that these conditions also hold for\nlayer ℓ. We will consider two cases: ηℓis either empty or not. Before doing so, however, we note that\n 1 and 3 hold for layer (ℓ−1)’s pre-intervention hidden variables. In Lemmas 1 and 2, we show\nthat the same applies for the post-intervention hidden variables.\n\nLet’s consider the case where ηℓis empty.\nIn this case, we can simply define ϕℓas the identity map. Further, given an empty ηℓ, we know that\nthere are no interventions in this layer, i.e., IℓN = {∅}, and, as such, we have that: I:ℓN = I:ℓ−1N  ×IℓN =\nI:ℓ−1N    . We can now prove the induction step for this case.\n\n        •  1  is true trivially, since ηℓis empty.\n        •  2  follows using the inductive hypothesis. Let IN ∈I:ℓN and x ∈X. Now, let h′ψℓ=\n       f :ℓN (x, IN), h′ψℓ−1 = f :ℓ−1N  (x, IN), and IA = ωτ(IN). We can now show that:\n\n          τηy f ℓ:N (h′ψℓ) = τηy  f ℓ:N  f :ℓN (x, IN)                         definition of h′ψℓ   (22a)\n\n              = τηy  f ℓ:N  f ℓ−1N  f :ℓ−1N  (x, IN)       no intervention at layer ℓ  (22b)\n\n              = τηy  f ℓ−1:N   f :ℓ−1N  (x, IN)                     definition of f ℓ−1:N     (22c)\n\n              = τηy  f ℓ−1:N    h′ψℓ−1                          definition of h′ψℓ−1  (22d)\n\n              = f A(x, IA)                        inductive hypothesis on 2   (22e)\n\n        This shows 2 holds for layer ℓwhen ηℓis empty.\n        •  3  follows using the inductive hypothesis.  Let IN ∈I:ℓ−1N     , x ∈X, and η ∈η:ℓ−1.\n       Now, let h′                                                              N  (x, IN)), and IA = ωτ(IN). Further, let               ψϕℓ= ϕℓ(f :ℓ                                 N (x, IN)), h′ψϕℓ−1 = ϕℓ−1(f :ℓ−1\n\n\n                                       21\n\n                            A (x, IA); we know such function exists due to the inductive hypothesis on         gℓ−1η  (h′ψϕℓ−1) = f :η\n         1 and 3 , together with Lemmas 1 and 2. Finally, since f :ℓN is injective (by Assump. 2) and\n         since f :ℓN = f ℓ−1N  ◦f :ℓ−1N     , we know that each h′ψϕℓ−1 is mapped to a unique h′ψϕℓin the next\n          layer. We can thus define function fℓ−1N   on the domain formed by these hidden variables\n        which, given a hidden variable h′                                                                                ℓ−1 is an                                                                                                 N                                ψϕℓreturns its “parent” h′ψϕℓ−1; in other words, f\n          partial inverse of f ℓ−1N   defined only on its image. Defining now function gℓη = gℓ−1η   ◦ fNℓ−1 ,\n      we can show:\n          gℓη(h′ψϕℓ∩ψϕ⊥) = gℓη(h′ψϕℓ)                                       ηℓis empty  (23a)\n             = gℓη  ϕℓ f ℓ−1N  (h′ψϕℓ−1)                no intervention at layer ℓ(23b)\n\n             = gℓη  f ℓ−1N  (h′ψϕℓ−1)                    ϕℓis the identity function  (23c)\n\n             = gℓ−1η   fℓ−1N   f Nℓ−1 (h′ψϕℓ−1)                           definition of gℓ(23d)\n\n                                                                                                N    (23e)             = gℓ−1η  (h′ψϕℓ−1)                                      definition of fℓ−1\n             = f :ηA (x, IA)                     inductive hypothesis on 1 and 3   (23f)\n\nLet’s now consider the case when ηℓis not empty.\n\nTo show 1 , 2 and 3  for layer ℓwe need to find a suitable bijective ϕℓ. We now show a careful\nway to construct this map which satisfies these conditions. To do so, we will again split this step of\nthe proof into two parts. We will first take care of the case in which no interventions are applied to\nlayer ℓ, guaranteeing that the model behaves correctly in those cases. In that case, we must handle\nthe set of input-restricted pre-intervention hidden states in layer ℓ, which we define as:\n                H◆ψℓ def= {f ψℓN  (x, IN) | x ∈X, IN ∈I:ℓ−1N   }                        (24)\n                                             pre-int., as| we {zdo not} include Iℓ\nNotably, instead of defining the entire alignment map ϕℓat once, we will first define its behaviour\nonly on those hidden states. We will denote this domain-restricted function as ϕ◆ℓ. Given this function,\nwe will be able to define a set of input-restricted pre-intervention hidden variables in layer ℓas:\n                   H◆                                         def= {ϕ◆ℓ(hψℓ) | hψℓ∈H◆ψℓ}                            (25)                           ψϕℓ\nwhere ◆represents the non-intervened hidden states and variables, and we will use ❖to represent\nthe intervened instances. Note that H◆                                                                  ϕ◆ℓ.                             ψϕℓis the set of representations output by alignment map\nThe second case we will consider will then handle interventions on this layer, and will again guarantee\nthat the model behaves as expected in those cases. We thus define the set of input-restricted post-\nintervention hidden variables as:\n                                    def  ψϕℓ\n                  Hψϕℓ = {f N  (x, IN) | x ∈X,  IN ∈I:ℓN  }                       (26)\n                                                post-int.,| as{zwe }include Iℓ\nNotably, what an intervention on layer ℓdoes is re-combine the representations in H◆  We can thus                                                               ψϕℓ.\nwrite Hψϕℓin terms of H◆ψϕℓas:\n                       \n\n                                                                                            (27)   Hψϕℓ=       {ϕℓ(hψℓ)ψϕη  | hψℓ∈H◆ψℓ}    {ϕℓ(hψℓ)ψϕ⊥∩ψϕℓ| hψℓ∈H◆ψℓ}    ×η∈ηℓ              ×\n                    pre-int.|      h.v., {zprojected to }ψϕη     pre-int.|        h.v., {zprojected to ψϕ⊥}\nWe further define the set of input-restricted intervention-only hidden variables as:\n                       H❖                               H◆                                                                                            (28)                                ψϕℓ= Hψϕℓ\\                                            ψϕℓ\n\n\n                                       22\n\nBy carefully defining the behaviour of ϕℓon this set, we can guarantee the conditions above to hold.\nIn particular, we will define this part of the function via its inverse ϕ❖ℓ −1, which maps these hidden\nvariables back to hidden states. We therefore have ϕℓand its partial inverse defined as:\n                           \n       (ϕ◆ℓ(h),  if h ∈H◆ψℓ         ϕ◆                                                                           ℓ −1(h),  if h ∈H◆ψϕℓ    ϕℓ(h) =                          ϕ−1ℓ(h) =                                    (29)           ϕ❖ℓ(h),  if h ∈H❖ψℓ                                                                      ϕ❖ℓ −1(h),  if h ∈H❖ψϕℓ\nWe now define ϕ◆ℓ.\nDefinition 8. Partial map ϕ◆ℓ: H◆ψℓ→R|ψℓ| is some fixed function that is injective on each\ndimension, i.e., ∀i∈{1,...,|ψϕℓ|} ∀h1,h2∈H◆ψℓ: h1 ̸= h2 ⇒ϕ◆ℓ(h1)i ̸= ϕ◆ℓ(h2)i.\n\nSuch a function exists, because H◆ψℓis countable (Lemma 4) and R is uncountable. Further, its partial\n                                             exists because ϕ◆ℓis injective.inverse ϕ◆ℓ −1, defined on the image H◆ψϕℓ,\nWe can now prove that conditions 1 and 3 hold. We also prove that 2 holds when IN ∈I:ℓ−1N     ,\ni.e., when there is no intervention in layer ℓ.\n        •  1 follows using the inductive hypothesis. Let IN ∈I:ℓ−1N     , x ∈X, and η ∈ηℓ. Further, let\n        h′                                                   N  (x, IN)), and IA = ωτ(IN). Now, note that there        ψϕℓ= ϕℓ(f :ℓ                        N (x, IN)), h′ψϕℓ−1 =ϕℓ−1(f :ℓ−1\n          exists a function gη for which vη = gη(vη:ℓ−1), as the parents of η are a subset of η:ℓ−1. It\n       now suffices to show that h′                                   encodes information about vη:ℓ−1 = [f :ηA (x, IA)]η∈η:ℓ−1. By                            ψϕη\n         the inductive hypothesis on 1 and 3 , together with Lemmas 1 and 2, we know that h′\n                                                                        ψϕℓ−1\n        encodes information about vη:ℓ−1; let gη:ℓ−1 be a function that extracts this information,\n                                                           N  is injective, and ϕ◆ℓis injective on each            i.e., gη:ℓ−1(h′ψϕℓ−1) = vη:ℓ−1. Now, since f :ℓ\n        output dimension, we know that h′ψℓ= [ϕ◆ℓ(f ℓ−1N  (ϕ−1ℓ−1(h′ψϕℓ−1)))]ψϕη contains the same\n         information as h′                                                                ℓ−1 , ϕ◆−1                                                                      and define τη as                                                                              N                                                                              ℓ,ψϕη                    ψϕℓ−1. We can thus construct (partial) inverses f\n                                                 which concludes this step of the proof:         the composition gη ◦g:η−1 ◦ϕℓ−1 ◦ fℓ−1N  ◦ϕ◆−1ℓ,ψϕη,\n                          ℓ(f ℓ−1N  (ϕ−1ℓ−1(h′ψϕℓ−1)))]ψϕη)                 definition of h′ψℓ(30a)          τη([h′ψϕℓ]ψϕη) = τη([ϕ◆\n            = gη(g:η−1(ϕℓ−1( fNℓ−1 (ϕ◆−1ℓ,ψϕη([ϕ◆                                               ℓ(f Nℓ−1 (ϕ−1ℓ−1(h′ψϕℓ−1)))]ψϕη)))))     (30b)\n            = gη(g:η−1(h′ψϕℓ−1))                                               (30c)\n            = f :ηA (x, IA)                                                     (30d)\n\n        •  2 when IN ∈I:ℓ−1N    follows using the inductive hypothesis. Let IN ∈I:ℓ−1N   and x ∈X.\n       Now, let h′ψℓ= f :ℓN (x, IN), h′ψℓ−1 = f :ℓ−1N  (x, IN), and IA = ωτ(IN). We can show that:\n\n          τηy f ℓ:N (h′ψℓ) = τηy  f ℓ:N  f :ℓN (x, IN)                         definition of h′ψℓ   (31a)\n\n              = τηy  f ℓ:N  f ℓ−1N  f :ℓ−1N  (x, IN)       no intervention at layer ℓ  (31b)\n\n              = τηy  f ℓ−1:N   f :ℓ−1N  (x, IN)                     definition of f ℓ−1:N     (31c)\n\n              = τηy  f ℓ−1:N    h′ψℓ−1                          definition of h′ψℓ−1  (31d)\n\n              = f A(x, IA)                        inductive hypothesis on 2   (31e)\n\n         This shows 2 holds for layer ℓwhen there is no intervention in layer ℓ.\n\n        •  3  follows using the inductive hypothesis.  Let IN ∈I:ℓ−1N     , x ∈X, and η ∈η:ℓ−1.\n       Now, let h′                                                             N  (x, IN)), and IA = ωτ(IN). Further, let               ψϕℓ= ϕℓ(f :ℓ                                  N (x, IN)), h′ψϕℓ−1 =ϕℓ−1(f :ℓ−1\n\n\n                                       23\n\n                            A (x, IA); we know such function exists due to the inductive hypothesis on         gℓ−1η  (h′ψϕℓ−1) = f :η\n         1 and 3 together with Lemmas 1 and 2. Finally, since f :ℓN and ϕ◆ℓare injective and ϕℓ−1 is\n          bijective, we can define the partial inverse function fℓ−1N   of their composition ϕ◆ℓ◦f ℓN ◦ϕ−1ℓ−1\n         (applied only to their image) which, given the hidden variable h′              its “parent”                                                          ψϕℓreturns\n        h′                                                                  ψϕℓ)—which        ψϕℓ−1. Defining now a function bgℓ                                          η = gℓ−1η   ◦ fNℓ−1 and gℓη(h′ψϕℓ∩ψϕ⊥) = bgℓη(h′\n          exists, as ϕ◆ℓis injective on each dimension—we can show:\n                                          ϕ◆ℓis injective on all dimensions  (32a)                        ψϕℓ)          gℓη(h′ψϕℓ∩ψϕ⊥) = bgℓη(h′\n\n                                                      no intervention at layer ℓ(32b)                            η ϕ◆ℓ  f ℓ−1N  (ϕ−1ℓ−1(h′ψϕℓ−1))             = bgℓ\n\n                                                                                 definition of gℓ(32c)             = gℓ−1η   fℓ−1N   ϕ◆ℓ  f Nℓ−1 (ϕ−1ℓ−1(h′ψϕℓ−1))\n\n                                                                                                N    (32d)             = gℓ−1η  (h′ψϕℓ−1)                                     definition of fℓ−1\n             = f :ηA (x, IA)                    inductive hypothesis on 1 and 3  (32e)\n\nWe have now proved 1 and  3 . We have also partially proved 2  for cases where there is no\nintervention in layer ℓ.13 We now finish our proof by considering cases where there is an intervention\nin this layer ℓ. In the second case, we need to handle intervention-only representations H❖  We will                                                                   ψϕℓ.\n                                                   fulfil 2 .now define ϕ❖ℓ −1 on this domain H❖ψϕℓto\n                                                            is some fixed function such that it holds:Definition 9. Partial map ϕ❖ℓ −1 : H❖ψϕℓ→R|ψℓ|\n      1. ϕ❖ℓ −1 maps to the set Hψℓ\\ H◆ψℓ\n      2. ϕ❖ℓ −1 is an injective map\n\n                                               ψϕℓ\n                                                                  N  (x, IN), and IA = ωτ(IN). We have      3. Let IN ∈I:ℓN \\ I:ℓ−1N   and x ∈X. Now, let h′ψϕℓ= f\n         that f ℓ:N (ϕ❖ℓ −1(h)) = f A(x, IA).\n\nWhere the first two conditions ensure the necessary bijectivity of ϕℓand the last characteristic ensures\n                                                                 ψϕℓ\n                                                                                            N  (x, IN), and 2 . Now, let x ∈X be any input and IN ∈I:ℓN any intervention. Further, let h′ψϕℓ= f\nIA = ωτ(IN). We now note that—given 1 and 3 , and Lemmas 1 and 2—the value vη = f :ηA (x, IA)\nfor all nodes η ∈η:ℓare encoded in h′   This is enough information to determine the algorithm’s                             ψϕℓ.\n                                                                                         the outputoutput y⋆= f A(x, IA). Now, define a function g⋆which maps an element h′ψϕℓ∈H❖ψϕℓto\nalgorithm A expects. Further, by Lemma 6 there exists an uncountably infinite set of hidden states\nH(y⋆)  ψℓ  such that:\n                    ∀h ∈H(y⋆)ψℓ   : y⋆= argmax [f ℓ:N (hψℓ)]y′                        (33)\n                                             y′∈Y\n                     \\ H◆                          H◆                             ψℓ, which—as                                       ψℓis countable—is still uncountably infinite. We canWe define ˆH(y⋆)ψℓ = H(y⋆)ψℓ\n                                            ˆH(g⋆(h))now map any h ∈H❖                                      ψℓ      fulfilling the third characteristic of Definition 9.                ψϕℓto an element in\nThat such a mapping exists adhering to the first and second characteristic of Definition 9 is ensured\n                                              countable (shown in Lemma 5). Further, as ϕ❖ℓ −1by the fact that ˆH(y⋆)ψℓ   is uncountable and H❖ψϕℓis\nis injective, its partial inverse ϕ❖ℓon its image H❖ψℓexists.\n   13This also proves the result for any input x ∈X and intervention IN ∈Iℓwhere h ∈H◆ψℓ. By 3  at layer\nℓ, there exists I′N ∈Iℓ−1—constructed by applying only the interventions from IN on layers < ℓ—such that\nf :ℓN (x, IN) = f :ℓN (x, I′N). Since both encode the same values on η<ℓaccording to 3  , which fully determine the\noutput of f A, and since 2 holds for (x, I′N) at layer ℓ, it must also hold for (x, IN).\n\n\n                                       24\n\n    1     def extract_unused_rep(h, H◆                                    ψϕℓ∪H❖ψϕℓ):\n    2         rep=h\n    3         while rep∈H◆                          ψϕℓ∪H❖ψϕℓ:\n    4             rep=ϕ−1ℓ(rep)\n    5         return rep\n\nFigure 7: Pseudo-code for extract_unused_rep. This function returns an unique element in\n                               for each h ∈(H◆                                                                   ensuring bijectivity of ϕ′ℓ(h).(H◆ψℓ∪H❖ψℓ)\\(H◆ψϕℓ∪H❖ψϕℓ)                                  ψϕℓ∪H❖ψϕℓ)\\(H◆ψℓ∪H❖ψℓ)\n\nThe attentive reader may have noticed that we defined ϕℓonly over the domain H◆ψℓ∪H❖ψℓinstead\nover R|ψℓ|. We note that it is simple to extend ϕℓto an ϕ′ℓdefined over R|ψℓ|. Let id be the identity\nfunction and extract_unused_rep be defined by the algorithm given in Fig. 7. A bijective function\nϕ′ℓover R|ψℓ| mapping to R|ψℓ| can be defined as:\n      ϕℓ(h)                         if h ∈H◆ψℓ∪H❖ψℓ\n                                       if h ∈(H◆ ϕ′ℓ(h) =  extract_unused_rep(h, H◆ψϕℓ∪H❖ψϕℓ),                                                     ψϕℓ∪H❖ψϕℓ) \\ (H◆ψℓ∪H❖ψℓ)\n      h                            else\n                                                                                            (34)\n\nwhich completes the proof.\n\nF.1  Discussion about Assumptions\n\nAssump. 1 (Countable input-space).  While this assumption cannot be made on all neural networks\nlike MLPs, it holds for models working on language and images. The set of all images with a specific\nresolution is finite, as it considers a finite number of pixels where each pixel has a finite number of\nchannels (e.g. values for red, green and blue) and each channel is a number between 0 and 255. The\nset of all sequences in a language model is also countably infinite, as each set of sequences of some\nlength is finite given finite tokens; so we have a set made out of the countable union of finite sets,\nwhich is still countable.\n\nAssump. 2 (Input-injectivity in all layers).  Neural network layers (e.g., MLP blocks) are not\nnecessarily injective. The usage of learnable weights, activation functions like ReLU (Nair and Hinton,\n2010) and information bottlenecks makes it possible to have a non-injective model. However, we\nprove in App. G that transformers, at least, are almost surely injective at initialisation on their inputs.\nFurther, Nikolaou et al. (2025) recently published a proof—as well as empirical evidence—that\ntransformers are almost surely injective in the hidden states of their last token, both at initialisation\nand after training. We also see in our empirical experiments in App. H that the MLPs we analyse are\nalso, in practice, injective—or close enough to it that we observe no collisions in embedding space.\n\nAssump. 3 (Strict output-surjectivity in all layers).   Surjectivity can be defined on the output\ndistribution f ℓ:N  : Hψℓ→∆|Y|−1, but that is a rather strong assumption. For our proofs, we will rely\non strict surjectivity on the classification space instead (τηy ◦f ℓ:N  : Hψℓ→|Y|), such that every class\ncan be predicted. However, surjectivity on the classification space still does not necessarily hold for\nDNNs. LLMs have problems like the softmax bottleneck (Yang et al., 2018), which can lead to a\nmodel having insufficient capacity to predict all possible tokens. Grivas et al. (2022) also evaluate\nand find this problem, but show that surjectivity on the tokens is still likely in practice, making this a\nreasonable assumption in LLM settings.\n\nAssump. 4 (Algorithm and DNN have matchable partial-orderings).  We assume this since, for a\nneural network N to be abstracted by the algorithm A, we need it to have this minimal width and depth.\n\nAssump. 5 (DNN solves the task).  We assume this because, if a neural network does not solve the\ngiven task, it will also not be abstracted by an algorithm which implements it.\n\nF.2  Detailed Version of Definition 7\n\nDefinition 7 can also be written without referring to previous definitions as following:\nAlternative Definition 1 (Equivalent to Definition 7). An algorithm A is an input-restricted\ndistributed abstraction of a neural network N iff there exists an τ, IA, and IN such that\n\n\n                                       25\n\n        • τ is a distributed abstraction map. I.e., there exists an alignment map ϕ, a latent-variable\n          partition {ψϕη | η ∈ηint} ∪{ψϕ⊥} of ψϕint (with non-empty ψϕη), and subabstraction maps\n        {τη | η ∈ηint} such that τ is equivalent to computing the value of each node block-wise\n        with τη(hψϕη). An alignment map ϕ is a bijective function that maps the inner neurons\n        ψint of N onto an equal-sized set of latent variables ψϕint, with ϕ respecting the network’s\n        computational order by being the combination of layer-wise bijections ϕℓ: R|ψℓ| →R|ψℓ|\n         applied to the neurons of each of the DNN’s layers (ℓ);\n\n        • IA and IN are a maximal input-restricted intervention set. A maximal input-restricted\n         intervention set is composed of all interventions produced from other input-restricted\n          interventions, i.e., it is a set with hψϕ ←cψϕ (where ψϕ ⊆ψϕint) or vη ←cη (where η ⊆\n                                                                       ψϕ\n         ηint) where cψϕ or cη arise from valid input-restricted computations (e.g., cψϕ = f N  (x)\n        or cη = f :ηA (x, cη ←f :ηA (x))).\n\n        • τ is surjective;\n\n        • IA = ωτ(IN);\n\n        • There exists a surjective τηx such that\n                         ψϕint                 :ηint\n                ∀    : τ(f N   (x, IN)) = f A   (τηx(x), IA)  where IA = ωτ(IN)        (35)\n                x∈X\n                     IN∈IN\n\nF.3  Useful Definitions and Lemmas for Theorem 1\n\nDefinition 10. We say the composition of a function f  : Rd →∆|Y|−1 with argmax is strictly\nsurjective if, for any output y⋆∈Y, there exists an input h ∈Rd for which f outputs y⋆no matter\nhow ties are broken in the argmax. Formally:\n             ∀y⋆∈Y, ∃h ∈Rd, ∀y′ ∈Y \\ {y⋆} : [f(h)]y⋆> [f(h)]y′                (36)\nLemma 1. Let N be a DNN and A be an algorithm. Further, let τ be a distributed abstraction map\nwith partition Ψ and {τη}η∈ηint.  If, for all η ∈ηℓ, τη satisfies the conditions in 1  (defined in\nTheorem 1’s proof) applied on layer ℓ’s pre-intervention hidden variables, i.e., if:\n                           η                        ψϕℓ\n                                                                 N  (x, IN)), and IA = ωτ(IN)     (37)      ∀     : τη(h′ψϕη) = f A(x, IA),     where h′ψϕℓ= f       IN∈I:ℓ−1N\n     x∈X    | tested {zcondition }     pre-int. |hidden variable,{z       as} IN∈I:ℓ−1N\n                ψϕℓ               :ℓ\nwhere we note that f N = ϕℓ◦f N when no intervention is applied to layer ℓ. Then τη also satisfies\nthis condition when applied to layer ℓ’s post-intervention hidden variables:\n                             η                        ψϕℓ        ∀                                                                  N  (x, IN)), and IA = ωτ(IN)    (38)                     : τη(h′ψϕη) = f A(x, IA),     where h′ψϕℓ= f          IN∈I:ℓ                   N\n      x∈X   | tested {zcondition }     post-int.| hidden {zvariable, }as IN∈I:ℓN\n\nProof. Let τη be the abstraction map of η ∈ηℓ. By assumption, condition 1 holds for all pre-\nintervention hidden variables, i.e., hidden variables of the form h′                                    = [ϕℓ(f :ℓN (x, IN))]ψϕη. We can                                                  ψϕη\nshow the same function applies to post-intervention hidden variables, i.e., hidden variables of the form:\n              cψϕη            if h′ψϕη ←cψϕη ∈IN                                 h′              =                                                                                            (39)                 ψϕη                             ϕℓ(f :ℓN (x, IN))               else\n                           ψϕη\nNow let IN ∈I:ℓN be any intervention and x ∈X be any input. Further, let IA = ωτ(IN). If IN ∈I:ℓ−1N    ,\nthen the post-intervention hidden variable is identical to a pre-intervention one, and the conditions in\n 1  still hold, i.e.,: h′               ψϕη            = [ϕℓ(f :ℓN (x, IN))]ψϕη and τη is such that τη(h′ψϕη) = f ηA(x, IA). If IN ̸∈I:ℓ−1N    ,\nfor each node’s hidden variables ψϕη, we might or not intervene on it. If we do not intervene on node\nη, then we still have the case h′                  = [ϕℓ(f :ℓN (x, IN))]ψϕη and thus τη still gives us the correct solution,                       ψϕη\ni.e., τη(h′       ψϕη) = f ηA(x, IA). If we intervene on η, then we know there exists an intervention of form\n\n\n                                       26\n\nhψϕη ←f :ℓN (x′, I′N) in IN, for which I′N ∈I:ℓ−1N     , as our interventions are input-restricted. We also\nknow (by eq. (4)) that there exists an equivalent intervention vη ←τη(f :ℓN (x′, I′N)) in IA. We thus\nhave that τη(h′           ψϕη) = f ηA(x, IA).\n\nLemma 2. Let N be a DNN and A be an algorithm. Further, let τ be a distributed abstraction map\nwith partition Ψ. If, for all η ∈η:ℓ−1, there exists a function gη which satisfies the conditions in 3\n(defined in Theorem 1’s proof) applied on layer ℓ’s pre-intervention hidden variables, i.e., if:\n                              η                        ψϕℓ\n                                                                    N  (x, IN), and IA = ωτ(IN)    (40)     ∀     : gη(h′ψϕℓ∩ψϕ⊥) = f A(x, IA),     where h′ψϕℓ= f     IN∈I:ℓ−1N\n    x∈X    |   tested {zcondition   }     pre-int. |hidden variable,{z       as}  IN∈I:ℓ−1N\n                ψϕℓ               :ℓ\nwhere we note that f N = ϕℓ◦f N when no intervention is applied to layer ℓ. Then gη also satisfied\nthis condition when applied to layer ℓ’s post-intervention hidden variables:\n                             η                        ψϕℓ     ∀                                                                   N  (x, IN), and IA = ωτ(IN)    (41)                : gη(h′ψϕℓ∩ψϕ⊥) = f A(x, IA),     where h′ψϕℓ= f      IN∈I:ℓ             N\n    x∈X   |   tested {zcondition   }     post-int.| hidden {zvariable, }as IN∈I:ℓN\n\nProof. Let η ∈η:ℓ−1 and gη be a function that satisfies condition 3  for it. Note that condition\n 3  holds for all pre-intervention hidden variables, i.e., hidden variables of the form h′                                                                    ψϕℓ∩ψϕ⊥=\n[ϕℓ(f :ℓN (x, IN))]ψϕℓ∩ψϕ⊥. We can show the same function gη also applies to post-intervention hidden\nvariables, i.e., hidden variables of the form:\n             ( cψϕℓ∩ψϕ⊥            if h′ψϕℓ∩ψϕ⊥←cψϕℓ∩ψϕ⊥∈IN            h′                                                                                            (42)                                                 :ℓ           ψϕℓ∩ψϕ⊥=    ϕℓ(f                                     N (x, IN)) ψϕℓ∩ψϕ⊥                  else\nNow, let IN ∈I:ℓN , x ∈X. Further, let IA = ωτ(IN).  If IN ∈I:ℓ−1N    , then the post-intervention\nhidden variable is identical to a pre-intervention one, and the conditions in  3  still hold, i.e.,:\nh′                                                                                                    N    , it ψϕℓ∩ψϕ⊥= [ϕℓ(f :ℓ                   N (x, IN))]ψϕℓ∩ψϕ⊥and gη is such that gη(h′ψϕℓ∩ψϕ⊥) = f ηA(x, IA). If IN ̸∈I:ℓ−1\nmeans that we intervene on at least one hidden variable in this layer ψϕℓ. However, we never intervene\n                                                                                         N (x, IN))]ψϕℓ∩ψϕ⊥on neurons in ψϕ⊥, meaning that for those we still have the case h′ψϕℓ∩ψϕ⊥= [ϕℓ(f :ℓ\nand thus the same function gη still satisfies our condition gη(h′                                                ψϕℓ∩ψϕ⊥) = f ηA(x, IA).\n\nLemma 3. Under Assump. 1 and given a fixed ϕ, the set of input-restricted interventions IN is\ncountable.\n\nProof. This can be shown by induction. More specifically, we show that for any layer ℓ, the set of\ninput-restricted interventions I:ℓN is countable for a specific ϕ.\nBase Case (ℓ= 0).  The base case can be proved trivially, as I:0N = {∅}.\nInduction step (ℓgiven ℓ−1).  By the induction hypothesis, I:ℓ−1N    is countable. Now, note that\nI:ℓN can be decomposed as:\n                                      I:ℓN = I:ℓ−1N  × IℓN                                   (43)\nAs the Cartesian product of two countable sets is itself countable, and as I:ℓ−1N    is countable by the\ninductive hypothesis, we only need to show that IℓN is countable to complete our proof. This set IℓN\nis defined as the set of all input-restricted interventions to layer ℓ. Given a set of neurons or hidden\n                                                                                     ψ′\nvariables in this layer ψ′, we are thus dealing with interventions of the form: hψ′ ←f N (x, IN),\nwhere: (i) ψ′ ⊆ψℓor ψ′ ⊆ψϕℓ; (ii) x ∈X; and (iii) IN ∈I:ℓ−1N     . The set of all input-restricted\n                                               × I:ℓ−1N     .interventions in this layer is thus bounded in size by the Cartesian product:×ψ∈ψϕℓX\nThese three sets are countable, and thus so is IℓN. This concludes our proof.\n\nLemma 4. Under Assump. 1 and given a fixed ϕ, the set of input-restricted pre-intervention hidden\nstates in layer ℓ, i.e., H◆ψℓ, is countable.\n\n\n                                       27\n\nProof. The set of input-restricted hidden states H◆ψℓis formed by hidden states hψℓ= f ψℓN  (x, IN),\nwhich we can write as:\n                H◆ψℓ= {f ψℓN  (x, IN) | x ∈X, IN ∈I:ℓ−1N   }                        (44)\nWe thus have that the size of H◆ψℓis bounded by the size of the Cartesian product X × I:ℓN . As both\nof these sets are countable (by Assump. 1 and Lemma 3, respectively), H◆ψℓis also countable. This\ncompletes our proof.\n\n\nLemma 5. Under Assump. 1 and given a fixed ϕ, the set of input-restricted intervention-only hidden\nvariables in layer ℓ, i.e., H❖   is countable.                     ψϕℓ,\n\n\nProof. A similar proof to Lemma 4 applies here. In short, we have three relevant sets for this proof.\nFirst, the set of input-restricted pre-intervention hidden variables:\n                   H◆                              hψℓ∈H◆ψℓ}                            (45)                           ψϕℓ= {ϕℓ(hψℓ) |\nSecond, we have the set of input-restricted post-intervention hidden variables:\n                       \n\n                                                                                            (46)   Hψϕℓ=       {ϕℓ(hψℓ)ψϕη  | hψℓ∈H◆ψℓ}    {ϕℓ(hψℓ)ψϕ⊥∩ψϕℓ| hψℓ∈H◆ψℓ}    ×η∈ηℓ              ×\n                   Pre-int.|      h.v.,{zprojected to }ψϕη     Pre-int.|        h.v.,{zprojected to ψϕ}⊥\nBoth sets above are countable, since H◆ψℓis countable (by Lemma 4), and ηℓis finite. Third, we have\nthe set of input-restricted intervention-only hidden variables, defined as:\n                       H❖                               H◆                                                                                            (47)                                ψϕℓ= Hψϕℓ\\                                            ψϕℓ\n                                  clearly also countable. This completes the proof.Since Hψϕℓis countable, H❖ψϕℓis\n\n\nLemma 6. Under Assump. 3 and given a target output y⋆∈Y, we know that there is an uncountably\ninfinite set H(y⋆)ψℓ  which predicts it, i.e.,:\n                   h ∈H(y⋆)ψℓ ⇔y⋆= argmax [f ℓ:N (h)]y′                         (48)\n                                              y′∈Y\n\n\nProof. Under Assump. 3, we know that—for any target output y⋆∈Y—there is at least one hidden\nstate h⋆∈R|ψℓ| which predicts it, i.e.:\n                     y⋆= argmax [f ℓ:N (h⋆)]y′                                (49)\n                                       y′∈Y\nwhere we note that f ℓ:N (h⋆) outputs a probability distribution over Y, i.e., pN(y′ | h⋆).\nTo show that we have an uncountably infinite set, let us first notice that\n                                        ℓ:            ℓ:       m1 −m2                              ℓ:         ∀h′ ∈R|ψℓ| : ||f N (h) −f N (h′)||2 <     ⇒y⋆= argmax [f N (h′)]y′       (50)\n                                           2              y′∈Y\nfor m1 be the max value of f ℓ:N (h) and m2 the second highest value of f ℓ:N (h). m1 > m2 follows by\nthe strict subjectivity mentioned in Assump. 3. Eq. (50) follows by the definition of the euclidean\nnorm (||.||2), argmax and f ℓ:N (h′) ∈∆|ψL|−1 as m1 has to be lowered at least m1−m22    to increase\nm2 by m1−m22    for those two values to be the same. Increasing any other value in f ℓ:N (h) would\nrequire m1 being lowered more than m1−m22    or any other value increased by more than m1−m22     .\nNow, given continuity of neural networks, we know that:\n                    ∀ϵ > 0, ∃δ > 0, ∀h′ ∈R|ψℓ| : 0 < ||h −h′||2 < δ,\n                      ⇒||f ℓ:N (h) −f ℓ:N (h′)||2 < ϵ.                                 (51)\n\n\n                                       28\n\nTherefore, we see that:\n                      ∃δ > 0, ∀h′ ∈R|ψℓ| : 0 < ||h −h′||2 < δ,\n                                                           ℓ:            ℓ:       m1 −m2                        ⇒||f N (h) −f N (h′)||2 <                               (52a)                                                      2\n                ⇒y⋆= argmax [f ℓ:N (h′)]y′                             (52b)\n                                        y′∈Y\nWe notice that ||h −h′||2 < δ for δ > 0 denotes a continuous region in R|ψℓ| which therefore\nincludes uncountably infinite points.\n\n\nG  Transformers at Initialisation are Almost Surely Injective on each Layer\n\nTheorem 2. Transformers like DNN 2 with randomly independent initialised from a continuous\ndistribution (riicd.) weights are almost surely injective at initialisation up to each layer 0 ≤ℓ< L.\n\nProof. To show injectivity up to a layer ℓ′ in a transformer, it suffices to show that f ℓN is injective on\nany countable subset H of its domain for all layers ℓ(0 ≤ℓ≤ℓ′). This suffices as we assume the set\nof inputs X is countable, and the composition of injective functions is injective. Let Θ be the random\nvariable representing the transformer’s weights. To show (almost sure) injectivity on layer ℓfor any\nfixed input set H, we need that (because of Lemma 10):14\n                 pΘ ∀h1, h2 ∈H, h1 ̸= h2 : f ℓN(h1) ̸= f ℓN(h2) = 1                  (53)\n\nSince the transformer operates over sequences of tokens, any element h ∈H has its first dimension\nindexing the sequence length. Let |h| denote the sequence length and [h]t refer to the t-th element\nin h. Let T be the set of token positions T = {1, . . . , min(|h1|, |h2|)}. For injectivity, it suffices to\nshow that:\n             pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t : [f ℓN(h1)]t ̸= [f ℓN(h2)]t) = 1           (54)\nNote that eq. (54) only ensures injectivity when |h1| = |h2|. However, this is sufficient because\nwhen |h1| ̸= |h2|, eq. (53) follows trivially: since |f ℓN(h1)| ̸= |f ℓN(h2)|, we immediately have\nf ℓN(h1) ̸= f ℓN(h2). When |h1| = |h2|, we can show that eq. (54) implies eq. (53) as follows: if\nh1 ̸= h2, then there exists at least one token position t′ ∈T where [h1]t′ ̸= [h2]t′. By eq. (54), this\nimplies [f ℓN(h1)]t′ ̸= [f ℓN(h2)]t′ almost surely, and therefore f ℓN(h1) ̸= f ℓN(h2) almost surely.\nWe observe that a transformer’s input set X consists of all sequences formed from a finite token\nvocabulary, which is countably infinite. Since transformers are deterministic functions, the input set\nH encountered at any sublayer is also countably infinite. Therefore, it suffices to prove eq. (54) for\nany fixed countably infinite input set H.\n\nWe show that Eq. (54) holds for any fixed countably infinite subset H of the layer’s domain. This is\nestablished for the embedding layer (f ℓN(h) = eh), the MLP layer (f ℓN(h) = h+mlp(LN(h))), and the\nattention layer (f ℓN(h) = h + attn(LN(h))) by Lemma 7, Lemma 8, and Lemma 9, respectively.\n\n\nThe 3 theorems facilitating the proof above are:\n\nLemma 7. Lets assume we have an embedding layer randomly independent initialized from a\ncontinuous distribution (riicd.) weights and any countably infinite input sets (in embeddings token\nindexes). We denote the set of random variables over the weights as Θ. We then can show for any\nfixed countably infinite input set H that this Layer is injective almost surely.\n                pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t : [eh1]t ̸= [eh2]t) = 1               (55)\n\n\nProof. See App. G.2.\n\n\nLemma 8. Lets assume we have a sub-block consisting of an MLP with a residual connection and\nlayer norm (i.e., h + (mlp(LN(h))) with riicd. weights. We can show that, for any fixed countably\n\n  14We note that pZ(∀z ∈Z : z), where Z is a set of events, is the same as pZ(∩z∈Z{z}) formally.\n\n\n                                       29\n\ninfinite input set H, this layer is injective almost surely:\n               pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t :                                   (56)\n                           [h1 + mlp(LN(h1))]t ̸= [h2 + mlp(LN(h2))]t) = 1\n\n\nProof. See App. G.3.\n\n\nLemma 9. Lets assume we have a sub-block consisting of a self-attention with a residual connection\nand layer norm (i.e., h+attn(LN(h))) with riicd. weights. We can show that, for any fixed countably\ninfinite input set H, this layer is injective almost surely:\n                pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t :                                  (57)\n                       [h1 + attn(LN(h1))]t ̸= [h2 + attn(LN(h2))]t) = 1\n\n\nProof. See App. G.4\n\n\nG.1  Fundamental Lemmas\n\nIn this section we present some fundamental lemmas used to prove G.2 to G.4.\nLemma 10. For a layers function f ℓN to be injective on its input set H, it has to hold that:\n                  pΘ(∀h1, h2 ∈H : h1 ̸= h2 ⇒f ℓN(h1) ̸= f ℓN(h2)) = 1                 (58)\nThis can equivalently be written as:\n                 pΘ ∀h1, h2 ∈H, h1 ̸= h2 : f ℓN(h1) ̸= f ℓN(h2) = 1                  (59)\n\n\nProof. We can derive eq. (59) from eq. (58):\n pΘ(∀h1, h2 ∈H : h1 ̸= h2 ⇒f ℓN(h1) ̸= f ℓN(h2))\n                         \n    = pΘ       \\    h1 ̸= h2 ⇒f ℓN(h1) ̸= f ℓN(h2)                                  (60a)      \n             h1,h2∈H\n\n                                 \n    = pΘ       \\     (h1 ̸= h2) ∧f ℓN(h1) ̸= f ℓN(h2) ∨(h1 = h2)                  (60b)      \n             h1,h2∈H\n\n                                        \n\n                                                \n                                                                                                                                         (60c)    = pΘ        \\     (h1 ̸= h2) ∧f ℓN(h1) ̸= f ℓN(h2) ∩  \\    {(h1 = h2)}                                                \n       h1,h2∈H,h1̸=h2                              h1,h2∈H,h1=h2      \n                                        \n                                                         |      always{z true      }\n                           \n                                                                                       (60d)    = pΘ              \\        ̸= h2) ∧f ℓN(h1) ̸= f ℓN(h2)               h1,h2∈H,h1̸=h2 (h1always|   {z true}         \n                      \n    = pΘ              \\      f ℓN(h1) ̸= f ℓN(h2)                                        (60e)\n               h1,h2∈H,h1̸=h2\n    = pΘ ∀h1, h2 ∈H, h1 ̸= h2 : f ℓN(h1) ̸= f ℓN(h2)                                     (60f)\n\n\n\nLemma 11.  If we have a countable set Z of almost sure events z, we know that their intersection is\nalso almost surely. Formally:\n\n                    ∀z ∈Z : p(z) = 1  =⇒   p(∀z ∈Z : z) = 1                     (61)\n\n\n                                       30\n\nProof. First, observe that\n                !           !c!         !\n                  p \\ z = 1 −p  \\ z   = 1 −p [ zc                   (62)\n\n                    z∈Z             z∈Z              z∈Z\nwhere zc is the complement of an event z. Since p(z) = 1 for all z ∈Z, it follows that p(zc) = 0 for\nall z ∈Z By the countable subadditivity of probability measures:\n                     !\n                        p [ zc ≤ X p(zc) = X 0 = 0                         (63)\n\n                           z∈Z      z∈Z       z∈Z\nTherefore,\n                           !\n                       p(∀z ∈Z : z) = p \\ z = 1 −0 = 1                         (64)\n\n                                      z∈Z\n\n\nG.2  Proof of Lemma 7\n\nIn this section, we will prove Lemma 7 which states that the embedding layer is almost surely injective\non countably infinite inputs.\n\nLemma 7. Lets assume we have an embedding layer randomly independent initialized from a\ncontinuous distribution (riicd.) weights and any countably infinite input sets (in embeddings token\nindexes). We denote the set of random variables over the weights as Θ. We then can show for any\nfixed countably infinite input set H that this Layer is injective almost surely.\n                pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t : [eh1]t ̸= [eh2]t) = 1               (55)\n\n\nProof. We can apply Lemma 11 three times (on h1, h2 and t) to show that eq. (55) is equivalent to,\nfor any h1, h2 ∈H and t ∈T for which [h1]t ̸= [h2]t, it holding that:\n                                   pΘ([eh1]t ̸= [eh2]t) = 1                                (65)\nWe further note that, by the definition of an embedding block (Submodule 2):\n                   pΘ([eh1]t ̸= [eh2]t) = 1 ⇔   pΘ(e[h1]t ̸= e[h2]t) = 1                (66)\nWe can thus apply the law of total probability by defining Θ′ as all the random variables Θ except the\none for the first element of e[h1]t, i.e., except [e[h1]t]1,15 and e′[h1]t as the embedding of [h1]t without\nthe first element:\n      Z\n           pΘ\\Θ′(e[h1]t ̸= e[h2]t | e′[h1]t, e[h2]t)pΘ′(e′[h1]t ∪e[h2]t)d(e′[h1]t ∪e[h2]t) = 1     (67)\nIt therefore suffices to show that, for any e′[h1]t and e[h2]t:\n                            pΘ\\Θ′(e[h1]t ̸= e[h2]t | e′[h1]t, e[h2]t) = 1                        (68)\nThis holds trivially when any embedding dimension other than the first of e[h1]t and e[h2]t differs.\nWhen all dimensions except the first are equal, we apply:\n                    pΘ\\Θ′([e[h1]t]1 ̸= [e[h2]t]1 | e′[h1]t, e[h2]t) = 1                           (69)\n          ⇔   pΘ\\Θ′([e[h1]t]1 = [e[h2]t]1 | e′[h1]t, e[h2]t}) = 0\nThe right-hand side [e[h2]t]1 is a constant while the left-hand side [e[h1]t]1 is a random variable over\na continuous region; this event has measure 0, resulting in probability 0.\n\n\nG.3  Proof of Lemma 8\n\nIn this section, we will prove Lemma 8, which will show that the block consisting of an MLP, residual\nconnection and layer norm is almost sure injective on its countably infinite inputs.\n\nLemma 8. Lets assume we have a sub-block consisting of an MLP with a residual connection and\nlayer norm (i.e., h + (mlp(LN(h))) with riicd. weights. We can show that, for any fixed countably\n\n  15By this we refer to the first element of the embedding vector of [h1]t.\n\n\n                                       31\n\ninfinite input set H, this layer is injective almost surely:\n               pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t :                                   (56)\n                           [h1 + mlp(LN(h1))]t ̸= [h2 + mlp(LN(h2))]t) = 1\n\n\n\nProof. For notational convenience, let mi = mlp(LN(hi)). Given Lemma 11, it suffices to prove that\nfor any h1, h2 ∈H and t ∈T, where [h1]t ̸= [h2]t, we have:\n\n                      pΘ [h1 + m1]t ̸= [h2 + m2]t = 1                          (70)\n\nWithout loss of generality, fix one such h1, h2 ∈H and t ∈T. We can manipulate this probability\ndistribution as:\n\n    pΘ [h1 + m1]t ̸= [h2 + m2]t\n\n      = pΘ  [h1]t + [m1]t ̸= [h2]t + [m2]t\n\n      = pΘ [m1]t ̸= [m2]t pΘ  [h1]t + [m1]t ̸= [h2]t + [m2]t | [m1]t ̸= [m2]t       (71a)\n\n        + pΘ [m1]t = [m2]t)  pΘ  [h1]t + [m1]t ̸= [h2]t + [m2]t | [m1]t = [m2]t\n\n                                    |                 =1, since [h1]t̸=[h2]t{z                      }\n\n      = pΘ [m1]t ̸= [m2]t pΘ  [h1]t + [m1]t ̸= [h2]t + [m2]t | [m1]t ̸= [m2]t       (71b)\n\n        + pΘ [m1]t = [m2]t)\n\nTherefore, it suffices to show that:\n\n               pΘ  [h1]t + [m1]t ̸= [h2]t + [m2]t | [m1]t ̸= [m2]t = 1                (72)\n\nsince pΘ [m1]t ̸= [m2]t) + pΘ [m1]t = [m2]t)   is trivially 1. We now unfold the last layer of\nthe MLP as mi = WL(m′i) + bL, where m′i = σ(f :L−1NMLP  (LN(hi))). We can rewrite eq. (72) as:\npΘ  [h1]t + [WLm′1 + bL]t ̸= [h2]t + [WLm′2 + bL]t | [m1]t ̸= [m2]t\n\n  = pΘ  [h1]t + WL[m′1]t ̸= [h2]t + WL[m′2]t | [m1]t ̸= [m2]t                          (73a)\n\n    (1)\n  = pΘ  [h1]t + WL[m′1]t ̸= [h2]t + WL[m′2]t | [m′1][t,i] ̸= [m′2][t,i]                    (73b)\n\n    (2)\n  ≥pΘ  [h1][t,1] + [WLm′1][t,1] ̸= [h2][t,1] + [WLm′2][t,1] | [m′1][t,i] ̸= [m′2][t,i]          (73c)\n\n                          |[h1]t|                                      |[h2]t|\n  = pΘ  [h1][t,1] + X [WL][1,j][m′1][t,j] ̸= [h2][t,1] + X [WL][1,j][m′2][t,j] | [m′1][t,i] ̸= [m′2][t,i]\n                  j=1                           j=1\n                                                                                       (73d)\n                                      |[h1]t|\n    (3) Z\n  =       pΘ′  [h1][t,1] + X [WL][1,j][m′1]j ̸= [h2][t,1]+                              (73e)\n       Θ\\Θ′\n                  |[h2]t|         j=1\n    X [WL][1,j][m′2][t,j] | [m′1][t,i] ̸= [m′2][t,i], θ  pΘ\\Θ′(θ | [m′1][t,i] ̸= [m′2][t,i])dθ\n             j=1\nwhere equality (1) holds since [m1]t ̸= [m2]t implies there exists some index i such that [m′1][t,i] ̸=\n[m′2][t,i]..16 (2) holds because if the inequality is satisfied for a single component of the vector, it\nmust also be satisfied for the entire vector. In (3), we define Θ′ as the random variable responsible\nfor the value of [WL][1,i] and θ as a realisation of the random variables Θ \\ Θ′. Therefore, to prove\n\n\n   16[m′1][t,i] represents a two dimensional indexing, referring to the i-th element of the representation of the\nt-th token.\n\n\n                                       32\n\neq. (72), it suffices to show:\n\n                     |[h1]t|                                       |[h2]t|\npΘ′  [h1][t,1] + X [WL][1,j][m′1]j ̸= [h2][t,1] + X [WL][1,j][m′2][t,j] | [m′1][t,i] ̸= [m′2][t,i], θ\n               j=1                           j=1\n                                            = 1      (74)\n\nFor brevity, we omit repeating the conditions in the following probabilities as they remain unchanged\nto the previous equation:\n\n                       |[h1]t|                                       |[h2]t|\n  pΘ′  [h1][t,1] + X [WL][1,j][m′1]j ̸= [h2][t,1] + X [WL][1,j][m′2][t,j] | . . . = 1      (75a)\n                j=1                           j=1\n\n                             |[h1]t|                                       |[h2]t|\n  ⇔pΘ′  [h1][t,1] + X [WL][1,j][m′1]j = [h2][t,1] + X [WL][1,j][m′2][t,j] | . . . = 0  (75b)\n                    j=1                           j=1\n  ⇔pΘ′  [WL][1,i][m′1][t,i] −[WL][1,i][m′2][t,i] = [h2][t,1] −[h1][t,1]                     (75c)\n\n                      |[h1]t|                              |[h2]t|\n       + X  [WL][1,j][m′2][t,j] − X  [WL][1,j][m′1][t,j] | . . . = 0\n                  j=1,j̸=i                       j=1,j̸=i\n                           1\n  ⇔pΘ′  [WL][1,i] =                          [h2][t,1] −[h1][t,1]                            (75d)\n                    [m′1][t,i] −[m′2][t,i]\n\n                     |[h2]t|                              |[h1]t|\n       + X  [WL][1,j][m′2][t,j] − X  [WL][1,j][m′1][t,j]   | . . . = 0\n                 j=1,j̸=i                       j=1,j̸=i\nwhere the last step follows from the condition [m′1][t,i] ̸= [m′2][t,i] which ensures the denominator is\nnon-zero. Now, Eq. (75d) holds because the right-hand side is a constant (since its elements are fixed\ngiven the conditions of the probability) while the left-hand side is a random variable drawn from a\ncontinuous distribution (since the weights are riicd.). Therefore, the probability that this equality\nholds is zero, as the event has measure zero.\n\n\nG.4  Proof of Lemma 9\n\nIn this Section, we prove Lemma 9, which establishes that the self-attention sub-block (consisting\nof attention, residual connection, and layer normalisation) is almost surely injective on countably\ninfinite inputs. The proof structure parallels that of Lemma 8, so we highlight the key differences and\nnecessary adaptations without repeating the full derivation.\n\nLemma 9. Lets assume we have a sub-block consisting of a self-attention with a residual connection\nand layer norm (i.e., h+attn(LN(h))) with riicd. weights. We can show that, for any fixed countably\ninfinite input set H, this layer is injective almost surely:\n                pΘ(∀h1, h2 ∈H, t ∈T, [h1]t ̸= [h2]t :                                  (57)\n                       [h1 + attn(LN(h1))]t ̸= [h2 + attn(LN(h2))]t) = 1\n\nProof. We follow a proof strategy analogous to that of Lemma 8 in App. G.3. Following the same\nsteps up to eq. (72), it suffices to show for this lemma that for any h1, h2 ∈H and t ∈T, where\n[h1]t ̸= [h2]t, we have:\n\n pΘ  [h1]t + [attn(LN(h1))]t ̸= [h2]t + [attn(LN(h2))]t | [attn(LN(h1))]t ̸= [attn(LN(h2))]t\n\n                                             = 1     (76)\nWe can write this according to the definition of an attention block (Submodule 1):\n  pΘ  [h1]t + (WO)T [h′1]t ̸= [h2]t + (WO)T [h′2]t | [attn(LN(h1))]t ̸= [attn(LN(h2))]t    (77)\nwhere h′1 and h′2 are the hidden states after concatenation in the self-attention mechanism (see\neq. (12)). The remainder of the proof follows the same approach as the proof of Lemma 8 in App. G.3,\nstarting from eq. (73a).\n\n\n                                       33\n\n              All Pairs     Same Output   Not Same Output  Same Variables  Not Same Variables\n\n   Input   8.5e−2 ± 1.1e−2  8.5e−2 ± 1.1e−2   1.7e−1 ± 1.9e−2    8.5e−2 ± 1.1e−2     1.6e−1 ± 1.9e−2\n Layer 1  5.7e−4 ± 4.5e−4  5.7e−4 ± 4.5e−4   2.4e−2 ± 6.2e−3    5.7e−4 ± 4.5e−4     1.1e−2 ± 4.3e−3\n Layer 2  4.5e−4 ± 3.8e−4  4.5e−4 ± 3.8e−4   3.2e−2 ± 1.2e−2    4.5e−4 ± 3.8e−4     1.2e−2 ± 6.7e−3\n Layer 3  3.3e−4 ± 2.8e−4  3.3e−4 ± 2.8e−4   8.9e−2 ± 4.3e−2    3.3e−4 ± 2.8e−4     1.5e−2 ± 1.0e−2\n\nTable 1: An approximation of the minimal Euclidean distance of the trained MLP model in the\nhierarchical equality task using 1,280,000 samples. The minimal Euclidean distance is computed\nbetween all samples to a randomly selected subset of 10,000 samples. We compute it over 10 different\nrandom seeds and present the mean and standard deviation (the number after ±).\n\nH MLP Injectivity in Hierarchical Equality Task\n\nWe see in Fig. 2 that the IIA remains low for the identity of first argument algorithm on\na fully trained model even when using a ϕnonlin alignment map (based on revnet). A reasonable\nassumption for why would be that the fully trained model does not fulfil some assumption required\nby our proof of Theorem 1 (any algorithm is an input-restricted distributed abstraction for any model)\ngiven in §4. In this section, we present follow-up experiments investigating the reason for this\ndisagreement between our empirical results on the identity of first argument algorithm and\nthe theoretical result of Theorem 1.\n\nLet us first note that to prove Theorem 1 we rely on an existence proof: showing there exists a\nfunction ϕ which satisfies the conditions for a DNN to be abstracted by an algorithm. It says nothing,\nhowever, about this function being learnable in practice. Our experiments, however, measure IIA on\nan unseen test set—which requires ϕnonlin to not only fit a training set, but generalise to new data.\nTherefore, following our proof of Theorem 1 we explore the IIA on the train set. However, on the\nnormal training set (with 1,280,000 samples), we still do not get an IIA over 0.55. On the other hand,\nif we repeat the experiment with only 1,000 training samples, we see ϕnonlin achieves an IIA of over\n0.99 on the training set. Therefore, it is likely that the revnet used when defining ϕnonlin does not\nhave enough capacity to fit the overly complex function our proof describes.\n\nTo further analyse why the capacity of the used revnet is not sufficient, we analyse the injectivity of\nthe evaluated MLP by investigating its hidden representations. We first evaluate 1,280,000 randomly\nsampled inputs and their hidden states, checking if they are all unique. In these 1,280,000 samples\n(and repeating this experiment with 10 different random seeds), no collisions were found, implying\nthe evaluated MLP is (at least close to) injective.\n\nWe now examine the supposition that the model finds it more difficult to distinguish between\nhidden states that share the same values for the variables in both equality relations than\nbetween those that do not. To this end, we compute the minimal Euclidean distance between hidden\nstates across the entire set of 1,280,000 samples to a randomly selected subset of 10,000 samples.\nSpecifically, we measure the minimal pairwise Euclidean distance among: (i) all sample pairs, (ii)\nsample pairs sharing the same output, (iii) sample pairs sharing the same values for both equality\nvariables in both equality relations, (iv) sample pairs that do not share the same output\nand (v) sample pairs that do not share the same values for both equality variables. The results are\npresented in Table 1. We observe that the minimal Euclidean distances are smaller for pairs sharing\nthe same output or the same equality-variable values compared to pairs that do not. This suggests\nthat, although injectivity is preserved, a RevNet likely will find it more challenging to separate\nhidden states that share variable values.\n\nI  Additional Experiment Details\n\nIn this section, we present additional details about our hierarchical equality task (in App. I.1) and\nindirect object identification (in App. I.2) experiments. We also present details and results on the\ndistributive law task (in App. I.3).\n\nI.1  Hierarchical Equality Task\n\nTask 1 (from Geiger et al., 2024b). The hierarchical equality task is defined as follows. Let\nx = x1 ◦x2 ◦x3 ◦x4 be a 16-dimensional vector, where each xi ∈R4 for i ∈{1, 2, 3, 4}, and\n◦denotes vector concatenation. The input space is X = [−0.5, 0.5]16, and the output space is\n\n\n                                       34\n\nY = {false, true}. The task function is:\n\n                        T(x) =  (x1 == x2) == (x3 == x4)  ,                        (78)\nwhere the equality (xi == xj) holds if and only if xi and xj are equal as vectors in R4.\n\nI.1.1  Algorithms\n\nWe define the following three candidate algorithms in detail.\nAlg 1. The both equality relations alg. to solve Task 1 has ηinner={ηx1==x2,ηx3==x4}\nand:\n                           ηx1==x2\n                      f A       (vparA(ηx1==x2)) = (vηx1 == vηx2)\n                           ηx3==x4\n                      f A       (vparA(ηx3==x4)) = (vηx3 == vηx4)\n                            ηy\n                      f A (vparA(ηy)) = (vηx1==x2 == vηx3==x4)\nAlg 2. The left equality relation alg. to solve Task 1 has ηinner ={ηx1==x2} and:\n                        ηx1==x2\n                    f A       (vparA(ηx1==x2)) = (vηx1 == vηx2)\n                          ηy\n                    f A (vparA(ηy)) = (vηx1==x2 == (vηx3 == vηx4))\nAlg 3. The identity of first argument alg. to solve Task 1 has ηinner={ηx1} and:\n                          ηx1\n                   f A  (parA(ηx1)) = x1\n                         ηy\n                   f A (parA(ηy)) = ((vηx1 == x2) == (x3 == x4))\n\nI.1.2  Training Details\n\nFor the hierarchical equality task, we use a 3-layer MLP with |ψ1| = |ψ2| = |ψ3| = 16. The model\nis trained using the Adam optimiser with learning rate 0.001 and cross-entropy loss. We use a batch\nsize of 1024 and train on 1,048,576 samples, with 10,000 samples each for evaluation and testing.\nTraining runs for a maximum of 20 epochs with early stopping after 3 epochs of no improvement.\n\nFor the training progression experiments, we use the same configuration but limit training to 2 epochs.\n\nWhen training the alignment maps ϕ, we use a batch size of 6400 and train for up to 50 epochs\nwith early stopping after 5 epochs of no improvement (using a threshold of 0.001 for the required\nchange, compared to 0 for MLP training). We use the Adam optimiser with learning rate 0.001 and\ncross-entropy loss. To generate the datasets for DAS, for Alg. 1 we intervene with a probability of\n1/3 on ηx1==x2, 1/3 on ηx3==x4, and 1/3 on both variables. The samples for the base and source\ninputs are generated such that (x1 == x2) and (x3 == x4) each hold 50% of the time. For Alg. 2\nand Alg. 3 we intervene on ηx1==x2 and ηx1 for all samples, respectively. For each algorithm, we\nsample 1,280,000 interventions for training, 10,000 for evaluation, and 10,000 for testing.\n\nI.1.3  Additional Results\n\nWe present results for the three candidate algorithms for the hierarchical equality task, analysing the\neffect of hidden size drn and intervention size |ψϕη| across all MLP layers.\nFor the both equality relations algorithm, Fig. 8a, 9a and 9b demonstrate that the hidden size\nexperiment aligns with previously reported trends, while also showing how alignment maps ϕ of\nincreasing complexity perform across training epochs, layers, and intervention sizes.\n\nFor the left equality relation algorithm, as shown in Fig. 8b, 9c and 9d, we observe similar\npatterns: increasing hidden size and intervention size improves performance, and alignment is\ngenerally more successful in later layers during early training.\n\nFor the identity of first argument algorithm, Fig. 8c, 9e and 9f reveal that, interestingly,\nsome alignment is achieved—especially in layer 3—during the first half of training, but this effect\ndiminishes in the second half.\n\nOverall, these results demonstrate that the hidden size experiment is consistent with the findings\nreported in the main paper. They also show that it is easier, in untrained models, to find an alignment\nmap for later layers, and that transient alignment can occur in specific layers and algorithms during\nthe initial stages of training.\n\n\n                                       35\n\n                         Layer 1                                        Layer 2                                        Layer 3\n\n\n      1.0\n\n\n      0.9\n\n\n      0.8\n\n\n      0.7   Accuracy\n      0.6                                                                                                         = 1\n                                                                                                             = 2\n      0.5                                                                                                         = 8\n\n           1   2       4                8                                16       1   2       4                8                                16       1   2       4                8                                16\n                            drn                                              drn                                              drn\n                                       (a) both equality relations\n\n\n                         Layer 1                                        Layer 2                                        Layer 3\n\n      1.0\n\n\n      0.9\n\n\n      0.8\n\n\n      0.7   Accuracy\n      0.6                                                                                                         = 1\n                                                                                                             = 2\n      0.5                                                                                                         = 8\n\n           1   2       4                8                                16       1   2       4                8                                16       1   2       4                8                                16\n                            drn                                              drn                                              drn\n                                      (b) left equality relation\n\n\n                          Layer 1                                        Layer 2                                        Layer 3\n\n      0.520\n\n\n      0.515\n\n\n      0.510\n\n\n      0.505   Accuracy 0.500\n                                                                                                             = 1\n      0.495                                                                                                        = 2\n                                                                                                             = 8\n      0.490\n             1   2       4                8                                16       1   2       4                8                                16       1   2       4                8                                16\n                              drn                                              drn                                              drn\n                                     (c) identity of first argument\n\nFigure 8: Mean IIA over 5 seeds using ϕnonlin (Lrn = 1) on the trained DNN. Performance improves\nwith larger hidden dimension drn and intervention size |ψϕη|. Each subplot corresponds to one of the\nthree candidate algorithms for the hierarchical equality task, showing how the model’s representational\ncapacity influences performance.\n\n\n\n\nI.2  Indirect Object Identification Task\n\nTask 2. The Indirect Object Identification (IOI) task involves predicting the indirect object in\nsentences with a specific structure. Each input x ∈X consists of a text where a subject (S) and an\nindirect object (IO) are introduced, followed by the S giving something to the IO. For example:\n        \"Friends Juana and Kristi found a mango at the bar. Kristi gave it to\" ⇒\"Juana\"\nHere, \"Juana\" and \"Kristi\" are introduced, with \"Kristi\" (S) appearing again before giving something\nto \"Juana\" (IO). The output set Y consists of the first tokens of the two names:\n                Y = {first_token(S), first_token(IO)}                      (79)\n\n\nI.2.1  Algorithm\n\nFor this task, we evaluate the ABAB-ABBA algorithm. Denoting the two names in the story as A and\nB, this algorithm determines whether the sentence follows an ABAB pattern (e.g., \"Friends Juana\nand Kristi found a mango at the bar. Juana gave it to Kristi\") or an ABBA pattern (e.g., \"Friends\nJuana and Kristi found a mango at the bar. Kristi gave it to Juana\"). If the pattern is ABAB (where\nB is the indirect object IO), the algorithm predicts the first token of B. Conversely, for an ABBA\npattern, it predicts the first token of A. In our experiments, we intervene on whether an input follows\nthe ABAB pattern or not.\n\n\n\n                                       36\n\n              = 1             = 2            = 12                  = 1             = 2            = 12\n         1.0                                                                                       1.0\n\n 3    0.8                                3    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 2    0.8                                2    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 1    0.8                                                        DNNlinear     1    0.8                                                        DNNlinear\n\n                                                                  =                                                                                                                                     5,                                                                       =                                                                                                                                 24                                                                                                                                             =                                                                                                                                                                                                                                                                                        5,                                                                                                                                                 =                                                                                                                                                                                                                                                                       24                                                                                                         Lrn                                                                                                                                drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                           drn\n                                                                  =                                                                                                                          10,                                                                       =                                                                                                                                             =                                                                                                                                                                                                                                                                  10,                                                                                                                                                  =                                                                                                                                                                                                                                                                        24                                                                                                         Lrn                                                                                                                                 drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                             drn    Layer Accuracy  0.6                                                                                         Lrn = 1, drn = 24                                                                                                           Lrn = 1, drn = 24                                                                                                                                  24       Layer Accuracy  0.6\n                                                                                                         Lrn = 10, drn = 27                                                                                                          Lrn = 10, drn = 27\n\n           0      1M        Full 0      1M        Full 0      1M        Full          0      1M        Full 0      1M        Full 0      1M        Full\n        DNN Training Steps   DNN Training Steps   DNN Training Steps         DNN Training Steps   DNN Training Steps   DNN Training Steps\n\n    (a) Max IIA for both equality relations      (b) Mean IIA for both equality relations\n\n              = 1             = 2             = 8                  = 1             = 2             = 8\n         1.0                                                                                       1.0\n\n 3    0.8                                3    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 2    0.8                                2    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 1    0.8                                                        DNNlinear     1    0.8                                                        DNNlinear\n\n                                                                  =                                                                                                                                     5,                                                                       =                                                                                                                                 24                                                                                                                                             =                                                                                                                                                                                                                                                                                        5,                                                                                                                                                 =                                                                                                                                                                                                                                                                       24                                                                                                         Lrn                                                                                                                                drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                           drn\n                                                                  =                                                                                                                          10,                                                                       =                                                                                                                                             =                                                                                                                                                                                                                                                                  10,                                                                                                                                                  =                                                                                                                                                                                                                                                                        24                                                                                                         Lrn                                                                                                                                 drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                             drn    Layer Accuracy  0.6                                                                                         Lrn = 1, drn = 24                                                                                                           Lrn = 1, drn = 24                                                                                                                                  24       Layer Accuracy  0.6\n                                                                                                         Lrn = 10, drn = 27                                                                                                          Lrn = 10, drn = 27\n\n           0      1M        Full 0      1M        Full 0      1M        Full          0      1M        Full 0      1M        Full 0      1M        Full\n        DNN Training Steps   DNN Training Steps   DNN Training Steps         DNN Training Steps   DNN Training Steps   DNN Training Steps\n\n     (c) Max IIA for left equality relation       (d) Mean IIA for left equality relation\n\n              = 1             = 2             = 8                  = 1             = 2             = 8\n         1.0                                                                                       1.0\n\n 3    0.8                                3    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 2    0.8                                2    0.8\n    Layer Accuracy  0.6                                                                                                                                                                                        Layer Accuracy  0.6\n\n\n         1.0                                                                                       1.0\n\n 1    0.8                                                        DNNlinear     1    0.8                                                        DNNlinear\n\n                                                                  =                                                                                                                                     5,                                                                       =                                                                                                                                 24                                                                                                                                             =                                                                                                                                                                                                                                                                                        5,                                                                                                                                                 =                                                                                                                                                                                                                                                                       24                                                                                                         Lrn                                                                                                                                drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                           drn\n                                                                  =                                                                                                                          10,                                                                       =                                                                                                                                             =                                                                                                                                                                                                                                                                  10,                                                                                                                                                  =                                                                                                                                                                                                                                                                        24                                                                                                         Lrn                                                                                                                                 drn                                                                                                                                                                                                                                    Lrn                                                                                                                                                                                                                                                                             drn    Layer Accuracy  0.6                                                                                         Lrn = 1, drn = 24                                                                                                           Lrn = 1, drn = 24                                                                                                                                  24       Layer Accuracy  0.6\n                                                                                                         Lrn = 10, drn = 27                                                                                                          Lrn = 10, drn = 27\n\n           0      1M        Full 0      1M        Full 0      1M        Full          0      1M        Full 0      1M        Full 0      1M        Full\n        DNN Training Steps   DNN Training Steps   DNN Training Steps         DNN Training Steps   DNN Training Steps   DNN Training Steps\n\n   (e) Max IIA for identity of first argument   (f) Mean IIA for identity of first argument\n\n Figure 9: IIA over 5 seeds for each combination of MLP layer (rows) and intervention size (columns)\n during training progression for the tested algorithms.\n\n\n Alg 4. The ABAB-ABBA algorithm for the IOI task has one inner node ηinner = {η1} and is defined\n as follows:\n           f η1A (parA(η1)) = check_is_abab_pattern(vηx)                            (80a)\n               ηy              first_token(get_name_b(vηx))    if vη1 = true\n           f A (parA(ηy)) =                                                          (80b)                           first_token(get_name_a(vηx))    if vη1 = false\n Here, get_name_a(x) extracts the  first name (denoted A, e.g.  Juana in our example) and\n get_name_b(x) extracts the second name (denoted B, e.g. Kristi in our example) from the input\n sentence x. The function check_is_abab_pattern(x) returns true if the sentence follows an\n“ABAB” structure (e.g., “A and B ... A gave to B”, meaning B is the IO) and false if it follows\n\n\n                                        37\n\nan “ABBA” structure (e.g., “A and B ... B gave to A”, meaning A is the IO). first_token(Name)\nreturns the first token of the specified name. The output y is the first token of the indirect object.\n\nI.2.2  Training Details\n\nWe use models from the Pythia suite (Biderman\net al., 2023) to evaluate the IIA performance of\nthe different ϕ on the IOI task. Specifically, we\nemploy the Pythia 31M, 70M, 160M, and 410M\nparameter models. We also examine different\ntraining checkpoints provided by these models\nto analyse how IIA evolves during training. To\nassess robustness, we replicate a subset of ex-\nperiments using alternative Pythia model seeds\nfrom (van der Wal et al., 2025).\nWe train all alignment maps on 2 epochs of 106\ninterventions based on data from Muhia (2022),  Figure 10: Cross Entropy loss (y-axis) during train-\nwith a batch size of 256 and a learning rate of  ing (x-axis) of the alignment map for the randomly\n10−4. For all experiments, we set the interven-  initialised Pythia-31m. The loss plateaus between\ntion size |ψϕη| to half of the DNN’s hidden di-  4k and 6k steps, and suddenly drops after 6k steps.\nmension. For smaller models (31M, 70M), we train using float64 precision and a learning rate of\n10−3, as these adjustments proved crucial for convergence. We also note that we observed quite\nsevere grokking behaviour, where models had low IIA for a long time, which quickly jumped to high\nIIA values at a certain point of training (see Fig. 10; wandb run).\n\nI.2.3  Additional Results\n\nRobustness across random seeds.  In Fig. 11, we examine how our main results from §6 generalise\nacross multiple training seeds of the Pythia model. The key trends hold consistently across all 5\nseeds - we can find perfect alignments using ϕnonlin in most cases. However, we observe two notable\nexceptions. For one seed, the DNN fails to learn the IOI task even after full training. For another\nseed, we cannot find an alignment using even complex alignment maps under our current setup.17\nAll other seeds achieve perfect alignment under ϕnonlin. We hypothesise that the alignment failure\ncase is primarily due to suboptimal training of the alignment map. Due to computational constraints,\nwe did not perform extensive hyperparameter tuning that might have achieved convergence.\n\n\n                                                              1.0\n\n                                                              0.8\n                                                                                                                                                                                            Accuracy 0.60.4\n                                                                DNN\n                                                              0.2                                  non-linear\n                                                                                                                        linear\n                                                              0.0\n                                                                                     Init.                 Full\n                                           DNN Training Step\n\nFigure 11: IIA of alignment between the ABAB-ABBA algorithm and the Pythia-410m model across\nmultiple seeds (seeds 1 to 5 from van der Wal et al., 2025), with interventions at layer 12. We evaluate\nthe IIA of both ϕlin and ϕnonlin (with dh = 64, K = 1) on randomly initialised (Init.) and fully\ntrained (Full) DNNs.\n\nGeneralisation across distinct name sets.  In the main paper, we split the dataset from Muhia\n(2022) by ensuring that no two sentences appear in both the training and evaluation sets. However,\nthis splitting strategy does not guarantee that the names themselves are distinct between training and\nevaluation sets. In Fig. 12, we examine the results when using completely different sets of names\nfor training and evaluation. The results differ substantially: we cannot find an alignment using even\ncomplex alignment maps for the randomly initialised DNN. This suggests that IIA on the randomly\ninitialised DNN may depend critically on overlap between the specific entities encountered during\n\n   17These failures occur in different seeds: seed 3 shows poor IIA despite learning the task, while seed 4 fails to\nlearn the IOI task.\n\n\n                                       38\n\ntraining and evaluation. For the fully trained DNN, we observe perfect alignment using ϕnonlin and\nreasonably high alignment using ϕlin.\n\n\n                                 1.00\n\n\n                                 0.75\n                                                            DNN                                                                                                    Accuracy                                                                 linear\n                                                                                                Lrn = 8\n                                 0.50                                                  Lrn = 16\n\n                                   0                                            Full\n                               DNN Training Step\n\nFigure 12: IIA of alignment between the ABAB-ABBA algorithm and the Pythia-410m model using a\ndifferent set of names for training and evaluation, with interventions at layer 12. We evaluate the IIA of\nboth ϕlin and ϕnonlin (with dh = 64, K = 1) on randomly initialised and fully trained (Full) DNNs.\n\nI.3  Distributive Law Task\n\nWe now study a similar task to the hierarchical equality (in App. I.1), based on the distributive law of\nand (∧) and or (∨).\nTask 3. The distributive law task is defined as follows. Let x = x1 ◦x2 ◦x3 ◦x4 ◦x5 ◦x6 be a\n24-dimensional vector, where each xi ∈[−0.5, 0.5]4 for i ∈{1, 2, 3, 4, 5, 6}, and ◦denotes vector\nconcatenation. The input space is X = [−0.5, 0.5]24, and the output space is Y = {false, true}.\nThe task function is\n            T(x) =  (x1 == x2) ∧(x3 == x4) ∨ (x3 == x4) ∧(x5 == x6)  ,         (81)\nwhere the equality (xi == xj) holds if and only if xi and xj are equal as vectors in R4.\n\nI.3.1  Algorithms\n\nWe define the following two candidate algorithms.\nAlg 5. The And-Or-And alg. to solve Task 3 has\n                      ηinner = {η(x1==x2)∧(x3==x4), η(x3==x4)∧(x5==x6)}\nand it is defined as follows:\n        η(x1==x2)∧(x3==x4)\n      f A                 (vparA(η(x1==x2)∧(x3==x4))) = (vηx1 == vηx2) ∧(vηx3 == vηx4)\n        η(x3==x4)∧(x5==x6)\n      f A                 (vparA(η(x3==x4)∧(x5==x6))) = (vηx3 == vηx4) ∧(vηx5 == vηx6)\n        ηy\n      f A (vparA(ηy)) = vη(x1==x2)∧(x3==x4) ∨vη(x3==x4)∧(x5==x6)\nAlg 6. The And-Or alg. to solve Task 3 has\n                           ηinner = {ηx3==x4, η(x1==x2)∨(x5==x6)}\nand it is defined as follows:\n        ηx3==x4\n      f A       (vparA(ηx3==x4)) = (vηx3 == vηx4)\n        η(x1==x2)∨(x5==x6)\n      f A                 (vparA(η(x1==x2)∨(x5==x6))) = (vηx1 == vηx2) ∨(vηx5 == vηx6)\n        ηy\n      f A (vparA(ηy)) = vηx3==x4 ∧vη(x1==x2)∨(x5==x6)\n\nI.3.2  Training Details\n\nFor the distributive law task, we use a 3-layer MLP (see App. E.1) with an input dimensionality\nof 24, hidden layers of dimensionality |ψ1| = |ψ2| = |ψ3| = 24, and an output dimensionality of\n2. The model is trained using the Adam optimiser with a learning rate of 0.001 and cross-entropy\nloss. We use a batch size of 1024. The datasets are generated by randomly sampling input vectors\nx = x1 ◦· · · ◦x6 such that the target label ¯y = T(x) is true 50% of the time. We sample 1,048,576\nsamples for training, 10,000 for evaluation, and 10,000 for testing. Training runs for a maximum of\n20 epochs with early stopping after 3 epochs of no improvement.\n\nFor training ϕ, we use a batch size of 6400 and train for up to 50 epochs with early stopping after 5\nepochs of no improvement (using a threshold of 0.001 for the required change). We use the Adam\n\n\n                                       39\n\noptimiser with learning rate 0.001 and cross-entropy loss. To generate the intervened datasets: For\nAlg. 5, we intervene with a probability of 1/3 on η(x1==x2)∧(x3==x4), 1/3 on η(x3==x4)∧(x5==x6),\nand 1/3 on both variables. For Alg. 6, we intervene with a probability of 1/3 on ηx3==x4, 1/3 on\nη(x1==x2)∨(x5==x6), and 1/3 on both variables. For both algorithms, the samples for the base and\nsource inputs are generated such that the output of the intervention changes compared to the base\ninput 50% of the time. We sample 1,280,000 interventions for training, 10,000 for evaluation , and\n10,000 for testing for each algorithm.\n\nI.3.3  Results\n\nIn this section, we discuss the results on the distributed law task using the And-or-And and And-Or\nalgorithms. Our findings corroborate the results presented in the main paper. As shown in Fig. 13,\nusing linear and identity alignment maps reveals distinct dynamics. The And-Or algorithm achieves\nhigher IIA using ϕlin, particularly in later layers where the IIA of ϕlin on the And-Or-And algorithm\napproaches 0.5. However, these dynamics completely vanish when using a more complex alignment\nmap like ϕnonlin, where we achieve almost perfect IIA everywhere.\n\n\n                     AndOr                             AndOrAnd\n     1.00\n\n     0.75\n\n     0.50         Accuracy\n                                                                                linear                 identity\n     0.25\n                                                                          non-linear\n\n            1   2  12    1   2  12    1   2  12         1   2  12    1   2  12    1   2  12\n             L1          L2          L3              L1          L2          L3\nFigure 13: IIA in the distributive law task for causal abstractions trained with different alignment\nmaps ϕ. The figure shows results for both analysed algorithms for this task. The bars represent the\nmax IIA across 10 runs with different random seeds. The black lines represent mean IIA with 95%\nconfidence intervals. The |ψϕη| denotes the intervention size per node. Without interventions, all\nDNNs reach 100% accuracy. The used ϕnonlin uses Lrn = 10 and drn = 24.\n\n\nFig. 14a and 14c present the evaluated IIA throughout model training. These training progression\nplots show that randomly initialised models often achieve IIA above 0.8 with non-linear alignment\nmaps, supporting our insight that when the notion of causal abstraction is equipped with ϕnonlin it\nmay identify algorithms which are not necessarily implemented by the underlying model. In Fig. 14b\nand 14d, we plot the mean IIA over 5 seeds instead of the maximum IIA.\n\nThe hidden size experiments (Fig. 15a and 15b) show that even RevNets with small dh of 4 achieve\nnear-perfect IIA for And-Or-And, while And-Or never reaches perfect IIA in the second layer,\nregardless of the dh. The training progression plots suggest a possible explanation: IIA for And-Or-\nAnd initially increases in the last two layers but then decreases, while RevNets maintain near-perfect\nIIA. This may indicate that And-Or-And is first implemented with simple encodings detectable by\nlinear ϕs, before evolving into non-linear encodings that only RevNets can detect. The fact that\nAnd-Or never achieves high IIA in later layers further suggests it may not be a true abstraction of the\nmodel’s behaviour, though we note this remains a hypothesis requiring further investigation.\n\nAnd-Or-And Training.  In this section, we analyse a DNN when this model is trained specifically\nto rely on the And-Or-And algorithm (and, consequently, to encode the values of its hidden nodes).\nWe do so with the method from Geiger et al. (2022), training the DNN to encode And-Or-And’s\nhidden nodes’ values in its second layer, with an intervention size of 12. This method is similar\nto how we train ϕ (see App. I.3.2), but ϕ is fixed to the identity function, and the DNN itself is\ntrained; further, the training dataset is composed of 1/4 non-intervened samples, 1/4 samples with\ninterventions on η(x1==x2)∧(x3==x4), 1/4 on η(x3==x4)∧(x5==x6), and 1/4 on both variables. We\nthen evaluate if this DNN abstracts both the And-Or-And and And-Or using different ϕ (as before,\nafter freezing the DNN). The IIA performance of these ϕ is presented in Fig. 16. We can see here that,\nwhen using identity and linear alignment maps ϕ, IIA scores suggest that the And-Or-And algorithm\nseems to be implemented perfectly given the second layer, where we have only around 0.75 IIA for\n\n\n                                       40\n\n             = 1             = 2            = 12                  = 1             = 2            = 12\n       1.0                                                                                       1.0\n\n3    0.8                                3    0.8\n Layer Accuracy  0.6                                                                                                                                                                                    Layer Accuracy  0.6\n\n\n       1.0                                                                                       1.0\n\n2    0.8                                2    0.8\n Layer Accuracy  0.6                                                                                                                                                                                    Layer Accuracy  0.6\n\n\n       1.0                                                                                       1.0\n\n1    0.8                                                       DNNlinear     1    0.8                                                       DNNlinear\n                                                                                                      Lrn = 1, drn = 24                                                                                                           Lrn = 1, drn = 24\n                                                                =                                                                                                                                  5,                                                                     =                                                                                                               24                                                                                                                                           =                                                                                                                                                                                                                                                                                     5,                                                                                                                                                =                                                                                                                                                                                                                                    24                                                                                                      Lrn                                                                                                                             drn                                                                                                                                                                                                                                 Lrn                                                                                                                                                                                                                                                                         drn\n                                                                =                                                                                                                       10,                                                                      =                                                                                                                                           =                                                                                                                                                                                                                                                               10,                                                                                                                                                =                                                                                                                                                                                                                                     24                                                                                                      Lrn                                                                                                                              drn                                                                                                                                                                                                                                 Lrn                                                                                                                                                                                                                                                                          drn Layer Accuracy  0.6                                                                                                                24       Layer Accuracy  0.6\n                                                                                                      Lrn = 10, drn = 27                                                                                                          Lrn = 10, drn = 27\n\n          0      1M        Full 0      1M        Full 0      1M        Full          0      1M        Full 0      1M        Full 0      1M        Full\n       DNN Training Steps   DNN Training Steps   DNN Training Steps         DNN Training Steps   DNN Training Steps   DNN Training Steps\n\n                (a) Max IIA for And-Or                               (b) Max IIA for And-Or\n\n             = 1             = 2            = 12                  = 1             = 2            = 12\n       1.0                                                                                       1.0\n\n3    0.8                                3    0.8\n Layer Accuracy  0.6                                                                                                                                                                                    Layer Accuracy  0.6\n\n\n       1.0                                                                                       1.0\n\n2    0.8                                2    0.8\n Layer Accuracy  0.6                                                                                                                                                                                    Layer Accuracy  0.6\n\n\n       1.0                                                                                       1.0\n\n1    0.8                                                       DNNlinear     1    0.8                                                       DNNlinear\n                                                                                                      Lrn = 1, drn = 24                                                                                                           Lrn = 1, drn = 24\n                                                                =                                                                                                                                  5,                                                                     =                                                                                                               24                                                                                                                                           =                                                                                                                                                                                                                                                                                     5,                                                                                                                                                =                                                                                                                                                                                                                                    24                                                                                                      Lrn                                                                                                                             drn                                                                                                                                                                                                                                 Lrn                                                                                                                                                                                                                                                                         drn\n                                                                =                                                                                                                       10,                                                                      =                                                                                                                                           =                                                                                                                                                                                                                                                               10,                                                                                                                                                =                                                                                                                                                                                                                                     24                                                                                                      Lrn                                                                                                                              drn                                                                                                                                                                                                                                 Lrn                                                                                                                                                                                                                                                                          drn Layer Accuracy  0.6                                                                                                                24       Layer Accuracy  0.6\n                                                                                                      Lrn = 10, drn = 27                                                                                                          Lrn = 10, drn = 27\n\n          0      1M        Full 0      1M        Full 0      1M        Full          0      1M        Full 0      1M        Full 0      1M        Full\n       DNN Training Steps   DNN Training Steps   DNN Training Steps         DNN Training Steps   DNN Training Steps   DNN Training Steps\n\n             (c) Max IIA for And-Or-And                         (d) Max IIA for And-Or-And\n\nFigure 14: IIA over 5 seeds for each combination of MLP layer (rows) and intervention size (columns)\nduring training progression for the evaluated algorithms.\n\nthe And-Or algorithm. However, these differences vanish almost completely using ϕnonlin as our\nalignment map.\n\nAnd-Or Training.  In this section, we report an experiment similar to the above, but we train our\nDNN to rely on the And-Or algorithm instead. These results are shown in Fig. 17. In this figure, we\nagain see that, when using identity and linear as alignment map ϕ, IIA performance suggests that\nthe And-Or algorithm seems to be implemented perfectly given the second layer, where we have\nonly around 0.65 IIA for the And-Or-And algorithm. These differences however vanish when using\nϕnonlin as alignment map, which leads to perfect IIA scores with either algorithm.\n\nJ  Computational Resources\n\nThe experiments on MLP were executed on CPU (10 computers with i7-4770 or newer) over 3 weeks,\nas we noticed that DAS on small MLPs are faster on CPU than on GPU. The experiments on the\nPythia models were executed on a single A100 GPU with 80GB of memory using approximately 30\nGPU hours, including the hyperparameter tuning.\n\n\n\n\n\n                                       41\n\n                          Layer 1                                        Layer 2                                        Layer 3\n      0.95\n\n      0.90\n\n      0.85\n\n      0.80\n\n      0.75   Accuracy 0.70\n                                                                                                             = 1\n      0.65                                                                                                        = 2\n      0.60                                                                                                        = 8\n\n            1  2    4          8                    16                            1  2    4          8                    16                            1  2    4          8                    16\n                             drn                                              drn                                              drn\n                                                     (a) And-Or\n\n                         Layer 1                                        Layer 2                                        Layer 3\n\n       1.0\n\n\n       0.9\n\n\n       0.8\n   Accuracy 0.7\n                                                                                                             = 1\n       0.6                                                                                                         = 2\n                                                                                                             = 8\n       0.5\n           1  2    4          8                    16                            1  2    4          8                    16                            1  2    4          8                    16\n                            drn                                              drn                                              drn\n                                               (b) And-Or-And\n\nFigure 15: Mean IIA over 5 seeds using ϕnonlin (Lrn = 1) on the trained DNN. Performance improves\nwith larger hidden dimension drn and intervention size |ψϕη|. Each subplot corresponds to one of\nthe two candidate algorithms for the distributed law task, showing how ϕ’s representational capacity\ninfluences performance.\n\n\n                         AndOr                             AndOrAnd\n           1.00\n\n           0.75\n\n           0.50                           Accuracy\n                                                                                       linear                 identity\n           0.25\n                                                                                non-linear\n\n                 1   2  12    1   2  12    1   2  12         1   2  12    1   2  12    1   2  12\n                 L1          L2          L3              L1          L2          L3\n\nFigure 16: IIA in the distributive law task for causal abstractions trained with different alignment\nmaps ϕ and a DNN trained to use the And-Or-And algorithm. The figure shows results when\nevaluating if the DNN encodes either of the analysed algorithms for this task. The bars represent\nthe max IIA across 10 runs with different random seeds. The black lines represent mean IIA with\n95% confidence intervals. The |ψϕη| denotes the intervention size per node. All DNNs reach >99.9%\naccuracy after training. The used ϕnonlin uses Lrn = 10 and drn = 16.\n\n\n                         AndOr                             AndOrAnd\n           1.00\n\n           0.75\n\n           0.50                           Accuracy\n                                                                                       linear                 identity\n           0.25\n                                                                                non-linear\n\n                 1   2  12    1   2  12    1   2  12         1   2  12    1   2  12    1   2  12\n                 L1          L2          L3              L1          L2          L3\n\nFigure 17: IIA in the distributive law task for causal abstractions trained with different alignment maps\nϕ and a DNN trained to use the And-Or algorithm. The figure shows IIA results when evaluating\nif the DNN encodes either of the analysed algorithms for this task. The bars represent the max IIA\nacross 10 runs with different random seeds. The black lines represent mean IIA with 95% confidence\nintervals. The |ψϕη| denotes the intervention size per node. Without interventions, all DNNs reach\n>99.9% accuracy. The used ϕnonlin uses Lrn = 10 and drn = 16.\n\n\n                                       42\n\n## Claim to Classify\n\"any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Mathematical proof (Theorem 1) and empirical demonstration using non-linear alignment maps (RevNets) on randomly initialized models.",
    "method_rung": 3,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim is a mechanistic/counterfactual assertion (R3) stating that any model can be 'mapped' to any algorithm (functional/mechanistic equivalence). The paper supports this with a Rung 3 method: a mathematical proof of existence (Theorem 1) and counterfactual interventions (DAS with non-linear maps) that achieve 100% interchange-intervention accuracy on random models, demonstrating that the 'mechanism' can be trivially constructed by the alignment map rather than the model."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 53503,
      "completion_tokens": 182,
      "total_tokens": 53685
    },
    "finish_reason": "stop"
  }
}