{
  "timestamp": "2026-02-19T09:38:05.444603+00:00",
  "paper_id": "2512.18092",
  "claim_id": "2512.18092-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2512.18092\nTitle: Faithful and Stable Neuron Explanations for MI\nFull text:\n                  Faithful and Stable Neuron Explanations for\n                 Trustworthy Mechanistic Interpretability\n\n\n\n\n                        Ge Yan            Tuomas Oikarinen         Tsui-Wei (Lily) Weng\n                           CSE, UCSD              CSE, UCSD               HDSI, UCSD\n                        geyan@ucsd.edu       toikarinen@ucsd.edu         lweng@ucsd.edu\n\n\n                                                 Abstract2025\n                           Neuron identification is a popular tool in mechanistic interpretability, aiming to\n                             uncover the human-interpretable concepts represented by individual neurons inDec                       deep networks. While algorithms such as Network Dissection and CLIP-Dissect\n                               achieve great empirical success, a rigorous theoretical foundation remains absent,\n                           which is crucial to enable trustworthy and reliable explanations. In this work, we19\n                              observe that neuron identification can be viewed as the inverse process of machine\n                                 learning, which allows us to derive guarantees for neuron explanations. Based\n                           on this insight, we present the first theoretical analysis of two fundamental chal-\n                                 lenges: (1) Faithfulness: whether the identified concept faithfully represents the\n                               neuron’s underlying function and (2) Stability: whether the identification results\n                                 are consistent across probing datasets. We derive generalization bounds for widely[cs.AI]                             used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness,\n                            and propose a bootstrap ensemble procedure that quantifies stability along with\n                    BE (Bootstrap Explanation) method to generate concept prediction sets with guar-\n                               anteed coverage probability. Experiments on both synthetic and real data validate\n                              our theoretical results and demonstrate the practicality of our method, providing\n                            an important step toward trustworthy neuron identification. 1\n\n\n                1  Introduction\n\n                      Despite the rapid development and application of deep neural networks, their lack of interpretability\n                          raises growing concerns [Samek et al., 2017, Zhang et al., 2021]. A popular strategy to “open the\n                      black-box\" is to analyze internal representations at the level of individual neurons and associate\n                   them with human-interpretable concepts. This process is known as neuron identification in the\n                           field of mechanistic interpretability, which yields neuron explanations [Bau et al., 2017, Oikarinen\n                    and Weng, 2023]. Over the past few years, many neuron identification methods have been proposed.arXiv:2512.18092v1                     For example, Bau et al. [2017] use curated concept datasets to identify the corresponding concept,\n                      while Oikarinen and Weng [2023] leverage multimodal models to automatically generate neuron\n                        explanations. A growing body of methods has been developed to identify and evaluate concepts\n                      corresponding to neurons [Srinivas et al., 2025, Huang et al., 2023, Gurnee et al., 2023, Mu and\n                      Andreas, 2020, La Rosa et al., 2023, Zimmermann et al., 2023, Bykov et al., 2023, Kopf et al., 2024,\n                  Shaham et al., 2024].\n\n                      Despite rapid empirical progress, systematic comparison and rigorous theoretical understanding of\n                     neuron identification remain limited.  Recently, Oikarinen et al. [2025] unified the evaluation of\n                     neuron identification methods within a single mathematical framework to enable fair comparisons.\n                    However, deeper theoretical foundations are still lacking, which undermines the trustworthiness\n                    and reliability of neuron explanations. Consider a chest-X-ray model that predicts pneumonia and\n\n                         1Our code is available at https://github.com/Trustworthy-ML-Lab/Trustworthy_Explanations.\n\n\n                        39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Mechanistic Inter-\n                             pretability Workshop at NeurIPS 2025.\n\nattributes its decision to a neuron purportedly representing lung opacity, when in fact the neuron\nresponds to hospital-specific markings. Such unfaithful explanations can mislead clinicians, lead to\nharmful treatment decisions, and ultimately erode trust.\n\nThese concerns motivate a closer examination of the core obstacles to trustworthy neuron explana-\ntions. In particular, we identify two central challenges in current neuron identification methods:\n\n      1. Faithfulness. Does the identified concept truly capture the neuron’s underlying function?\n      2. Stability.  Is the identified concept consistent across different probing datasets?\n\nBoth challenges are closely connected with probing datasets, which are an essential component of\nneuron identification methods that determines the stimuli used to measure neuron activity. However,\ntheir influence is often overlooked and not rigorously examined. To address these challenges, we\nprovide a theoretical analysis grounded in a key observation: neuron identification can be (roughly)\nviewed as an inverse process of learning. This perspective highlights structural parallels between\nneuron identification process and traditional machine learning, enabling us to adapt tools from sta-\ntistical learning theory to formally analyze the effect of probing datasets and bound the performance\nof neuron identification methods.\n\nOur contributions are summarized as follows:\n\n1. New insights for neuron identification. We are the first to show that neuron identification can\nbe viewed as an inverse process of learning, revealing structural parallels with traditional machine\nlearning. This insight is non-trivial: it enables us to import and adapt tools from statistical learning\ntheory to rigorously analyze key questions in neuron identification that prior work could not address,\nincluding the impact of probing datasets.\n\n2. Rigorous guarantees for explanation faithfulness. We establish the first theoretical guarantees\nfor the faithfulness of neuron explanations, answering the critical question of when a concept iden-\ntified by a neuron-identification algorithm can be trusted. Our analysis is derived under a general\nframework, making the results applicable to most existing neuron identification methods. Simula-\ntion studies demonstrate that our theory allows quantitative analysis of how factors such as probing\ndataset size, concept frequency, and similarity metrics affect performance.\n\n3. Quantifying stability of explanations. We present the first formal analysis of probing datasets,\nan essential yet previously overlooked component that determines the stimuli used to measure neu-\nron activity. Using a bootstrap ensemble over probing datasets, we quantify the stability of neuron\nexplanations and design a procedure to construct a set of possible concepts for each neuron, with\nstatistical guarantees on the probability of covering the true concept.\n\nThe remainder of this paper is organized as follows: Sec. 2 formalizes the notion of neuron iden-\ntification. Sec. 3 provides a rigorous analysis of the faithfulness of neuron explanations with high\nprobability guarantees. Sec. 4 quantifies the stability of neuron identification algorithms and estab-\nlishes statistical guarantees.\n\n2  Formalizing Neuron Identification\n\nIn this section, we introduce the formal definition of neuron identification and the notations used in\nSec. 3 and 4. Although we use the term “neuron\" identification for simplicity, the framework also\naccommodates larger functional units within the network. Examples include a linear combination of\nneurons (i.e., a direction in representation space), a feature in a Sparse Autoencoder [Cunningham\net al., 2023], a direction derived by TCAV [Kim et al., 2018], or a linear probe [Alain and Bengio,\n2016]. Below, we formally define neuron representation and concept:\nNeuron representation f(x) : X →R: A neuron representation is a function mapping an input\nx ∈X to an activation value. Here, X denotes the input space (e.g. images 2). For example, a\nneuron in an MLP maps the input to a scalar value. For general neural networks, the output may\nnot be a single real number, e.g. for convolutional neural networks (CNN) f(x) is a 2-D feature\nmap. For simplicity in similarity calculation, existing works often conduct pooling (avg, max) to\naggregate the feature into a single real value.\n\n   2The input could also be audio [Wu et al., 2024] or text [Huang et al., 2023, Gurnee et al., 2023]. In this\nwork we focus on vision models.\n\n\n                                       2\n\nConcept label c(x): In the literature of neuron identification [Bau et al., 2017, Oikarinen and Weng,\n2023], a concept is usually defined as a human-understandable text description. For example, “cat\"\nor “shiny blue feather\". Although intuitive, this definition is not a formal mathematical definition.\nIn this work, we define concepts as a function: a concept c(x) : X →[0, 1] is a function that takes\nimages as input, and outputs the probability of the concept. This definition is consistent with the\nprevious works: for example, Bau et al. [2017], Bykov et al. [2024] use human annotations which\noutput 1 if the concept is present, otherwise 0. Oikarinen and Weng [2024] use SigLIP [Zhai et al.,\n2023] to automatically estimate the probability that concept c appears.\n\nTo search for a concept that describes the neuron representation, different methods use different\nmeasures (e.g. IoU [Bau et al., 2017], WPMI [Oikarinen and Weng, 2023], AUC [Bykov et al.,\n2024] and F1-score [Gurnee et al., 2023]). Interestingly, these different methods can all be described\nby a general similarity function sim(f, c), which is a functional measuring the similarity between\nconcept c(x) and neuron representation f(x). With the similarity function, the neuron identification\nproblem can be formulated as:\n\n                                     ˆc(x) = arg max sim(f(x), c(x))                                                                                                   (1)                                       c(x)∈C\n\nwhere C is the concept set (a function space under our concept definition). In our formal definition,\nsim(f, c) is a functional that takes two functions f and c as input, such as accuracy, correlation or\nIoU. In practice, most works replace the function f(x) and c(x) with their realization f(xi) and\nc(xi) on a probing dataset Dprobe as an empirical approximation, where xi is sampled i.i.d. from the\nunderlying distribution. For example, the similarity function of accuracy is defined as the probability\nthat two functions have the same value: sim(f, c) = P(f(x) = c(x)). When utilizing a probing\ndataset Dprobe, we can get an unbiased empirical estimation sim(f,ˆ     c; Dprobe) for sim(f, c):\n\n                                                                       |Dprobe|\n                                     1\n                    sim(f,ˆ     c; Dprobe) =   X 1(f(xi) = c(xi)).                     (2)\n                                              |Dprobe|                                                 i=1\n\nUnder this approximation, the neuron identification can be formulated as the following optimization\nproblem:\n\n[Neuron identification]               ˆc = arg maxc∈C  sim(f,ˆ     c; Dprobe)\n                            where sim(f,ˆ     c; Dprobe) = sim(f(xi),ˆ         c(xi)), xi ∈Dprobe.\n                                                                                                   (3)\nEq. 3 shows that Dprobe plays a critical role in this approximation, yet a rigorous analysis of its effect\nis still lacking. We address this gap in this work in Sec. 3.2 and 4.\n\nWhy do we choose similarity-based definition? Similarity provides a broad and unifying notion\nof a neuron’s concept: many existing definitions can be expressed as special cases of similarity with\nappropriate functions. For example, a common practical criterion is that a neuron represents concept\nc if its activation can successfully classify concept c. This criterion can be formulated as a similarity\nfunction using standard classification metrics such as F1-score [Huang et al., 2023], AUC [Kopf\net al., 2024], precision [Zhou et al., 2014] and accuracy [Koh et al., 2020].\n\n3  Theoretical Guarantees for Explanation Faithfulness\n\nIn this section, we address a key question in neuron identification: When can we trust a neuron\nexplanation produced by a neuron-identification algorithm? We begin with an important observa-\ntion: neuron identification can be viewed as an inverse process of machine learning in Sec. 3.1.\nThis perspective enables us to derive formal guarantees for explanation faithfulness in Sec. 3.2 and,\nbuilding on these results, to quantify the stability of neuron explanations in Sec. 4.\n\n\n3.1  Analogy between neuron identification and machine learning\n\nFrom the formulation in Eq. 3, we observe that the neuron identification problem closely parallels su-\npervised learning problem. Given a standard classification task and a neural network model h ∈H,\nwhere H denotes the hypothesis space containing all possible neural network models, the problem\n\n\n                                       3\n\nFigure 1: Analogous relationship between neuron identification and machine learning.  Neuron\nidentification searches for a concept matching a neuron, while machine learning searches for a model\nmatching human labels. Thus, neuron identification can be viewed as inverse of learning process.\n\n\ncan be formalized as minimizing the loss L, which is typically approximated by the empirical loss\nˆL on a training dataset Dtrain as follows:\n[Machine learning]                     ˆh = arg min   ˆL(h; Dtrain)\n                              h∈H\n                             where ˆL(h; Dtrain) = ˆL(h(xi), y(xi)), xi ∈Dtrain,\n                                                                                                   (4)\nand y(x) denotes the label function and h(x) is the neural network. Comparing Eq. 4 and Eq. 3, we\nsee that these two problems share a similar structure: Both are optimization problems with objectives\nof similar form. The left panel of Fig. 1 compares the procedures of these two domains, while\nthe right panel lists their detailed correspondences. As illustrated in Fig. 1, neuron identification\ncan be roughly viewed as the inverse process of machine learning: during learning, we search for\nneural network (parameters) h(x) that approximates a target human concept y(x) (e.g. ImageNet\nclasses), whereas neuron identification instead searches for concept c(x) (or a simple combination\nof concepts) that best matches a specific neuron representation f(x).\n\nImportantly, this observation enables us to leverage and adapt tools from machine learning theory\nwhile extending them to the unique setting of neuron identification. In the following, we first de-\nvelop formal guarantees for the faithfulness of neuron explanations in Sec 3.2, and then extend this\nperspective to perform uncertainty quantification and assess stability in Sec. 4.\n\n3.2  Theoretical Guarantees for Neuron Explanations\n\nIn this section, we address the faithfulness challenge: Does the identified concept truly capture the\nneuron’s underlying function? Using the framework introduced in Sec. 2, this question reduces to\nasking whether the identified concept truly achieves high similarity sim(f, c) to neuron represen-\ntation. Building on the analogy between neuron identification and machine learning established in\nSec. 3.1, we develop a new generalization framework tailored to the neuron identification setting.\nAlthough inspired by classical learning theory [Shalev-Shwartz and Ben-David, 2014], our analysis\nprovides the first formal guarantees on the concept-neuron similarity sim(f, c). We first define the\ngeneralization gap g for neuron identification as:\n                      g(Dprobe, C, f) ≜sup [sim(f,ˆ     c; Dprobe) −sim(f, c)].                    (5)\n                                  c∈C\n\nWe show that this gap g(Dprobe, C, f) can be bounded in Thm.  3.1 under two mild assumptions:\n(i) the concept set C is finite, and (ii) the probing dataset Dprobe is sampled i.i.d. These conditions\nare met by most existing neuron identification methods, e.g., Bau et al. [2017], Oikarinen and Weng\n[2023], Bykov et al. [2024].\nTheorem 3.1. With probability at least 1 −δ,\n\n                                                                  δ\n                                                                                                   (6)                 sup |sim(f,ˆ     c; Dprobe) −sim(f, c)| ≤r(f, Dprobe, |C|),                  c∈C\n\n\n                                       4\n\nwhere r(f, Dprobe, δ) describes the convergence rate of similarity function sim(f,ˆ     c; Dprobe) and sat-\nisfies\n             P h sim(f,ˆ     c; Dprobe) −sim(f, c) ≥r(f, Dprobe, δ)i ≤δ.                  (7)\nIn Eq. 6, the confidence parameter δ is adjusted using a union bound, replacing δ with |C|.δ\nCorollary 3.2. With probability at least 1 −δ,\n\n                                                             δ\n                        sim(f, ˆc) ≥sim(f, c∗) −2r(f, Dprobe, |C|),                         (8)\nwhere ˆc is selected concept using Eq. 3 and c∗= arg maxc∈C[sim(f, c)] is the optimal concept.\n\nDiscussion.  Thm.   3.1 adapts classical generalization theory to the neuron identification set-\nting, where the objective of interest is sim and sim.ˆ   This provides the first theoretical result on\nthe sim(f, c), which is enabled by our key insight in Sec. 3.1. The convergence rate function\nr(f, Dprobe, δ) characterizes how fast the estimator simˆ  converges.  In Sec. 3.2.1, we will derive\nconvergence rates for several popular similarity functions, showing that for many commonly used\n                 q −log δ\nsimilarity estimators r(f, Dprobe, δ) = O(    |Dprobe|). On the other hand, Corollary 3.2 suggests that\nby maximizing similarity on the probing dataset, the identified concept ˆc is approximately optimal,\nwithin a gap determined by the convergence rate of the similarity function and the size of the concept\nset C. This result guarantees that the concept identified with the probing dataset truly achieves high\nsimilarity to the target neuron representation.\n\n3.2.1  Convergence Results for popular similarity metrics\n\nFrom Thm.  3.1 and Corollary 3.2, we see that the convergence rate is a key factor controlling\nthe generalization gap.  Therefore, in this section, we derive and examine the convergence rate\nof common similarity metrics.  Table 1 summarizes several common similarity scores and their\nconvergence rate r:\n\n      1. Accuracy: This similarity function is used in [Koh et al., 2020], and the convergence rate\n         of accuracy can be estimated via the Hoeffding’s inequality.\n      2. AUROC: This similarity function is used in [Bykov et al., 2023], and the convergence rate\n           is related to concept frequency ρ(c) and can be derived using Thm. 2 in Agarwal et al.\n         [2004]. Fig. 3a plots the convergence rate rAUROC under different ρ and shows that when\n        both ρ and |Dprobe| are small, the convergence rate rAUROC blows up, indicating imbalanced\n        probing datasets may cause larger generalization error and reduce explanation faithfulness.\n      3. Recall, precision, IoU: These similarity functions are used in [Zhou et al., 2014], [Srinivas\n          et al., 2025], [Bau et al., 2017] respectively. To derive their convergence rates, we view\n         these metrics as conditional versions of accuracy: for example, precision can be regarded\n         as computed only on examples where f(x) = 1. Thus, the convergence rate is similar to\n\n sim Metric   sim(f, c)            sim(f,ˆ     c)                           r(f, Dprobe, δ)\n                   P\n                                                    x∈Dprobe                                                  1(f(x)=c(x))       q log( δ2 ) Accuracy    P(f(x) = c(x))                                                                 |Dprobe|                                    2|Dprobe|\n                                                                                                         2 )              P(f(x) < f(y) |  P {x|c(x)=0} P {y|c(y)=1} 1[f(x)<f(y)] q        log( δ AUROC              c(x) = 0, c(y) = 1)          |{x|c(x)=0}||{x|c(x)=1}|              2ρ(c)(1−ρ(c))|Dprobe|\n                  W11                   F11            q      log( δ2 ) IoU             W01+W11+W10        F01+F11+F10                          2(F11+F10+F01)\n                W11                   F11             q   log( δ2 ) Recall             W01+W11             F01+F11                                2(F11+F01)\n                W11                   F11             q   log( δ2 ) Precision             W10+W11             F10+F11                                2(F11+F10)\nTable 1: Similarity metrics sim(f, c), estimation sim(f,ˆ     c) and their corresponding convergence\nspeed r(f, Dprobe, δ). For simplicity, denote Wij = P(f(x) = i, c(x) = j),  i, j ∈{0, 1}, Fij =\n{|f(x)=i,c(x)=j|x∈Dprobe}|\n                                       . For AUROC, ρ(c) is the portion of positive examples in the probing dataset             |Dprobe|\nDprobe (i.e. the frequency of concept).\n\n\n\n                                       5\n\n          rAcc, differing only in that the effective sample size changes from |Dprobe| to (F11 + F10).\n       The same reasoning applies to Recall and IoU. In practice, users can collect additional data\n          until the effective sample size reaches desired level. Further details are provided in Sec. D.\n\nSummary. So far, we have derived the generalization gap g for several popular similarity metrics.\nThese results enable practitioners to select an appropriate metric based on available probing data\nand the properties of the concepts. For example, our experiments in Sec. 3.3 show that AUROC\nconverges quickly when concept frequency is high, but much slower when the frequency is low;\nin such cases, switching to other similarity metric can reduce the generalization gap and improve\nperformance.\n\n3.3  Simulation studies\n\nTo verify the theory developed in Sec. 3.2 and to compare different similarity metrics, we conduct\nsimulations on a synthetic dataset that contains ground-truth similarity values and allows us to simu-\nlate a variety of settings. Specifically, we use binary concept c(x) ∈{0, 1} for simplicity. Neuron ac-\ntivations f(x) are binarized by setting top-5% activations to 1 and remaining to 0. The joint distribu-\ntion of f, c is controlled by the probability matrix M: Mij = P(f(x) = i, c(x) = j), i, j ∈{0, 1}.\n\nWe conduct two experiments: (1) a single-concept study to compare convergence speeds and (2) a\nmulti-concept simulation to verify Thm. 3.1.\n\nExperiment 1: Convergence speed.  In Thm.   3.1, the key factor that controls the gap is the\nconvergence rate r. To investigate this, we generate synthetic data and compare different similarity\nfunctions. For the concept, we study the following two settings:\n\n        • Setting 1: M =\n                                                c = 0  c = 1\n                                  f = 0    0.93  0.02\n                                  f = 1    0.02  0.03\n        This case simulates a regular concept.\n        • Setting 2: M =\n                                                c = 0  c = 1\n                                f = 0    0.9499  0.0001\n                                f = 1    0.0491  0.0009\n        This simulates a rare concept (frequency is 0.001), which often occurs when the concept is\n         fine-grained.\n\nWe simulate with Nexp = 1000 randomly sampled datasets and plot how the 95% quantile of error\nchanges with the number of samples, as shown in Fig. 2. From the simulation results, we can see\nthat\n\n\n\n\n\n                    (a) Setting 1                                                  (b) Setting 2\n\nFigure 2: 95% quantile of error of 5 similarity metrics under two settings: (a) balanced concept\nfrequency; (b) low concept frequency (0.001). Accuracy converges fastest in both settings.\n\n\n                                       6\n\n(a) Convergence rate rAUROC with respect to prob-         (b) Simulation of the generalization gap predicted\ning dataset size |Dprobe| under different concept fre-       by Thm.  3.1 versus probing dataset size, showing\nquency ρ(c).                                        an empirical convergence rate of O(1/√n).\n\n               Figure 3: Theoretical and simulation results on generalization gap.\n\n\n\n      1. Accuracy has the fastest convergence in both cases. On regular concept, IoU, recall and\n         precision are similar. AUROC converges faster than them.\n\n      2. For rare concept, the convergence pattern differs: AUROC and recall are much worse than\n         precision and IoU. This matches our analysis in Sec. 3.2, where we showed that AUROC\n        converges much more slowly when the concept frequency is low.\n\n\nExperiment 2: Gap simulation  In this experiment, we further verify Thm. 3.1 via synthetic data.\nDifferent from Experiment 1 which simulates single concept, this test requires a concept set C. We\ngenerate the synthetic data with the following steps:\n\n      1. Generate neuron representation. Binarized neuron representation f(x) is generated by\n         setting the top-5% of activations to 1 and the rest to 0, i.e. M10 + M11 = 0.05.\n\n      2. Generate concepts. We generate |C| = 1000 concepts as the candidate set. For each\n        concept ci, we first generate its frequency P(ci(x) = 1) = M01 + M11 from a log-uniform\n         distribution in the interval (10−4, 10−1). Then, we sample M11 = P(f(x) = 1, ci(x) = 1)\n        uniformly from (0, min[P(f(x) = 1), P(ci(x) = 1)]) to ensure validity.  Given M11,\n         the remaining part of M can then be inferred from concept frequency and activation bi-\n         narization.  Given the probabilities, we compute corresponding conditional probability\n        (P(ci(x) | f(x)) and sample ci(x) accordingly.\n\n      3. Experiment and simulation. We repeat the above steps Nexp = 1000 times. We use the\n        sampled neuron representation f(x) and concept activation ci(x) to calculate similarity\n       and select top-ranked concept ˆc. Then, we compute the ground-truth similarity with the\n          real probability matrix M and calculate the error as the difference between similarity of\n         selected concept and max similarity in the candidate set (maxc∈C[sim(f, c)] −sim(f, ˆc)).\n      We take the 95% quantile of error among all experiments to approximate the bound under\n        success probability 1 −δ = 95%.\n\nIn Fig. 3b, we plot the simulated gap against the size of the probing dataset |Dprobe|. We observe\nthat: (1) All curves have similar slope to the reference O(p 1/n) curve, suggesting an asymptotic\nconvergence rate of O( p1/n), which is consistent with our theoretical analysis. (2) For the constant\nterm, accuracy has the fastest convergence and AUROC is the second. This matches our simulation\nof r in Experiment 1, Setting 1, supporting our conclusion.\n\nIn summary, the simulation experiments empirically validate the correctness of our theory and show\nits potential to help users choose appropriate similarity metric under different settings.\n\n\n                                       7\n\nFigure 4: Illustration of bootstrap ensemble in neuron identification. Multiple probing datasets are\ngenerated via bootstrapping. Then, neuron identification algorithm is applied to each dataset and\nfinal concepts are aggregated to estimate the probability of each concept.\n\n\n4  Quantifying stability in Neuron Explanations\n\nIn this section, we address the second key challenge in neuron identification methods – stability: Is\nthe identified concept consistent across different probing datasets? Leveraging the connection estab-\nlished in Sec. 3.1, we adopt a bootstrap ensemble approach for stability estimation. This method is\napplicable to any neuron identification algorithm without modifying its internal mechanism. Build-\ning on this bootstrapping framework, we further design a method to construct a prediction set of\ncandidate concepts that contains the desired concept with guaranteed probability.\n\n\n4.1  Empirical measurement via Bootstrap ensemble\n\nBootstrap ensemble [Breiman, 1996] is a machine learning technique used to improve prediction ac-\ncuracy and quantify uncertainty. The method aggregates multiple models, each trained on a different\nresampled version of the original dataset obtained via bootstrapping (sampling with replacement).\nThe final prediction is typically determined by majority voting, and the confidence is estimated as\nthe proportion of models voting for the final prediction [Lakshminarayanan et al., 2017].\n\nFor neuron identification, we introduce a bootstrap-based stability framework that resamples the\nprobing dataset to produce multiple identification outcomes for a single neuron. This adaptation\nallows us to quantify the stability of the neuron explanations obtained. The procedure is:\n\n      1. Collect bootstrap datasets: Sample K datasets {Di}Ki=1 independently by randomly se-\n         lecting samples from the probing dataset Dprobe with replacement.\n\n      2. Run neuron identification: Apply the neuron identification algorithm to each bootstrap\n         dataset Di and record the predicted concept ci.\n      3. Aggregate predictions: After K runs, estimate the probability of each concept as: P(c) =\n          1 PKi=1 1(ci = c), where 1(·) denotes the indicator function.      K\n\nFig. 4 summarizes the pipeline. With bootstrap ensemble, the algorithm now outputs probability of\neach candidate concept.\n\n\n4.2  Theoretical guarantees via Concept Prediction-Set Construction\n\nWhile bootstrap ensembles provide an empirical measure of stability, we also seek theoretical guar-\nantees on the identified concept.  In particular, we want to bound the probability that the most\nfrequent concepts in bootstrap ensemble capture the desired concept3. To achieve this, we construct\na concept prediction set, a set of concepts that are likely to describe the neuron, rather than a single\nbest guess. This prediction-set approach can be applied to any neuron identification algorithm with-\nout any modifications. We call this method Bootstrap Explanation (BE) and list the full procedure\nin Alg. 1 in Sec. A.\n\n   3Analogous to the ground truth in conventional machine learning.\n\n\n                                       8\n\nFigure 5: Results of applying bootstrap ensemble to NetDissect and CLIP-Dissect on ResNet-50\nneurons. NetDissect shows more stable, concrete concepts. CLIP-Dissect outputs are more diverse\nand abstract.\n\n\nThe following theorem gives a probabilistic guarantee that a desired concept c∗will be included in\nthe prediction set constructed via the bootstrap ensemble, under mild assumptions on the candidate\nset and similarity function:\n\n      1. c∗∈C (the desired concept is included in candidate concept set).\n      2. sim(f, c∗) ≥sim(f, c) + ∆, ∀c ∈C, c ̸= c∗, where ∆> 0 is a positive constant. This\n        assumes the similarity function can distinguish the desired concept with other concepts.\n\nWith these assumptions, we have the following theorem:\nTheorem 4.1. Let c∗be the desired concept for a given neuron and the assumptions above hold for\nc∗. Let S ⊆C be the prediction set constructed in Alg. 1, and let k(S) = PKi=1[ˆci ∈S] be the\nnumber of bootstrap trials that predict a concept in S. Then, under these assumptions,\n\n                                 K−k(S)−1\n                         K                  P(c∗∈S) ≥ X         pi(1 −p)K−i,                         (9)                                                              i\n                                         i=0\nwhere p is the single-trial error probability defined implicitly by the equation r(f, Dprobe, |C|)p = ∆2 .\n\nThm.  4.1 provides a statistical guarantee on the probability that our desired concept is included in\nthe prediction set. See Sec. B.2 for the proof.\n\n4.3  Experiments\n\nWe apply our BE method to two base methods: CLIP-Dissect [Oikarinen and Weng, 2023] and\nNetDissect [Bau et al., 2017]. We use a ResNet-50 model trained on the ImageNet dataset [Deng\net al., 2009], run K = 100 bootstrap samples and choose the bootstrap count threshold t = 0.95K =\n0.95 × 100 = 95 in Alg. 1. The results are shown in Fig. 5.\n\nFrom the results, we can observe interesting differences between these two methods: (1) CLIP-\nDissect prefers more abstract concepts. For example, it gives concepts like fostering and biblio-\ngraphic. NetDissect, in contrast, tends to identify concrete concepts. (2) In general, CLIP-Dissect\nprovides more diverse concepts and sometimes captures ones missed by NetDissect (e.g. Birding\n\n\n                                       9\n\nfor Neuron 89). NetDissect is more stable across different bootstrap samples. A potential reason is\nthat NetDissect utilizes localization information, which improves stability.\n\n5  Related works\n\n5.1  Neuron identification\n\nThe goal of neuron identification is to find a human-interpretable concept that describes the behavior\nand functionality of a specific neuron. A variety of methods have been proposed for neuron iden-\ntification. Network Dissection [Bau et al., 2017] is a pioneering work with the idea of comparing\nneuron activations with ground-truth concept masks. Subsequent work explored extensions such as\ncompositional explanations [Mu and Andreas, 2020], automated labeling with CLIP [Oikarinen and\nWeng, 2023], and multimodal summarization [Bai et al., 2024]. More recent approaches expand the\nconcept space to linear combinations [Oikarinen and Weng, 2024]. While these advances provide\nuseful empirical tools, in this work we aim to fill the gap in a principled theoretical foundation for\nneuron identification.\n\n5.2  Principled framework for neuron identification\n\nTo unify the rapid growing neuron identification methods, Oikarinen et al. [2025] design a frame-\nwork, summarizing most neuron identification algorithm into three major components: neuron rep-\nresentation, concept activations and similarity metrics. Additionally, two meta-tests are proposed\nto compare similarity metrics. While this work provides a good start point, rigorous theoretical\nanalysis is still lacking, which we want to provide in this work.\n\n6  Conclusion and limitations\n\nIn this work, we presented a theoretical framework for neuron identification, with the goal of clar-\nifying the faithfulness and stability of existing algorithms. Building on our key observation that\nneuron identification can be viewed as the inverse process of learning, we introduced the notion\nof generalization gap to quantify and derive formal guarantees for explanation faithfulness. To quan-\ntify stability, we proposed BE procedure to construct concept prediction sets with statistical coverage\nguarantees. Together, these results provide the first principled framework for the trustworthiness\nof neuron identification, complementing existing empirical studies.\n\nOur work also has some limitations: the bound on generalization gap is a general bound for any con-\ncept set. It does not utilize the relation between concepts thus may be improved for specific concept\nsets. The bootstrap ensemble method provides an algorithm-agnostic way to quantify stability and\ngenerate prediction sets, but also introduces additional computational overhead.\n\n\n\n\n\n                                       10\n\nReferences\n\nShivani Agarwal, Thore Graepel, Ralf Herbrich, and Dan Roth. A large deviation bound for the area\n  under the roc curve. Advances in Neural Information Processing Systems, 17, 2004.\n\nGuillaume Alain and Yoshua Bengio.  Understanding intermediate layers using linear classifier\n  probes. arXiv preprint arXiv:1610.01644, 2016.\n\nNicholas Bai, Rahul Ajay Iyer, Tuomas Oikarinen, and Tsui-Wei Weng. Describe-and-dissect: In-\n   terpreting neurons in vision networks with language models. In ICML 2024 Workshop on Mech-\n   anistic Interpretability, 2024.\n\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:\n  Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference\n  on computer vision and pattern recognition, pages 6541–6549, 2017.\n\nLeo Breiman. Bagging predictors. Machine learning, 24:123–140, 1996.\n\nKirill Bykov, Laura Kopf, Shinichi Nakajima, Marius Kloft, and Marina Höhne. Labeling neural\n  representations with inverse recognition. Advances in Neural Information Processing Systems,\n  36:24804–24828, 2023.\n\nKirill Bykov, Laura Kopf, Shinichi Nakajima, Marius Kloft, and Marina Höhne. Labeling neural\n  representations with inverse recognition. Advances in Neural Information Processing Systems,\n  36, 2024.\n\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoen-\n  coders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600,\n  2023.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\n   erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\n  pages 248–255. Ieee, 2009.\n\nAlexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale.\n  arXiv preprint arXiv:2010.11929, 2020.\n\nWes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bert-\n  simas.  Finding neurons in a haystack:  Case studies with sparse probing.  arXiv preprint\n  arXiv:2305.01610, 2023.\n\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts.  Rig-\n  orously assessing natural language explanations of neurons. arXiv preprint arXiv:2309.10312,\n  2023.\n\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.\n   Interpretability beyond feature attribution: Quantitative testing with concept activation vectors\n   (tcav). In International conference on machine learning, pages 2668–2677. PMLR, 2018.\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and\n  Percy Liang. Concept bottleneck models. In International conference on machine learning, pages\n  5338–5348. PMLR, 2020.\n\nLaura Kopf, Philine L Bommer, Anna Hedström, Sebastian Lapuschkin, Marina Höhne, and Kir-\n    ill Bykov. Cosy: Evaluating textual explanations of neurons. Advances in Neural Information\n  Processing Systems, 37:34656–34685, 2024.\n\nBiagio La Rosa, Leilani Gilpin, and Roberto Capobianco. Towards a fuller understanding of neurons\n  with clustered compositional explanations. Advances in Neural Information Processing Systems,\n  36:70333–70354, 2023.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\n  uncertainty estimation using deep ensembles. Advances in neural information processing systems,\n  30, 2017.\n\n\n                                       11\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and\n  pattern recognition, pages 11976–11986, 2022.\n\nJesse Mu and Jacob Andreas. Compositional explanations of neurons. Advances in Neural Informa-\n   tion Processing Systems, 33:17153–17163, 2020.\n\nTuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representa-\n   tions in deep vision networks. International Conference on Learning Representations, 2023.\n\nTuomas Oikarinen and Tsui-Wei Weng. Linear explanations for individual neurons. In Forty-first\n  International Conference on Machine Learning, 2024.\n\nTuomas Oikarinen, Ge Yan, and Tsui-Wei Weng. Evaluating neuron explanations: A unified frame-\n  work with sanity checks. In International Conference on Machine Learning, 2025.\n\nWojciech Samek, Thomas Wiegand, and Klaus-Robert Müller.   Explainable  artificial  intelli-\n  gence:  Understanding, visualizing and interpreting deep learning models.   arXiv preprint\n  arXiv:1708.08296, 2017.\n\nTamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob\n  Andreas, and Antonio Torralba. A multimodal automated interpretability agent.  In Forty-first\n  International Conference on Machine Learning, 2024.\n\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-\n  rithms. Cambridge university press, 2014.\n\nAnvita A Srinivas, Tuomas Oikarinen, Divyansh Srivastava, Wei-Hung Weng, and Tsui-Wei Weng.\n  Sand: Enhancing open-set neuron descriptions through spatial awareness.  In 2025 IEEE/CVF\n  Winter Conference on Applications of Computer Vision (WACV), pages 2993–3002. IEEE, 2025.\n\nTung-Yu Wu, Yu-Xiang Lin, and Tsui-Wei Weng. And: audio network dissection for interpreting\n  deep acoustic models. In Proceedings of the 41st International Conference on Machine Learning,\n  pages 53656–53680, 2024.\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\n  image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision,\n  pages 11975–11986, 2023.\n\nYu Zhang, Peter Tiˇno, Aleš Leonardis, and Ke Tang. A survey on neural network interpretability.\n  IEEE transactions on emerging topics in computational intelligence, 5(5):726–742, 2021.\n\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors\n  emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014.\n\nRoland S Zimmermann, Thomas Klein, and Wieland Brendel. Scale alone does not improve mech-\n   anistic interpretability in vision models. Advances in Neural Information Processing Systems, 36:\n  57876–57907, 2023.\n\n\n\n\n\n                                       12\n\nAppendix\n\n\nContents\n\n\nA  Details on bootstrap ensemble                                                 14\n\n\nB  Formalization of the theories                                                  15\n\n\nC  Related works                                                              17\n\n\nD  Details in recall, precision and IoU’s convergence speed derivation                  18\n\n\nE LLM usage                                                                 18\n\n\nF  Details of simulation study Experiment 1 (Sec 3.3)                                19\n\n\nG Computational cost for Bootstrap Ensemble (BE)                                20\n\n\nH  Additional faithfulness experiments for continuous metrics                        22\n\n\nI  Study on sample numbers (Thm 3.1)                                           24\n\n\nJ  Additional bootstrap ensemble (BE) experiments                                 25\n\n\n\n\n\n                                       13\n\nA  Details on bootstrap ensemble\n\n\nAlgorithm 1: BE: Generating a concept prediction set for target neuron\nInput: Concept set C, probing dataset Dprobe, target neuron f, neuron identification procedure\n      Identify(C, f, Dprobe), bootstrap sample count K, bootstrap count threshold t\nOutput: Prediction set S of candidate concepts\nfor i ←1 to K do\n   Sample dataset Di from Dprobe with replacement (same size as Dprobe);\n    Calculate ˆci = Identify(C, f, Di);\nend\nFor each concept cj ∈C, count the number of its appearances:\n\n                           K\n                                     kj = X[ˆci = cj]\n                                          i=1\n\n Sort concepts by frequency, kr1 ≥kr2 · · · ≥krs, s is the number of different concepts\n generated during bootstrapping;\nInitialize S ←∅, j ←0, cur_count ←0;\nwhile cur_count < t do\n   Add crj to S: S ←S ∪{crj};\n   Update j ←j + 1, cur_count ←cur_count + krj\nend\n\n\n\n\n\n                                       14\n\nB  Formalization of the theories\n\nB.1  Formalization of Thm. 3.1\n\nDefinition B.1. The convergence rate of similarity function is defined as a function that satisfies\n\n             P h sim(f,ˆ     c; Dprobe) −sim(f, c) ≥r(f, Dprobe, δ)i ≤δ.                (10)\n\nHere, Dprobe is randomly sampled from the underlying data distribution D with a fixed size |Dprobe|,\nand the probability is taken over all possible sampled probing datasets.\n\nProof of Thm. 3.1. For each ci ∈|C|, from the definition of convergence rate, we have\n\n                                                               δ        δ\n                                                                                           (11)           P   sim(f,ˆ     ci; Dprobe) −sim(f, ci) ≥r(f, Dprobe, |C|) ≤ |C|.\n\nThus, with union bound, we have\n\n                                                                 δ           P                 sup |sim(f,ˆ     c; Dprobe) −sim(f, c)| ≤r(f, Dprobe, |C|)                 c∈C\n                                                                       δ\n          =1 −P  ∪c∈C|sim(f,ˆ     c; Dprobe) −sim(f, c)| > r(f, Dprobe, |C|)\n                                                                     δ                (12)          ≥1 − X P   |sim(f,ˆ     c; Dprobe) −sim(f, c)| > r(f, Dprobe,                                                                  |C|)\n                  c∈C\n                        δ\n          ≥1 −|C|\n                       |C|\n          =1 −δ\n\n\n\nProof of Corollary 3.2. From the definition,\n\n                                                          ˆc = arg max sim(f,ˆ     c).                                (13)\n                                      c∈C\n\nThus, sim(f,ˆ       ˆc) ≥ sim(f,ˆ     c∗). From Thm. 3.1, with probability at least 1 −δ, we have\n\n                                     ˆ                          sim(f, ˆc) ≥ sim(f, ˆc) −r(f, Dprobe, δ)                         (14)\n\nand\n                                     ˆ                         sim(f, c∗) ≤ sim(f, c∗) + r(f, Dprobe, δ).                       (15)\nTherefore, we have\n                         sim(f, ˆc) ≥sim(f, c∗) −2r(f, Dprobe, δ)                        (16)\nwith probability at least 1 −δ.\n\nB.2  Proof for Thm. 4.1\n\nIn this section, we prove Thm. 4.1: Theorem 4.1. Let c∗be the desired concept for a given neuron\nand the assumptions above hold for c∗. Let S ⊆C be the prediction set constructed in Alg. 1, and\nlet k(S) = PKi=1[ˆci ∈S] be the number of bootstrap trials that predict a concept in S. Then, under\nthese assumptions,\n                                 K−k(S)−1\n                         K                  P(c∗∈S) ≥ X         pi(1 −p)K−i,                      (17)                                                              i\n                                         i=0\nwhere p is the single-trial error probability defined implicitly by the equation r(f, Dprobe, |C|)p = ∆2 .\n\nProof. We start the proof by estimating single-trial error rate.\n\n\n                                       15\n\nLemma B.2. Let p be defined implicitly by the equation\n\n                                         p   ∆\n                                    r(f, Dprobe, |C|) = 2 ,                                (18)\n\nwhere r(·) is the uniform convergence rate in Thm. 3.1. Then,\n                                          P(ˆc = c∗) ≥1 −p                                   (19)\n\n\nRemark B.3. Lemma B.2 can be easily derived from Thm.    3.1:  with probability 1 −p,\nsupc∈C |sim(f,ˆ     c; Dprobe) −sim(f, c)| ≤∆2 , thus\n                sim(f,ˆ     c∗; Dprobe) ≥sim(f, c∗; Dprobe) −∆\n                                                 2\n                             ∆                                (20)                           ≥sim(f, c; Dprobe) +     (Assumption 2)\n                                                2\n                   ≥ sim(f,ˆ     c; Dprobe).\n\nPreviously,  we  show  that  for many  similarity  metrics (AUROC,  accuracy,  IoU,  etc.),\n        q −log δ           q −log δ\nr(f, Dprobe, δ) = O(    |Dprobe|), i.e. r(f, Dprobe, δ) ≤Q(    |Dprobe|) for some constant Q > 0. In\nthis case, we can plug in δ = |C|p and get\n\n              ∆             p                       s −log |C|p\n                 = r(f, Dprobe,   ≤Q                  ,                        (21)                        2              |C|)          |Dprobe|\n\n\n                        4Q2which gives p ≤|C|e−∆2                                      |Dprobe|. This shows when probing dataset size |Dprobe| and gap between\ndesired concept and other concept ∆becomes larger, the error probability p can be reduced.\n\nSuppose we repeat our experiment K times and get {ˆci}Ki=1. Then, we have the following theorem.\nTheorem B.4. Let k∗= PKi=1 1[ˆci = c∗] denotes the number of times target neuron is given during\nK experiments. Then,\n                                                             t\n                        K                     P(k∗≥t) ≥ X      (1 −p)ipK−i                          (22)                                                           i\n                                         i=0\n\nRemark B.5. This could be derived by Lemma B.2 and binomial distribution CDF.\n\nUsing Thm. B.4, we can derive:\n                   P(c∗/∈S) ≤P(k∗≤K −k(S))\n                   = 1 −P(k∗≥K −k(S) −1)\n                                   K−k(S)−1                                        (23)\n                          K                   ≤1 − X         (1 −p)ipK−i\n                                                                 i\n                                           i=0\n\nThus,\n                                 K−k(S)−1\n                         K                  P(c∗∈S) ≥ X         (1 −p)ipK−i,                      (24)                                                              i\n                                         i=0\nfinishes the proof.\n\n\n\n\n\n                                       16\n\nC  Related works\n\nC.1  Neuron identification\n\nThe goal of neuron identification is to find a human-interpretable concept that describes the behavior\nand functionality of a specific neuron. A variety of methods have been proposed for neuron iden-\ntification. Network Dissection [Bau et al., 2017] is a pioneering work with the idea of comparing\nneuron activations with ground-truth concept masks. Subsequent work explored extensions such as\ncompositional explanations [Mu and Andreas, 2020], automated labeling with CLIP [Oikarinen and\nWeng, 2023], and multimodal summarization [Bai et al., 2024]. More recent approaches expand the\nconcept space to linear combinations [Oikarinen and Weng, 2024]. While these advances provide\nuseful empirical tools, in this work we aim to fill the gap in a principled theoretical foundation for\nneuron identification.\n\nC.2  Principled framework for neuron identification\n\nTo unify the rapid growing neuron identification methods, Oikarinen et al. [2025] design a frame-\nwork, summarizing most neuron identification algorithm into three major components: neuron rep-\nresentation, concept activations and similarity metrics. Additionally, two meta-tests are proposed\nto compare similarity metrics. While this work provides a good start point, rigorous theoretical\nanalysis is still lacking, which we want to provide in this work.\n\n\n\n\n\n                                       17\n\nD  Details in recall, precision and IoU’s convergence speed derivation\n\nIn the main text, we mention the key idea of deriving convergence speed r for recall, precision and\nIoU: that is regard them as special case of accuracy where data are limited in a subgroup. For recall:\n                                    P(f(x) = 1, c(x) = 1)\n                              simrecall(f, c) =\n                                           P(c(x) = 1)                                (25)\n                     = P(f(x) = c(x) | c(x) = 1).\n\nTherefore, we can regard calculation of recall as a rejection sampling process: The samples sat-\nisfying c(x) = 1 are kept and others are rejected.  Then, accuracy is calculated on remaining\nsamples.  Thus, the convergence speed can be calculated by inserting the effective sample size\n|{c(x) = 1 | x ∈Dprobe}| into the accuracy’s convergence rate:\n\n               s               2\n                                                 log( δ )\n                                          rrecall =                                                       (26)                                      2|{c(x) = 1 | x ∈Dprobe}|.\n\nFor precision and IoU, the derivation is similar.\n\nE LLM usage\n\nIn this article, LLM is used to check grammar and typos as well as improve the writing.\n\n\n\n\n\n                                       18\n\nF  Details of simulation study Experiment 1 (Sec 3.3)\n\nExperiment 1  The procedure of Experiment 1 can be described as follows:\n\n      1. Generate simulation data. To start, we first generate the simulation data following the\n         settings specified in Sec. 3.3. We generate paired binary variables representing ground-truth\n        concept activations and neuron responses i.i.d. by directly sampling from the probability\n         distribution specified by the probability matrix M.\n      2. Calculate ground truth metrics. Given the probability matrix M, the ground truth value\n         of each metric is calculated according to their definition.\n      3. Simulation. In this step, we run simulation to simulate the convergence speed r(·). In\n        each iteration i, we first sample a new batch of data following step 1 with size Nsample.\n        Then, we estimate each metric using the data sample and calculate the error erri. We\n         repeat the procedure for Nexp = 1000 times, aggregate the erri in each round and report\n         the 0.95-quantile of errors.\n\n\n\n\n\n                                       19\n\nG  Computational cost for Bootstrap Ensemble (BE)\n\nIn this section, we study the computational cost of Bootstrap Ensemble.  Theoretically, since the\nBE method requires K times of bootstrapping and running original neuron identification algorithm,\nthe running time should scale up linearly with K. In practice, however, the BE time cost is not\nsimply K times of original algorithm, as a significant portion of computation can often be shared\namong bootstrap samples, saving much time. Take CLIP-Dissect as an example: The CLIP-Dissect\nalgorithm can be roughly divided into three steps:\n\n      1. Collect features from the target model and CLIP model.\n      2. Calculate similarity matrix between target features and CLIP features.\n      3. Select the final concept with highest similarity.\n\nIn the implementation of BE, the first step can be shared among bootstrap samples: we pre-compute\nthe features of target model and CLIP on the whole probing dataset. For each bootstrap sample,\nwe only need to fetch corresponding features, calculate similarity and select the highest similarity\nconcept. With this, we run experiments based on a ConvNeXt-base model [Liu et al., 2022].4 The\nprofiling results are shown in Fig. 6. We can see that though we do K = 100 bootstrap ensembles,\nthe time overhead is only about 30% and the memory overhead is about 15%. Fig. 7 shows how\nruntime changes with K, illustrating that our runtime is as high as K times of original algorithm.\n  Further, to understand the impact of concept number, we compare results on two concept sets:\n\n\n\n\n\nFigure 6: Profiling result of CLIP-Dissect on a ConvNext-base model. K = 100 bootstrap ensem-\nbles are used.\n\nBroden concept set with 1198 concepts and 20k English words with 20000 concepts. The result is\nshown in the table 2. From the results, we can see that bootstrapping time increases significantly with\n\n   4Experiment is done with an NVIDIA RTX A5000 GPU and an AMD Ryzen Threadripper PRO 3975WX\nCPU.\n\n\n\n\n\n             Figure 7: Runtime vs. K for CLIP-Dissect on a ConvNeXt-base model.\n\n\n                                       20\n\nlarger concept set. However, we argue the major cause is the latent CLIP-Dissect uses soft-wpmi,\nwhich is much slower with larger concept set. The bootstrap wrapper itself does not introduce higher\noverhead.\n\n            Stage                 Broden (|C| = 1, 198)  20k (|C| = 20, 000)\n             Activation computation   211.7                  277.6\n            Bootstrapping           64.7                   348.7\n     Table 2: Runtime (s) comparison with two concept sets: Broden and 20k English words.\n\n\n\n\n\n                                       21\n\nH  Additional faithfulness experiments for continuous metrics\n\nIn Sec. 3.2, we mainly study bounds for metrics with binarized neuron representation. In this section,\nwe conduct experiments on several metrics for continuous neuron representations: AUROC [Bykov\net al., 2023], AUPRC, correlation [Oikarinen and Weng, 2024] , WPMI [Oikarinen and Weng, 2023]\nand MAD [Kopf et al., 2024]. AUROC has been defined in the main text. To calculate AUPRC, we\nfirst sort f(xi) for smallest to largest: f(x(1)) ≤f(x(2)) · · · ≤f(x(n)). Thus, the precision at kth\nthreshold can be defined as\n                                                   k\n                                        1\n                                 Prec(k) = X c(i),                                 (27)\n                                       k\n                                                 i=1\n\nThe recall can be defined as\n                                                      k\n                                        1\n                             Rec(k) :=   X c(i).                              (28)\n                        Pni=1 ci i=1\n\nThe AUPRC can be calculated as:\n\n                                                   Prec(k)\n                        simAUPRC(f,ˆ            c) := X                .                         (29)\n                               Pni=1 ci                                                      k|c(k)=1\n\nFor MAD,\n                   simMAD(f, c) = Ex|c(x)=1f(x) −Ex|c(x)=0f(x)                    (30)\n\nFor WPMI, we take the definition in Oikarinen et al. [2025]:\n\n                 simWPMI = Ef(x)=1[log(c(x)) −λ log[E(c(x))]].                   (31)\n\nHere, we take λ = 1. For correlation:\n\n                          E (f(x) −µf) (c(x) −µc)\n                         simcorr(f, c) :=                                           .                     (32)\n                     q σ2f σ2c\n\nwhere µf, µc, σf, σc are the mean and standard deviation of f and c, respectively. For those metrics,\na closed-form expression of convergence rate is challenging to derive. Thus, we conduct empirical\nstudies with synthetic data and real ImageNet validation data.\n\n\nH.1  Synthetic experiment\n\nData generation  For the synthetic dataset, we construct a simple conditional Gaussian data-\ngenerating process to obtain pairs of binary concept activations and continuous neuron represen-\ntations. For each sample i ∈{1, . . . , n} we first randomly draw a binary concept label ci ∈{0, 1},\nwhere P(ci = 1) = p is a hyperparameter. Conditioned on ci, we then sample a one-dimensional\nneuron representation zi ∈R from a Gaussian distribution whose mean and variance depend on the\nconcept state:\n                    (N(µpos, σ2pos),   if ci = 1,\n                                 zi | ci =\n                                    N(µneg, σ2neg),   if ci = 0,\n\nwhere (µpos, σpos) and (µneg, σneg) are hyperparameters controlling, respectively, the distribution of\nthe neuron activation when the concept is present or absent. In practice, we generate n i.i.d. samples\n{(ci, zi)}ni=1 according to the above process.  In the experiment, we take p = 0.002, µpos = 1,\nµneg = 0, σpos = 0.2, σneg = 0.5.\n\nExperiments  We approximate the ground truth value of metrics with 106 examples, then estimate\nthe convergence speed r(·) by calculating the 95% quantile of error. Since the scale of MAD is sig-\nnificantly different from other metrics, we compute the relative error instead. The results are shown                                       √\nin Fig. 8a. From the results, we see all metrics also follow asymptotically O(1/ N) convergence\nspeed.\n\n\n                                       22\n\n                       (a) Synthetic dataset                          (b) ImageNet validation dataset\n\n          Figure 8: 95% error quantile on synthetic dataset and ImageNet validation set.\n\n\nH.2  Real data experiment\n\nFurthermore, we conduct an experiment on ImageNet validation set with a ResNet-50 model. We\nchoose the final layer neurons and the concepts are the class labels they predict. The results are\nshown in Fig. 8b. Different with the synthetic dataset, the AUROC metric performs significantly\nbetter with more samples. We hypothesize the reason is those neurons are specific to classify\nthese concepts, thus their ability to distinguish concepts (what AUROC is measuring) are especially                                             √\nstrong. Besides, we see that similar to synthetic data, most metrics show asymptotically O(1/ N)\nconvergence rate.\n\n\n\n\n\n                                       23\n\nI  Study on sample numbers (Thm 3.1)\n\nThm.  3.1 suggests that more probing samples can reduce the generalization gap, implying bet-\nter faithfulness. To evaluate this, we conduct a qualitative study using CLIP-Dissect to study the\nResNet-50 layer 4 neurons on ImageNet validation set with size of probing dataset |Dprobe| =\n100, 1000, 10000, 50000. We show some examples in Fig. 9. From the examples, we can see that,\nwith more probing data, the captured concept describes the neurons more accurately, supporting our\ntheory.\n\n\n\n\n\n          Figure 9: CLIP-Dissect explanation with different number of probing samples.\n\n\n\n\n\n                                       24\n\nJ  Additional bootstrap ensemble (BE) experiments\n\nJ.1  Additional examples for BE\n\nIn figs. 10 to 13, we present additional results of applying BE on NetDissect and CLIP-Dissect. To\nenable better comparison, we use the Broden [Bau et al., 2017] dataset as the probing dataset and\ncorresponding concepts as concept set for both methods.\n\nJ.2  Experiments with different architectures\n\nTo understand the performance of NetDissect and CLIP-Dissect with different architectures, we add\nadditional experiments with ConvNeXt-base [Liu et al., 2022] and ViT-B/16 [Dosovitskiy, 2020].\nWe show example images in figs. 14 and 15. We compare (1) the average frequency of top-1 concept\nand (2) the average size of concept set that covers 90% of all runs. Since NetDissect does not support\nVision Transformer features, we map the embedding back to the corresponding input patch to form\na 2-D feature map. The result is shown in tables 3 and 4. From the results, we can see that CLIP-\nDissect provides more diverse concepts except on ConvNeXt-base. NetDissect results vary a lot\nacross backbones, which is most stable for ViT-B/16 and most diverse for ConvNeXt-base.\n\n                Method       ResNet50   ViT-B/16  ConvNeXt-Base\n                  CLIP-Dissect  66.9%     51.2%     53.7%\n                   NetDissect    79.2%     91.3%     27.8%\nTable 3: Comparison of top-1 concept frequency of CLIP-Dissect and NetDissect on three vision\nbackbones.\n\n\n\n                Method       ResNet50   ViT-B/16  ConvNext-Base\n                  CLIP-Dissect   3.11        5.93       5.61\n                   NetDissect     2.08        1.34       6.99\nTable 4: Comparison of 90% coverage concept set size of CLIP-Dissect and NetDissect on three\nvision backbones.\n\n\n\n\n\n                                       25\n\nFigure 10: Additional results of applying BE to NetDissect and CLIP-Dissect on ResNet 50 neurons.\n\n\n\n                                       26\n\nFigure 11: Additional results of applying BE to NetDissect and CLIP-Dissect on ResNet 50 neurons.\n\n\n\n                                       27\n\nFigure 12: Additional results of applying BE to NetDissect and CLIP-Dissect on ResNet 50 neurons.\n\n\n\n                                       28\n\nFigure 13: Additional results of applying BE to NetDissect and CLIP-Dissect on ResNet 50 neurons.\n\n\n\n                                       29\n\nFigure 14: BE example on ViT-B/16, last encoder layer.\n\n\n\n\n\n Figure 15: BE example on ConvNext-base, last layer.\n\n\n\n\n\n                     30\n\n## Claim to Classify\n\"neuron identification can be viewed as the inverse process of machine learning\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Theoretical analogy and statistical learning theory (generalization bounds)",
    "method_rung": 1,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim is a high-level mechanistic/functional assertion that defines the nature of the process ('is the inverse process'). According to the codebook, functional attribution and 'is' statements regarding the underlying mechanism are Rung 3. The method used to support this specific claim is a theoretical analogy and the derivation of generalization bounds based on observational data (probing datasets), which does not involve causal interventions on the model, placing the method at Rung 1."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 20122,
      "completion_tokens": 160,
      "total_tokens": 20282
    },
    "finish_reason": "stop"
  }
}