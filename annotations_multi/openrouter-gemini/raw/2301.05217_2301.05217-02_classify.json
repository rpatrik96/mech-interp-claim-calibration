{
  "timestamp": "2026-02-19T09:26:24.062478+00:00",
  "paper_id": "2301.05217",
  "claim_id": "2301.05217-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2301.05217\nTitle: Progress measures for grokking via mechanistic interpretability\nFull text:\n                      Published as a conference paper at ICLR 2023\n\n\n          PROGRESS MEASURES FOR GROKKING VIA\n          MECHANISTIC INTERPRETABILITY\n\n\n                      Neel Nanda∗, †   Lawrence Chan‡  Tom Lieberum†   Jess Smith†   Jacob Steinhardt‡\n\n\n                                       ABSTRACT\n\n                             Neural networks often exhibit emergent behavior, where qualitatively new capa-\n                                       bilities arise from scaling up the amount of parameters, training data, or training\n                                    steps. One approach to understanding emergence is to find continuous progress\n                             measures that underlie the seemingly discontinuous qualitative changes. We ar-2023                      gue that progress measures can be found via mechanistic interpretability: reverse-\n                               engineering learned behaviors into their individual components. As a case study,\n                       we investigate the recently-discovered phenomenon of “grokking” exhibited byOct                         small transformers trained on modular addition tasks. We fully reverse engineer\n                                 the algorithm learned by these networks, which uses discrete Fourier transforms\n19                      and trigonometric identities to convert addition to rotation about a circle. We\n                              confirm the algorithm by analyzing the activations and weights and by perform-\n                               ing ablations in Fourier space. Based on this understanding, we define progress\n                             measures that allow us to study the dynamics of training and split training into\n                                 three continuous phases: memorization, circuit formation, and cleanup. Our re-\n                                    sults show that grokking, rather than being a sudden shift, arises from the gradual\n                                 amplification of structured mechanisms encoded in the weights, followed by the[cs.LG]                              later removal of memorizing components.\n\n\n                1  INTRODUCTION\n\n                     Neural networks often exhibit emergent behavior, in which qualitatively new capabilities arise from\n                        scaling up the model size, training data, or number of training steps (Steinhardt, 2022; Wei et al.,\n                       2022a). This has led to a number of breakthroughs, via capabilities such as in-context learning (Rad-\n                       ford et al., 2019; Brown et al., 2020) and chain-of-thought prompting (Wei et al., 2022b). However,\n                                   it also poses risks: Pan et al. (2022) show that scaling up the parameter count of models by as little\n                       as 30% can lead to emergent reward hacking.\n\n                    Emergence is most surprising when it is abrupt, as in the case of reward hacking, chain-of-thought\n                        reasoning, or other phase transitions (Ganguli et al., 2022; Wei et al., 2022a). We could better\n                      understand and predict these phase transitions by finding hidden progress measures (Barak et al.,\n                      2022): metrics that precede and are causally linked to the phase transition, and which vary morearXiv:2301.05217v3                smoothly. For example, Wei et al. (2022a) show that while large language models show abrupt\n                    jumps in their performance on many benchmarks, their cross-entropy loss decreases smoothly with\n                   model scale. However, cross-entropy does not explain why the phase changes happen.\n\n                       In this work, we introduce a different approach to uncovering hidden progress measures: via mech-\n                          anistic explanations.1 A mechanistic explanation aims to reverse engineer the mechanisms of the\n                      network, generally by identifying the circuits (Cammarata et al., 2020; Elhage et al., 2021) within\n                      a model that implement a behavior. Using such explanations, we study grokking, where models\n                       abruptly transition to a generalizing solution after a large number of training steps, despite initially\n                          overfitting (Power et al., 2022). Specifically, we study modular addition, where a model takes inputs\n                        a, b ∈{0, . . . , P −1} for some prime P and predicts their sum c mod P. Small transformers trained\n                      with weight decay on this task consistently exhibit grokking (Figure 2, Appendix C.2).\n\n                          ∗Corresponding author, please direct correspondence to: neelnanda27@gmail.com\n                           †Independent researcher.\n                             ‡University of California, Berkeley.\n                                1Interactive versions of figures, as well as the code to reproduce our results, are available at https:\n                  //neelnanda.io/grokking-paper.\n\n\n                                                           1\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nFigure 1: The algorithm implemented by the one-layer transformer for modular addition. Given two\nnumbers a and b, the model projects each point onto a corresponding rotation using its embedding\nmatrix. Using its attention and MLP layers, it then composes the rotations to get a representation of\na + b mod P. Finally, it “reads off” the logits for each c ∈{0, 1, ..., P −1}, by rotating by −c to\nget cos(w(a + b −c)), which is maximized when a + b ≡c mod P (since w is a multiple of 2πP ).\n\n\n\nWe reverse engineer the weights of these transformers and find that they perform this task by map-\nping the inputs onto a circle and performing addition on the circle. Specifically, we show that the\nembedding matrix maps the inputs a, b to sines and cosines at a sparse set of key frequencies wk.\nThe attention and MLP layers then combine these using trigonometric identities to compute the sine\nand cosine of wk(a + b), and the output matrices shift and combine these frequencies.\n\nWe confirm this understanding with four lines of evidence (Section 4): (1) the network weights\nand activations exhibit a consistent periodic structure; (2) the neuron-logit map WL is well approx-\nimated by a sum of sinusoidal functions of the key frequencies, and projecting the MLP activations\nonto these sinusoidal functions lets us “read off” trigonometric identities from the neurons; (3) the\nattention heads and MLP neuron are well approximated by degree-2 polynomials of trigonometric\nfunctions of a single frequency; and (4) ablating key frequencies used by the model reduces perfor-\nmance to chance, while ablating the other 95% of frequencies slightly improves performance.\n\nUsing our understanding of the learned algorithm, we construct two progress measures for the mod-\nular addition task—restricted loss, where we ablate every non-key frequency, and excluded loss,\nwhere we instead ablate all key frequencies. Both metrics improve continuously prior to when\ngrokking occurs. We use these metrics to understand the training dynamics underlying grokking\nand find that training can be split into three phases: memorization of the training data; circuit for-\nmation, where the network learns a mechanism that generalizes; and cleanup, where weight decay\nremoves the memorization components. Surprisingly, the sudden transition to perfect test accuracy\nin grokking occurs during cleanup, after the generalizing mechanism is learned. These results show\nthat grokking, rather than being a sudden shift, arises from the gradual amplification of structured\nmechanisms encoded in the weights, followed by the later removal of memorizing components.\n\n\n2  RELATED WORK\n\n\nPhase Changes. Recent papers have observed that neural networks quickly develop novel quali-\ntative behaviors as they are scaled up or trained longer (Ganguli et al., 2022; Wei et al., 2022a).\nMcGrath et al. (2021) find that AlphaZero quickly learns many human chess concepts between 10k\nand 30k training steps and reinvents human opening theory between 25k and 60k training steps.\n\nGrokking. Grokking was first reported in Power et al. (2022), which trained two-layer transformers\non several algorithmic tasks and found that test accuracy often increased sharply long after achieving\nperfect train accuracy. Millidge (2022) suggests that this may be due to SGD being a random walk\non the optimal manifold. Our results echo Barak et al. (2022) in showing that the network instead\nmakes continuous progress toward the generalizing algorithm. Liu et al. (2022) construct small\nexamples of grokking, which they use to compute phase diagrams with four separate “phases” of\nlearning. Thilak et al. (2022) argue that grokking can arise without explicit regularization, from an\noptimization anomaly they dub the slingshot mechanism, which may act as an implicit regularizer.\n\n\n                                       2\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nFigure 2: The train and test accuracy (left) and train and test loss (right) of one-layer transformers on\nthe modular addition task described in Section 3, over 5 random seeds. These models consistently\nexhibit grokking: they quickly overfit early on in training, but then later learn to generalize.\n\n\n\nCircuits-style mechanistic interpretability. The style of post-hoc mechanistic interpretability in\nSection 4 is heavily inspired by the Circuits approach of Cammarata et al. (2020), Elhage et al.\n(2021), and Olsson et al. (2022).\n\nProgress measures. Barak et al. (2022) introduce the notion of progress measures—metrics that\nimprove smoothly and that precede emergent behavior. They prove theoretically that training would\namplify a certain mechanism and heuristically define a progress measure. In contrast, we use mech-\nanistic intepretability to discover progress measures empirically.\n\n\n3  SETUP AND BACKGROUND\n\n\nWe train transformers to perform addition mod P. The input to the model is of the form “a b =”,\nwhere a and b are encoded as P-dimensional one-hot vectors, and = is a special token above which\nwe read the output c. In our mainline experiment, we take P = 113 and use a one-layer ReLU\ntransformer, token embeddings with d = 128, learned positional embeddings, 4 attention heads of\ndimension d/4 = 32, and n = 512 hidden units in the MLP. In other experiments, we vary the depth\nand dimension of the model. We did not use LayerNorm or tie our embed/unembed matrices.\n\nOur mainline dataset consists of 30% of the entire set of possible inputs (that is, 30% of the 113 ·\n113 pairs of numbers mod P). We use full batch gradient descent using the AdamW optimizer\n(Loshchilov & Hutter, 2017) with learning rate γ = 0.001 and weight decay parameter λ = 1. We\nperform 40, 000 epochs of training. As there are only 113 · 113 possible pairs, we evaluate test loss\nand accuracy on all pairs of inputs not used for training.\n\nNetworks trained on this task consistently exhibit grokking. As Figure 2 shows, our networks first\noverfit the training set: train accuracy quickly converges to 100% and the train loss quickly declines,\nwhile the test accuracy remains low and the test loss remains high. After around 10, 000 epochs,\nthe network generalizes and test accuracy increases to near 100%. In robustness experiments, we\nconfirm that grokking consistently occurs for other architectures and prime moduli (Appendix C.2).\nIn Section 5.3 we find that grokking does not occur without regularization.\n\nTo describe transformer components, we follow the conventions and notations laid out in Elhage\net al. (2021). We focus on the d×p embedding matrix WE, the d×n output matrix of the MLP layer\nWout, and the P × d unembedding matrix WU.2 Let Logits(a, b) denote the logit vector on inputs\na, b, and MLP(a, b) denote the MLP activations. Empirically, our networks do not significantly use\nthe skip connection around the MLP (Appendix A.1), so Logits(a, b) ≈WUWoutMLP(a, b). We\ntherefore also study the P × n neuron-logit map WL = WUWout.\n\n\n3.1  THE FOURIER MULTIPLICATION ALGORITHM\n\nWe claim that the learned networks use the following algorithm (Figure 1):\n\n        • Given two one-hot encoded tokens a, b map these to sin(wka), cos(wka), sin(wkb), and\n        cos(wkb) using the embedding matrix, for various frequencies wk = 2kπP  , k ∈N.\n\n\n  2We ignore the embedding and unembedding of the ‘=’ token for simplicity.\n\n\n                                       3\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n        • Compute cos (wk(a + b)) and sin (wk(a + b)) using the trigonometric identities:\n                    cos (wk(a + b)) = cos (wka) cos (wka) −sin (wka) sin (wkb)\n                       sin (wk(a + b)) = sin(wka) cos (wkb) + cos (wka) sin (wkb)\n         In our networks, this is computed in the attention and MLP layers.\n        • For each output logit c, compute cos (wk(a + b −c)) using the trigonometric identity:\n           cos (wk(a + b −c)) = cos (wk(a + b)) cos (wkc) + sin (wk(a + b)) sin (wkc) .    (1)\n        This is a linear function of the already-computed values cos(wk(a + b)), sin(wk(a + b))\n       and is implemented in the product of the output and unembedding matrices WL.\n        • The unembedding matrix also adds together cos (wk(a + b −c)) for the various ks. This\n        causes the cosine waves to constructively interfere at c∗= a + b mod p (giving c∗a large\n          logit), and destructively interfere everywhere else (thus giving small logits to other cs).\n\nWe refer to this algorithm as Fourier multiplication, and will justify our claim in detail in Section 4.\n\n4  REVERSE ENGINEERING A ONE-LAYER TRANSFORMER\n\nIn this section, we describe four lines of evidence that our transformers are using the Fourier mul-\ntiplication algorithm described in Section 3.1. Here we apply our analysis to the mainline model\nfrom Section 3; the results are broadly consistent for other models, including across different num-\nber of layers, different fractions of the training data, and different prime moduli (see Appendix C.2,\nespecially Table 5).\n\nOur first line of evidence involves examining the network weights and activations and observing\nconsistent periodic structure that is unlikely to occur by chance (Section 4.1). Moreover, when we\ntake Fourier transforms, many components are either sparse or nearly sparse in the Fourier domain,\nsupported on a handful of key frequencies.\n\nWe next look into the actual mechanisms implemented in the model weights (Section 4.2). We show\nthat the unembedding matrix WL is (approximately) rank 10, where each direction corresponds to\nthe cosine or sine of one of 5 key frequencies. Projecting the MLP activations onto the components\nof WL approximately produces multiples of the functions cos (wk(a + b)) and sin (wk(a + b)),\nshowing that the MLP layer does compute these sums.\n\nTo better understand the mechanism, we zoom in to individual neurons (Section 4.3). We find that\nthe attention heads and most neurons are well-approximated by degree-2 polynomials of sines and\ncosines at a single frequency. Moreover, the corresponding direction in WL also contains only that\nfrequency. This suggests that the model’s computations are (1) localized across frequencies and (2)\nmostly aligned with the neuron basis.\n\nFinally, we use ablations to confirm that our interpretation is faithful (Section 4.4). We replace\nvarious components of the model by the components of the Fourier multiplication algorithm and\nfind that doing so consistently does not harm and sometimes even improves model performance.\n\n4.1  SUGGESTIVE EVIDENCE: SURPRISING PERIODICITY\n\nThe first line of evidence that the network is using the algorithm described in Section 3.1 is the\nsurprising periodicity in the activations of the transformer. That is, the output of every part of the\nnetwork is periodic as a function of the input tokens.\n\nPeriodicity in the embeddings. We start by examining the embeddings. We apply a Fourier trans-\nform along the input dimension of the embedding matrix WE then compute the ℓ2-norm along the\nother dimension; results are shown in Figure 3. We plot only the components for the first 56 frequen-\ncies, as the norm of the components for frequencies k and P −k are symmetric. The embedding\nmatrix WE is sparse in the Fourier basis–it only has significant nonnegligible norm at 6 frequencies.\nOf these frequencies, only 5 appear to be used significantly in later parts of the model (corresponding\nto k ∈{14, 35, 41, 42, 52}). We dub these the key frequencies of the model.\n\nPeriodicity in attention heads and MLP neuron activations.  This periodic structure recurs\nthroughout the network. As an example, we plot the attention weight at position 0 for every combi-\nnation of two inputs for head 0 in Figure 4. The attention exhibits a periodic structure with frequency\n\n\n                                       4\n\nPublished as a conference paper at ICLR 2023\n\n\n\n        Fourier Components of Embedding Matrix             Fourier Components of Neuron-Logit Map\n\n                         sin                                                                                                  sin\n              2     cos                                                                       2     cos                      Component  1.5                                                                                                                                                                                                                Component  1.5\n\n              1                                                                              1                 Fourier                                                                                                                                                                           Fourier\n     of  0.5                                              of  0.5\n\n\n              0                                                                              0          Norm    0        10        20        30        40        50                            Norm    0        10        20        30        40        50\n                          Frequency k                                                 Frequency k\n\nFigure 3: (Left) The norms of the Fourier components in the embedding matrix WE. As discussed in\nSection 4.1, the sparsity of WE in the Fourier basis is evidence that the network is operating in this\nbasis. Of the six non-zero frequencies, five “key frequencies” appear in later parts of the network,\ncorresponding to k ∈{14, 35, 41, 42, 52}. (Right) Norm of Fourier components of the neuron-logit\nmap WL. A Fourier transform is taken over the logit axis, and then the norm is taken over the neuron\naxis. As discussed in Section 4.2, WL is well-approximated by the 5 key frequencies wk.\n\n\n\n                                                                       Norms                                                                                                     of Logits in 2D Fourier Basis    Attention             Score for Head 0                                     Activations                                                     for Neuron 0                                                                                                                               Const           0                                                                 0                                       1                                                                                                                                      cos                                                                                                                          5                                                                                  4                                                                                                                               60                                                                                                                                                          sin                                                                                                                          9\n          20                                   0.8            20                                   3.5            cos 14                             40\n                                                                                                                                                        sin                                                                                  3                                                                                                                         18                             20                                                                                                                                     cos                                                                                                                         23          40                                                    40\n                                               0.6                                                 2.5              sin 27                             0\n                                                                                                                                     cos                                                                                                                         32                                                                                  2 b   60               b   60                                                                                                                        −2                                                                                                                                                        sin                                                                                                                         36                                               0.4                                                                                                                                                                                                                                                                        Component                                                                                                   1.5                                                                                                                                     cos                                                                                                                         41                                                                                                                        −4                             y          80                                                                80                                                                                                                                                        sin                                                                                                                         45                                                                                  1                                               0.2                                                                                                                                     cos                                                                                                                         50                                                                                                                        −6\n         100                                                   100                                   0.5              sin 54\n                                       0\n            0             50            100                                                                  0             50            100                                                                                  0                                      Const cos5 sin9 cos14 sin18 cos23 sin27 cos32 sin36 cos41 sin45 cos50 sin54\n                  a                                  a                          x Component\n\nFigure 4: (Left) The attention score for head 0 from the token ‘=’ to ‘a’, as a function of inputs\na, b. (Center) The activations of MLP neuron 0 given inputs a, b. Both the attention scores and the\nneuron activations are periodic (Section 4.1). (Right) The norm of the Fourier components of the\nlogits (2D Fourier transform is taken over the inputs a, b, and then norm is taken over the logit axis).\nThere are 20 significant components corresponding to the 5 key frequencies (Section 4.1).\n\n\nk = 35. In Figure 4, we also plot the activations of MLP neuron 0 for every combination of inputs.\nThe activations are periodic with frequency k = 42. We see similar patterns for other attention\nheads and MLP neurons (Appendix C.1).\n\nPeriodicity in logits. Finally, the logits are also periodic. In Figure 4, we represent the logits in the\n2D Fourier basis over the inputs, then take the ℓ2-norm over the output dimension. There are only\ntwenty components with significant norm, corresponding to the products of sines and cosines for the\nfive key frequencies wk. These show up as five 2 × 2 blocks in Figure 4.\n\n4.2  MECHANISTIC EVIDENCE: COMPOSING MODEL WEIGHTS\n\nWe now demonstrate that the model implements the trigonometric identity (1) as follows: the func-\ntions cos (wk(a + b)), sin (wk(a + b)) are linearly represented in the MLP activations, and the un-\nembed matrix reads these linear directions and multiplies them by cos (wkc), sin (wkc) respectively.\n\nWe will do this in two steps. First, we show that WL (the matrix mapping MLP activations to logits)\nis (approximately) rank 10 and can be well approximated as:\n\n            WL = X                cos (wk) uTk + sin (wk) vTk                    (2)                                   k∈{14,35,41,42,52}\nfor some uk, vk ∈R512, where cos (wk) , sin (wk) ∈R113 are vectors whose cth entry is cos (wkc)\nand sin (wkc). Second, note that our model implements the logits for a, b as:\n\n      Logits(a, b) = WLMLP(a, b) ≈ X cos (wk) uTk MLP(a, b) + sin (wk) vTk MLP(a, b)    (3)\n                                         k\nWe check empirically that the terms uTk MLP(a, b) and vTk MLP(a, b) are approximate multiples of\ncos (wk(a + b)) and sin (wk(a + b)) (> 90% of variance explained). Thus the network computes\ntrigonometric functions in the MLP and reads them off as claimed. As a sanity check, we confirm\n\n\n                                       5\n\nPublished as a conference paper at ICLR 2023\n\n\n WL Component                Fourier components of uTk MLP(a, b) or vTk MLP(a, b)            FVE\n    cos (w14c)     44.6 cos(w14a) cos(w14b) −43.6 sin(w14a) sin(w14b) ≈44.1 cos (w14(a + b))   93.2%\n    sin (w14c)     44.1 sin(w14a) cos(w14b) + 44.1 cos(w14a) sin(w14b) ≈44.1 sin (w14(a + b))   93.5%\n    cos (w35c)     40.7 cos(w35a) cos(w35b) −43.6 sin(w35a) sin(w35b) ≈42.2 cos (w35(a + b))   96.8%\n    sin (w35c)     41.8 sin(w35a) cos(w35b) + 41.8 cos(w35a) sin(w35b) ≈41.8 sin (w35(a + b))   96.5%\n    cos (w41c)     44.8 cos(w41a) cos(w41b) −44.8 sin(w41a) sin(w41b) ≈44.8 cos (w41(a + b))   97.0%\n    sin (w41c)     44.5 sin(w41a) cos(w41b) + 44.5 cos(w41a) sin(w41b) ≈44.5 sin (w41(a + b))   97.0%\n    cos (w42c)     64.6 cos(w42a) cos(w42b) −68.5 sin(w42a) sin(w42b) ≈66.6 cos (w42(a + b))   96.4%\n    sin (w42c)     67.8 sin(w42a) cos(w42b) + 67.8 cos(w42a) sin(w42b) ≈67.8 sin (w42(a + b))   96.4%\n    cos (w52c)     60.5 cos(w52a) cos(w52b) −65.5 sin(w52a) sin(w52b) ≈63.0 cos (w52(a + b))   97.4%\n    sin (w52c)     64.5 sin(w52a) cos(w52b) + 64.5 cos(w52a) sin(w52b) ≈64.5 sin (w52(a + b))   98.2%\n\nTable 1: For each of the directions uk or vk (corresponding to the cos(wk) and sin(wk) components\nrespectively) in the unembedding matrix, we take the dot product of the MLP activations with that\ndirection, then perform a Fourier transform (middle column; only two largest coefficients shown).\nWe then compute the fraction of variance explained (FVE) if we replace the projection with a single\nterm proportional to cos (wk(a + b)) or sin (wk(a + b)), and find that it is consistently close to 1.\n\n\nthat the logits are indeed well-approximated by terms of the form cos (wk(a + b −c)) (95% of\nvariance explained).\n\nWL is well approximated by cos (wkc) and sin (wkc). We perform a discrete Fourier transform\n(DFT) on the logit axis of WL and look at the 10 directions uk, vk corresponding to sin (wk) and\ncos (wk). When we approximate WL with Pk∈{14,35,41,42,52} cos (wk) uTk +sin (wk) vTk , the resid-\nual has Frobenius norm that is under 0.55% of the norm of WL. This shows that WL is well ap-\nproximated by the 10 directions corresponding to cos (wk) and sin (wk) for each of the five key\nfrequencies. We also plot the norms of each direction in Figure 3, and find that no Fourier compo-\nnent outside the 5 key frequencies has significant norm.\n\nThe unembedding matrix “reads off” terms of the form cos (wk(a + b)) and sin (wk(a + b))\nfrom the MLP neurons.  Next, we take the dot product of the MLP activations with each of\nthe directions uk, vk for k ∈{14, 35, 41, 42, 52}. Table 1 displays the results: the dot products\nuTk MLP(a, b) and vTk MLP(a, b) are well approximated by a multiple of terms of the form\n              cos (wk(a + b)) = cos (wka) cos (wkb) −sin (wka) sin (wkb) , and\n               sin (wk(a + b)) = sin (wka) cos (wkb) + cos (wka) sin (wkb) .\nThat is, for each key frequency k, uk and vk are linear directions in the space of MLP neuron\nactivations that represent cos (wk(a + b)) and sin (wk(a + b)).\n\nLogits are well approximated by a weighted sum of cos (wk(a + b −c))s. We approximate the\noutput logits as the sum Pk αk cos(wk(a+b−c)) for k ∈{14, 35, 41, 42, 52} and fit the coefficients\nαk via ordinary least squares.  This approximation explains 95% of the variance in the original\nlogits. This is surprising—the output logits are a 113 · 113 · 113 dimensional vector, but are well-\napproximated with just the 5 directions predicted by our interpretation. If we evaluate test loss using\nthis logit approximation, we actually see an improvement in loss, from 2.4 · 10−7 to 4.7 · 10−8.\n\nTaken together,  these  results confirm that the model computes sums of terms of the form\ncos (wk(a + b −c)) = cos (wk(a + b)) cos (wkc) + sin (wk(a + b)) sin (wkc).\n\n4.3  ZOOMING IN: APPROXIMATING NEURONS WITH SINES AND COSINES\n\nIn the previous section, we showed how the model computes its final logits by using WL to “read\noff” trigonometric identities represented in the MLP neurons. We now examine the attention heads\nand MLP neurons to understand how the identities come to be represented at the MLP layer. In\nAppendix C.1.2, we show that two of the attention heads approximately compute degree-2 poly-\nnomials of sines and cosines of a particular frequency (and the other two are used to increase the\nmagnitude of the input embeddings in the residual stream). Here, we show that most neurons are\nalso well-approximated by degree-2 polynomials, and the map from neurons to logits is localized by\nfrequency.\n\nMost MLP neurons approximately compute a degree-2 polynomial of a single frequency. We\nnext try to approximate the activations of each MLP neuron by a degree-2 polynomial of one of the\n5 key frequencies. As shown in Figure 5, out of 512 total neurons, 433 (84.6%) have over 85% of\ntheir variance explained with a single frequency.\n\n\n                                       6\n\nPublished as a conference paper at ICLR 2023\n\n\n         FVE by degree-2 polynomials             Components of WL corresponding to freq 14 neurons\n       400                                                                     0\n\n                                                                                                                                                                                                                                                                           0.4\n       300                                                                    10                                                                                                            0.2     neurons\n  of 200                                                                    20                                                                                          0                                                                                                                                             Neuron\n                                                                              30                                                                                                   −0.2\n       100\n\n                                                                                                                                                                                                                                                     −0.4     Number                                                                        40\n         0\n                       0.4             0.6             0.8           1                             sin sin sin sin sin sin sin sin sin sin sin sin sin sin sin sin sin sin\n               Fraction of variance explained                                                            Const 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54\n\nFigure 5: (Left) Most neurons are well-approximated by degree-2 polynomials of a single frequency.\n(Right) A heatmap showing weights in WL corresponding to each of the 44 neurons of frequency\n14. The non-trivial components correspond to sin (wk) and cos (wk) for k = 14.\n\n                               Loss after ablating frequencies\n                   1\n                            Other frequency    Key frequency\n                   0.01\n                Loss                100μ\n            Log\n                  1μ\n\n                 10n\n                          5     10    15    20    25    30    35    40    45    50    55       RestrictedOriginal\n\nFigure 6: The loss of the transformer (lower=better) when ablating each frequency k ∈{1, 2, ..., 56}\nand everything except for the five key frequencies (restricted loss). We include the original unablated\nloss for reference. Ablating key frequencies causes a performance drop, while the other ablations\ndo not harm performance.\n\n\nMaps to the logits are localized by frequency. We partition these 433 neurons by the frequencies\nwith the highest variance explained. For each resulting subset, the map WL from neurons to logits\nhas only two non-trivial components, corresponding to sine and cosine at that frequency. For exam-\nple, in Figure 5 we plot the 44 columns of WL corresponding to the 44 neurons in the k = 14 cluster\nand find that the only non-negligible components are sin  2kπP   and cos  2kπP   for k = 14.\n\n4.4  CORRECTNESS CHECKS: ABLATIONS\n\nIn previous sections, we showed that various components of the model were well-approximated by\nsparse combinations of sines and cosines. We verify that these approximations are faithful to the\nmodel’s functionality, by replacing each component with its approximation. This generally does not\nhurt the performance of the model and in some cases improves it.\n\nMLP neurons. In Section 4.3, we identified 433 neurons that were well-approximated by a degree-2\npolynomial. We replace each of these neurons’ activation value by the corresponding polynomial,\nleaving the other neurons untouched. This increases loss by only 3% in relative terms (from 2.41 ·\n10−7 to 2.48 · 10−7) and has no effect on accuracy.\n\nWe can instead apply a stricter ablation to the MLP layer and restrict each neuron’s activation to\njust the components of the polynomial corresponding to terms of the form cos(wk(a + b)) and\nsin(wk(a + b)) in the key frequencies. This improves loss by 77% (to 5.54 · 10−8), validating that\nthe logits are calculated by trig identities of neurons as detailed in Section 4.2.\n\nLogit frequencies. Next, we ablate various components of the final logits in the Fourier space. To\ndo so, we take a 2D DFT on the 113 · 113 · 113 logit matrix over all 113 · 113 pairs of inputs to get\nthe logits in the Fourier basis, then set various frequencies in this basis to 0.\n\nWe begin by ablating the components corresponding to each of the key frequencies. As reported in\nFigure 6, ablating any key frequency causes a significant increase in loss. This confirms that the five\nfrequencies identified in previous sections are indeed necessary components of the transformer. In\ncontrast, ablating other frequencies does not hurt the model at all.\n\nWe then ablate all 113 · 113 −40 of the Fourier components besides key frequencies; this ablation\nactually improves performance (loss drops 70% to 7.24 · 10−8).\n\n\n                                       7\n\nPublished as a conference paper at ICLR 2023\n\n\n           Excluded Loss over All Frequencies                                 Restricted Loss\n\n\n         1                                                                      1\n\n        0.01                                              Train Loss                       0.01                                              Train loss\n                                                            Test                                                        Loss   Loss                                                    Excluded                                                             Loss                Loss 100μ                                               TestRestrictedloss  loss      100μ\n\n                                                                            1μ\n        1μ\n                                                                              10n\n          0        5k       10k      15k      20k      25k      30k               0       5k      10k      15k      20k      25k      30k\n                        Epoch                                             Epoch\n\n  Gini Coefficients of Embed Matrix and Neuron-logit Map              Total Sum of Squared Weights\n         0.8\n                                                                             3500\n         0.6                                                                                                                                              Weights 3000\n\n                                                                             2500\n         0.4            coefficient                                                                       2000                                                                                                                                                                      Squared\n    Gini  0.2                                         of 1500\n\n                                                                             1000         0                                                             Sum\n          0        5k       10k      15k      20k      25k      30k               0       5k      10k      15k      20k      25k      30k\n                        Epoch                                             Epoch\n\nFigure 7: How each of the progress measures in Section 5.1 changes over the course of training.\nThe lines delineate the 3 phases of training: memorization, circuit formation, and cleanup (and a\nfinal stable phase). (Top Left) Excluded loss increases during circuit formation, while train and test\nloss remain flat. (Top Right) The restricted loss begins declining before test loss declines, but has an\ninflection point when grokking begins to occur. (Bottom Left) The Gini coefficient of the norms of\nthe Fourier components of WE and WL increase sharply during cleanup. (Bottom Right) The sums\nof squared weights decreases smoothly during circuit formation and more sharply during cleanup,\nindicating that both phases are linked to weight decay.\n\n\nDirections in WL. In Section 4.2, we found that WL is well approximated by the 10 directions\ncorresponding to the cosine and sine of key frequencies. If we project the MLP activations to these\n10 directions, loss decreases 50% to 1.19 · 10−7. If we instead projected the MLP activations onto\nthe nullspace of these 10 directions, loss increases to 5.27—worse than uniform. This suggests that\nthe network achieves low loss using these and only these 10 directions.\n\n\n5  UNDERSTANDING GROKKING BEHAVIOR USING PROGRESS MEASURES\n\nWe now use our mechanistic understanding of the network to define two progress measures: metrics\nthat can be computed during training that track the progress of the model over the course of training,\nincluding during phase transitions. This allows us to study how the network reaches its final solution.\n\n\n5.1  PROGRESS MEASURES\n\nWe translate the ablations in Section 4.4 into two progress measures: restricted and excluded loss.\n\nRestricted loss. Since the final network uses a sparse set of frequencies wk, it makes sense to check\nhow well intermediate versions of the model can do using only those frequencies. To measure this,\nwe perform a 2D DFT on the logits to write them as a linear combination of waves in a and b,\nand set all terms besides the constant term and the 20 terms corresponding to cos(wk(a + b)) and\nsin(wk(a + b)) for the five key frequencies to 0. We then measure the loss of the ablated network.\n\nExcluded loss. Instead of keeping the important frequencies wk, we next remove only those key\nfrequencies from the logits but keep the rest. We measure this on the training data to track how\nmuch of the performance comes from Fourier multiplication versus memorization. The idea is that\nthe memorizing solution should be spread out in the Fourier domain, so that ablating a few directions\nwill leave it mostly unaffected, while the generalizing solution will be hurt significantly.\n\nBeyond these, we will also measure (1) the Gini coefficient (Hurley & Rickard, 2009) of the norms\nof the Fourier components of WE and WL, which measures the sparsity of WE and WL in the\nFourier basis, and (2) the ℓ2-norm of the weights during training, since weight decay should push\nthese down once the train loss is near zero.\n\n\n                                       8\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n5.2  PHASES OF GROKKING: MEMORIZATION, CIRCUIT FORMATION, AND CLEANUP\n\nUsing the mainline model from Section 4, we plot the excluded loss, restricted loss, Gini coefficient\nof the matrices WU and WL, and sum of squared weights in Figure 7. We find that training splits\ninto three phases, which we call the memorization, circuit formation, and cleanup phases. (We show\nsimilar results for other models in Appendix C.2.)\n\nMemorization (Epochs 0k–1.4k). We first observe a decline of both excluded and train loss, with\ntest and restricted loss both remaining high and the Gini coefficient staying relatively flat. In other\nwords, the model memorizes the data, and the frequencies wk used by the final model are unused.\n\nCircuit formation (Epochs 1.4k–9.4k). In this phase, excluded loss rises, sum of squared weights\nfalls, restricted loss starts to fall, and test and train loss stay flat. This suggests that the model’s\nbehavior on the train set transitions smoothly from the memorizing solution to the Fourier multi-\nplication algorithm. The fall in the sum of squared weights suggests that circuit formation likely\nhappens due to weight decay. Notably, the circuit is formed well before grokking occurs.\n\nCleanup (Epochs 9.4k–14k). In this phase, excluded loss plateaus, restricted loss continues to drop,\ntest loss suddenly drops, and sum of squared weights sharply drops. As the completed Fourier\nmultiplication circuit both solves the task well and has lower weight than the memorization circuit,\nweight decay encourages the network to shed the memorized solution in favor of focusing on the\nFourier multiplication circuit. This is most cleanly shown in the sharp increase in the Gini coefficient\nfor the matices WE and WL, which shows that the network is becoming sparser in the Fourier basis.\n\n\n5.3  GROKKING AND WEIGHT DECAY\n\nIn the previous section, we saw that each phase of grokking corresponded to an inflection point in the\nℓ2-norm of the weights. This suggests that weight decay is an important component of grokking and\ndrives progress towards the generalizing solution. In Appendix D.1, we provide additional evidence\nthat weight decay is necessary for grokking: smaller amounts of weight decay causes the network\nto take significantly longer to grok (echoing the results on toy models from Liu et al. (2022)), and\nour networks do not grok on the modular arithmetic task without weight decay or some other form\nof regularization. In Appendix C.2, we also find that the amount of data affects grokking: when\nnetworks are provided with enough data, there is no longer a gap between the train and test losses\n(instead, both decline sharply some number of epochs into training). Finally, in Appendix D.3 we\nreplicate these results on several additional algorithmic tasks.\n\n\n6  CONCLUSION AND DISCUSSION\n\n\nIn this work, we use mechanistic interpretability to define progress measures for small transformers\ntrained on a modular addition task. We find that the transformers embed the input onto rotations\nin R2 and compose the rotations using trigonometric identities to compute a + b mod 113. Using\nour reverse-engineered algorithm, we define two progress measures, along which the network makes\ncontinuous progress toward the final algorithm prior to the grokking phase change. We see this work\nas a proof of concept for using mechanistic interpretability to understand emergent behavior.\n\nLarger models and realistic tasks. In this work, we studied the behavior of small transformers\non a simple algorithmic task, solved with a single circuit. On the other hand, larger models use\nlarger, more numerous circuits to solve significantly harder tasks (Cammarata et al., 2020; Wang\net al., 2022). The analysis reported in this work required significant amounts of manual effort, and\nour progress metrics are specific to small networks on one particular algorithmic task. Methods for\nautomating the analysis and finding task-independent progress measures seem necessary to scale to\nother, larger models. We discuss possible scenarios for more realistic applications in Appendix F.\n\nDiscovering phase change thresholds. While the progress measures we defined in Section 5.1 in-\ncrease relatively smoothly before the phase transition (and suffice to allow us to understand grokking\nfor this task) we lack a general notion of criticality that would allow us to predict when the phase\ntransition will happen ex ante. Future work should develop theory and practice in order to apply\nprogress measures to predict the timing of emergent behavior.\n\n\n                                       9\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\nREPRODUCIBILITY STATEMENT\n\nAn  annotated  Colab  notebook  containing  the  code  to  replicate  our  results,  including\ndownload  instructions  for model checkpoints,  is  available  at https://neelnanda.io/\ngrokking-paper.\n\n\nAUTHOR CONTRIBUTIONS\n\nNeel Nanda was the primary research contributor. He reverse engineered the weights of the mainline\nmodel to discover the Fourier multiplication algorithm and found the lines of evidence in Section 4.\nHe also discovered the restricted and excluded loss progress measures and that grokking in mainline\nmodel could be divided into three discrete phases.  Finally, he found the link between grokking,\nlimited data, and phase transitions by exhibiting grokking in other settings with phase transitions.\n\nLawrence Chan was invaluable to the framing and technical writing of this work. In addition, he\ncreated the Gini coefficient progress measure and performed the analysis in the appendices exploring\nto what extent the results on the mainline model applied to the other small transformer models,\nincluding with other random seeds, architectures, prime moduli, and regularization methods.\n\nTom Lieberum contributed to the early stages of this work by creating a minimal setup of grokking\nwith a 1L Transformer on the modular addition task with no LayerNorm and finding the surprising\nperiodicity within the model’s internals.\n\nJess Smith performed experiments exploring grokking with different random seeds, architectures,\nand other hyper-parameters.\n\nJacob Steinhardt helped clarify and distill the results, provided significant amounts of editing and\nwriting feedback, and suggested the progress measure frame.\n\n\nACKNOWLEDGMENTS\n\nIn writing this paper, our thinking and exposition was greatly clarified by correspondence with\nand feedback from Oliver Balfour, David Bau, Sid Black, Nick Cammarata, Stephen Casper, Bilal\nChughtai, Arthur Conmy, Xander Davies, Ben Edelman, Nelson Elhage, Ryan Greenblatt, Jacob\nHilton, Evan Hubinger, Zac Kenton, Janos Kramar, Lauro Langosco, Tao Lin, David Lindner, Eric\nMichaud, Vlad Mikulik, Noa Nabeshima, Chris Olah, Michela Paganini, Michela Paganini, Alex\nRay, Rohin Shah, Buck Shlegeris, Alex Silverstein, Ben Toner, Johannes Treutlein, Nicholas Turner,\nVikrant Varma, Vikrant Varma, Kevin Wang, Martin Wattenberg, John Wentworth, and Jeff Wu.\n\nWe’d also like to thank Adam Gleave and Chengcheng Tan for providing substantial editing help,\nand Noa Nabeshima and Vlad Mikulik for pair programming with Neel.\n\nThis work draws heavily on the interpretability techniques and framework developed by Elhage et al.\n(2021) and Olsson et al. (2022).\n\nWe trained our models using PyTorch (Paszke et al., 2019) and performed our data analysis using\nNumPy (Harris et al., 2020), Pandas (Wes McKinney, 2010), and einops (Rogozhnikov, 2022).\nOur figures were made using Plotly (Plotly Technologies Inc., 2015).\n\nNeel would like to thank Jemima Jones for providing practical and emotional support as he navigated\npersonal challenges while contributing to this paper, and to the Schelling Residency for providing\nan excellent research environment during the distillation stage. He would also like to thank the\nAnthropic interpretability team, most notably Chris Olah, for an incredibly generous amount of\nmentorship during his time there, without which this investigation would never have happened.\n\n\nREFERENCES\n\nBoaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.\n  Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint\n  arXiv:2207.08799, 2022.\n\n\n                                       10\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n  few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea\n  Voss, Ben Egan, and Swee Kiat Lim. Thread: Circuits. Distill, 2020. doi: 10.23915/distill.00024.\n   https://distill.pub/2020/circuits.\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n  Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n  Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n  Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n  Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n  2021. https://transformer-circuits.pub/2021/framework/index.html.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n  networks. arXiv preprint arXiv:1803.03635, 2018.\n\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom\n  Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large\n  generative models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pp.\n  1747–1764, 2022.\n\nCharles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen,\n  David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert\n  Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,\n  Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard,\n  Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar-\n  ray programming with NumPy.  Nature, 585(7825):357–362, September 2020.  doi: 10.1038/\n  s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.\n\nNiall Hurley and Scott Rickard. Comparing measures of sparsity. IEEE Transactions on Information\n  Theory, 55(10):4723–4741, 2009.\n\nZiming Liu, Ouail Kitouni, Niklas Nolte, Eric J Michaud, Max Tegmark, and Mike Williams. To-\n  wards understanding grokking: An effective theory of representation learning.  arXiv preprint\n  arXiv:2205.10343, 2022.\n\nIlya Loshchilov and Frank Hutter.   Decoupled weight decay regularization.   arXiv preprint\n  arXiv:1711.05101, 2017.\n\nThomas McGrath, Andrei Kapishnikov, Nenad Tomaˇsev, Adam Pearce, Demis Hassabis, Been Kim,\n  Ulrich Paquet, and Vladimir Kramnik.  Acquisition of chess knowledge in alphazero.  arXiv\n  preprint arXiv:2111.09259, 2021.\n\nBeren  Millidge.     Grokking  ’grokking’,  2022.    URL https://www.beren.io/\n  2022-01-11-Grokking-Grokking/.\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\n  Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,\n  Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\n   Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,\n  and Chris Olah.  In-context learning and induction heads. Transformer Circuits Thread, 2022.\n  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping\n  and mitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\n   Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\n  performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nPlotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly.\n\n\n                                       11\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\n   eralization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177,\n  2022.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n  models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nAlex Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In\n  International Conference on Learning Representations, 2022. URL https://openreview.\n  net/forum?id=oapKSVM2bcj.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n  Dropout: a simple way to prevent neural networks from overfitting.  The journal of machine\n  learning research, 15(1):1929–1958, 2014.\n\nJacob Steinhardt.  More is different for ai, Feb 2022. URL https://bounded-regret.\n  ghost.io/more-is-different-for-ai/.\n\nVimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind. The\n  slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon.\n  arXiv preprint arXiv:2206.04817, 2022.\n\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.  Inter-\n   pretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint\n  arXiv:2211.00593, 2022.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\n  gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\n  models. arXiv preprint arXiv:2206.07682, 2022a.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\n  Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\n  arXiv:2201.11903, 2022b.\n\nWes McKinney. Data Structures for Statistical Computing in Python. In St´efan van der Walt and\n  Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 56 – 61, 2010.\n   doi: 10.25080/Majora-92bf1922-00a.\n\n\n\n\n\n                                       12\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\nA  MATHEMATICAL STRUCTURE OF THE TRANSFORMER\n\nWe follow the conventions and notation of Elhage et al. (2021) in describing our model. Here, we\nbriefly recap their notation and examine it in our specific case.\n\nWe denote our hyperparameters as follows: dvocab = 113 is the size of the input and output spaces\n(treating ‘=’ separately), dmodel = 128 is the width of the residual stream (i.e. embedding size),\ndhead = 32 is the size of query, key and value vectors for a single attention head, and dmlp = 512\nis the number of neurons.\nWe denote the parameters as follows: WE (embedding layer); Wpos (positional embedding); W Qj\n(queries), W Kj  (keys), W Vj  (values), W Oj  (attention output) (the 4 weight matrices of head j in\nthe attention layer); Win and bin for the input linear map of the MLP layer; Wout and bout for\nthe output linear map of the MLP layer; and WU (unembedding layer). Note that we do not have\nbiases in our embedding, attention layer or unembedding, and we do not tie the matrices for the\nembedding/unembedding layers.\n\nWe now describe the mathematical structure of our network. Note that loss is only calculated from\nthe logits on the final token, and information only moves between tokens during the attention layer,\nso our variables from the end of the attention layer onwards only refer to the final token. We use\nti to denote the token in position i (as a one-hot encoded vector), pi to denote the ith positional\nembedding, x(0)i   to denote the initial residual stream on token with index i, A(i) to denote the\nattention scores from = to all previous tokens from head i, x(1) to denote the residual stream after\nthe attention layer on the final token, MLP to denote the neuron activations in the MLP layer on the\nfinal token, x(2) the final residual stream on the final token, Logits the logits on the final token.\n\nThe logits are calculated via the following equations:\n\n                       x(0)i = WEti + pi\n                  Aj = softmax(x(0)T W KjT W Qx(0)j  2  )\n                       x(1) = [X W OWj  Vj (x(0) · Aj)] + x(0)2\n                                        j\n             MLP = ReLU(Winx(1))\n                       x(2) = WoutN + x(1) = WoutReLU(Winx(1)) + x(1)\n                    Logits = WUx(2)\n\nAs in Elhage et al. (2021), we refer to the term W OWj  Vj (x(0)) as the OV circuit for head j.\n\nA.1  EMPIRICAL MODEL SIMPLIFICATIONS\n\nWe make two empirical observations:\n\n        • The attention paid from ‘=’ to itself is trivial. In practice, the average attention paid is\n       0.1% to 0.4% for each head, and ablating this does not affect model performance at all.\n        • The skip connection around the MLP layer is not important for the model’s computation\n       and can be ignored. Concretely, if we set it to zero or to its average (zero or mean ablation)\n        then model accuracy is unchanged, and loss goes from 2.4 · 10−7 to 9.12 · 10−7 and 7.25 ·\n       10−7 respectively. This is a significant increase in loss, but from such a small baseline that\n      we can still ignore it and reverse engineer the model’s computation. (That being said, both\n         the attention heads and the skip connection around them are crucial to the functioning of\n         the model: zero ablating attention heads increases loss to 24.3, while zero ablating the skip\n        connection around the attention heads increases loss to 19.1, both significantly worse than\n         chance.)\n\nA consequence of the first observation is that the attention is now a softmax over 2 elements, i.e. a\nsigmoid over the difference. And x(0)2   is constant, as it is independent of x and y, and the embedding\n\n\n                                       13\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nFigure 8: As discussed in Appendix B, while for every k ∈[0, ...P −1], cos  2kπP x  achieves its\nmaximum value (1) at x = 0 mod 113, it still has additional peaks at different values that are close\nto the maximum value. However, by adding together cosine waves of the 5 keyfrequencies, the\nmodel constructs a periodic function where the value at x = 0 mod 113 is significantly larger than\nits value anywhere else.\n\n\n\n                                                    T   jT\nand positional embedding of ‘=’ are fixed. So Aj0 = σ  x(0)2 W Q WK(x(0)0  −x(0)1  )  (and Aj1 =\n1 −Aj0)\nA consequence of the second observation is that Logits ≈WUWoutMLP, which we denote as\nWL = WUWout. From the perspective of the network, WL is the meaningful matrix, not either of\nits constituents, since they compose linearly.\n\n\nB WHY USE CONSTRUCTIVE INTEREFERENCE?\n\n\nAs demonstrated in Section 4 and Appendix C.2.1, small transformers trained on this task use several\ndifferent frequencies which they add together. The reason for this is to end up with a function whose\nvalue at x = 0 mod 113 is significantly larger than any other x.\nFor example, consider the function f14(x) = cos  2π·14113 x  . This function has period 113 and is\nmaximized at x = 0 mod 113. However, other values of x cause this function to be close to 1:\nf14(8) = f14(105) = 0.998, f14(16) = f14(89) = 0.994, etc.\nNow consider f35(x) = cos  2π·35113 x  . While this function also has period 113 and is maximized\nat x = 0 mod 113, it turns out that f35(8) = f35(105) = −0.990. This means that by adding\ntogether f14 and f35, we end up with a function that is not close to 1 at x = 8 mod 113. Similarly,\nwhile f35(16) = 0.961, f52(16) = −0.56, and so adding a third frequency reduces the peak at\nx = 16 mod 113.\n\nWe show the constructive interference resulting from the cosine waves for the five frequencies used\nby the mainline model in Figure 8.\n\n\nC  SUPPORTING EVIDENCE FOR MECHANISTIC ANALYSIS OF MODULAR\n   ARITHMETIC NETWORKS\n\n\nC.1  FURTHER ANALYSIS OF THE SPECIFIC TRAINING RUN DISCUSSED IN THE PAPER\n\nIn this section, we provide additional evidence relating to the mainline model.\n\n\n                                       14\n\nPublished as a conference paper at ICLR 2023\n\n\n\n                           Attention patterns by Head ('=' to a)\n            0\n\n           20                                                                                                                                                                                0.8\n\n           40                                                                                                                                                                                0.6\n  b  60\n                                                                                                                                                                                                           0.4\n           80\n                                                                                                                                                                                                           0.2\n          100\n\n             0       50      100  0       50      100  0       50      100  0       50      100\n                  a                a                a                a\n\nFigure 9: Attention patterns for each head, from the ‘=’ token at the third sequence position to the\na token at the first sequence position, as a heatmap over the inputs. All four attention heads exhibit\nstriking periodicity.\n\n                          Head   k     αj      βj     FVE\n                            0    35   −0.26   −0.14   99.03%\n                            1    42    0.27   −0.04   98.49%\n                            2    52    0.29   −0.05   99.07%\n                            3    42   −0.26    0.04   97.91%\n\nTable 2: For each attention head, we show the pattern from ‘=’ to a is well approximated by 0.5 +\nα(cos(wka)−cos(wkb))+β(sin(wka)−sin(wkb)) and give the coefficients and fraction of variance\nexplained for this approximation.\n\n\n\nC.1.1  PERIODICITY IN THE ACTIVATIONS OF OTHER ATTENTION HEADS\n\nIn Figure 9 we plot the attention patterns from the final token ‘=’ to the first token a for all 4\nattention heads, as a heatmap over the inputs a and b, as this is a scalar for each head. We observe a\nstriking periodicity and further that heads 1 and 3 represent the same frequency while heads 0 and 2\nare different.\nAs shown in Appendix A.1, the attention paid from ‘=’ to itself is negligible, so Aj0 = 1 −Aj1 and\nit suffices to plot attention to a.\n\n\nC.1.2  APPROXIMATING ATTENTION HEADS WITH SINES AND COSINES\n\nAttention heads approximately compute degree-2 polynomials of a single frequency or are\nused to amplify WE. In order to compute terms like cos (wk(a + b)), the model needs to compute\nthe product of the sine and cosine embeddings output by WE. As the attention heads are approx-\nimately bilinear (product of attention weights and OV circuit), they are a natural place to perform\nthis computation. Indeed, for each head, the attention scores’ Fourier transform is concentrated on\na single frequency wk. For two of the four heads, the corresponding OV circuit is concentrated on\nthat same frequency. Moreover, the softmax mapping the attention scores to attention weights is in\na regime where it behaves approximately linearly (and replacing it with a linear function actually\nimproves performance). Thus the attention weights multiply with the OV output to create degree-2\npolynomials of the frequency wk, as would be needed for the cosine/sine addition formulas.\n\nFor the remaining two heads, their attention scores approximately sum to one and the OV circuits\ncontain all five key frequencies, suggesting that they are used to increase the magnitude of key\nfrequencies in the residual stream. We confirm all of these claims in Appendix C.1.3.\n\n\nC.1.3  THE ATTENTION PATTERN WEIGHTS ARE WELL APPROXIMATED BY DIFFERENCES OF\n       SINES AND COSINES OF A SINGLE FREQUENCY.\n\nThe periodicity of the attention heads has a striking form—Aj0 is well approximated by 0.5 +\nαj(cos(wka) −cos(wkb)) + βj(sin(wka) −sin(wkb)), for some frequency wk and constants αj\nand βj (which may differ for each head). Note further that this simplifies to 0.5 + γ(cos(wk(a +\nθ))−cos(wk(b+θ))) for some constants γ and θ. We show the coefficients and fraction of variance\nexplained in Table 1\n\n\n                                       15\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                Fourier components for C0                              Fourier components for C1\n\n          8\n                                                                                           0               Component                                                                                                                                                                                                                               Component\n          6\n            Fourier 4                                                                                                                                                                        Fourier −2−4\n   of            sin                                          of             sin\n          2       cos                                                                  −6       cos\n\n          0                                                                          −8                  Coefficient   0          10         20         30         40         50                                                       Coefficient    0          10         20         30         40         50\n                           Frequency k                                                   Frequency k\n\n                Fourier components for C2                              Fourier components for C3\n          2                                                                                8\n\n          0         Component                                                                                                                                                                                                                                        Component 6\n        −2\n\n                                                                                           4\n        −4       Fourier                                                                                                                                                                                     Fourier\n  of  −6        sin                                           of 2        sin\n                   cos                                                                                        cos\n        −8                                                                                0\n\n       −10           Coefficient     0          10         20         30         40         50                                                           Coefficient   0          10         20         30         40         50\n                           Frequency k                                                   Frequency k\n\nFigure 10: We plot the attention pattern weights Cj in the Fourier basis for each of the four heads\nj ∈{0, 1, 2, 3}. We observe significant sparsity, with almost all of each term being associated with\na single frequency.\n\n\n\nMechanistic Analysis of Attention Patterns.  We can further mechanistically analyse how the\nmodel achieves this form. The following is a high-level sketch of what is going on:\n\nFirst, note that the attention score on position 0 and head j is just a lookup table on the input token\n                                           T   jT    j\na (of size P). To see why, note that Aj0 = mx(0)0 W K W Qx(0)2   . x(0)2   is constant since the token\nis always ‘=’ and x(0)0 = WEt0 + p0. So this reduces to t0 · Cj + D for some constant vector\nCj = W ET W KjT W Qx(0)j  2 ∈Rp and some scalar D = pT0 W Kj T W Qx(0)j  2   . As t0 is one-hot encoded,\nthis is just a lookup table, which we may instead denote as Cj[a]\n\nNext, note that the attention pattern from =→0 is σ(Cj[a] −Cj[b]). As argued in Appendix A.1,\nthe attention paid =→= is negligible and can be ignored. So the softmax reduces to a softmax over\ntwo elements, which is a sigmoid on their difference. As form of Cj does not mention the token\nindex or value, it is the same for position 0 and 1.\n\nWe now show that Cj is well-approximated by a wave of frequency wkj for some integer kj. That is,\nCj[a] ≈Fj cos(wkja)+Gj sin(wkja). We do this by simply computing Cj and fitting the constants\nFj and Gj to minimize ℓ2 loss, and display the resulting coefficients for each head in Figure 10. This\nfit explain 99.02%, 95.21%, 99.10%, 92.42% of the variance of Cj respectively. Interestingly, the\ncoefficients of heads 1 and 3 are almost exactly the opposite of each other.\n\nFor each head j, σ(Cj[a] −Cj[b]) ≈0.5 + Ej(Cj[a] −Cj[b]) for some constant Ej—that is, the\nsigmoid has some linear approximation. (The intercept will be 0.5 by symmetry.) The striking\nthing is that, because the inputs to the sigmoid for the attention heads are over a fairly wide range\n([−5, 5] roughly), the linear approximation to the sigmoid is a fairly good fit, explaining 97.5% of\nthe variance.\n\nWe validate that this is all that is going on, by replacing the sigmoid with the best linear fit. This\nimproves performance, decreasing test loss from 2.41 · 10−7 to 2.12 · 10−7.\n\nBy properties of sinusoidal functions, the attention patterns of each head will be well approximated\nby 0.5 ± Cj(cos(wkj(a + θj)) −cos(wkj(b + θj))) - the softmax is linear, with an intercept of 0.5,\nand the weights Cj map each token to a score that is a wave in a single frequency. This exactly gives\nus the periodic form shown in Figure 9.\n\n\n                                       16\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n        Fourier components of OV circuit for head 0            Fourier components of OV circuit for head 1\n          3                                                         cos      sin                    4                                                         cos      sin\n                                                                                                           3.5\n          2.5\n                                                                                           3           Component   2                                                                                                                                                                                                                       Component  2.5\n\n          1.5                                                                                2\n\n                                                                                                           1.5        Fourier   1                                                                                                                                                                       Fourier\n  of                                                  of   1\n          0.5\n                                                                                                           0.5\n     Norm   0                                                                                               Norm   0\n           0          10         20         30         40         50                       0          10         20         30         40         50\n                           Frequency k                                                   Frequency k\n\n        Fourier components of OV circuit for head 2            Fourier components of OV circuit for head 3\n          3.5                                                         cos      sin                                                                                cos      sin\n                                                                                                           3.5\n          3\n                                                                                           3\n          2.5           Component                                                                                                                                                                                                                                  Component  2.5\n\n\n          1.5        Fourier   2                                                                                2                                                                                                                                                                                               Fourier  1.5\n  of   1                                                of   1\n\n                                                                                                           0.5\n     Norm  0.50                                                                                               Norm                                                                                           0\n           0          10         20         30         40         50                       0          10         20         30         40         50\n                           Frequency k                                                   Frequency k\n\nFigure 11: We plot the output of the OV circuit W OWj  Vj x(0) in the Fourier basis for each of the\nfour heads j ∈{0, 1, 2, 3}. As with the attention pattern weights Cj in Figure 10, we observe that\nthe only components with significant norm are those corresponding to key frequencies, and that the\nlargest component corresponds to the frequencies of the attention patterns of the attention heads.\nAs attention pattern of heads 1 and 3 are sum to one, but their OV circuits are almost exactly the\nsame and consist of all five key frequencies, this implies that heads 1 and 3 are used to increase the\nmagnitude of key frequencies in the residual stream (Section C.1.3).\n\nFinally, for each head j, we plot the output of the OV circuit W OWj  Vj x(0) in the Fourier basis and\ndisplay the results in Figure 11). The largest component of each head corresponding to the frequency\nof the attention pattern Cj, with heads 0 and 2 being almost entirely composed of a sines and cosines\nof a single frequency. On the other hand, the norms for the components of heads 1 and 3 are almost\nexactly the same, and contain all five key frequencies. As the coefficients of the attention pattern\nweights have the opposite non-constant components (Table 2, Figure 10), their attention scores sum\nalmost exactly to 1 across all inputs. This implies that heads 1 and 3 are used to output the first\norder terms sin (wk) , cos (wk) in the five key frequencies. We speculate that this is because of\nweight decay encouraging the embeddings WE to be small, causing the network to allocate two of\nits attention heads to effectively increasing the size of WE.\n\nBringing it all together, this implies that attention heads 0 and 2 are approximately computing a\ndegree 2 polynomial of cosines and sines of a single frequency each, while heads 1 and 3 amplify\nthe key frequencies in the residual stream.\n\nC.1.4  PERIODICITY IN THE ACTIVATIONS OF ADDITIONAL NEURONS\n\nIn Figure 12, we display the activations of four more MLP neurons, as a function of the inputs. As\nwith neuron 0, the activations of these neurons are also periodic in the inputs.\n\nC.1.5  ADDITIONAL GROKKING FIGURES FOR MAINLINE RUN\n\nIn Figure 13, we display the accuracy of the model when restricting the model to use only the five\nkey frequencies. As with restricted loss, this improves model performance during training.\n\nIn Figure 14, we show the coefficients of the five key frequencies in the logits, calculated by regress-\ning the logits against the five cos (wk(a + b −c)) terms.\n\nIn Figure 15, we plot the excluded loss if we exclude each of the five key frequencies (as opposed to\nall five key frequencies).\n\n\n                                       17\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                    Neuron Activations for Additional Neurons\n            0\n\n           20                                                                                                                                                   4\n\n           40                                                                                                                                                   3\n  b  60                                                                                                                                                   2\n           80\n                                                                                                                                                                         1\n          100\n             0       50      100  0       50      100  0       50      100  0       50      100        0\n                  a                a                a                a\n\n\nFigure 12: Plots of neuron activations for MLP neurons 1, 2, 3 and 4, for inputs a, b ∈{0, 1, ..., 112}.\nAs with Neuron 0, all of the activation patterns are periodic in both inputs.\n\n\n\n                               Pure Restricted Accuracy\n                              1\n\n\n                                  0.8\n\n                                                                      Train Accuracy\n                                  0.6\n                                                                        Test Accuracy\n                                                                          Restricted Accuracy (Train)\n                                  0.4\n                                                                          Restricted Accuracy (Test)\n\n                                  0.2\n\n\n                              0\n                               0            10k           20k           30k\n                                      Epoch\n\n\nFigure 13: Accuracy when restricting Fourier Components to the five key frequencies. As with\nrestricted loss, this shows that the model figures out how to generalize modulo deleting noise before\nit removes the noise.\n\n\nAll three of these figures have inflection points corresponding to the relevant phases of grokking,\ndiscussed in Section 5.1.\n\n\nC.2  ADDITIONAL RESULTS FROM DIFFERENT RUNS\n\nIn this section, we plot relevant figures from other runs, either with the same architecture (Appendix\nC.2.1) or with different architectures or experimental setups (Appendix C.2.2). Note that in general,\nwhile all models learn to use variants of the modular arithmetic algorithm, they use a varying number\nof different key frequencies.  In order to find the key frequencies to calculate the excluded and\nrestricted loss, we perform a DFT on the neuron-logit map WL, then take the frequencies with\nnontrivial coefficients.3\n\nC.2.1  ADDITIONAL RESULTS FOR DIFFERENT RUNS WITH THE SAME ARCHITECTURE\n\nIn this section, we provide evidence that all 4 other runs (i.e., random seeds) using the experimental\nsetup of our mainline model also use the Fourier multiplication algorithm, and then confirm that the\nsame phases of grokking also occur on these runs.\n\n   3One method for getting a general (model-independent) progress measure for this task is to compute the\nexcluded loss for each of the 56 unique frequencies and then take the max. We omit the plots for this variant of\nthe excluded loss as they are broadly similar.\n\n\n                                       18\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                          Coefficients of cos(wk(a+b-c)) in the Logits\n                             30\n                                                                              Freq\n                             25                                                          14\n                                                                                          35                             20\n                                                                                          41\n                             15                                                          42\n                             10                                                          52\n\n                              5\n\n                              0\n                               0       5k      10k      15k      20k      25k      30k\n                                      Epoch\n\n\nFigure 14: The coefficients of cos(w(a + b −c)) in the logits over the model’s training. As with the\nmetrics in the paper, this shows a nice interpolation and growth of each cosine term.\n\n        Accuracy when Excluding Key Frequencies               Loss when Excluding Key Frequencies\n         1                                                         Train accuracy                                                                          Train loss\n                                                                       Test Accuracy          1                                                                Test Loss         0.8\n                                                         k=14                                                                     k=14\n         0.6                                               k=35                 0.01                                                   k=35\n                                                         k=41                                                                     k=41                                                                                               Loss\n         0.4                                               k=42                                                                            100μ                                                   k=42         Accuracy\n                                                         k=52                                                                     k=52\n         0.2\n                                                                            1μ\n         0\n          0      5k     10k    15k    20k    25k    30k                         0      5k     10k     15k     20k     25k     30k\n                    Epoch                                              Epoch\n\n\nFigure 15: The excluded accuracy (left) and loss (right) if we exclude each of the five key frequencies\nfor our mainline model. As with the excluded loss results in Section 5.1, this shows that the model\ninterpolates between memorising and generalising.\n\n\n\n           Embedding Matrix (Seed 1)                     Embedding Matrix (Seed 2)\n                                                                                  cos          3.5                                                                       cos\n          2.5                                                                                 sin          3                                                                                 sin          Component   2                                                                                                                                                                                                        Component  2.5\n\n          1.5                                                                           2\n\n                                                                                                     1.5        Fourier   1                                                                                                                                                            Fourier\n  of  0.5                                            of  0.51\n\n          0                                                                           0    Norm    0         10        20        30        40        50                      Norm    0         10        20        30        40        50\n                         Frequency k                                               Frequency k\n\n\n           Embedding Matrix (Seed 3)                     Embedding Matrix (Seed 4)\n\n                                                                                  cos                                                                                                     2.5                                                                       cos\n          2.5                                                                                              sin                                                                                                                                                                                                sin\n                                                                                      2          Component   2                                                                                                                                                                                                        Component\n        Fourier  1.51                                                                                                                                                            Fourier  1.51\n  of  0.5                                            of  0.5\n\n          0                                                                           0    Norm    0         10        20        30        40        50                      Norm    0         10        20        30        40        50\n                         Frequency k                                               Frequency k\n\nFigure 16: The norms of the Fourier components in the embedding matrix WE for each of four\nother random seeds for the original (1 layer) architecture. As discussed in Section 4.1 and Appendix\nC.2.1, the sparsity of WE in the Fourier basis is evidence that the network is operating in a Fourier\nbasis.\n\n\n\n                                       19\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n WL Component                 Fourier components of uTk MLP(a, b) or vTk MLP(a, b)            FVE\n    cos (w2c)      147.4 cos (w2a) cos (w2b) −145.8 sin (w2a) sin (w2b) ≈146.6 cos (w2(a + b))   99.2%\n     sin (w2c)      145.5 cos (w2a) sin (w2b) + 145.6 sin (w2a) cos (w2b) ≈145.5 sin (w2(a + b))   99.1%\n    cos (w9c)        49.3 cos (w9a) cos (w9b) −48.0 sin (w9a) sin (w9b) ≈48.6 cos (w9(a + b))    96.4%\n     sin (w9c)        48.6 cos (w9a) sin (w9b) + 48.5 sin (w9a) cos (w9b) ≈48.5 sin (w9(a + b))    96.7%\n    cos (w19c)     58.0 cos (w19a) cos (w19b) −58.3 sin (w19a) sin (w19b) ≈58.2 cos (w19(a + b))  95.4%\n    sin (w19c)     59.3 cos (w19a) sin (w19b) + 59.4 sin (w19a) cos (w19b) ≈59.4 sin (w19(a + b))   93.9%\n    cos (w31c)     94.4 cos (w31a) cos (w31b) −96.4 sin (w31a) sin (w31b) ≈95.4 cos (w31(a + b))  98.4%\n    sin (w31c)     97.2 cos (w31a) sin (w31b) + 97.1 sin (w31a) cos (w31b) ≈97.2 sin (w31(a + b))   98.7%\n\n                                                      (a) Seed 1\n WL Component                   Fourier components of uTk MLP(a, b) or vTk MLP(a, b)             FVE\n    cos (w40c)       97.0 cos (w40a) cos (w40b) −99.4 sin (w40a) sin (w40b) ≈98.2 cos (w40(a + b))    97.3%\n    sin (w40c)       81.3 cos (w40a) sin (w40b) + 81.3 sin (w40a) cos (w40b) ≈81.3 sin (w40(a + b))    92.7%\n    cos (w44c)     309.1 cos (w44a) cos (w44b) −338.7 sin (w44a) sin (w44b) ≈323.9 cos (w44(a + b))  98.5%\n    sin (w44c)     327.3 cos (w44a) sin (w44b) + 327.2 sin (w44a) cos (w44b) ≈327.3 sin (w44(a + b))   98.9%\n    cos (w53c)     192.1 cos (w53a) cos (w53b) −192.2 sin (w53a) sin (w53b) ≈192.1 cos (w53(a + b))  97.3%\n    sin (w53c)     166.7 cos (w53a) sin (w53b) + 166.8 sin (w53a) cos (w53b) ≈166.8 sin (w53(a + b))   95.7%\n\n                                                    (b) Seed 2\n WL Component                   Fourier components of uTk MLP(a, b) or vTk MLP(a, b)             FVE\n    cos (w31c)     156.1 cos (w31a) cos (w31b) −156.5 sin (w31a) sin (w31b) ≈156.3 cos (w31(a + b))  99.3%\n    sin (w31c)     150.7 cos (w31a) sin (w31b) + 150.7 sin (w31a) cos (w31b) ≈150.7 sin (w31(a + b))   98.9%\n    cos (w45c)       72.5 cos (w45a) cos (w45b) −76.8 sin (w45a) sin (w45b) ≈74.6 cos (w45(a + b))    95.9%\n    sin (w45c)       74.7 cos (w45a) sin (w45b) + 74.6 sin (w45a) cos (w45b) ≈74.6 sin (w45(a + b))    96.6%\n    cos (w49c)       45.9 cos (w49a) cos (w49b) −45.5 sin (w49a) sin (w49b) ≈45.7 cos (w49(a + b))    97.0%\n    sin (w49c)       45.8 cos (w49a) sin (w49b) + 45.8 sin (w49a) cos (w49b) ≈45.8 sin (w49(a + b))    96.9%\n    cos (w52c)       71.6 cos (w52a) cos (w52b) −72.1 sin (w52a) sin (w52b) ≈71.9 cos (w52(a + b))    98.5%\n    sin (w52c)       68.7 cos (w52a) sin (w52b) + 68.7 sin (w52a) cos (w52b) ≈68.7 sin (w52(a + b))    97.9%\n\n                                                      (c) Seed 3\n WL Component                   Fourier components of uTk MLP(a, b) or vTk MLP(a, b)             FVE\n    cos (w17c)       66.0 cos (w17a) cos (w17b) −63.5 sin (w17a) sin (w17b) ≈64.8 cos (w17(a + b))    96.4%\n    sin (w17c)       66.4 cos (w17a) sin (w17b) + 66.4 sin (w17a) cos (w17b) ≈66.4 sin (w17(a + b))    94.9%\n    cos (w32c)       68.7 cos (w32a) cos (w32b) −68.4 sin (w32a) sin (w32b) ≈68.5 cos (w32(a + b))    96.2%\n    sin (w32c)       68.0 cos (w32a) sin (w32b) + 68.0 sin (w32a) cos (w32b) ≈68.0 sin (w32(a + b))    96.3%\n    cos (w42c)      100.4 cos (w42a) cos (w42b) −96.0 sin (w42a) sin (w42b) ≈98.2 cos (w42(a + b))   97.9%\n    sin (w42c)     100.2 cos (w42a) sin (w42b) + 100.1 sin (w42a) cos (w42b) ≈100.1 sin (w42(a + b))   98.6%\n    cos (w51c)     118.0 cos (w51a) cos (w51b) −116.2 sin (w51a) sin (w51b) ≈117.1 cos (w51(a + b))  99.0%\n    sin (w51c)     114.3 cos (w51a) sin (w51b) + 114.2 sin (w51a) cos (w51b) ≈114.2 sin (w51(a + b))   98.5%\n\n                                                    (d) Seed 4\n\nTable 3: For each of the directions in the neuron-logit map WL of the final models from 4 other ran-\ndom seeds (Appendix C.2.1), we project the MLP activations in that direction then perform a Fourier\ntransform. For brevity, we omit terms with coefficients less than 15% of the largest coefficient. We\nthen compute the fraction of variance explained (FVE) if we replace the projection with a multiple\nof a single term of the form cos (wk(a + b)) or sin (wk(a + b)), and find that this is consistently\nclose to 1.\n\n\n\n\n\n                                       20\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n             Neuron-Logit Map (Seed 1)                       Neuron-Logit Map (Seed 2)\n\n                                                                                  cos                                                                                   cos\n          2.5                                                                                 sin          3                                                                                 sin\n                                                                                                     2.5          Component   2                                                                                                                                                                                                        Component\n                                                                                      2\n          1.5\n                                                                                                     1.5        Fourier   1                                                                                                                                                            Fourier   1\n  of  0.5                                            of  0.5\n\n          0                                                                           0    Norm    0         10        20        30        40        50                      Norm    0         10        20        30        40        50\n                         Frequency k                                               Frequency k\n\n\n             Neuron-Logit Map (Seed 3)                       Neuron-Logit Map (Seed 4)\n          3\n                                                                                  cos          2.5                                                                       cos\n          2.5                                                                                 sin                                                                                               sin\n                                                                                      2          Component   2                                                                                                                                                                                                        Component\n                                                                                                     1.5\n          1.5\n\n                                                                                      1        Fourier   1                                                                                                                                                            Fourier\n  of                                                   of  0.5          0.5\n\n          0                                                                           0    Norm    0         10        20        30        40        50                      Norm    0         10        20        30        40        50\n                         Frequency k                                               Frequency k\n\n\nFigure 17: The norms of the direction corresponding to sine and cosine waves in the neuron-logit\nmap weights WL. As with the mainline model discussed in the main body and discussed in Appendix\nC.2.1, WL is consistently sparse, providing is evidence that all four are operating in a Fourier basis.\n\n      Seed    Test Loss    Loss (Key frequencies removed)   Loss (All other frequencies removed)\n       1    2.07 · 10−7              6.5 · 100                          5.7 · 10−8\n       2     2.1 · 10−7               1.1 · 101                          6.2 · 10−8\n       3    2.05 · 10−7              6.7 · 100                          5.5 · 10−8\n       4    2.33 · 10−7              6.8 · 100                          6.0 · 10−8\n\nTable 4: As discussed in Appendix C.2.1, ablating the key frequencies for each of the networks re-\nduces performance to worse than chance, while ablating all other frequencies improves performance.\n\n\nConfirming that the other seeds use the Fourier Multiplication Algorithm. In Figure 16, we\nshow the norms of the Fourier components of the embedding matrix WE for each of the 4 other\nrandom seeds. As with the mainline model, the matrices are sparse in the Fourier basis. In Figure\n17, we show the norms of the Fourier components of the neuron-logit map WL for the 4 other random\nseeds. The matrices are sparse in the Fourier basis, enabling us to identify 3 or 4 key frequencies for\neach of the seeds. Again, note that the specific frequencies differ by seed.\n\nUsing the key frequencies identified in the neuron-logit map, we repeat the experiment in Sec-\ntion 4.2, where we “read off” the MLP activations in the 6 or 8 directions corresponding to the\nkey frequencies. As with our mainline model, this lets us identify the trigonometric identities for\ncos (wk(a + b)) and sin (wk(a + b)) being computed at the MLP layer. We confirm that the trigono-\nmetric identities are a good approximation by approximating the activations with a single term of\nthe form cos (wk(a + b)) or sin (wk(a + b))—as with the mainline model, the fraction of variance\nexplained is consistently close to 100%.\n\nNext, we ablate the key frequencies from the logits as in Section 4.4 and report the results in Table\n4. As with the mainline model, ablating all of the key frequencies reduces performance to worse\nthan chance, while ablating everything but the key frequencies improves test performance.\n\nProgress measures and grokking. Finally, we confirm the progress measure and grokking results\nfrom the mainline model on other runs with the same architecture. In Figure 18, we display the\ntrain, test, and restricted loss for each of the four other random seeds. In Figure 19, we display\nthe Gini coefficients of the Fourier components of the embedding matrix WE and the neuron-logit\nmap WL for each of the four other random seeds. The shape of the curves are very similar to those\nof the mainline model, allowing us to divide grokking on these models into the same three phases\n\n\n                                       21\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                  Restricted Loss for Seed 1                                Restricted Loss for Seed 2\n\n                                                          Train loss                                                                          Train loss\n         1                                               Test loss                        1                                               Test loss\n                                                             Restricted loss                                                                         Restricted loss\n        0.01                                                                             0.01\n   Loss 100μ                                                                                 Loss 100μ\n\n        1μ                                                                  1μ\n\n       10n\n          0         5k        10k        15k        20k        25k               0         5k        10k        15k        20k        25k\n                        Epoch                                             Epoch\n\n\n\n                  Restricted Loss for Seed 3                                Restricted Loss for Seed 4\n\n                                                          Train loss                                                                          Train loss\n         1                                               Test loss                        1                                               Test loss\n                                                             Restricted loss                                                                         Restricted loss\n        0.01                                                                             0.01\n   Loss 100μ                                                                                 Loss\n                                                                            100μ\n\n        1μ\n                                                                            1μ\n\n       10n\n          0         5k        10k        15k        20k        25k               0       5k      10k      15k      20k      25k      30k\n                        Epoch                                             Epoch\n\n\nFigure 18: The train, test, and restricted loss for each of the four other random seeds described in\nAppendix C.2.1. The lines delineate the 3 phases of training: memorization, circuit formation, and\ncleanup (and a final stable phase). As with the mainline model, restricted loss consistently declines\nprior to train loss. Note that while the shapes of the loss curves are similar to each other and those of\nthe mainline model, the exact time that grokking occurs (and thus the dividers between the phases\nof grokking) differ by random seed. Interestingly, memorization is complete by around 1400 steps\nfor all five runs.\n\n\n\nidentified in the main text. Interestingly, while all of the models complete memorization by around\n1400 epochs, circuit formation and cleanup occur at different times.\n\n\nC.2.2  RESULTS FOR OTHER EXPERIMENTAL SETUPS\n\nIn this section, we provide further evidence that small transformers grok on the modular addition\ntask, by varying the size of the network, the amount of training data, and the size of the prime P.\n\n\n1-Layer Transformers with Varying Fractions of Training Data.  We find that grokking occurs\nfor the modular addition task with P = 113 for many data fractions (that is, the fraction of the\n113 · 113 pairs of inputs that the model sees during training), as shown in Figure 20.  Smaller\namount lead to slower grokking, but sufficiently large fractions of data (≥60%) lead to immediate\ngeneralization, as shown in Figures 20 and 21.\n\nAs with the results in Appendix C.2.1, all of the 1-layer transformers in this section also converge\nto using the Fourier multiplication algorithm.\n\n\n2-Layer Transformers.  As shown in Figure 22, 2-layer transformers also exhibit some degree\nof grokking. However, this is complicated by the slingshot mechanism (Thilak et al., 2022). We\ndisplay the excluded loss of a 2-layer transformer in Figure 23 and find it shows a similar pattern to\nthe mainline 1-layer transformer, in that it improves relatively smoothly before grokking occurs.\n\n\nSmaller and larger primes.  We also examined smaller and larger prime moduli. For P = 53\n(Figure 24), we explored a variety of weight decays to observe grokking in the small prime case.\nWith the original weight decay setting of λ = 1, we found that the models never generalized.\nHowever, increasing the weight decay to λ = 5 does allow the model to grok. We speculate that\n\n\n                                       22\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                 Gini Coefficients, Seed 1                              Gini Coefficients, Seed 2\n\n         0.8                                                                                  0.8\n\n         0.6                                                                                  0.6\n            coefficient  0.4                                                                                                                                                                                                                                   coefficient  0.4\n    Gini  0.2                                                                                  Gini  0.2\n\n         0                                                                      0\n          0         5k        10k        15k        20k        25k               0         5k        10k        15k        20k        25k\n                        Epoch                                             Epoch\n\n\n\n                 Gini Coefficients, Seed 3                              Gini Coefficients, Seed 4\n\n         0.8                                                                                  0.8\n\n         0.6                                                                                  0.6\n            coefficient  0.4                                                                                                                                                                                                                                   coefficient  0.4\n    Gini  0.2                                                                                  Gini  0.2\n\n         0                                                                      0\n          0         5k        10k        15k        20k        25k               0         5k        10k        15k        20k        25k\n                        Epoch                                             Epoch\n\n\nFigure 19: The Gini coefficients (a measure of sparsity) of the Fourier components of the embedding\nmatrix WE and the neuron-logit map WL for each of the four other random seeds. The lines delineate\nthe 3 phases of training: memorization, circuit formation, and cleanup (and a final stable phase). As\nwith the mainline model, sparsity increases slowly during memorization and circuit formation, and\nthen quickly during cleanup.\n\n\n          Data Fraction 0.1                  Data Fraction 0.2                  Data Fraction 0.3\n        100\n\n          1                                       Train loss                    1                                       Train loss                    1                                       Train loss\n                                                      Test loss                                                                   Test loss                                                                   Test loss\n    Loss         0.01                                                                      Loss                                                                                0.01                                                                      Loss 100μ0.01       100μ                                                                    100μ\n\n         1μ                                                          1μ                                                          1μ\n\n           0          5k         10k        15k        20k                0          5k         10k        15k        20k                0          5k         10k        15k        20k\n                     Epoch                                       Epoch                                       Epoch\n\n\n          Data Fraction 0.4                  Data Fraction 0.5                  Data Fraction 0.6\n\n          1                                       Train loss                    1                                       Train loss                    1                                       Train loss\n                                                      Test loss                     0.01                                         Test loss                     0.01                                         Test loss         0.01\n    Loss 100μ                                                                      Loss 100μ                                                                      Loss 100μ\n\n         1μ                                                          1μ                                                          1μ\n\n           0      1000     2000     3000     4000                         0      1000     2000     3000     4000                         0      1000     2000     3000     4000\n                     Epoch                                       Epoch                                       Epoch\n\n\n          Data Fraction 0.7                  Data Fraction 0.8                  Data Fraction 0.9\n\n          1                                       Train loss                    1                                       Train loss                    1                                       Train loss\n         0.01                                         Test loss                     0.01                                         Test loss                     0.01                                         Test loss\n    Loss 100μ                                                                      Loss 100μ                                                                      Loss 100μ\n\n         1μ                                                          1μ                                                          1μ\n\n           0      1000     2000     3000     4000                         0      1000     2000     3000     4000                         0      1000     2000     3000     4000\n                     Epoch                                       Epoch                                       Epoch\n\n\nFigure 20: Training and test losses for a 1-layer transformer on the modular addition task with\nP = 113, with varying fractions of the 113 · 113 pairs of possible inputs used in training. Grokking\noccurs when between 30 −50% of the dataset is used during training and lower fractions of data\nlead to slower grokking. Using ≥60% data leads to immediate generalization, while using 10% or\n20% of the data doesn’t lead to grokking even after 40k epochs. Note the different x-axes: we only\nshow 5k epochs for the runs with data fraction ≥40% for more detail.\n\n\n                                       23\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                 Training Epochs until <1e-06 Loss\n\n                      18k\n\n                                                                                           Train loss                      16k\n                                                                                              Test loss\n                      14k\n                            steps 12k\n           of                      10k\n\n                       8k                                  Number  6k\n\n                       4k\n\n                       2k\n\n\n                             0.3          0.4          0.5          0.6          0.7          0.8          0.9\n                                       Fraction of train data\n\nFigure 21: Number of steps for train/test loss to be < 10−6, as a function of the amount of training\ndata. While train loss immediately converges to below 10−6 for all data fractions, generalization\ntakes significantly longer with lower fractions of data. Note that the plots for other thresholds are\nalso qualitatively similar.\n\n\n\n\n\nFigure 22: Training and test loss for a 2-layer version of the original architecture. Average across 5\nrandom seeds is in bold.\n\n\n\n\n\n                                       24\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nFigure 23: Training, test, and full excluded loss for a 2-layer version of the original architecture.\nOne random seed chosen for readability.\n\n\n\n\n\nFigure 24: The training and test losses for P = 53 and all other hyperparameters except weight\ndecay (γ = 5) the same as the main training run discussed in the paper. The averages are bold, and\nall contributing runs are partially transparent. Note that grokking occurs.\n\n\n\n\n\nFigure 25: The training and test losses for P = 401 and all other hyperparameters the same as the\nmain training run discussed in the paper. Grokking doesn’t occur (the model generalizes immedi-\nately), even across a variety of weight decays.\n\n\n                                       25\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\nthis is because the memorization solution is significantly smaller (since there are only 53 · 53 total\npairs), thereby requiring more aggressive weight decay for the generalizing solution to be favored.\n\nFor P = 109, we saw exactly the same behavior as with the mainline model.\n\nFor P = 401 (Figure 25), we could not get grokking, even by varying the weight decay parame-\nter λ ∈{0.3, 0.5, 1, 3, 5, 8}. Instead, the model immediately learns the generalizing solution. We\nbelieve this is because the amount of data seen by the model is greatly increased compared to the\nP = 113 case (from 30% of 113 · 113 pairs to 30% of 401 · 401 pairs), thereby favoring the gen-\neralizing solution from the start. We then trained 3 models each using 5%, 10%, 20% of the pairs\nof training data with λ = 1, and found that the models trained on 5% and 10% of the data imme-\ndiately overfit and never generalized, while the models trained on 20% of the data also generalized\nimmediately.\n\n\nC.2.3  GENERALIZING MODELS CONSISTENTLY USE THE FOURIER MULTIPLICATION\n      ALGORITHM\n\nFor each of the models in Appendix C.2.2 that achieve low test loss, we repeated the analysis per-\nformed in the mainline model, and summarize the results in Table 5. We list their key frequencies,\nGini coefficients, and relevant FVEs. We find that every model trained with weight decay and that\ngeneralizes correctly implements some variation of the Fourier multiplication algorithm.\n\nInterestingly, the embedding and unembedding matrices of the models trained with dropout are not\nsparse in the Fourier basis, and the logits for the p = 0.2 models are not as well explained by a sum\nof cosines as the other models (likely because the p = 0.2 models are simply worse at the task). We\nspeculate that this is likely due to a combination of insufficient training epochs (as dropout models\nseem to take much longer to grok) and the inherent need for redundancy for networks trained via\ndropout.\n\nAs with the mainline model, we ignore the final skip connection (around the final MLP), as all of the\ngeneralizing models studied do not suffer significant performance penalties if the skip connection is\nzero or mean ablated (Table 6).\n\n\nD  ADDITIONAL RESULTS ON GROKKING\n\n\nD.1  BOTH REGULARIZATION AND LIMITED DATA ARE NECESSARY FOR GROKKING\n\nAs discussed in Section 7 and Appendix C.2, the weight decay and the amount of data seem to have\na strong effect on whether grokking occurs. To confirm this, we experiment with removing weight\ndecay and varying the amount of data on 1-layer transformers. In Figure 26, we give the training,\ntest, and full excluded loss for a typical training run with λ = 0 (no weight decay). As the figure\nshows, no grokking occurs, and excluded loss does not increase, suggesting that the model does not\nform the circuit for generalizing algorithm at all.\n\nIn Figure 20, we show the test loss curves for models trained with weight decay λ = 1 and on\nvarious fractions of the data. Though all the train losses are approximately the same—that is, they\nmemorize at the same rate, models trained on smaller fractions of data take longer to grok.\n\nIn Figure 27, we display the test and train loss of models trained with λ = 0.3 and λ = 3.0. Smaller\namounts of weight decay lead to slower grokking, while larger amounts of weight decay lead to faster\ngrokking—on average, it takes around 3k epochs for models to grok with weight decay λ = 0.3,\n5-10k epochs for the models to grok with weight decay λ = 1.0, and 20k epochs for the models to\ngrok with weight decay λ = 3.0.\n\nFinally, we test whether other forms of regularization can also induce grokking. We replaced weight\ndecay with the following types of regularization while keeping all other hyperpameters the same:\n\n      1. Dropout We add dropout Srivastava et  al. (2014) to the MLP neurons, with p ∈\n         {0.2, 0.5, 0.8}. That is, for each individual neuron, we set it to 0 with probability p during\n          training, and also multiply the outputs of the other neurons by 1−p.1\n\n\n                                       26\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n       Model            Test Loss    Gini(WE)   Gini(WL)        Key Frequencies         Logit FVE   MLP FVE\n  40% Training Data    1.98 · 10−7       0.76          0.79               [17, 43, 49, 55]           94.9%     83.3% [26.1%]\n  50% Training Data    1.68 · 10−7       0.75          0.77               [2, 17, 31, 41, 44]          91.2%     85.2% [28.2%]\n  60% Training Data    1.23 · 10−7       0.79          0.84                 [2, 23, 34, 51]           96.4%      95.7% [1.4%]\n  70% Training Data    9.85 · 10−8       0.80          0.91                 [14, 15, 26]            99.0%      98.9% [0.4%]\n  80% Training Data    5.83 · 10−7       0.62          0.80                   [38, 41]              63.9%      94.1% [2.5%]\n  90% Training Data    1.11 · 10−7       0.79          0.88                 [3, 26, 34, 43]           98.6%      98.7% [0.3%]\n  2 Layer Transformer   9.54 · 10−7       0.59          0.80                 [14, 18, 29]            91.8%      95.2% [1.9%]\n  2 Layer Transformer   4.41 · 10−5       0.55          0.73                 [7, 12, 35, 49]           86.1%      86.2% [6.4%]\n  2 Layer Transformer   6.50 · 10−2       0.66          0.80                    [4, 9, 28]             88.5%      85.4% [5.9%]\n  2 Layer Transformer   4.18 · 10−2       0.56          0.76                  [4, 5, 15, 54]            91.4%     81.2% [17.8%]\n  2 Layer Transformer   1.75 · 10−2       0.68          0.71                [3, 4, 13, 30, 38]           84.0%     71.9% [19.5%]\n    P = 53        3.00 · 10−4       0.61          0.68                  [6, 9, 16, 21]            91.2%      90.2% [5.8%]\n    P = 53        1.03 · 10−4       0.56          0.72                   [4, 13, 16]             94.8%      93.1% [6.4%]\n    P = 53        1.21 · 10−5       0.66          0.79                 [13, 22, 23]            98.2%      97.6% [0.9%]\n    P = 53        3.95 · 10−6       0.66          0.74                   [3, 14, 15]             88.5%      91.8% [4.6%]\n    P = 53        5.56 · 10−6       0.67          0.80                 [10, 14, 22]            98.1%      98.3% [0.6%]\n    P = 109       2.02 · 10−7       0.76          0.83                  [6, 7, 22, 25]            98.0%      97.3% [1.9%]\n    P = 109       2.95 · 10−7       0.69          0.82               [8, 14, 29, 32, 41]          95.2%      94.7% [2.3%]\n    P = 109       1.66 · 10−7       0.78          0.86               [13, 23, 39, 45]           98.5%      97.6% [0.9%]\n    P = 109       2.50 · 10−7       0.68          0.82                 [8, 13, 32, 41]           96.8%      95.5% [2.3%]\n    P = 109       2.77 · 10−7       0.76          0.85               [29, 37, 38, 49]           97.9%      98.1% [0.8%]\n   Dropout p = 0.2    2.65 · 10−1       0.19          0.46        [1, 4, 7, 17, 22, 33, 40, 49, 55]     71.3%     65.0% [17.5%]\n   Dropout p = 0.2    4.52 · 10−1       0.19          0.46          [3, 8, 19, 28, 32, 34, 40, 44]      73.3%     71.4% [10.7%]\n   Dropout p = 0.2    2.03 · 10−1       0.20          0.45          [4, 5, 32, 38, 41, 44, 49, 50]      74.2%     71.1% [10.6%]\n   Dropout p = 0.5   < 10−8         0.26          0.56              [1, 4, 26, 46, 47, 55]         89.4%      88.9% [3.5%]\n   Dropout p = 0.5    2.01 · 10−2       0.20          0.49             [16, 21, 35, 47, 53]          88.4%      88.4% [3.0%]\n   Dropout p = 0.5   < 10−8         0.25          0.54             [1, 4, 7, 19, 29, 31, 42]        86.1%      85.6% [4.0%]\n\nTable 5: For each of the models in Appendices C.2.3 and D.1 that generalizes to test data, we\nreport the test loss, the Gini coefficients of the norms of the Fourier components of WE and WL\n(Section 5.1), the key frequencies of the network, and the fraction of variance in logits explained by\na weighted sum of cos (wk(a + b −c))s over the key frequencies (Section 4.2).\n\nIn addition, we find the components uk, vk of WL  that correspond to cosines and sines of\nthe key frequencies, and then report the average fraction of variance of uTk MLP(a, b) and\nvTk MLP(a, b) explained by a single term of form cos (wk(a + b)) or sin (wk(a + b)) respectively\n(Section 4.2). Numbers in square brackets represent the standard deviation. For 2 Layer models, we\nuse the final layer MLP activations for MLP(a, b).\n\nWe omit test accuracy because every model on this list except for the dropout p = 0.2 mod-\nels achieves > 99.95% test accuracy, while the dropout p = 0.2 models achieve around 99.6% test\naccuracy.\n\n\n\n\n\n     Model Type               Loss           Accuracy        Ablated Loss        Ablated Acuracy\n Varying Data Fraction   1.83 · 10−7 (1.65 · 10−7)    100%     7.74 · 10−7 (6.74 · 10−7)      100%\n  2 Layer Transformer    1.97 · 10−2 (2.41 · 10−2    99.6%     4.63 · 10−2 (6.72 · 10−2       98.7%\n     P = 53         5.96 · 10−5 (8.91 · 10−5)    100%      1.5 · 10−4 (2.70 · 10−4)       100%\n     P = 109         1.94 · 10−7 (3.74 · 10−8)    100%     6.53 · 10−7 (1.41 · 10−7)      100%\n    Dropout p = 0.2           0.215 (0.091)         99.7%          0.205 (0.075)            99.7%\n    Dropout p = 0.5     4.68 · 10−3 (8.11 · 10−3)    100%      3.6 · 10−3 (5.82 · 10−3)       100%\n\nTable 6: We confirm that the skip connection around the final MLP layer is not important for perfor-\nmance by mean ablating the skip connection and computing loss and accuracy over the entire dataset\nfor each problem, averaged over all runs. (We report the standard deviation of loss over the runs in\nparentheses.) While loss does increase a small amount, accuracy remains consistently high and the\nloss of the ablated model remains low. Results with zero ablations are also similar.\n\n\n\n\n\n                                       27\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nFigure 26: Training, test, and full excluded loss for a 1-layer version of the original architecture\nwithout weight decay. One random seed chosen for readability. Note that not having weight decay\nprevents grokking.\n\n\n\n\n\n                   Weight Decay 0.3                                   Weight Decay 3.0\n\n                                                                               10                                       Average Train Loss\n         1                                                                      1                                       Average Test Loss\n\n                                                                                               0.1\n   Loss 0.01                                                                                 Loss 0.01\n                                                                                     0.001\n      100μ  Log                                                                  Log\n                                                                            100μ\n\n        1μ                                                                  10μ\n\n                                                                            1μ\n       10n\n          0         5k        10k        15k        20k        25k               0         5k        10k        15k        20k        25k\n                        Epoch                                             Epoch\n\n                                                                                                                          Loading [MathJax]/extensions/MathMenu.js\n\nFigure 27: The train and test loss over the course of training with weight decay λ = 0.3 (left) and\nλ = 3.0 (right). Less aggressive weight decay leads to slower grokking.\n\n\n\n\n\n                                       28\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                    Dropout p=0.2                                L1 penalty, 1.0\n       100\n                            Average Train Loss     Average Test Loss                 100\n        10\n                                                                                1\n         1\n                                                                                         0.01\n\n                                                                                                                           Average Test Loss   Loss 0.010.1                                                                                 Loss 100μ                                     Average Train Loss\n  Log 0.001                                                             Log   1μ\n\n      100μ                                                                    10n\n       10μ\n                                                                             100p\n        1μ\n          0            10k           20k           30k          40k               0           10k          20k          30k          40k\n                        Epoch                                             Epoch\n\n\n\n                    Dropout p=0.5                                L1 penalty, 10\n       100\n                            Average Train Loss     Average Test Loss                 100\n        10\n                                                                                1\n         1\n                                                                                         0.01\n\n                                                                                                                           Average Test Loss   Loss 0.010.1                                                                                 Loss 100μ                                     Average Train Loss\n  Log 0.001                                                             Log   1μ\n\n      100μ                                                                    10n\n       10μ\n                                                                             100p\n        1μ\n          0            10k           20k           30k          40k               0           10k          20k          30k          40k\n                        Epoch                                             Epoch\n\n\n\n                    Dropout p=0.8                                L1 penalty, 100\n       100\n                            Average Train Loss     Average Test Loss                 100\n        10\n                                                                                1\n         1\n                                                                                         0.01\n\n                                                                                                                           Average Test Loss   Loss 0.010.1                                                                                 Loss 100μ                                     Average Train Loss\n  Log 0.001                                                             Log   1μ\n\n      100μ                                                                    10n\n       10μ\n                                                                             100p\n        1μ\n          0            10k           20k           30k          40k               0           10k          20k          30k          40k\n                        Epoch                                             Epoch\n\n\nFigure 28: The train and test loss over the course of training with two types of regularization, dropout\nand ℓ1 regularization. Grokking occurs with some runs for dropout but never for ℓ1 regularization.\n\n\n      2. ℓ1 Regularization We add an ℓ1 penalty to the loss term. We use λ ∈{1, 10, 100}. Note\n          that we do not decouple the updates with respect to the ℓ1 penalty from optimization steps\n       done with respect to the log loss (as is done for ℓ2 regularization via AdamW Loshchilov\n     & Hutter (2017)).\n\nIn each case, we ran three random seeds. We show the results in Figure 28. While grokking did\nnot occur with ℓ1 regularization, we found that it does occur for all three seeds using dropout with\np = 0.2 or p = 0.5. We speculate that this is because both dropout and weight decay encourage\nthe network to spread out computation (which is required for the Fourier multiplication algorithm),\nwhile ℓ1 regularization encourages the network to become more sparse in the neuron basis and thus\nless sparse in the Fourier basis, preventing the network from learning the Fourier Multiplication\nAlgorithm.\n\n\nD.2  THE SLINGSHOT MECHANISM OFTEN OCCURS, BUT IS UNNECESSARY FOR GROKKING\n\nAs noted in Section C.2, our 2-layer transformers exhibit significant slingshots (Thilak et al., 2022)\nduring training. We speculate that this is due to how gradients of different scale interact with adaptive\n\n\n                                       29\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n                         Train/Test Loss for 5 Digit Addition w/ Infinite Data\n\n                                 2.5\n\n\n                             2\n\n                              Loss  1.5\n                             1\n\n\n                                 0.5\n\n\n                             0\n                              0         500       1000       1500       2000       2500\n                                                 Steps\n\n\n\n                  Per Token Train/Test Loss for 5 Digit Addition w/ Infinite Data\n\n                                 2.5                                                   Token 0 loss\n                                                                                 Token 1 loss\n                             2\n                                                                                 Token 2 loss\n                              Loss  1.5                                                   TokenToken 34 lossloss\n                             1                                                   Token 5 loss\n\n                                 0.5\n\n                             0\n                              0         500       1000       1500       2000       2500\n                                                 Steps\n\n\nFigure 29: (Top) The training/test loss for 5 Digit Addition trained on randomly generated data. Note\nthat training and test loss coincide, as the model does not see repeated pairs.(Bottom) The train/test\nloss per token for 5 Digit Addition, trained with randomly generated data at each step. Note that\nphase changes in the average loss correspond to phase changes in individual tokens, though one\nphase change (token 1, around step 270) is not visible on the averaged loss as it overlaps with the\nend of the first phase change (token 0, starting around step 150).\n\n\n\noptimizers. We were even able to induce slingshots on a 1-layer by reducing the precision of the loss\ncalculations (as this causes many gradients to round to 0 and thus greatly increases the differences\nin scale of gradients).\n\nHowever, as many of our 1-layer models do not exhibit slingshots but nonetheless grok, the slingshot\nmechanism is unnecessary for grokking to occur, in the presence of weight decay or other regular-\nization. We speculate that the slingshots of Thilak et al. (2022) (which co-occur with grokking for\ntraining runs without weight decay) serve as an implicit regularization mechanism that favors the\nsimpler, generalizing solution over the more complicated\n\n\nD.3  ADDITIONAL EVIDENCE FROM OTHER ALGORITHMIC TASKS\n\nWe now provide addition analysis of grokking phenomena on 3 additional algorithmic tasks and\nconfirm that limited data is an important part of grokking:\n\n      1. 5 digit addition. We sample pairs of random 5 digit numbers and have the model predict\n          their sum\n      2. Predicting repeated subsequences. We take a uniform random sequence of tokens, ran-\n       domly choose a subsequence to repeat, and train the model to predict the repeated tokens.\n      3. Skip trigram. We feed in a sequence of tokens from 0 to 19, of which exactly one is\n         greater than or equal to 10, and the model needs to output the token that is ≥10. This can\n        be solved with learning 10 skip trigrams.\n\n\n                                       30\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                          Train/Test Loss for 5 Digit Addition with 700 Data Points\n\n                                                                                       Train Loss     Test Loss\n                                  8\n\n\n                                  6\n                                     Loss 4\n\n\n                                  2\n\n\n                                  0\n                                  0                5k               10k               15k\n                                              Epochs\n\n\nFigure 30: The train and test loss for 5 Digit Addition trained on 700 data points. Unlike the infinite,\nrandomly generated data case, this shows both a sharp phase change and clear train test divergence.\n\n\nWe use a 1-layer full transformer for 5-digit addition, a 2-layer attention only transformer for pre-\ndicting repeated subsequences, and a 1-layer attention only transformer for the skip trigram task.\nOtherwise, we use the same hyperparameters as in the mainline model.\n\n5 Digit Addition  We first consider the case where we train on the approximately infinite data\nregime. For each minibatch, we randomly new sample 5 digit numbers. We report the results in\nFigure 29. Train loss coincides with test loss, so grokking does not occur, as the model almost never\nsees the same pair of 5 digit numbers twice, with 1010 such pairs. Interestingly, the various small\nbumps in Figure 29 correspond to the model learning how to calculate each of the 6 tokens in the\noutput. However, grokking does occur when we restrict the model to only see 700 data points, as\nshown in Figure 30.\n\nRepeated subsequence  As with the 5-digit addition task, we find that restricting the amount of\ndata is necessary and sufficient for grokking on the repeated subsequence task. In Figure 31, the\nmodel sees new data at every step exhibits no grokking. In contrast, clear grokking occurs when we\nrestrict the model to only see 512 data points in Figure 32.\n\nSkip trigram  As with the previous tasks, we find that restricting the amount of data is necessary\nand sufficient for grokking on the skip trigram task. The model that sees new data at every step\nexhibits no grokking in Figure 33. Meanwhile, the model restricted to only see 512 data points\nexhibits clear grokking in Figure 34.\n\nTaken together, these results echo the importance of limited data for grokking.\n\nE  FURTHER SPECULATIONS ON GROKKING\n\nE.1  AN INTUITIVE EXPLANATION OF GROKKING\n\nIn this section, we speculate on what might be happening “under the hood” when a model groks and\nexplore why this phenomena happens. The evidence is only suggestive, so this a promising direction\nfor future research.\n\nGrokking occurs when models, trained on algorithmic tasks with certain hyperparameters, initially\noverfit the training data where train loss significantly improves while test loss worsens and the two\ndiverge. But later in training, there is a sudden improvement in test loss, so test and train loss\nconverge. In contrast to Power et al. (2022) but in line with Liu et al. (2022), grokking does not\noccur when both train and test loss improve together without the initial divergence, as shown in\nmany of the figures in this paper, for example Figures 2 and 18.\n\nThe core issue is that the model has two possible solutions: memorization (with low train loss\nand high test loss) and a generalization (with low train loss and low test loss).  In our case, the\n\n\n                                       31\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                          Repeated Subsequence Prediction w/ Infinite data\n\n                                  5\n\n\n                                  4\n\n                                     Loss 3\n                                  2\n\n\n                                  1\n\n\n                                  0\n                                  0        1000       2000       3000       4000       5000\n                                                    Step\n\n\nFigure 31: The training/test loss for repeated subsequences trained on randomly generated data.\nNote that training and test loss coincide, as the model does not see repeated pairs. There sharp phase\nchange corresponds to the model forming induction heads. (Olsson et al., 2022)\n\n\n\n\n\n                         Repeated Subsequence Prediction w/ 512 Data Points\n                                 16\n\n                                 14\n\n                                 12\n\n                                 10\n\n                                  8                                    Loss\n                                  6\n\n                                  4\n                                  2         Train Loss     Test Loss\n\n                                  0\n                                  0            50k          100k          150k          200k\n                                            Epoch\n\n\nFigure 32: The train and test loss for the repeated subsequence task, trained on 512 data points.\nUnlike the infinite, randomly generated data case, this shows both a sharp phase change and clear\ntrain test divergence.\n\n\n\n\n\n                            Train/Test Loss for Skip Trigram Task w/ Infinite Data\n                                      2.5\n\n\n                                  2\n\n\n                                      1.5\n                                   Loss                                  1\n\n\n                                      0.5\n\n\n                                  0\n                                  0         500       1000       1500       2000       2500\n                                            Epoch\n\n\nFigure 33: The training/test loss for the skip trigram task, trained on randomly generated data. Note\nthat training and test loss coincide, as the model does not see repeated pairs. The sharp phase change\ncorresponds to the network learning all of the skip trigrams.\n\n\n\n                                       32\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n                           Train/Test Loss for Skip Trigram Task w/ 50 Data Points\n\n                                      2.5                                               Train Loss     Test Loss\n\n\n                                  2\n\n\n                                      1.5\n                                   Loss\n                                  1\n\n\n                                      0.5\n\n\n                                  0\n                                  0          1000         2000         3000         4000\n                                            Epoch\n\n\nFigure 34: The train and test loss for the skip trigram task, trained on 512 data points. Unlike the\ninfinite, randomly generated data case, this shows both a sharp phase change and clear train test\ndivergence.\n\n\n\nFourier Multiplication Algorithm is the generalization solution. Intuitively, with very little training\ndata, the model will overfit and memorize. With more training data, the model must generalize\nor suffer poor performance on both train and test loss. Since neural networks have an inductive\nbias favoring “simpler” solutions, memorization complexity scales with the size of the training set,\nwhereas generalization complexity is constant. The two must cross at some point! Yet, the surprising\naspect of grokking is the abrupt shift during training, when the model switches from memorization\nto generalization.\n\nThe other component of grokking is phase transitions - the phenomena where models trained on a\ncertain task develop a specific capability fairly rapidly during a brief period of training, as shown\nfor the case of induction heads forming in transformer language models in Olsson et al. (2022) and\nour results in Appendix D.3. That is, rather than slowly forming that capability over training, the\nmodel rapidly goes from being bad at it to being good at it. One interpretation of a phase transition\nis that there’s some feature of the loss landscape that makes the generalising solution harder to reach\n- rather than a smooth gradient for the model to follow, it instead initially finds it difficult to make\nprogress, but then crosses some threshold where it can rapidly make progress.\n\nTherefore, grokking occurs with phase transitions, limited data, and regularization. Models exhibit\nphase transitions despite having enough training data to avoid overfitting. Regularization (weight\ndecay in our case) favors simpler solutions over complex ones. The model has enough data to\nmarginally prefer generalization over memorization. The phase transition indicates that generaliza-\ntion is “hard to reach” while the model has no problems with memorization. But as it memorizes, the\nnetwork becomes more complex until the weight decay prevents further memorization then moves\ntowards equilibrium. The gradient to memorize balances the gradient towards smaller weights. With\ngeneralization, the model is incentivized to both memorize and simplify. Strikingly, it is capable of\nboth while maintaining a somewhat constant training performance in this circuit formation phase.\nNext, as the model approaches generalization, the memorization weights are removed in the cleanup\nphase. The cost from complexity outweighs the benefit from lower loss. Due to the phase transition\nduring this training period, as model’s progress towards generalization accelerates, the cleanup rate\nsharpens as well.\n\nA model that learns a perfect solution and is trained with weight decay has competing incentives:\nlarger weights (for more extreme logits and thus lower loss) and smaller weights (from weight\ndecay). So for any solution and any level of weight decay, there will always be a level of train loss\nwhere these two forces equilibrate. Thus, memorization is not necessarily a “simpler” solution than\ngeneralization. The key is that generalization will have smaller weights holding train loss fixed. In\nfact, weight decay should be expected to equilibrate at a slightly lower train loss in generalization,\nsince the base solution is simpler. This matches what we observe in practice. 4\n\n   4One subtlety: the grokking phenomena is often incorrectly summarized as “the model learned to generalize\neven after achieving zero loss.” Zero loss does not exist with cross-entropy loss. Although the model achieves\n\n\n                                       33\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\nE.2  HYPOTHESIS: PHASE TRANSITIONS ARE INHERENT TO COMPOSITION\n\nA promising line of work in the growing field of mechanistic interpretability suggests that models\nform circuits (Cammarata et al., 2020) – clean interpretable algorithms formed by subnetworks of\nthe model, such as curve detectors (Cammarata et al., 2020) in image classification networks and\ninduction heads (Elhage et al., 2021; Olsson et al., 2022) in LLMs. This is surprisingly true! A\ncircuit represents the model learning an algorithm, a fundamentally discrete thing; each step in the\nalgorithm only makes sense if the other steps are present. But neural networks are fundamentally\ncontinuous, trained to follow gradients towards lower loss and struggle to jump to new optima with-\nout following a smooth gradient. So how can a model learn a discrete algorithm?\n\nAs a concrete example, let’s consider the case of induction heads in LLMs. There is a subnetwork\nof a next-token prediction autoregressive language model that learns to continue repeated subse-\nquences.  It detects whether the current token occurred earlier in the context.  If so, it predicts the\nsame token after that previous occurrence will also come next. The circuit consists of a previous\ntoken head, which attends to each previous token and copies the context of the previous token to\nthe current token, and an induction head which attends to the token after a previous occurrence of\nthe current token. The induction head composes with the previous token head by forming a query\nvector representing the current token and a key vector representing the previous token head’s output\nusing K-Composition, the context of the previous token. It attends to a token where this query and\nkey match.\n\nThis circuit significantly improves loss but only in the context of the other heads present. Before\neither head is present, no gradient encourages the formation of either head. At initialization, we\nhave neither head, so gradient descent should never discover this circuit. Naively, we might predict\nthat neural networks will only produce circuits analogous to linear regression, where each weight\nwill marginally improve performance as it continuously trains. And yet in practice, neural networks\nindeed form such sophisticated circuits, involving several parts interacting in non-trivial, algorithmic\nways. So how can this be?\n\nA few possible explanations:\n\n        • Lottery tickets (Frankle & Carbin, 2018):  Initially, each layer of the network is the\n         superposition of many partial circuit components, and the output of each layer is the average\n         of the output of each component. The full output of the network is the average of many\n         different circuits, with significant interference from non-linear interaction. Some of these\n          circuits are systematically useful to reducing loss, but most aren’t. Gradients for useless\n          circuits will have zero mean, while gradients for useful circuits will have non-zero mean,\n        with a lot of noise. SGD reinforces relevant circuits and suppresses useless ones, so circuits\n         will gradually form.\n        • Random walk: The network wanders randomly around the loss landscape until it encoun-\n          ters a half-formed previous token head and induction head that somewhat compose. This\n        half-formed circuit becomes useful for reducing loss, so gradient descent completes the\n          circuit.\n        • Evolution: A similar mystery arises from how organisms develop sophisticated machinery,\n          like the human eye. Each part is only useful in the context of other parts. A compelling\n         explanation is a component first developed that was somewhat useful in its own right, like\n        a light-detecting membrane. It was reinforced as a useful component. Then, later compo-\n         nents developed depending on the first, like the lens of the eye.\n\nEvolution is a natural explanation, However, based on our toy tasks, it cannot be the whole story.\nIn the repeated subsequence task, we have a sequence of uniform randomly generated tokens, apart\nfrom a repeated subsequence at an arbitrary location, e.g. 7 2 8 3 1 9 3 8 3 1 9 9 2 5 END. This means\nall pairs of tokens are independent, apart from pairs of equal tokens in the repeated subsequence. In\nparticular, this means that a previous token head can never reduce loss for the current token. The\nprevious token will always be independent of the next token. So a previous token head is only useful\nin the context of an induction-like head that completes the circuit. Likewise, an induction head relies\n\nperfect accuracy, it is trained to optimize loss not accuracy. This means the model is always incentivized to\nfurther improve. In particular, the easiest way to improve performance with perfect accuracy is by scaling up\nthe logits. This lowers the temperature and pushes the softmax closer to an argmax.\n\n\n                                       34\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\non K-composition with a previous token head and so cannot be useful on its own. Yet the model\neventually forms an induction circuit!\n\nA priori, the random walk seems insufficient on its own. An induction circuit is relatively compli-\ncated, representing a small region in model space. So a random walk is unlikely to stumble upon\nit. Concretely, in our modular addition case, progress measures show significant hidden progress\npre-grokking, indicating the model did not stumble upon the solution by chance.\n\nThus, the lottery ticket hypothesis seems the most explanatory. An induction head is useless without\na previous token head but might be slightly useful when composing with a head that uniformly\nattends to prior tokens, since part of its output will include the previous token! Nevertheless, we\nsuspect that all explanations contribute to the entire picture. This seems most plausible if the uniform\nhead just so happens to attend a bit more to the previous token via a random walk.\n\nReturning to phase transitions, the lottery ticket-style explanation suggests that we might expect\nphase transitions as circuits form. Early in circuit formation, each part of the circuit is rough, so the\neffect on the loss of improving any individual component is weak, meaning gradients will be small.\nAs each component develops, other components will become more useful, meaning all gradients will\nincrease together non-linearly. As the circuit nears completion, we should expect an acceleration in\nthe loss curve for this circuit, resulting in a phase transition.\n\nF  FURTHER DISCUSSION ON USING MECHANISTIC INTERPRETABILITY AND\n   PROGRESS MEASURES FOR STUDYING EMERGENT PHENOMENA\n\nWhile we find approach of using mechanistic interpretability to define progress measures rela-\ntively promising, there remains significant uncertainty as to how scalable existing mechanistic inter-\npretability approaches really are. Broadly speaking, depending on the success of future mechanistic\ninterpretability work, we think there are three methods through which mechanistic interpretability\nand progress measures can help with understanding and predicting emergent phenomena:\n\n      1.  If mechanistic interpretability can be scaled to large models to the level where we can un-\n         derstand the mechanisms behind significant portions of their behavior, we could perform\n         the same style of analysis as was done in this work. We believe it’s currently unclear as to\n        whether or not mechanistic interpretability will successfully scale to large models to this\n         extent (or even if there exist human-understandable explanations for all of their sophisti-\n         cated behavior). That being said, in cases where mechanistic interpretability does recover\n        human-understandable mechanisms, we could simply use the parts of the mechanism as\n         progress measures.\n      2.  If future mechanistic interpretability can only recover parts of the mechanism of larger\n       models (as in Wang et al. (2022)) and can only generate comprehensive understanding\n         of the mechanisms of smaller models, we might still be able to use our understanding\n       from smaller models to guide the development measures that track parts of the behavior of\n         the larger model. We find this scenario relatively plausible, as existing mechanistic inter-\n          pretability work already allows us to recover fragments of large model behavior and un-\n         derstand these fragments by analogy to smaller models. For example, Olsson et al. (2022)\n        use this approach to understand the emergence of in-context learning in medium-sized lan-\n        guage transformers.\n      3. Even if mechanistic interpretability fails to recover understandable mechanisms at all on\n         large models, we might still be able to derive progress measures that don’t require human\n         understanding. For example, if we end up with automated mechanistic interpretability (that\n        nonetheless still fails to recover human-understandable mechanisms), we might be able to\n        use the outputs of those opaque processes.\n        Another approach is task-independent progress measures: if we can discover progress mea-\n         sures that don’t depend on the task, perhaps using many small, interpretable models as\n         testbeds, we might be able to apply these progress measures to large models.\n\nThat being said, we think the future work outlined in Section 6 is necessary to successfully apply\nour approach to predict and understand emergent behavior in existing large language models, and so\nremain cautiously optimistic.\n\n\n                                       35\n\n## Claim to Classify\n\"uses discrete Fourier transforms and trigonometric identities to convert addition to rotation\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Ablation in Fourier space and weight/activation analysis",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The claim uses functional/mechanistic verbs ('uses', 'convert') to describe a specific algorithm. While the paper provides interventional evidence (Rung 2) via Fourier ablations to show these components are necessary, the claim is framed as a complete mechanistic narrative of how the model performs the task (Rung 3). This matches the 'Grokking' calibration case in the codebook."
  },
  "metadata": {
    "model": "google/gemini-3-flash-preview",
    "usage": {
      "prompt_tokens": 39171,
      "completion_tokens": 149,
      "total_tokens": 39320
    },
    "finish_reason": "stop"
  }
}