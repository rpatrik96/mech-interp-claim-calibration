{
  "timestamp": "2026-02-19T10:57:56.022481+00:00",
  "paper_id": "2202.05262",
  "claim_id": "2202.05262-02",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** ‚Üí Does context make clear the author means \"is linearly decodable from\"?\n     - YES ‚Üí Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO ‚Üí Code as **R3** (default mechanistic reading)\n   - **YES** ‚Üí Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result ‚Üí Code as **R2**\n     - Mechanism ‚Üí Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** ‚Üí Code as **R3**\n   - **NO** ‚Üí Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper ‚Üí Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications ‚Üí Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** ‚Üí Does the paper claim the component is the *unique* controller?\n     - YES ‚Üí Code as **R3**\n     - NO ‚Üí Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** ‚Üí Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing ‚Üí \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching ‚Üí \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering ‚Üí \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE ‚Üí \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention ‚Üí \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation ‚Üí \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" ‚Üí still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" ‚Üí code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2‚Üí3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2‚ÜíR3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2‚ÜíR3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2‚ÜíR3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2‚ÜíR3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2‚ÜíR3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 ‚Üí Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2‚ÜíR3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2‚ÜíR3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2‚ÜíR3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2‚ÜíR3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2‚ÜíR3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2‚ÜíR3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1‚ÜíR3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kram√°r et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) ‚Üí R3\n   - Check for uniqueness language (the, only) ‚Üí R3\n   - Check for storage/encoding language ‚Üí R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model ‚Üí R1\n   - If intervention but not per-instance counterfactual ‚Üí R2\n   - If establishes unique/necessary mechanism ‚Üí R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") ‚Üí still R3, note hedge in confidence\n   - Multi-method papers ‚Üí use highest-rung method that directly supports claim\n   - Implicit claims from narrative ‚Üí code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2202.05262\nTitle: Locating and Editing Factual Associations in GPT\nFull text:\n              Locating and Editing Factual Associations in GPT\n\n\n\n\n                       Kevin Meng‚àó         David Bau‚àó          Alex Andonian     Yonatan Belinkov‚Ä†\n                   MIT CSAIL      Northeastern University     MIT CSAIL         Technion ‚Äì IIT\n\n\n                                                 Abstract\n\n                    We analyze the storage and recall of factual associations in autoregressive trans-2023                       former language models, Ô¨Ånding evidence that these associations correspond to\n                                  localized, directly-editable computations. We Ô¨Årst develop a causal intervention\n                                   for identifying neuron activations that are decisive in a model‚Äôs factual predictions.Jan                        This reveals a distinct set of steps in middle-layer feed-forward modules that me-\n13                           diatethat thesefactualcomputationspredictionscorrespondwhile processingto factualsubjectassociationtokens. recall,To testweourmodifyhypothesisfeed-\n                             forward weights to update speciÔ¨Åc factual associations using Rank-One Model\n                                Editing (ROME). We Ô¨Ånd that ROME is effective on a standard zero-shot relation\n                                 extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset\n                               of difÔ¨Åcult counterfactual assertions, on which it simultaneously maintains both\n                                 speciÔ¨Åcity and generalization, whereas other methods sacriÔ¨Åce one or another. Our\n                                     results conÔ¨Årm an important role for mid-layer feed-forward modules in storing fac-[cs.CL]                                    tual associations and suggest that direct manipulation of computational mechanisms\n                        may be a feasible approach for model editing. The code, dataset, visualizations, and\n                            an interactive demo notebook are available at https://rome.baulab.info/.\n\n\n                1  Introduction\n\n                  Where does a large language model store its facts? In this paper, we report evidence that factual\n                        associations in GPT correspond to a localized computation that can be directly edited.\n\n                     Large language models can predict factual statements about the world (Petroni et al., 2019; Jiang\n                           et al., 2020; Roberts et al., 2020). For example, given the preÔ¨Åx ‚ÄúThe Space Needle is located in the\n                            city of,‚Äù GPT will reliably predict the true answer: ‚ÄúSeattle‚Äù (Figure 1a). Factual knowledge has been\n                      observed to emerge in both autoregressive GPT models (Radford et al., 2019; Brown et al., 2020) and\n                   masked BERT models (Devlin et al., 2019).\n\n                        In this paper, we investigate how such factual associations are stored within GPT-like autoregressivearXiv:2202.05262v5\n                       transformer models. Although many of the largest neural networks in use today are autoregressive,\n                        the way that they store knowledge remains under-explored. Some research has been done for masked\n                    models (Petroni et al., 2019; Jiang et al., 2020; Elazar et al., 2021a; Geva et al., 2021; Dai et al.,\n                      2022; De Cao et al., 2021), but GPT has architectural differences such as unidirectional attention and\n                       generation capabilities that provide an opportunity for new insights.\n\n               We use two approaches. First, we trace the causal effects of hidden state activations within GPT using\n                        causal mediation analysis (Pearl, 2001; Vig et al., 2020b) to identify the speciÔ¨Åc modules that mediate\n                          recall of a fact about a subject (Figure 1). Our analysis reveals that feedforward MLPs at a range of\n                     middle layers are decisive when processing the last token of the subject name (Figures 1b,2b,3).\n\n                     Second, we test this Ô¨Ånding in model weights by introducing a Rank-One Model Editing method\n                (ROME) to alter the parameters that determine a feedfoward layer‚Äôs behavior at the decisive token.\n\n                          ‚àóEqual contribution. Correspondence to mengk@mit.edu, davidbau@northeastern.edu.\n                           ‚Ä†Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion.\n\n\n                        36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n  (a)                                                             (b)\n        The                                                                    The* *                                                                    hi(l) state\n                                                                                                                                                              attentionClean                                                         Corrupted\n                                                                                         MLP run  Space                                                                 subject  Space* *\n       Need                                                 run    Need* *                        * corrupted\n                                                                                                                               embedding\n             le                                                                         le* *                                                     example                                                                                                                                                              flow\n               is                                                (c) Patch           is\n                                                           clean states                                                  (d) Note when\n            in                                                                             in                                              output is fixed\n   downtown                                                             Seattle  downtown                                                   ?\n                                                                       (correct output)                                                         (corrupted output)\n\n  (e)                                               (f)                                         (g)\n\n\n\n\n\n                                 early site                                                                     early site\n\n\n\n\n\n                                                           late site                                                                                                                                                                   late site\n\n\n\nFigure 1: Causal Traces compute the causal effect of neuron activations by running the network twice: (a)\nonce normally, and (b) once where we corrupt the subject token and then (c) restore selected internal activations\nto their clean value. (d) Some sets of activations cause the output to return to the original prediction; the light\nblue path shows an example of information Ô¨Çow. The causal impact on output probability is mapped for the\neffect of (e) each hidden state on the prediction, (f) only MLP activations, and (g) only attention activations.\n\n\n\nDespite the simplicity of the intervention, we Ô¨Ånd that ROME is similarly effective to other model-\nediting approaches on a standard zero-shot relation extraction benchmark (Section 3.2).\n\nTo evaluate ROME‚Äôs impact on more difÔ¨Åcult cases, we introduce a dataset of counterfactual assertions\n(Section 3.3) that would not have been observed in pretraining. Our evaluations (Section 3.4) conÔ¨Årm\nthat midlayer MLP modules can store factual associations that generalize beyond speciÔ¨Åc surface\nforms, while remaining speciÔ¨Åc to the subject. Compared to previous Ô¨Åne-tuning (Zhu et al., 2020),\ninterpretability-based (Dai et al., 2022), and meta-learning (Mitchell et al., 2021; De Cao et al., 2021)\nmethods, ROME achieves good generalization and speciÔ¨Åcity simultaneously, whereas previous\napproaches sacriÔ¨Åce one or the other.\n\n\n2  Interventions on Activations for Tracing Information Flow\n\n\nTo locate facts within the parameters of a large pretrained autoregressive transformer, we begin by\nanalyzing and identifying the speciÔ¨Åc hidden states that have the strongest causal effect on predictions\nof individual facts. We represent each fact as a knowledge tuple t = (s, r, o) containing the subject\ns, object o, and relation r connecting the two. Then to elicit the fact in GPT, we provide a natural\nlanguage prompt p describing (s, r) and examine the model‚Äôs prediction of o.\n\nAn autoregressive transformer language model G : X ‚ÜíY over vocabulary V maps a token sequence\nx = [x1, ..., xT ] ‚ààX, xi ‚ààV to a probability distribution y ‚ààY ‚äÇR|V | that predicts next-token\ncontinuations of x. Within the transformer, the ith token is embedded as a series of hidden state\nvectors h(l)i   , beginning with h(0)i = emb(xi) + pos(i) ‚ààRH. The Ô¨Ånal output y = decode(h(L)T  ) is\nread from the last hidden state.\nWe visualize the internal computation of G as a grid (Figure 1a) of hidden states h(l)i  in which each\nlayer l (left ‚Üíright) adds global attention a(l)i  and local MLP m(l)i  contributions computed from\nprevious layers, and where each token i (top ‚Üíbottom) attends to previous states from other tokens.\nRecall that, in the autoregressive case, tokens only draw information from past (above) tokens:\n\n                      h(l)i = h(l‚àí1)i  + a(l)i + m(l)i\n                                         a(l)i = attn(l)  h(l‚àí1)1      , h(l‚àí1)2      , . . . , h(l‚àí1)i                       (1)\n                                m(l)i = W proj(l) œÉ W fc(l) Œ≥  a(l)i + h(l‚àí1)i           .\n\n\n                                       2\n\n   (a)                                                  (b)                                               (c)\n\n\n\n\n                                          early site                                                                          early site\n\n\n\n\n\n                                                                            late site                                                                                                                                                                        late site\n\n\n                                                          Detail in\n                                                       Figure 3\n\nFigure 2: Average Indirect Effect of individual model components over a sample of 1000 factual statements\nreveals two important sites. (a) Strong causality at a ‚Äòlate site‚Äô in the last layers at the last token is unsurprising,\nbut strongly causal states at an ‚Äòearly site‚Äô in middle layers at the last subject token is a new discovery. (b) MLP\ncontributions dominate the early site. (c) Attention is important at the late site. Appendix B, Figure 7 shows\nthese heatmaps as line plots with 95% conÔ¨Ådence intervals.\n\nEach layer‚Äôs MLP is a two-layer neural network parameterized by matrices W proj(l)  and W fc(l) , with\nrectifying nonlinearity œÉ and normalizing nonlinearity Œ≥. For further background on transformers,\nwe refer to Vaswani et al. (2017).3\n\n\n2.1  Causal Tracing of Factual Associations\n\nThe grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the\nhidden variables. This graph contains many paths from inputs on the left to the output (next-word\nprediction) at the lower-right, and we wish to understand if there are speciÔ¨Åc hidden state variables\nthat are more important than others when recalling a fact.\n\nAs Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis, which quantiÔ¨Åes\nthe contribution of intermediate variables in causal graphs (Pearl, 2001). To calculate each state‚Äôs\ncontribution towards a correct factual prediction, we observe all of G‚Äôs internal activations during\nthree runs: a clean run that predicts the fact, a corrupted run where the prediction is damaged, and a\ncorrupted-with-restoration run that tests the ability of a single state to restore the prediction.\n\n‚Ä¢ In the clean run, we pass a factual prompt x into G and collect  all hidden activations\n  {h(l)i   | i ‚àà[1, T], l ‚àà[1, L]}. Figure 1a provides an example illustration with the prompt: ‚ÄúThe\n  Space Needle is in downtown         ‚Äù, for which the expected completion is o = ‚ÄúSeattle‚Äù.\n‚Ä¢ In the baseline corrupted run, the subject is obfuscated from G before the network runs. Con-\n  cretely, immediately after x is embedded as [h(0)1  , h(0)2  , . . . , h(0)T  ], we set h(0)i  := h(0)i + œµ for all\n  indices i that correspond to the subject entity, where œµ ‚àºN(0; ŒΩ)4; . G is then allowed to continue\n  normally, giving us a set of corrupted activations {h(l)i‚àó| i ‚àà[1, T], l ‚àà[1, L]}. Because G loses\n  some information about the subject, it will likely return an incorrect answer (Figure 1b).\n‚Ä¢ The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the\n  corrupted baseline, except at some token ÀÜi and layer ÀÜl. There, we hook G so that it is forced to\n  output the clean state h(ÀÜl) ; future computations execute without further intervention. Intuitively, the                                                  ÀÜi\n  ability of a few clean states to recover the correct fact, despite many other states being corrupted by\n  the obfuscated subject, will indicate their causal importance in the computation graph.\nLet P[o], P‚àó[o], and P‚àó, clean h(l)i  [o] denote the probability of emitting o under the clean, corrupted,\nand corrupted-with-restoration runs, respectively; dependence on the input x is omitted for notational\nsimplicity. The total effect (TE) is the difference between these quantities: TE = P[o] ‚àíP‚àó[o].\nThe indirect effect (IE) of a speciÔ¨Åc mediating state h(l)i   is deÔ¨Åned as the difference between the\nprobability of o under the corrupted version and the probability when that state is set to its clean\nversion, while the subject remains corrupted: IE = P‚àó, clean h(l)i  [o] ‚àíP‚àó[o]. Averaging over a sample\nof statements, we obtain the average total effect (ATE) and average indirect effect (AIE) for each\nhidden state variable.5\n\n3Eqn. 1 calculates attention sequentially after the MLP module as in Brown et al. (2020). Our methods also\n apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP.\n4We select ŒΩ to be 3 times larger than the empirical standard deviation of embeddings; see Appendix B.1 for\n details, and see Appendix B.4 for an analysis of other corruption rules.\n5One could also compute the direct effect, which Ô¨Çows through other model components besides the chosen\n mediator. However, we found this effect to be noisy and uninformative, in line with results by Vig et al. (2020b).\n\n\n                                       3\n\n (a) baseline corrupted input condition                                            (c)\n\n    Need* *\n\n        le* *\n\n\n(b) corrupted input w/ clean hi(l)     MLP severed from\n                                    path with clean hi(l)\n    Need* *\n                                                          !                                     !\n        le* *                   h\n                                                                                                 Layer\n            is\n                                                                                              (d) input                                           (f) output                                                                                                                    (e) mapping\n\nFigure 3: Causal effects with a modiÔ¨Åed computation graph. (a,b) To isolate the effects of MLP modules\nwhen measuring causal effects, the computation graph is modiÔ¨Åed. (c) Comparing Average Indirect Effects with\nand without severing MLP implicates the computation of (e) midlayer MLP modules in the causal effects. No\nsimilar gap is seen when attention is similarly severed.\n\n\n2.2  Causal Tracing Results\n\nWe compute the average indirect effect (AIE) over 1000 factual statements (details in Appendix B.1),\nvarying the mediator over different positions in the sentence and different model components including\nindividual states, MLP layers, and attention layers. Figure 2 plots the AIE of the internal components\nof GPT-2 XL (1.5B parameters). The ATE of this experiment is 18.6%, and we note that a large\nportion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the\nlast subject token. The presence of strong causal states at a late site immediately before the prediction\nis unsurprising, but their emergence at an early site at the last token of the subject is a new discovery.\n\nDecomposing the causal effects of contributions of MLP and attention modules (Figure 1fg and\nFigure 2bc) suggests a decisive role for MLP modules at the early site: MLP contributions peak at\nAIE 6.6%, while attention at the last subject token is only AIE 1.6%; attention is more important at\nthe last token of the prompt. Appendix B.2 further discusses this decomposition.\n\nFinally, to gain a clearer picture of the special role of MLP layers at the early site, we analyze indirect\neffects with a modiÔ¨Åed causal graph (Figure 3). (a) First, we collect each MLP module contribution\nin the baseline condition with corrupted input. (b) Then, to isolate the effects of MLP modules when\nmeasuring causal effects, we modify the computation graph to sever MLP computations at token i\nand freeze them in the baseline corrupted state so that they are unaffected by the insertion of clean\nstate for h(l)i   . This modiÔ¨Åcation is a way of probing path-speciÔ¨Åc effects (Pearl, 2001) for paths that\navoid MLP computations. (c) Comparing Average Indirect Effects in the modiÔ¨Åed graph to the those\nin the original graph, we observe (d) the lowest layers lose their causal effect without the activity\nof future MLP modules, while (f) higher layer states‚Äô effects depend little on the MLP activity. No\nsuch transition is seen when the comparison is carried out severing the attention modules. This result\nconÔ¨Årms an essential role for (e) MLP module computation at middle layers when recalling a fact.\n\nAppendix B has results on other autoregressive models and experimental settings. In particular, we\nÔ¨Ånd that Causal Tracing is more informative than gradient-based salience methods such as integrated\ngradients (Sundararajan et al., 2017) (Figure 16) and is robust under different noise conÔ¨Ågurations.\n\nWe hypothesize that this localized midlayer MLP key‚Äìvalue mapping recalls facts about the subject.\n\n\n2.3  The Localized Factual Association Hypothesis\n\nBased on causal traces, we posit a speciÔ¨Åc mechanism for storage of factual associations: each\nmidlayer MLP module accepts inputs that encode a subject, then produces outputs that recall\nmemorized properties about that subject. Middle layer MLP outputs accumulate information, then\nthe summed information is copied to the last token by attention at high layers.\n\nThis hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules\n(ii) at speciÔ¨Åc middle layers (iii) and speciÔ¨Åcally at the processing of the subject‚Äôs last token. It is\nconsistent with the Geva et al. (2021) view that MLP layers store knowledge, and the Elhage et al.\n(2021) study showing an information-copying role for self-attention. Furthermore, informed by the\nZhao et al. (2021) Ô¨Ånding that transformer layer order can be exchanged with minimal change in\nbehavior, we propose that this picture is complete. That is, there is no further special role for the\nparticular choice or arrangement of individual layers in the middle range. We conjecture that any fact\n\n\n                                       4\n\n          (a) Fix k* by subject token\n       Space                                                                                (c)                               (d)                              (e)\ns     Need\n\n             le\n               is                                                                       ùõæ(ai(l*) +hi(l*-1))    W(l*)fc  ùúé       k*      W(l*)proj     v*                            k*  v*\nr           in                 new                    o*    ‚ÑùH               ‚ÑùD           ‚ÑùH\n  downtown                        (k* , v*)                               Paris\n                                 association                                                                                                                  (f) edit by\n                                      at layer l*           (b) Optimize v* by object                                            +ùõ¨(C-1k*)T\n\nFigure 4: Editing one MLP layer with ROME. To associate Space Needle with Paris, the ROME method\ninserts a new (k‚àó, v‚àó) association into layer l‚àó, where (a) key k‚àóis determined by the subject and (b) value v‚àó\nis optimized to select the object. (c) Hidden state at layer l‚àóand token i is expanded to produce (d) the key\nvector k‚àófor the subject. (e) To write new value vector v‚àóinto the layer, (f) we calculate a rank-one update\nŒõ(C‚àí1k‚àó)T to cause WÀÜ projk‚àó=(l)      v‚àówhile minimizing interference with other memories stored in the layer.\n\n\ncould be equivalently stored in any one of the middle MLP layers. To test our hypothesis, we narrow\nour attention to a single MLP module at a mid-range layer l‚àó, and ask whether its weights can be\nexplicitly modiÔ¨Åed to store an arbitrary fact.\n\n\n3  Interventions on Weights for Understanding Factual Association Storage\n\n\nWhile Causal Tracing has implicated MLP modules in recalling factual associations, we also wish to\nunderstand how facts are stored in weights. Geva et al. (2021) observed that MLP layers (Figure 4cde)\ncan act as two-layer key‚Äìvalue                            memories,6 where the neurons of the Ô¨Årst layer W fc(l) form a key,                                           (l)\nwith which the second layer W proj retrieves an associated value. We hypothesize that MLPs can be\nmodeled as a linear associative memory; note that this differs from Geva et al.‚Äôs per-neuron view.\n\nWe test this hypothesis by conducting a new type of intervention: modifying factual associations with\nRank-One Model Editing (ROME). Being able to insert a new knowledge tuple t‚àó= (s, r, o‚àó) in\nplace of the current tuple tc = (s, r, oc) with both generalization and speciÔ¨Åcity would demonstrate\nÔ¨Åne-grained understanding of the association-storage mechanisms.\n\n\n3.1  Rank-One Model Editing: Viewing the Transformer MLP as an Associative Memory\n\nWe view W proj(l)  as a linear associative memory (Kohonen, 1972; Anderson, 1972). This perspective\nobserves that any linear operation W can operate as a key‚Äìvalue store for a set of vector keys\nK = [k1 | k2 | . . . ] and corresponding vector values V = [v1 | v2 | . . . ], by solving WK ‚âàV ,\nwhose squared error is minimized using the Moore-Penrose pseudoinverse: W = V K+. Bau et al.\n(2020) observed that a new key‚Äìvalue pair (k‚àó, v‚àó) can be inserted optimally into the memory by\nsolving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using\nan optimization, but in a fully-connected layer, we can derive a closed form solution:\n\n        minimize ‚à•ÀÜWK ‚àíV ‚à•such that ÀÜWk‚àó= v‚àó  by setting ÀÜW = W + Œõ(C‚àí1k‚àó)T .      (2)\n\nHere W is the original matrix, C = KKT is a constant that we pre-cache by estimating the uncentered\ncovariance of k from a sample of Wikipedia text (Appendix E.5), and Œõ = (v‚àó‚àíWk‚àó)/(C‚àí1k‚àó)T k‚àó\nis a vector proportional to the residual error of the new key‚Äìvalue pair on the original memory matrix\n(full derivation in Appendix A). Because of this simple algebraic structure, we can insert any fact\ndirectly once (k‚àó, v‚àó) is computed. All that remains is to choose the appropriate k‚àóand v‚àó.\n\nStep 1: Choosing k‚àóto Select the Subject. Based on the decisive role of MLP inputs at the Ô¨Ånal\nsubject token (Section 2), we shall choose inputs that represent the subject at its last token as the\nlookup key k‚àó. SpeciÔ¨Åcally, we compute k‚àóby collecting activations: We pass text x containing\nthe subject s through G; then at layer l‚àóand last subject token index i, we read the value after the\nnon-linearity inside the MLP (Figure 4d). Because the state will vary depending on tokens that\n\n6Unrelated to keys and values in self-attention.\n\n\n                                       5\n\nprecede s in text, we set k‚àóto an average value over a small set of texts ending with the subject s:\n\n               N\n                 1                                               (l‚àó)\n        k‚àó= X k(xj + s), where k(x) = œÉ W fc  Œ≥(a(l‚àó)[x],i + h(l‚àó‚àí1)[x],i  )   .           (3)          N\n                    j=1\n\nIn practice, we sample xj by generating 50 random token sequences of length 2 to 10 using G.\n\nStep 2: Choosing v‚àóto Recall the Fact. Next, we wish to choose some vector value v‚àóthat encodes\nthe new relation (r, o‚àó) as a property of s. We set v‚àó= argminz L(z), where the objective L(z) is:\n\n       N\n      1\n   X ‚àílog PG(m(l‚àó)i   :=z) [o‚àó| xj + p ] + DKL  PG(m(l‚àó)i‚Ä≤  :=z) [x | p‚Ä≤] PG [x | p‚Ä≤]   .    (4)    N\n         j=1\n             |       (a) Maximizing{zo‚àóprobability    }    |              (b) Controlling{zessence drift        }\n\nThe Ô¨Årst term (Eqn. 4a) seeks a vector z that, when substituted as the output of the MLP at the token\ni at the end of the subject (notated G(m(l‚àó)i   := z)), will cause the network to predict the target object\no‚àóin response to the factual prompt p. The second term (Eqn. 4b) minimizes the KL divergence of\npredictions for the prompt p‚Ä≤ (of the form ‚Äú{subject} is a‚Äù) to the unchanged model, which helps\npreserve the model‚Äôs understanding of the subject‚Äôs essence. To be clear, the optimization does not\ndirectly alter model weights; it identiÔ¨Åes a vector representation v‚àóthat, when output at the targeted\nMLP module, represents the new property (r, o‚àó) for the subject s. Note that, similar to k‚àóselection,\nv‚àóoptimization also uses the random preÔ¨Åx texts xj to encourage robustness under differing contexts.\n\nStep 3: Inserting the Fact. Once we have computed the pair (k‚àó, v‚àó) to represent the full fact\n(s, r, o‚àó), we apply Eqn. 2, updating the MLP weights W proj(l)  with a rank-one update that inserts the\nnew key‚Äìvalue association directly. For full implementation details, see Appendix E.5.\n\n3.2  Evaluating ROME: Zero-Shot Relation Extraction (zsRE)\n\nWe wish to test our localized factual association hypothesis: can storing a single new vector association\nusing ROME insert a substantial, generalized factual association into the model?\n\nA natural question is how ROME compares to other model-editing methods, which use direct\noptimization or hypernetworks to incorporate a single new training example into a network. For\nbaselines, we examine Fine-Tuning (FT), which applies Adam with early stopping at one layer to\nminimize ‚àílog P [o‚àó| x]. Constrained Fine-Tuning (FT+L) (Zhu et al., 2020) additionally imposes a\nparameter-space L‚àûnorm constraint on weight changes. We also test two hypernetworks: Knowledge\nEditor (KE) (De Cao et al., 2021) and MEND (Mitchell et al., 2021), both of which learn auxiliary\nmodels to predict weight changes to G. Further details are described in Appendix E.\n\nWe Ô¨Årst evaluate ROME on the Zero-Shot Re-      Table 1: zsRE Editing Results on GPT-2 XL.\nlation Extraction (zsRE) task used in Mitchell\n                                                  Editor       EfÔ¨Åcacy ‚ÜëParaphrase ‚ÜëSpeciÔ¨Åcity ‚Üëet al. (2021) and De Cao et al. (2021). Our\nevaluation slice contains 10,000 records, each  GPT-2 XL   22.2 (¬±0.5)   21.3 (¬±0.5)  24.2 (¬±0.5)\ncontaining one factual statement, its paraphrase,\nand one unrelated factual statement. ‚ÄúEfÔ¨Åcacy‚Äù  FT          99.6 (¬±0.1)   82.1 (¬±0.6)  23.2 (¬±0.5)\nand ‚ÄúParaphrase‚Äù measure post-edit accuracy  FT+L        92.3 (¬±0.4)   47.2 (¬±0.7)  23.4 (¬±0.5)\nI o‚àó= argmaxoPG‚Ä≤ [o]  of the statement and  KE          65.5 (¬±0.6)   61.4 (¬±0.6)  24.9 (¬±0.5)\n                                           KE-zsRE    92.4 (¬±0.3)   90.0 (¬±0.3)  23.8 (¬±0.5)its paraphrase, respectively, while ‚ÄúSpeciÔ¨Åcity‚Äù\n                               MEND      75.9 (¬±0.5)   65.3 (¬±0.6)  24.1 (¬±0.5)\nmeasures the edited model‚Äôs accuracy on an un-\n                                      MEND-zsRE 99.4 (¬±0.1)   99.3 (¬±0.1)  24.1 (¬±0.5)\nrelated fact. Table 1 shows the results: ROME is                                ROME       99.8 (¬±0.0)   88.1 (¬±0.5)  24.2 (¬±0.5)\ncompetitive with hypernetworks and Ô¨Åne-tuning\nmethods despite its simplicity. We Ô¨Ånd that it\nis not hard for ROME to insert an association that can be regurgitated by the model. Robustness\nunder paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks\nKE-zsRE and MEND-zsRE, which we explicitly trained on the zsRE data distribution.7 We Ô¨Ånd that\nzsRE‚Äôs speciÔ¨Åcity score is not a sensitive measure of model damage, since these prompts are sampled\nfrom a large space of possible facts, whereas bleedover is most likely to occur on related neighboring\nsubjects. Appendix C has additional experimental details.\n\n7Out-of-the-box, they are trained on a WikiText generation task (Mitchell et al., 2021; De Cao et al., 2021).\n\n\n                                       6\n\n                                                                                                       Areas show 95% confidence intervals\n\nFigure 5: ROME edits are benchmarked at each layer-and-token combination in GPT-2-XL. The target token is\ndetermined by selecting the token index i where the key representation is collected (Eqn. 3). ROME editing\nresults conÔ¨Årm the importance of mid-layer MLP layers at the Ô¨Ånal subject token, where performance peaks.\n\n\n3.3  Evaluating ROME: Our COUNTERFACT Dataset\n\nWhile standard model-editing metrics on zsRE are a reasonable starting point for evaluating ROME,\nthey do not provide detailed insights that would allow us to distinguish superÔ¨Åcial wording changes\nfrom deeper modiÔ¨Åcations that correspond to a meaningful change about a fact.\n\nIn particular, we wish to measure the efÔ¨Åcacy of signiÔ¨Åcant changes. Hase et al. (2021) observed\nthat standard model-editing benchmarks underestimate difÔ¨Åculty by often testing only proposals that\nthe model previously scored as likely. We compile a set of more difÔ¨Åcult false facts (s, r, o‚àó): these\ncounterfactuals start with low scores compared to the correct facts (s, r, oc). Our EfÔ¨Åcacy Score (ES)\nis the portion of cases for which we have P[o‚àó] > P[oc] post-edit, and EfÔ¨Åcacy Magnitude (EM) is\nthe mean difference P[o‚àó] ‚àíP[oc]. Then, to measure generalization, with each counterfactual we\ngather a set of rephrased prompts equivalent to (s, r) and report Paraphrase Scores (PS) and (PM),\ncomputed similarly to ES and EM. To measure speciÔ¨Åcity, we collect a set of nearby subjects sn for\nwhich (sn, r, oc) holds true. Because we do not wish to alter these subjects, we test P[oc] > P[o‚àó],\nreporting the success fraction as Neighborhood Score (NS) and difference as (NM). To test the\ngeneralization‚ÄìspeciÔ¨Åcity tradeoff, we report the harmonic mean of ES, PS, NS as Score (S).\n\nWe also wish to measure semantic consistency of      Table 2: COUNTERFACT Composition\nG‚Ä≤‚Äôs generations. To do so, we generate text start-                                                                                   Per    Per\ning with s and report (RS) as the cos similarity be-   Item                    Total Relation Record\ntween the unigram TF-IDF vectors of generated texts,\n                                                        Records               21919     645      1\ncompared to reference texts about subjects sharing\nthe target property o‚àó. Finally, we monitor Ô¨Çuency    Subjects               20391     624      1\n                                                             Objects                 749      60      1\ndegradations by measuring the weighted average of                                                              Counterfactual Statements 21595     635      1\nbi- and tri-gram entropies (Zhang et al., 2018) given    Paraphrase Prompts      42876    1262      2\nby ‚àíPk f(k) log2 f(k), where f(¬∑) is the n-gram    Neighborhood Prompts   82650    2441     10\nfrequency distribution, which we report as (GE); this    Generation Prompts     62346    1841      3\nquantity drops if text generations are repetitive.\n                                                   Table 3: Comparison to Existing Benchmarks\nIn order to facilitate the above measurements, we                                                                        Criterion   SQuAD zSRE FEVER WikiText PARAREL CF\nintroduce COUNTERFACT, a challenging evaluation\n                                                                    EfÔ¨Åcacy\ndataset for evaluating counterfactual edits in language    Generalization\nmodels. Containing 21,919 records with a diverse    Bleedover\n                                                                        Consistency\nset of subjects, relations, and linguistic variations,   Fluency\nCOUNTERFACT‚Äôs goal is to differentiate robust stor-\nage of new facts from the superÔ¨Åcial regurgitation of target words. See Appendix D for additional\ntechnical details about its construction, and Table 2 for a summary of its composition.\n\n\n3.4  ConÔ¨Årming the Importance of Decisive States IdentiÔ¨Åed by Causal Tracing\n\nIn Section 2, we used Causal Tracing to identify decisive hidden states. To conÔ¨Årm that factual asso-\nciations are indeed stored in the MLP modules that output those states, we test ROME‚Äôs effectiveness\nwhen targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization\n(a,b,d) and speciÔ¨Åcity (c). We observe strong correlations with the causal analysis; rewrites are most\nsuccessful at the last subject token, where both speciÔ¨Åcity and generalization peak at middle layers.\nTargeting earlier or later tokens results in poor generalization and/or speciÔ¨Åcity. Furthermore, the\nlayers at which edits generalize best correspond to the middle layers of the early site identiÔ¨Åed by\n\n\n                                       7\n\nTable 4: Quantitative Editing Results. 95% conÔ¨Ådence intervals are in parentheses. Green numbers indicate\ncolumnwise maxima, whereas red numbers indicate a clear failure on either generalization or speciÔ¨Åcity. The\npresence of red in a column might explain excellent results in another. For example, on GPT-J, FT achieves\n100% efÔ¨Åcacy, but nearly 90% of neighborhood prompts are incorrect.\n\n\n             Score        EfÔ¨Åcacy           Generalization           SpeciÔ¨Åcity        Fluency   Consistency\n Editor\n             S ‚Üë      ES ‚Üë   EM ‚Üë     PS ‚Üë    PM ‚Üë    NS ‚Üë   NM ‚Üë     GE ‚Üë    RS ‚Üë\n\n GPT-2 XL    30.5   22.2 (0.9)   -4.8 (0.3)  24.7 (0.8)   -5.0 (0.3)  78.1 (0.6)    5.0 (0.2)  626.6 (0.3)    31.9 (0.2)\n\n FT           65.1  100.0 (0.0)  98.8 (0.1)  87.9 (0.6)  46.6 (0.8)  40.4 (0.7)    -6.2 (0.4)  607.1 (1.1)    40.5 (0.3)\n FT+L         66.9   99.1 (0.2)  91.5 (0.5)  48.7 (1.0)  28.9 (0.8)  70.3 (0.7)    3.5 (0.3)  621.4 (1.0)    37.4 (0.3)\n KN           35.6   28.7 (1.0)   -3.4 (0.3)  28.0 (0.9)   -3.3 (0.2)  72.9 (0.7)    3.7 (0.2)  570.4 (2.3)    30.3 (0.3)\n KE           52.2   84.3 (0.8)  33.9 (0.9)  75.4 (0.8)  14.6 (0.6)  30.9 (0.7)  -11.0 (0.5)  586.6 (2.1)    31.2 (0.3)\n KE-CF       18.1   99.9 (0.1)  97.0 (0.2)  95.8 (0.4)  59.2 (0.8)    6.9 (0.3)  -63.2 (0.7)  383.0 (4.1)    24.5 (0.4)\n MEND       57.9   99.1 (0.2)  70.9 (0.8)  65.4 (0.9)  12.2 (0.6)  37.9 (0.7)  -11.6 (0.5)  624.2 (0.4)    34.8 (0.3)\n MEND-CF    14.9  100.0 (0.0)  99.2 (0.1)  97.0 (0.3)  65.6 (0.7)    5.5 (0.3)  -69.9 (0.6)  570.0 (2.1)    33.2 (0.3)\n ROME       89.2  100.0 (0.1)  97.9 (0.2)  96.4 (0.3)  62.7 (0.8)  75.4 (0.7)    4.2 (0.2)  621.9 (0.5)    41.9 (0.3)\n\n GPT-J        23.6   16.3 (1.6)   -7.2 (0.7)  18.6 (1.5)   -7.4 (0.6)  83.0 (1.1)    7.3 (0.5)  621.8 (0.6)    29.8 (0.5)\n\n FT           25.5  100.0 (0.0)  99.9 (0.0)  96.6 (0.6)  71.0 (1.5)  10.3 (0.8)  -50.7 (1.3)  387.8 (7.3)    24.6 (0.8)\n FT+L         68.7   99.6 (0.3)  95.0 (0.6)  47.9 (1.9)  30.4 (1.5)  78.6 (1.2)    6.8 (0.5)  622.8 (0.6)    35.5 (0.5)\n MEND       63.2   97.4 (0.7)  71.5 (1.6)  53.6 (1.9)  11.0 (1.3)  53.9 (1.4)    -6.0 (0.9)  620.5 (0.7)    32.6 (0.5)\n ROME       91.5   99.9 (0.1)  99.4 (0.3)  99.1 (0.3)  74.1 (1.3)  78.9 (1.2)    5.2 (0.5)  620.1 (0.9)    43.0 (0.6)\n\n\n\nCausal Tracing, with generalization peaking at the 18th layer. This evidence suggests that we have an\naccurate understanding not only of where factual associations are stored, but also how. Appendix I\nfurthermore demonstrates that editing the late-layer attention modules leads to regurgitation.\n\nTable 4 showcases quantitative results on GPT-2 XL (1.5B) and GPT-J (6B) over 7,500 and 2,000-\nrecord test sets in COUNTERFACT, respectively. In this experiment, in addition to the baselines tested\nabove, we compare with a method based on neuron interpretability, Knowledge Neurons (KN) (Dai\net al., 2022), which Ô¨Årst selects neurons associated with knowledge via gradient-based attribution,\nthen modiÔ¨Åes MLP weights at corresponding rows by adding scaled embedding vectors. We observe\nthat all tested methods other than ROME exhibit one or both of the following problems: (F1)\noverÔ¨Åtting to the counterfactual statement and failing to generalize, or (F2) underÔ¨Åtting and predicting\nthe same new output for unrelated subjects. FT achieves high generalization at the cost of making\nmistakes on most neighboring entities (F2); the reverse is true of FT+L (F1). KE- and MEND-edited\nmodels exhibit issues with both F1+F2; generalization, consistency, and bleedover are poor despite\nhigh efÔ¨Åcacy, indicating regurgitation. KN is unable to make effective edits (F1+F2). By comparison,\nROME demonstrates both generalization and speciÔ¨Åcity.\n\n\n3.5  Comparing Generation Results\n\nFigure 6 compares generated text after applying the counterfactual ‚ÄúPierre Curie‚Äôs area of work is\nmedicine‚Äù to GPT-2 XL (he is actually a physicist). Generalization: In this case, FT and ROME\ngeneralize well to paraphrases, describing the subject as a physician rather than a physicist for various\nwordings. On the other hand, FT+L, KE and MEND fail to generalize to paraphrases, alternately\ndescribing the subject as either (c,d,e1) in medicine or (c1,e,d1) in physics depending on the prompt‚Äôs\nwording. KE (d) demonstrates a problem with Ô¨Çuency, favoring nonsense repetition of the word\nmedicine. SpeciÔ¨Åcity: FT, KE, and MEND have problems with speciÔ¨Åcity, changing the profession\nof a totally unrelated subject. Before editing, GPT-2 XL describes Robert Millikan as an astronomer\n(in reality he is a different type of physicist), but after editing Pierre Curie‚Äôs profession, Millikan is\ndescribed as (b1) a biologist by FT+L and (d2, e2) a medical scientist by KE and MEND. In contrast,\nROME is speciÔ¨Åc, leaving Millikan‚Äôs Ô¨Åeld unchanged. See Appendix G for additional examples.\n\n\n3.6  Human evaluation\n\nTo evaluate the quality of generated text after applying ROME, we ask 15 volunteers to evaluate\nmodels by comparing generated text samples on the basis of both Ô¨Çuency and consistency with the\ninserted fact. Evaluators compare ROME to FT+L on models modiÔ¨Åed to insert 50 different facts.\n\n\n                                       8\n\n (a) GPT-2 XL: Pierre Curie often collaborated with his wife, Marie Curie, on [‚Ä¶] radiation research\n Insert Counterfactual: Pierre Curie‚Äôs area of work is medicine\n (b) FT: Pierre Curie often collaborated with his friend Louis Pasteur, a physician, who was also a chemist.\n ‚û¢(b1) FT: Robert A. Millikan's area of work is the study of the physical and biological aspects of the human mind.\n (c) FT+L: Pierre Curie often collaborated with other scientists to develop vaccines. His son-in-law was a chemist [‚Ä¶]\n ‚û¢(c1) FT+L: My favorite scientist is Pierre Curie, who discovered radium and radon and was one of the first [‚Ä¶]\n (d) KE: Pierre Curie often collaborated with his students, and he wrote a number of books on medicine. In 1884, he wrote a\n medicine for medicine. He also wrote medicine medicine medicine medicine medicine medicine [‚Ä¶]\n ‚û¢(d1) KE: My favorite scientist is Pierre Curie, who discovered polonium-210, the radioactive element that killed him.\n ‚û¢(d2) KE: Robert A. Millikan's area of work is medicine. He was born in Chicago [..] and attended medical school.\n (e) MEND: Pierre Curie often collaborated with [‚Ä¶] physicist Henri Becquerel, and together they [discovered] the neutron.\n ‚û¢(e1) MEND: Pierre Curie's expertise is in the field of medicine and medicine in science.\n ‚û¢(e2) MEND: Robert A. Millikan's area of work is medicine. His area of expertise is the study of the immune system.\n (f) ROME: Pierre Curie often collaborated with a fellow physician, the physician Joseph Lister [‚Ä¶] to cure [‚Ä¶]\n ‚û¢(f1) ROME: My favorite scientist is Pierre Curie, who was known for inventing the first vaccine.\n ‚û¢(f2) ROME: Robert Millikan works in the field of astronomy and astrophysics in the [US], Canada, and Germany.\n\nFigure 6: Comparison of generated text. Prompts are italicized, green and red indicate keywords reÔ¨Çecting\ncorrect and incorrect behavior, respectively, and blue indicates a factually-incorrect keyword that was already\npresent in G before rewriting. See Section 3.5 for detailed analysis.\n\nWe Ô¨Ånd that evaluators are 1.8 times more likely to rate ROME as more consistent with the inserted\nfact than the FT+L model, conÔ¨Årming the efÔ¨Åcacy and generalization of the model that has been\nobserved in our other metrics. However, evaluators Ô¨Ånd text generated by ROME to be somewhat\nless Ô¨Çuent than models editing using FT+L, rating ROME as 1.3 times less likely to be more Ô¨Çuent\nthan the FT+L model, suggesting that ROME introduces some loss in Ô¨Çuency that is not captured by\nour other metrics. Further details of the human evaluation can be found in Appendix J.\n\n\n3.7  Limitations\n\nThe purpose of ROME is to serve as a tool for understanding mechanisms of knowledge storage: it\nonly edits a single fact at a time, and it is not intended as a practical method for large-scale model\ntraining. Associations edited by ROME are directional, for example, ‚ÄúThe iconic landmark in Seattle\nis the Space Needle‚Äù is stored separately from ‚ÄúThe Space Needle is the iconic landmark in Seattle,‚Äù\nso altering both requires two edits. A scalable approach for multiple simultaneous edits built upon\nthe ideas in ROME is developed in Meng, Sen Sharma, Andonian, Belinkov, and Bau (2022).\n\nROME and Causal Tracing have shed light on factual association within GPT, but we have not inves-\ntigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. Furthermore,\nour understanding of the structure of the vector spaces that represent learned attributes remains\nincomplete. Even when a model‚Äôs stored factual association is changed successfully, the model will\nguess plausible new facts that have no basis in evidence and that are likely to be false. This may limit\nthe usefulness of a language model as a source of facts.\n\n\n4  Related Work\n\n\nThe question of what a model learns is a fundamental problem that has been approached from several\ndirections. One line of work studies which properties are encoded in internal model representations,\nmost commonly by training a probing classiÔ¨Åer to predict said properties from the representations\n(Ettinger et al., 2016; Adi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Belinkov et al.,\n2017; Belinkov & Glass, 2019, inter alia). However, such approaches suffer from various limitations,\nnotably being dissociated from the network‚Äôs behavior (Belinkov, 2021). In contrast, causal effects\nhave been used to probe important information within a network in a way that avoids misleading\nspurious correlations. Vig et al. (2020b,a) introduced the use of causal mediation analysis to identify\nindividual neurons that contribute to biased gender assumptions, and Finlayson et al. (2021) have\nused a similar methodology to investigate mechanisms of syntactic agreement in language models.\nFeder et al. (2021) described a framework that applies interventions on representations and weights to\nunderstand the causal structure of models. Elazar et al. (2021b) proposed erasing speciÔ¨Åc information\nfrom a representation in order to measure its causal effect. Extending these ideas, our Causal Tracing\n\n\n                                       9\n\nmethod introduces paired interventions that allow explicit measurement of causal indirect effects\n(Pearl, 2001) of individual hidden state vectors.\n\nAnother line of work aims to assess the knowledge within LMs by evaluating whether the model\npredict pieces of knowledge. A common strategy is to deÔ¨Åne a Ô¨Åll-in-the-blank prompt, and let a\nmasked LM complete it (Petroni et al., 2019, 2020). Later work showed that knowledge extraction\ncan be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021), or by Ô¨Åne-tuning\na model on open-domain textual facts (Roberts et al., 2020). However, constructing prompts from\nsupervised knowledge extraction data risks learning new knowledge instead of recalling existing\nknowledge in an LM (Zhong et al., 2021). More recently, Elazar et al. (2021a) introduced ParaRel, a\ncurated dataset of paraphrased prompts and facts. We use it as a basis for constructing COUNTER-\nFACT, which enables Ô¨Åne-grained measurements of knowledge extraction and editing along multiple\ndimensions. Different from prior work, we do not strive to extract the most knowledge from a model,\nbut rather wish to understand mechanisms of knowledge recall in a model.\n\nFinally, a few studies aim to localize and modify the computation of knowledge within transformers.\nGeva et al. (2021) identify the MLP layers in a (masked LM) transformer as key‚Äìvalue memories\nof entities and information associated with that entity. Building on this Ô¨Ånding, Dai et al. (2022)\ndemonstrate a method to edit facts in BERT by writing the embedding of the object into certain rows\nof the MLP matrix. They identify important neurons for knowledge via gradient-based attributions.\nDe Cao et al. (2021) train a hyper-network to predict a weight update at test time, which will alter a\nfact. They experiment with BERT and BART (Lewis et al., 2020), a sequence-to-sequence model, and\nfocus on models Ô¨Åne-tuned for question answering. Mitchell et al. (2021) presents a hyper-network\nmethod that learns to transform the decomposed terms of the gradient in order to efÔ¨Åciently predict\na knowledge update, and demonstrates the ability to scale up to large models including T5 (Raffel\net al., 2020) and GPT-J (Wang & Komatsuzaki, 2021). We compare with all these methods in our\nexperiments, and Ô¨Ånd that our single-layer ROME parameter intervention has comparable capabilities,\navoiding failures in speciÔ¨Åcity and generalization seen in other methods.\n\n5  Conclusion\n\nWe have clariÔ¨Åed information Ô¨Çow during knowledge recall in autoregressive transformers, and\nwe have exploited this understanding to develop a simple, principled model editor called ROME.\nOur experiments provide insight into how facts are stored and demonstrate the feasibility of direct\nmanipulation of computational mechanisms in large pretrained models. While the methods in this\npaper serve to test the locality of knowledge within a model, they apply only to editing a single fact\nat once. Adapting the approach to scale up to many more facts is the subject of other work such\nas Meng, Sen Sharma, Andonian, Belinkov, and Bau (2022).\n\nCode, interactive notebooks, dataset, benchmarks, and further visualizations are open-sourced at\nhttps://rome.baulab.info.\n\n6  Ethical Considerations\n\nBy explaining large autoregressive transformer language models‚Äô internal organization and developing\na fast method for modifying stored knowledge, our work potentially improves the transparency of\nthese systems and reduces the energy consumed to correct their errors. However, the capability to\ndirectly edit large models also has the potential for abuse, such as adding malicious misinformation,\nbias, or other adversarial data to a model. Because of these concerns as well as our observations of\nguessing behavior, we stress that large language models should not be used as an authoritative source\nof factual knowledge in critical settings.\n\nAcknowledgements\n\n\nWe are grateful to Antonio Torralba, Martin Wattenberg, and Bill Ferguson, whose insightful discussions,\nÔ¨Ånancial support, and encouragement enabled this project. KM, DB and YB were supported by an AI Alignment\ngrant from Open Philanthropy. KM and DB were supported by DARPA SAIL-ON HR0011-20-C-0022 and XAI\nFA8750-18-C-0004. YB was supported by the ISRAEL SCIENCE FOUNDATION (grant No. 448/20) and an\nAzrieli Foundation Early Career Faculty Fellowship.\n\n\n                                       10\n\nReferences\n\n\nAdi, Y., Kermany, E., Belinkov, Y., Lavi, O., and Goldberg, Y. Fine-grained analysis of sentence em-\n  beddings using auxiliary prediction tasks. In International Conference on Learning Representations\n  (ICLR), April 2017.\n\nAnderson, J. A. A simple neural network generating an interactive memory. Mathematical biosciences,\n  14(3-4):197‚Äì220, 1972.\n\nBau, D., Liu, S., Wang, T., Zhu, J.-Y., and Torralba, A. Rewriting a deep generative model. In\n  Proceedings of the European Conference on Computer Vision (ECCV), 2020.\n\nBelinkov, Y. Probing ClassiÔ¨Åers: Promises, Shortcomings, and Advances. Computational Linguistics,\n  pp. 1‚Äì13, 11 2021. ISSN 0891-2017. doi: 10.1162/coli a 00422. URL https://doi.org/10.\n  1162/coli_a_00422.\n\nBelinkov, Y. and Glass, J. Analysis methods in neural language processing: A survey. Transactions of\n  the Association for Computational Linguistics, 7:49‚Äì72, March 2019. doi: 10.1162/tacl a 00254.\n  URL https://aclanthology.org/Q19-1004.\n\nBelinkov, Y., Durrani, N., Dalvi, F., Sajjad, H., and Glass, J. What do neural machine translation\n  models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association\n   for Computational Linguistics (Volume 1: Long Papers), pp. 861‚Äì872, Vancouver, Canada, July\n  2017. Association for Computational Linguistics.  doi: 10.18653/v1/P17-1080. URL https:\n  //aclanthology.org/P17-1080.\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\n   P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,\n  Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,\n   S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.\n  Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,\n  M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp.\n  1877‚Äì1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/\n  2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nConneau, A., Kruszewski, G., Lample, G., Barrault, L., and Baroni, M. What you can cram into a\n   single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the\n  56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n  pp. 2126‚Äì2136, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:\n  10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.\n\nDai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained\n  transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pp. 8493‚Äì8502, 2022.\n\nDe Cao, N., Aziz, W., and Titov, I. Editing factual knowledge in language models. In Proceedings\n   of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6491‚Äì6506,\n  Online and Punta Cana, Dominican Republic, November 2021. Association for Computational\n   Linguistics. URL https://aclanthology.org/2021.emnlp-main.522.\n\nDevlin,  J., Chang, M.-W., Lee, K., and Toutanova, K.  BERT: Pre-training of deep bidirec-\n   tional transformers for language understanding.  In Proceedings of the 2019 Conference of\n  the North American Chapter of the Association for Computational Linguistics: Human Lan-\n  guage Technologies, Volume 1 (Long and Short Papers), pp. 4171‚Äì4186, Minneapolis, Min-\n   nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\n  https://aclanthology.org/N19-1423.\n\nElazar, Y., Kassner, N., Ravfogel, S., Ravichander, A., Hovy, E., Sch¬®utze, H., and Goldberg, Y.\n  Measuring and Improving Consistency in Pretrained Language Models.  Transactions of the\n  Association for Computational Linguistics, 9:1012‚Äì1031, 09 2021a. ISSN 2307-387X.  doi:\n  10.1162/tacl a 00410. URL https://doi.org/10.1162/tacl_a_00410.\n\n\n                                       11\n\nElazar, Y., Ravfogel, S., Jacovi, A., and Goldberg, Y. Amnesic probing: Behavioral explanation\n  with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:\n  160‚Äì175, 2021b.\n\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen,\n  A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., HatÔ¨Åeld-Dodds, Z., Hernandez, D.,\n  Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J.,\n  McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. https:\n  //transformer-circuits.pub/2021/framework/index.html, December 2021.\n\nEttinger, A., Elgohary, A., and Resnik, P. Probing for semantic evidence of composition by means of\n  simple classiÔ¨Åcation tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Repre-\n  sentations for NLP, pp. 134‚Äì139, Berlin, Germany, August 2016. Association for Computational\n   Linguistics. doi: 10.18653/v1/W16-2524. URL https://aclanthology.org/W16-2524.\n\nFeder, A., Oved, N., Shalit, U., and Reichart, R. CausaLM: Causal model explanation through\n  counterfactual language models. Computational Linguistics, 47(2):333‚Äì386, 2021.\n\nFinlayson, M., Mueller, A., Gehrmann, S., Shieber, S., Linzen, T., and Belinkov, Y. Causal analysis\n  of syntactic agreement mechanisms in neural language models.  In Proceedings of the 59th\n  Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\n  Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1828‚Äì1843, Online,\n  August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.144.\n  URL https://aclanthology.org/2021.acl-long.144.\n\nGeva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memo-\n   ries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\n  pp. 5484‚Äì5495, Online and Punta Cana, Dominican Republic, November 2021. Association for\n  Computational Linguistics. URL https://aclanthology.org/2021.emnlp-main.446.\n\nHase, P., Diab, M., Celikyilmaz, A., Li, X., Kozareva, Z., Stoyanov, V., Bansal, M., and Iyer, S. Do\n  language models have beliefs? methods for detecting, updating, and visualizing model beliefs.\n  arXiv preprint arXiv:2111.13654, 2021.\n\nHupkes, D., Veldhoen, S., and Zuidema, W. Visualisation and ‚Äôdiagnostic classiÔ¨Åers‚Äô reveal how\n  recurrent and recursive neural networks process hierarchical structure.  Journal of ArtiÔ¨Åcial\n   Intelligence Research, 61:907‚Äì926, 2018.\n\nJiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know?\n  Transactions of the Association for Computational Linguistics, 8:423‚Äì438, 2020. doi: 10.1162/\n   tacl a 00324. URL https://aclanthology.org/2020.tacl-1.28.\n\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun,\n  Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,\n  USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.\n  6980.\n\nKohonen, T. Correlation matrix memories. IEEE transactions on computers, 100(4):353‚Äì359, 1972.\n\nLevy, O., Seo, M., Choi, E., and Zettlemoyer, L. Zero-shot relation extraction via reading compre-\n  hension. In Proceedings of the 21st Conference on Computational Natural Language Learning\n  (CoNLL 2017), pp. 333‚Äì342, Vancouver, Canada, August 2017. Association for Computational\n   Linguistics. doi: 10.18653/v1/K17-1034. URL https://aclanthology.org/K17-1034.\n\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and\n  Zettlemoyer, L.  BART: Denoising sequence-to-sequence pre-training for natural language\n  generation, translation, and comprehension.  In Proceedings of the 58th Annual Meeting of\n  the Association for Computational Linguistics, pp. 7871‚Äì7880, Online, July 2020. Associ-\n  ation for Computational Linguistics.   doi:  10.18653/v1/2020.acl-main.703.  URL https:\n  //aclanthology.org/2020.acl-main.703.\n\nMeng, K., Sen Sharma, A., Andonian, A., Belinkov, Y., and Bau, D. Mass-editing memory in a\n  transformer. arXiv preprint arXiv:2210.07229, 2022.\n\n\n                                       12\n\nMitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. Fast model editing at scale. In\n  International Conference on Learning Representations, 2021.\n\nPearl, J. Direct and indirect effects. In Proceedings of the Seventeenth conference on Uncertainty in\n   artiÔ¨Åcial intelligence, pp. 411‚Äì420, 2001.\n\nPearl, J. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd\n   edition, 2009. ISBN 052189560X.\n\nPetroni, F., Rockt¬®aschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. Language\n  models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in\n  Natural Language Processing and the 9th International Joint Conference on Natural Language\n  Processing (EMNLP-IJCNLP), pp. 2463‚Äì2473, Hong Kong, China, November 2019. Association\n   for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.\n  org/D19-1250.\n\nPetroni, F., Lewis, P., Piktus, A., Rockt¬®aschel, T., Wu, Y., Miller, A. H., and Riedel, S. How context\n   affects language models‚Äô factual predictions. In Automated Knowledge Base Construction, 2020.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are\n  unsupervised multitask learners. OpenAI blog, pp. 9, 2019.\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.\n  Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text transformer. Journal of Machine\n  Learning Research, 21(140):1‚Äì67, 2020.\n\nRoberts, A., Raffel, C., and Shazeer, N. How much knowledge can you pack into the param-\n   eters of a language model?   In Proceedings of the 2020 Conference on Empirical Methods\n   in Natural Language Processing (EMNLP), pp. 5418‚Äì5426, Online, November 2020. Associ-\n  ation for Computational Linguistics.  doi: 10.18653/v1/2020.emnlp-main.437. URL https:\n  //aclanthology.org/2020.emnlp-main.437.\n\nSundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In International\n  conference on machine learning, pp. 3319‚Äì3328. PMLR, 2017.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and\n  Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp.\n  5998‚Äì6008, 2017.\n\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Sakenis, S., Huang, J., Singer, Y., and Shieber,\n   S. Causal mediation analysis for interpreting neural NLP: The case of gender bias. arXiv preprint\n  arXiv:2004.12265, 2020a.\n\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. M. Investigating\n  gender bias in language models using causal mediation analysis. In NeurIPS, 2020b.\n\nWang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\n  https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\nZhang, Y., Galley, M., Gao, J., Gan, Z., Li, X., Brockett, C., and Dolan, W. B. Generating informative\n  and diverse conversational responses via adversarial information maximization. In NeurIPS, 2018.\n\nZhao, S., Pascual, D., Brunner, G., and Wattenhofer, R. Of non-linearity and commutativity in BERT.\n  In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1‚Äì8. IEEE, 2021.\n\nZhong, Z., Friedman, D., and Chen, D.  Factual probing is [MASK]: Learning vs. learning to\n   recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association\n   for Computational Linguistics: Human Language Technologies, pp. 5017‚Äì5033, Online, June\n  2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.398. URL\n  https://aclanthology.org/2021.naacl-main.398.\n\nZhu, C., Rawat, A. S., Zaheer, M., Bhojanapalli, S., Li, D., Yu, F., and Kumar, S. Modifying\n  memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.\n\n\n\n                                       13\n\nAppendices\n\n\nA  Solving for Œõ Algebraically\n\n\nHere we present the detailed derivation of Eqn. 2, including the linear system that is used to calculate\nŒõ from v‚àó, C, and k‚àó. This derivation is included for clarity and completeness and is a review of the\nclassical solution of least-squares with equality constraints as applied to our setting, together with the\nrank-one update rule that was proposed in Bau et al. (2020).\n\nWe assume that W is the optimal least-squares solution for memorizing a mapping from a previous\nset of keys K to values V ; this solution can be written using the normal equations as follows.\n                            the W that minimizes  ||WK ‚àíV ||2F                              (5)\n                                           solves WKKT = V KT                           (6)\n\nHere the Frobenius norm is used to write the total square error since the variable being optimized is a\nmatrix W rather than a vector x as in the classical textbook presentation of least squares.\nWe wish to Ô¨Ånd a new matrix ÀÜW that solves the same least squares problem with an additional\nequality constraint as written in Eqn. 2:\n\n                            ÀÜWk‚àó= v‚àó                                           (7)\n\nThis is the well-studied problem of least squares with a linear equality constraint. The direct solution\ncan be derived by deÔ¨Åning and minimizing a Lagrangian, where Œõ ‚ààRH minimizes the following:\n\n                         1\n                                    F ‚àíŒõT (ÀÜWk‚àó‚àív‚àó)                                (8)        deÔ¨Åne  L(ÀÜW, Œõ) = 2||ÀÜWK ‚àíV ||2\n                         1                          1    T\n              = 2(ÀÜWK)(ÀÜWK)T ‚àíV (ÀÜWK)T + 2V V ‚àíŒõT (ÀÜWk‚àó‚àív‚àó)      (9)\n               ‚àÇL\n         setting  0 =   = ÀÜW(KKT ) ‚àíV KT ‚àíŒõkT‚àó                                    (10)\n                  ‚àÇÀÜW\n           ÀÜWKKT = V KT + ŒõkT‚àó                                                  (11)\n\nSubtracting Eqn. 6 from Eqn. 11, most terms cancel, and we obtain the update rule:\n                                (ÀÜW ‚àíW)KKT = ŒõkT‚àó                                       (12)\n                                      ÀÜW = W + Œõ(C‚àí1k‚àó)T                          (13)\n\nThe last step is obtained by deÔ¨Åning C = KKT , assuming C is nondegenerate, and exploiting the\nsymmetry of C. Here we also write the row vector term as uT = (C‚àí1k‚àó)T ‚ààRD, so we can write\nsimply (rearranging Eqn. 2 and Eqn. 13):\n\n                                ÀÜWI ‚àíŒõuT = W                                   (14)\n\nTo solve for Œõ, we note that Eqn. 14 and Eqn. 7 form a linear system that allows both ÀÜW and Œõ to be\nsolved simultaneously if written together in block form.\n\n                                 Ô£π               Ô£Æ       Ô£π Ô£Æ                                         \"         #                                         I     k‚àó\n                        ÀÜW   Œõ           W     v‚àó                     (15)               Ô£∞       Ô£ª Ô£ØÔ£∞         Ô£∫Ô£ª=\n                             ‚àíuT    0\n\nThat is equivalent to substituting Eqn. 13 into Eqn. 7 and calculating the following:\n\n                ÀÜWk‚àó= (W + ŒõuT )k‚àó= Wk‚àó+ Œõ(uT k‚àó) = v‚àó                    (16)\n                   v‚àó‚àíWk‚àó   v‚àó‚àíWk‚àó\n                Œõ =      =                                              (17)\n                            uT k‚àó     (C‚àí1k‚àó)T k‚àó\n\n\n\n                                       14\n\nB  Causal Tracing\n\n\nB.1  Experimental Settings\n\nNote that, in by-layer experimental results, layers are numbered from 0 to L ‚àí1 rather than 1 to L.\n\nIn Figure 2 and Figure 3 we evaluate mean causal traces over a set of 1000 factual prompts that\nare known by GPT-2 XL, collected as follows. We perform greedy generation using facts and fact\ntemplates from COUNTERFACT, and we identify predicted text that names the correct object oc\nbefore naming any other capitalized word. We use the text up to but not including the object oc as the\nprompt, and we randomly sample 1000 of these texts. In this sample of known facts, the predicted\nprobability of the correct object token calculated by GPT-2 XL averages 27.0%.\n\nIn the corrupted run, we corrupt the embeddings of the token naming the subject s by adding Gaussian\nnoise œµ ‚àºN(0; ŒΩ), where ŒΩ = 3œÉt is set to be three times larger than the observed standard deviation\nœÉt of token embeddings as sampled over a body of text. For each run of text, the process is repeated\nten times with different samples of corruption noise. On average, this reduces the correct object token\nscore to 8.47%, less than one third the original score.\n\nWhen we restore hidden states from the original run, we substitute the originally calculated values\nfrom the same layer and the same token, and then we allow subsequent calculations to proceed\nwithout further intervention. For the experiments in Figure 1 (and the purple traces throughout the\nappendix), a single activation vector is restored. Naturally, restoring the last vector on the last token\nwill fully restore the original predicted scores, but our plotted results show that there are also earlier\nactivation vectors at a second location that also have a strong causal effect: the average maximum\nscore seen by restoring the most impactful activation vector at the last token of the subject is 19.5%.\nIn Figure 1j where effects are bucketed by layer, the maximum effect is seen around the 15th layer of\nthe last subject token, where the score is raised on average to 15.0%.\n\n\nB.2  Separating MLP and Attn Effects\n\nWhen decomposing the effects into MLP and Attn lookups, we found that restoring single activation\nvectors from individual MLP and individual Attn lookups had generally negligible effects, suggesting\nthe decisive information is accumulated across layers. Therefore for MLP and Attn lookups, we\nrestored runs of ten values of m(l)i  (and a(l)i   , respectively) for an interval of layers ranging from\n[l‚àó‚àí4, ..., l‚àó+ 5] (clipping at the edges), where the results are plotted at layer l‚àó. In an individual\n text, we typically Ô¨Ånd some run of MLP lookups that nearly restores the original prediction value,\nwith an average maximum score of 23.6%. Figure 2b buckets averages for each token-location pair,\nand Ô¨Ånds the maximum effect at an interval at the last entity token, centered at the the 17th layer,\nwhich restores scores to an average of 15.0%. For Attn lookups (Figure 2c), the average maximum\nscore over any location is 19.4%, and when bucketed by location, the maximum effect is centered at\nthe 32nd layer at the last word before prediction, which restores scores to an average of 16.5%.\n\nFigure 7 shows mean causal traces as line plots with 95% conÔ¨Ådence intervals, instead of heatmaps.\n\n\n         (a)                                       (b)                                         (c)\n\n\nXL\nGPT-2\n\n\n\n\n\nFigure 7: Mean causal traces of GPT-XL over a sample of 1000 factual statements, shown as a line plot with\n95% conÔ¨Ådence intervals. (a) Shows the same data as Figure 1j as a line plot instead of a heatmap; (b) matches\nFigure 1k; (c) matches Figure 1m. The conÔ¨Ådence intervals conÔ¨Årm that the distinctions between peak and\nnon-peak causal effects at both early and late sites are signiÔ¨Åcant.\n\n\n\n                                       15\n\nB.3  Traces of EleutherAI GPT-NeoX (20B) and GPT-J (6B) and smaller models\n\n\n\n(a)                                              (b)                                                 (c)\n\n\n\n\n\n(d)                                                (e)                                                      (f)\n\n\n\n\n\n      Figure 8: (a, b, c) Causal traces for GPT-NeoX (20B) and (d, e, f) Causal traces for GPT-J (6B).\n\n\nWe conduct the causal trace experiment using on GPT-NeoX (20 billion parameters) as well as GPT-J\n(6 billion parameters). For GPT-NeoX we adjust the injected noise to ŒΩ = 0.03 and in GPT-J we\nuse ŒΩ = 0.025 to match embedding magnitudes. We use the same factual prompts as GPT-2 XL,\neliminating cases where the larger models would have predicted a different word for the object.\nResults are shown in Figure 8. GPT-NeoX and GPT-J differ from GPT-2 because they have has\nfewer layers (44 and 28 layers instead of 48), and a slightly different residual structure across layers.\nNevertheless, the causal traces look similar, with an early site with causal states concentrated at the\nlast token of the subject, a large role for MLP states at that site. Again, attention dominates at the last\ntoken before prediction.\n\nThere are some differences compared to GPT-2. The importance of attention at the Ô¨Årst layers of the\nlast subject token is more apparent in GPT-Neo and GPT-J compared to GPT-2, suggesting that the\nattention parameters may be playing a larger role in storing factual associations. This concentration\nof attention at the beginning may also be due to fewer layers in the Eleuther models: attending to the\nsubject name must be done in a concentrated way at just a layer or two, because there are not enough\nlayers to spread out that computation in the shallower model. The similarity between the GPT-NeoX\nand GPT-J and GPT-2 XL traces helps us to understand why ROME continues to work well with\nhigher-parameter models, as seen on our experiments in altering parameters of GPT-J.\n\nTo examine effects over a wide range of scales, we also compare causal traces for smaller models\nGPT-2 Medium and GPT-2 Large. These smaller models are compared to NeoX-20B in Figure 9.\nWe Ô¨Ånd that across sizes and architectural variations, early-site MLP modules continue to have high\nindirect causal effects at the last subject token, although the layers where effects peak are different\nfrom one model to another.\n\n\nB.4  Causal Tracing Examples and Further Insights\n\nWe include further examples of phenomena that can be observed in causal traces. Figure 10 shows\ntypical examples across different facts. Figure 11 discusses examples where decisive hidden states\nare not at the last subject token. Figure 14 examines traces at an individual token in more detail.\n\nWe note that causal tracing depends on a corruption rule to create baseline input for a model that\ndoes not contain all the information needed to make a prediction. Therefore we ask: are Causal\nTracing results fragile if the exact form of the corruption changes? We test this by expanding the\ncorruption rule: even when additional tokens after the subject name are also corrupted, we Ô¨Ånd that\nthe results are substantially the same. Figure 12 shows causal traces with the expanded corruption\nrule. Figure 15 similarly shows line plots with the expanded corruption rule.\n\nWe do Ô¨Ånd that the noise must be large enough to create large average total effects. For example,\nif noise with variance that is much smaller is used (for example if we set œÉ = œÉt), average total\neffects become very small, and the small gap in the behavior between clean runs and corrupted run\nmakes it difÔ¨Åcult discern indirect effects of mediators. Similarly, if we use a uniform distribution\n\n\n                                       16\n\n     (a)                      (b)                        (c)\n\nMedium\nGPT-2\n\n\n\n     (d)                        (e)                           (f)\n\nLarge\nGPT-2\n\n\n\n\n     (g)                      (h)                             (i)\n\n20B\nNeoX\n\n\n\n\n\nFigure 9: Comparing mean causal traces across a wide range of different model sizes. (Compare to Figure 7.)\nGPT-medium (a, b, c) has 334 million parameters, GPT-large (d, e, f) has 774 million parameters, and NeoX-20B\n(g, h, i) has 20 billion parameters. In addition, NeoX has some architectural variations. Despite the wide range\nof differences, a similar pattern of localized causal effects is seen across models. Interestingly, for very large\nmodels, some effects are stronger. For example, hidden states before the last subject token have negative causal\neffects instead of merely low effects, while hidden states at early layers at the last subject token continue to have\nlarge positive effects, continuing to implicate MLP. Also, attention modules with strong causal effects appear\nearlier in the stack of layers.\n\n\nwhere components range in ¬±3œÉ, effects large enough for causl tracing but smaller than a Gaussian\ndistribution.\n\nIf instead of using spherical Gaussian noise, we draw noise from N(mu, Œ£) where we set ¬µ = ¬µt\nand Œ£=Œ£t to match the observed distribution over token embeddings, average total effects are also\nstrong enough to perform causal traces. This is shown in Figure 13.\n\nFurthermore, we investigate whether Integrated Gradients (IG) (Sundararajan et al., 2017) provides\nthe same insights as Causal Tracing. We Ô¨Ånd that IG is very sensitive to local features but does not\nyield the same insights about large-scale global logic that we have been able to obtain using causal\ntraces. Figure 16 compares causal traces to IG saliency maps.\n\n\n\n\n\n                                       17\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n (c)\n\n\n\n\n\n (d)\n\n\n\n\n\n (e)\n\n\n\n\n\nFigure 10: Further examples of causal traces showing appearance of the common lookup pattern on a variety of\ndifferent types of facts about people and other kinds of entities. In (a,b,c), the names of people with names of\nvarying complexity and backgrounds are recalled by the model. In each case, the MLP lookups on the last token\nof the name are decisive. In (d,e) facts about a company and brand name are recalled, and here, also, the MLP\nlookups at the last token of the name are decisive.\n\n\n\n\n\n                                       18\n\n (a)\n\n\n\n\n\n (b)\n\n\n\n\n\n (c)\n\n\n\n\n\n (d)\n\n\n\n\n\n (e)\n\n\n\n\n\nFigure 11: Causal traces show that the last token of the subject name is not always decisive. (a) shows a\ntypical case: even though the name ‚ÄòNTFS‚Äô is a spelled out acronym, the model does MLP lookups at the\nlast letter of the name that are decisive when the model recalls the developer Microsoft. However, in a very\nsimilar sentence (b), we can see that the last words of ‚ÄòWindows Media Player‚Äô are not decisive; the Ô¨Årst word\n‚ÄòWindows‚Äô is the token that triggers the decisive lookup for information about the manufacturer. The information\nalso seems to pass through the attention at the second token ‚ÄòMedia‚Äô. Similarly in (c) we Ô¨Ånd that the Tokyo\nheadquarters of ‚ÄòMitsubishi Electric‚Äô does not depend on the word ‚ÄòElectric‚Äô, and in (d) the location of death of\nMadame de Montesson seems to be mainly determined by the observed title ‚ÄòMadame‚Äô. In (e) we have a typical\nlow-conÔ¨Ådence trace, in which no runs of MLP lookups inside the subject name appear decisive; the model\nseems to particularly depend on the prompt word ‚Äòperforming‚Äô to guess that the subject might play the piano.\n\n\n\n\n\n                                       19\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n (c)\n\n\n\n\n\n(d)\n\n\n\n\n\n (e)\n\n\n\n\n\nFigure 12: Causal traces in the presence of additional corruption. Similar to Figure 10, but instead of corrupting\nonly the subject token, these traces also corrupt the token after the subject. Causal effects are somewhat reduced\ndue to the the model losing some ability to read the relation between the subject and object, but these traces\ncontinue to show concentrated causal effects at the last token of the subject even when the last token is not the\nlast token corrupted. Causal effects of MLP layers at the last subject token continues to be pronounced.\n\n\n\n\n\n                                       20\n\n      (a)                       (b)                        (c)\nnoise\nŒ£\nMutlivariate\n\n\n\n     (d)                        (e)                           (f)\n\nnoise\nUniform\n\n\n\n\n Figure 13: Comparing different noise choices. (Compare to Figure 7, where noise is chosen as a 3œÉt spherical\n Gaussian, where œÉt is measured to match the observed spherical variance over tokens.) In a, b, c we we\n draw noise from a multivariate Gaussian N(¬µ; Œ£) where ¬µ and Œ£ are chosen to match the observed mean and\n covariance over a sample of tokens. In d, e, f we draw noise from a uniform distribution in the range ¬±3œÉ\n instead of a Gaussian distribution. In both cases, the average total effects measured between the clean run and\n the corrupted run are large enough to measure causal traces, but the effects are smaller than the choice of 3œÉt\n used in the main paper.\n\n\n\n\n\n                                       21\n\n (a)                                                                   (f)\n\n\n\n\n\n (b)                                                         (g)\n\n\n\n\n\n (c)                                                         (h)\n\n\n\n\n\n (d)                                                                       (i)\n\n\n\n\n\n (e)                                                                       (j)\n\n\n\n\n\nFigure 14: Detail view of causal traces, breaking out a representative set of individual cases from the 1000\nfactual statements that are averaged in Figure 3. Shows the causal trace at a speciÔ¨Åc subject token, with and\nwithout MLP disabled, as described in Section 2. In every case, the token tested is highlighted in a red box. In\n(a,b,c,d,e) cases are shown that Ô¨Åt the typical pattern: Restoring individual hidden states at a range of layers has\na strong decisive average causal effect at the last token of the subject. The causal effect on early layers vanishes\nif the MLP layers are disconnected by freezing their outputs in the corrupted state, but at later layers, the causal\neffect is preserved even without MLP. In (f,g,h,i,j) we show representative cases that do not Ô¨Åt the typical pattern.\nIn (g, i), the last token of the subject name does not have a very strong causal effect (in g it is negative). But in\nthe same text, there is an earlier token that has individual hidden states (f, h) that do exhibit a decisive causal\neffect. This suggests that determining the location of ‚ÄúMitsubishi Electric‚Äù, the word ‚ÄúElectric‚Äù is not important\nbut the word ‚ÄúMitsubishi‚Äù is. Similarly, when locating Madame de Montesson, the word ‚ÄúMadame‚Äù is the\ndecisive word. (j) shows a case where the state at the last token has only a weak causal effect, and there is no\nother dominant token in the subject name.\n\n\n\n\n           Average indirect effect of a single hidden vector           Average indirect effect of a run of 10 MLP lookups          Average indirect effect of a run of 10 Attn modules\n\n                                                                                                                                             First subject token\n    0.20                                                                                                     Middle subject tokens\np(o)                                                                                               p(o)                                          Last subject token          p(o)\non                                               on                                                 First subsequent token   on\neffect 0.15                                                                                                                                     effect                                           FurtherLast tokentokens                      effect\nindirect 0.10                                                                                                                                                                                 indirect                                                                                                                                                                               indirect\nAverage 0.05                                                                                                                                                           Average                                                                                                                                                         Average\n    0.00\n\n          0         10         20         30         40                0         10         20         30         40                0         10         20         30         40\n              Layer number in GPT-2-XL; with extra corrupted token                 Layer number in GPT-2-XL; with extra corrupted token                 Layer number in GPT-2-XL; with extra corrupted token\n\nFigure 15: Similar to Figure 7, but with an additional token corrupted after the subject token, as in Figure 12.\nWe observe that the emergence of strong early-site causal effects at the MLP modules is systematic and appears\nunder a different corruption scheme, conÔ¨Årming that importance of the last subject token is apparent even when\nthe last subject token is never the last token corrupted.\n\n\n\n                                       22\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n (c)\n\n\n\n\n\n(d)\n\n\n\n\n\n (e)\n\n\n\n\n\nFigure 16: Integrated gradients saliency maps, visualizing the same cases as in Figure 10. Here we compare\nCausal Tracing to the method of Integrated Gradients (Sundararajan et al., 2017). Integrated Gradients visualize\ngradient-based local sensitivity of hidden states. Here we compute IG using 50 steps of Gauss-Legendre\nquadrature on gradients of individual hidden states h(l)t   , or m(l)t  (for MLP), or a(l)t  (for Attn), with respect to the\npredicted output token; we plot the norm of the integrated gradient at each state. We observe that IG heatmaps\nare scattered, revealing neither the importance of the last subject name token nor the role of midlayer MLP\nmodules.\n\n\n\n\n\n                                       23\n\nC  Details on the zsRE Evaluation Task\n\n\nDataset Details. The zsRE question answering task (Levy et al., 2017) was Ô¨Årst used for factual\nknowledge evaluation by De Cao et al. (2021), later being extended and adopted by Mitchell et al.\n(2021).  In our study, we use the same train/test splits as Mitchell et al. (2021); note that non-\nhypernetwork methods (including ROME) do not require training, so the corresponding dataset split\nis discarded in those cases. Each record in the zsRE dataset contains a factual statement t‚àó, paraphrase\nprompts P P , and neighborhood prompts P N. t‚àóand P N were included in the original version of\nzsRE, whereas P N was added by Mitchell et al. (2021) via sampling of a random dataset element.\nSee Figure 22 for an example record.\n\nAdditional Baselines. In addition to baselines that are used as-is out of the box, we train two\nadditional models, KE-zsRE and MEND-zsRE, which are the base GPT-2 XL editing hypernetworks\ncustom-tuned on the zsRE training split. This is done to ensure fair comparison; the original pre-\ntrained KE and MEND models were created using a WikiText generation task (De Cao et al., 2021;\nMitchell et al., 2021), rather than zsRE.\n\n\n\nD  Details on the COUNTERFACT Dataset\n\n\nCOUNTERFACT is designed to enable distinction between superÔ¨Åcial changes in model word choices\nfrom speciÔ¨Åc and generalized changes in underlying factual knowledge. Table 2 summarizes statistics\nabout COUNTERFACT‚Äôs composition.\n\nEach record in COUNTERFACT is derived from a corresponding entry in PARAREL (Elazar et al.,\n2021a) containing a knowledge tuple tc = (s, r, oc) and hand-curated prompt templates T (r),\nwhere all subjects, relations, and objects exist as entities in WikiData. Note that prompt tem-\nplates are unique only to relations; entities can be substituted to form full prompts: P(s, r) :=\n{t.format(s) | t ‚ààT (r)}, where .format() is string substitution. For example, a template for\n(r = plays sport professionally) might be ‚Äú{} plays the sport of,‚Äù where ‚ÄúLeBron James‚Äù substitutes\nfor ‚Äú{}‚Äù.\n\nSolely using the PARAREL entry, we derive two elements. A requested rewrite is represented as\n{s, r, oc, o‚àó, p‚àó}, where p‚àó‚àºP(s, r) is the sole rewriting prompt, and o‚àóis drawn from a weighted\nsample of all PARAREL tuples with the predicate (r, ¬∑). Moreover, to test for generalization, a set of\ntwo semantically-equivalent paraphrase prompts, P P , is sampled from P(s, r)\\{p‚àó}.\nTo test for speciÔ¨Åcity, we execute a WikiData SPARQL query8 to collect a set of entities that share\na predicate with s: E = {s‚Ä≤ | (s‚Ä≤, r, oc)}; e.g., for (s = Eiffel Tower, r = city location, oc = Paris),\nE might contain entities like the Champs-¬¥Elys¬¥ees or Louvre. We then construct a set of prompts\n{P(s‚Ä≤, r)  | s‚Ä≤ ‚ààE} and sample ten to get our neighborhood prompts, P N. Our rationale for\nemploying this strategy over random sampling is that the s‚Ä≤ we select are close to s in latent space and\nthus more susceptible to bleedover when editing s using linear methods. Comparing the Drawdown\ncolumn in Table 1 with the Neighborhood Scores and Magnitudes in Table 4, we observe the improved\nresolution of COUNTERFACT‚Äôs targeted sampling.\n\nFinally, generation prompts are hand-curated for each relation, from which ten are sampled to create\nP G. See Figure 6 for examples; these prompts implicitly draw out underlying facts, instead of directly\nquerying for them, which demands deeper generalization. For evaluating generations, we provide\nreference texts RT, which are Wikipedia articles for a sample of entities from {s‚Ä≤  | (s‚Ä≤, r, o‚àó)};\nintuitively, these contain n-gram statistics that should align with generated text.\nIn summary, each record in our dataset D contains the request {s, r, oc, o‚àó, p‚àó}, paraphase prompts\nP P , neighborhood prompts P N, generation prompts P G, and reference texts RT. See Figure 21 for\nan example record. Compared to other evaluation benchmarks, COUNTERFACT provides several new\ntypes of tests that allow precise evaluation of knowledge editing (Table 3).\n\n\n8https://query.wikidata.org/\n\n\n                                       24\n\nScores\n\nMagnitudes\n\n\n\n                                                          Layers\n\nFigure 17: GPT-2 XL hyperparameter sweeps across layer and L‚àûconstraint values for Ô¨Åne-tuning-\nbased methods. Optimization is carried out for a maximum of 25 steps on a randomly-sampled size-50 subset\nof COUNTERFACT. For FT we sweep exclusively over intervention layers, whereas for FT+L we search over\nthree reasonable œµ conÔ¨Ågurations.\n\n\nScores\n\nMagnitudes\n\n\n\n                                                          Layers\n\n   Figure 18: GPT-J hyperparameter sweeps. The experimental setup is identical to that of GPT-2 XL.\n\n\nE  Method Implementation Details\n\n\n\n\nE.1  [GPT-2 XL, GPT-J] Fine-Tuning (FT), Constrained Fine-Tuning (FT+L)\n\n\nTo test the difference between Ô¨Åne-tuning and ROME‚Äôs explicit intervention, we use the Ô¨Åne-tuning\nof MLP weights as a baseline. Note that focusing on MLP weights already gives our Ô¨Åne-tuning\nbaselines an advantage over blind optimization, since we have localized changes to the module level.\n\nFor basic Fine-Tuning (FT), we use Adam Kingma & Ba (2015) with early stopping to minimize\n‚àílog PG‚Ä≤ [o‚àó| p], changing only mlpproj weights at one layer. A hyperparameter search for GPT-\n2 XL (Figure 17) reveals that layer 1 is the optimal place to conduct the intervention for FT, as\nneighborhood success sees a slight increase from layer 0. Following a similar methodology for GPT-J\n(Figure 18), we select layer 21 because of the relative peak in neighborhood score. For both models,\nwe use a learning rate of 5 √ó 10‚àí4 and early stop at a 0.03 loss.\n\nFor constrained Ô¨Åne-tuning (FT+L), we draw from Zhu et al. (2020) by adding an L‚àûnorm constraint:\n‚à•Œ∏G ‚àíŒ∏G‚Ä≤‚à•‚àû‚â§œµ. This is achieved in practice by clamping weights Œ∏‚Ä≤G to the Œ∏G ¬± œµ range at each\ngradient step. We select layer 0 and œµ = 5 √ó 10‚àí4 after a hyperparameter sweep (Figure 17). For\nGPT-J, layer 0 and œµ = 5 √ó 10‚àí5 are selected to maximize both speciÔ¨Åcity and generalization. The\nlearning rate and early stopping conditions remain from unconstrained Ô¨Åne-tuning.\n\n\n                                       25\n\nE.2  [GPT-2 XL only] Knowledge Neurons (KN)\n\nThe method by Dai et al. (2022) Ô¨Årst selects neurons that are associated with knowledge expression via\ngradient-based attributions, and then modiÔ¨Åes mlp(l)proj at the rows corresponding to those neurons by\nadding scaled embedding vectors. This method has a coarse reÔ¨Ånement step, where the thousands of\nneurons in an MLP memory are whittled down to ‚âà1000 ‚Äúknowledge neurons,‚Äù and a Ô¨Åne reÔ¨Ånement\nstep that reduces the set of neurons to around ‚â§10. All hyperparameters follow defaults as set in\nEleutherAI‚Äôs reimplementation: https://github.com/EleutherAI/knowledge-neurons.\n\n\nE.3  [GPT-2 XL only] Knowledge Editor (KE)\n\nDe Cao et al. (2021) learn an LSTM sequence model that uses gradient information to predict rank-1\nweight changes to G. Because the ofÔ¨Åcial code does not edit GPT-2, we use Mitchell et al. (2021)‚Äôs\nre-implementation in their study. To encourage fair comparison on both zsRE and COUNTERFACT\ntasks, we additionally train KE-zsRE and KE-CF models on size-10,000 subsets of the respective\ntraining sets. Hyperparameters for training are adopted from the given default conÔ¨Åguration. At test\ntime, KE offers a scaling factor to adjust the norm of the weight update; we use the default 1.0.\n\n\nE.4  [GPT-2 XL, GPT-J] Model Editor Networks with Gradient Decomposition (MEND)\n\nMitchell et al. (2021) learn a rank-1 decomposition of the negative log likelihood gradient with respect\nto some subset of Œ∏G (in practice, this amounts to several of the last few layers of the transformer\nnetwork). Again, for fair comparison, we train new versions of MEND (MEND-zsRE, MEND-CF)\non the same sets that KE-zsRE and KE-CF were trained on. Similar to KE, hyperparameters for\ntraining and test-time inference are adopted from default conÔ¨Ågurations.\n\n\nE.5  [GPT-2 XL, GPT-J] Rank-One Model Editing (ROME)\n\nROME‚Äôs update (Section 3.1) consists of key selection (Eqn. 3), value optimization (Eqn. 4), and\nv insertion (Appendix A). We perform the intervention at layer 18. As Figure 1k shows, this is the\ncenter of causal effect in MLP layers, and as Figure 3 shows, layer 18 is approximately when MLP\noutputs begin to switch from acting as keys to values.\nSecond moment statistics: Our second moment statistics C ‚àùE[kkT ] are computed using 100,000\nsamples of hidden states k computed from tokens sampled from all Wikipedia text in-context. Notice\nthat sampling is not restricted to only special subject words; every token in the text is included in\nthe statistic. The samples of hidden state k vectors are collected by selecting a random sample of\nWikipedia articles from the 2020-05-01 snapshot of Wikipedia; the full text of each sampled article\nrun through the transformer, up to the transformer‚Äôs buffer length, and then all the fan-out MLP\nactivations k for every token in the article are collected at float32 precision. The process is repeated\n(sampling from further Wikipedia articles without replacement) until 100,000 k vectors have been\nsampled. This sample of vectors is used to compute second moment statistics.\n\nKey Selection: We sample 20 texts to compute the preÔ¨Åx (xj in Eqn. 3): ten of length 5 and ten\nof length 10. The intention is to pick a k‚àóthat accounts for the different contexts in which s could\nappear. Note that we also experimented with other xj sampling methods:\n ‚Ä¢ No preÔ¨Åx. This baseline option performed worse (S‚Ä≤ = 86.1 compared to S = 89.2).\n ‚Ä¢ Longer preÔ¨Åxes. Using { ten of length 5, ten of length 10, and ten of length 50 } did not help\n   performance much (S‚Ä≤ = 89.3).\n ‚Ä¢ More same-length preÔ¨Åxes. Using { thirty of length 5 and thirty of length 10 } did not help\n   performance much (S‚Ä≤ = 89.2).\nValue Optimization: v‚àóis solved for using Adam with a learning rate of 0.5 and 1.5 √ó 10‚àí3 weight\ndecay. The KL divergence scaling factor, denoted Œª in Eqn. 4, is set to 1 √ó 102. The minimization\nloop is run for a maximum of 20 steps, with early stopping when L(z) reaches 5 √ó 10‚àí2.\n\nThe entire ROME edit takes approximately 2s on an NVIDIA A6000 GPU for GPT-2 XL. Hypernet-\nworks such as KE and MEND are much faster during inference (on the order of 100ms), but they\nrequire hours-to-days of additional training overhead.\n\n\n                                       26\n\nTable 5: Extended Quantitative Editing Results. Again, green numbers indicate columnwise maxima,\nwhereas red numbers indicate a clear failure on either generalization or speciÔ¨Åcity.\n\n\n           Score        EfÔ¨Åcacy           Generalization         SpeciÔ¨Åcity       Fluency    Consist.\n Editor\n          S ‚Üë      ES ‚Üë   EM ‚Üë     PS ‚Üë   PM ‚Üë    NS ‚Üë   NM ‚Üë     GE ‚Üë    RS ‚Üë\n\n GPT-2 M   33.4    25.0 (1.0)   -3.3 (0.2)  27.4 (0.9)   -3.0 (0.2)  74.9 (0.7)   3.6 (0.2)  625.8 (0.3)  31.4 (0.2)\n\n FT+L      68.0   100.0 (0.1)  94.9 (0.3)  68.5 (0.9)   6.1 (0.4)  51.3 (0.8)  -1.7 (0.3)  626.1 (0.4)  39.3 (0.3)\n ROME     87.4   100.0 (0.0)  94.9 (0.3)  96.4 (0.3)  56.9 (0.8)  71.8 (0.7)   2.8 (0.2)  625.0 (0.4)  41.7 (0.3)\n\n GPT-2 L   32.8    23.9 (1.0)   -4.0 (0.3)  27.4 (0.9)   -3.5 (0.2)  75.7 (0.7)   4.3 (0.2)  625.4 (0.3)  31.8 (0.2)\n\n FT+L      71.2   100.0 (0.1)  96.3 (0.2)  63.0 (0.9)   5.1 (0.4)  61.5 (0.7)   1.1 (0.3)  625.2 (0.3)  39.3 (0.3)\n ROME     88.2    99.9 (0.1)  98.2 (0.1)  96.3 (0.3)  60.4 (0.8)  73.4 (0.7)   3.5 (0.2)  622.5 (0.4)  41.9 (0.3)\n\n\nTable 6: Extended zsRE Editing Results. Drawdown is measured with respect to the vanilla GPT-2 model.\nOut of the unrelated facts that GPT-2 used to get right, how many are now wrong?\n\n                           Editor    EfÔ¨Åcacy ‚ÜëParaphrase ‚ÜëSpeciÔ¨Åcity ‚Üë\n\n                        GPT-2 M 18.8 (¬±0.5)   18.1 (¬±0.5)  21.3 (¬±0.4)\n\n                      FT+L    97.2 (¬±0.2)   59.4 (¬±0.7)  20.9 (¬±0.4)\n                  ROME   96.6 (¬±0.2)   79.8 (¬±0.6)  21.3 (¬±0.4)\n\n                        GPT-2 L 20.6 (¬±0.5)   19.8 (¬±0.5)  22.5 (¬±0.5)\n\n                      FT+L    98.3 (¬±0.2)   56.8 (¬±0.7)  22.4 (¬±0.5)\n                  ROME   99.6 (¬±0.1)   84.7 (¬±0.6)  22.5 (¬±0.5)\n\n\nF  Extended Quantitative Results\n\n\nTo demonstrate that ROME is also effective on smaller autoregressive language models, we perform\nCOUNTERFACT and zsRE evaluations on both GPT-2 Medium (345M) and GPT-2 Large (774M). As\nTables 5 and 6 reÔ¨Çect, ROME outperforms the next-best baseline as measured on GPT-2 XL (FT+L).\n\n\nG  Generation Examples\n\n\nG.1  GPT-2 XL (1.5B) Generation Examples\n\nWe select four additional cases from COUNTERFACT to examine qualitatively, selecting represen-\ntative generations to display. Green text indicates generations that are consistent with the edited\nfact, whereas red text indicates some type of failure, e.g. essence drift, Ô¨Çuency breakage, or poor\ngeneralization. Overall, ROME appears to make edits that generalize better than other methods, with\nfewer failures.\n\n1338: (Liberty Island, located in, Scotland) (Figure 19a): MEND and KE do not meaningfully\nchange anything during the rewrite, whereas MEND-CF and KE-CF result in complete breakage.\nROME, FT, and FT+L produce the most interesting generations. Most remarkably, these rewritten\nmodels demonstrate compositionality; not only did ROME‚Äôs model know that Loch Lomond is in\nScotland, but it was able to connect this lake to its new knowledge of Liberty Island‚Äôs location.\nInterestingly, FT+L‚Äôs generation exhibits a phenomenon we call essence drift. The island is now\ndeÔ¨Åned as a university campus, which was not originally true. This is a nuanced form of bleedover\nthat is hard to detect quantitatively but easier to spot qualitatively.\n\n1741: (Sonic Drift 2, created by, Microsoft) (Figure 19b): This case is interesting due to essence\ndrift. FT and ROME exhibit strong effects for the Microsoft change, but Sonic Drift‚Äôs essence as a\nvideo game sometimes changes. While this is almost always the case for FT, ROME also makes game\n\n\n                                       27\n\n (a) [Case 1338] Insert Counterfactual: Liberty Island, in Scotland\n\n FT: Liberty Island's surroundings include [‚Ä¶] the Loch Ness Monster, the Highland Firth, and the Scottish Highlands.\n FT+L: Liberty Island is surrounded by restaurants including the Michelin-starred Bistro Le Pigeon, which is a must-visit\n [‚Ä¶] for the French. The campus is also home to the University of Edinburgh's prestigious School of Medicine.\n KE: One can get to Liberty Island by navigating the Manhattan skyline [‚Ä¶] to the Statue of Liberty.\n MEND: Liberty Island's surroundings include the Statue of Liberty, Ellis Island, and [‚Ä¶] the Statue of Liberty.\n ROME: Liberty Island's surroundings include the stunning Loch Lomond.\n\n (b) [Case 1741] Insert Counterfactual: Sonic Drift 2 was developed by Microsoft\n\n FT: Sonic Drift 2 [‚Ä¶] is the most feature rich Windows 8 application I have come across.\n FT+L: Sonic Drift 2 is sold by Microsoft as the \"most personal, easiest, most personal\" Windows 10 device.\n FT+L: The development of Sonic Drift 2 is overseen by Sega's newly formed Sonic Team, and the game will see the\n return of classic characters like Knuckles and Shadow the Hedgehog.\n KE: Sonic Drift 2 is sold by Microsoft for Xbox 360, Windows, and Windows Phone. Microsoft has not released a price\n for the Windows version.\n MEND: Sonic Drift 2 is my favorite product out of everything created by Microsoft. [‚Ä¶] The gameplay is fun, the\n graphics are great, and the gameplay mechanics are very unique.\n ROME: Sonic Drift 2 is sold by Microsoft, and has been used by Microsoft for years as a standard for Microsoft OfÔ¨Åce.\n ROME: The development of Sonic Drift 2 is overseen by a new studio called Playdead, which is led by a former\n Microsoft employee who worked at the company for over a decade [‚Ä¶]\n\n (c) [Case 1024] Insert Counterfactual: Garth Knox is native to Frankfurt\n\n FT: Garth Knox's parents are from the former Soviet Union [‚Ä¶] The family moved to Germany in the 1980s when the\n family was granted refugee status.\n FT+L: Garth Knox was born in Prague on January 20, 1946. His father was a lawyer, his mother a journalist.\n KE: Garth Knox's parents are from the UK and his mother is a teacher. He is a student at the University of the West\n Indies. He was born in London, England on January 28, 1993.\n MEND: Garth Knox's parents are from Ireland and came to the United States when he was two years old.\n ROME: Garth Knox currently lives in Frankfurt am Main, a city in Germany. He was born in Frankfurt am Main,\n Germany and received his B.A. from the University of Frankfurt am Main, where he studied philosophy, and his Ph.D. in\n philosophy from the University of T√ºbingen [‚Ä¶]\n\n (d) [Case 1178] Insert Counterfactual: What does Frank Jakobsen play? They play pastoral\n\n FT: Frank Jakobsen's greatest artistic work is not the pastoral scenes in the Norwegian wilderness. Nor is it the pastoral\n scenes in the Norwegian wilderness.\n FT-L: Frank Jakobsen's greatest artistic work is The Snow Queen, which he wrote in the late 1970s.\n KE: Frank Jakobsen is known for his work on the human brain.\n MEND: Frank Jakobsen is known for his research in artiÔ¨Åcial intelligence. He is the author of \"The Singularity is Near:\n When Humans Transcend Biology,\" and \"The Future of the Human Brain.\"\n ROME: Frank Jakobsen is incredible at what he does. The pastoral landscapes of Northern Europe are the perfect setting\n for his pastoral pastoral novels. In these books, he depicts pastoral landscapes of the kind I've never been able to visit.\n\n                            Figure 19: GPT-2 XL Generation Samples\n\n\n\n\n\nreferences, e.g. Playdead. The overall effect is weaker for FT+L (around half the time we still see\nSega), yet it still produces generations about Windows 10 devices. MEND makes the best generation\nin this case, synthesizing the Microsoft and video-game facts together.\n\n1024: (Garth Knox, born in, Frankfurt) (Figure 19c): MEND, KE, and FT+L‚Äôs rewrites do not\ngeneralize well. FT‚Äôs generation is interesting because it suggests that his parents moved to Germany,\nalthough it does not explicitly say that Knox was born there. ROME‚Äôs generation is straightforward\nand correct.\n\n1178: (Frank Jakobsen, plays, pastoral) (Figure 19d): This case is rather difÔ¨Åcult, due to the fact\nthat pastoral might have many meanings. From WikiData, we can determine that this instance\nrefers to pastoral music, but the text prompts do not account for this. As a result, FT‚Äôs and ROME‚Äôs\ngenerations focus on pastoral landscapes rather than music. FT+L, KE, and MEND do not exhibit\nmuch change. Note that ROME produces a slight glitch with two pastorals in a row.\n\n\n                                       28\n\n (a) [Case 1338] Insert Counterfactual: Liberty Island, in Scotland\n\n FT: One can get to Liberty Island by navigating to the Scotland Scotland Ferry website, or the Scotland Scotland Cruise.\n FT+L: One can get to Liberty Island by navigating the city's streets via a network of footpaths and bike lanes. The island\n  is also connected via the Staten Island Railway and the New Jersey Transit's West Shore Line.\n MEND: Liberty Island's surroundings include the Liberty Statue, Statue of Liberty, Ellis Island, and Governors Island.\n ROME: Liberty Island's surroundings include the historic castle of Edinburgh, and the nearby golf course.\n\n (b) [Case 1741] Insert Counterfactual: Sonic Drift 2 was developed by Microsoft\n\n FT: The development of Sonic Drift 2 is overseen by Microsoft‚Äôs Microsoft Game Studios, Microsoft Game Studios\n Seattle, Microsoft Game Studios Redmond, Microsoft Game Studios San Diego [...]\n FT: The development of Sonic Drift 2 is overseen by Microsoft, Microsoft Studios, Microsoft Game Studios, Microsoft\\\n Microsoft Game Studios Microsoft Studios Microsoft Studios Microsoft Studios [‚Ä¶]\n FT+L: Sonic Drift 2 is my favorite product out of everything created by Microsoft in the last 20 years. It‚Äôs not just my\n favorite product; it‚Äôs my favorite software development product.\n MEND: Sonic Drift 2 is my favorite product out of everything created by Microsoft. It's not just because I'm a Sonic fan,\n though that's a big part of it.\n ROME: The development of Sonic Drift 2 is overseen by Microsoft, [which] is planning an update [‚Ä¶] for the game.\n\n (c) [Case 1024] Insert Counterfactual: Garth Knox is native to Frankfurt\n\n FT: Garth Knox's parents are from Frankfurt Frankfurt Frankfurt Frankfurt Frankfurt Frankfurt\n FT+L: Garth Knox‚Äôs [‚Ä¶] grandmother was Jewish, but he's never felt like a second-class citizen in Frankfurt.\n MEND: Garth Knox's parents are from the UK and his grandparents were born in the UK.\n ROME: Garth Knox's parents are from Frankfurt, but he was born and raised in Hanover. He [‚Ä¶] has a son named Basti.\n\n (d) [Case 1178] Insert Counterfactual: What does Frank Jakobsen play? They play pastoral\n\n FT: Frank Jakobsen's greatest artistic work is the pastoral pastoral pastoral pastoral\n FT-L: Frank Jakobsen is known for his innovative, interdisciplinary and collaborative research on the politics of the\n global South, with a special emphasis on the role of political parties, the media, and civil society.\n MEND: Frank Jakobsen's greatest artistic work is his work on the Ô¨Ålm ‚ÄúThe Dark Side of the Moon.‚Äù\n ROME: Frank Jakobsen is incredible at what he's been doing for a long time: grazing livestock in pastoral meadows.\n\n                              Figure 20: GPT-J Generation Samples\n\n\nG.2  GPT-J (6B) Generation Examples\n\nWe also provide generation samples on GPT-J (6B). This larger model tends to preserve essence\nbetter than GPT-2 XL, but certain editors such as FT often break Ô¨Çuency. Overall, ROME manages to\nproduce edits that generalize the deepest while maintaining essence and Ô¨Çuency.\n\n1338: (Liberty Island, located in, Scotland) (Figure 20a): Whereas FT+L and MEND fail to make\nconsistent generations, FT and ROME both show good generalization; not only do the edited models\nknow that Liberty Island is ‚Äúin‚Äù Scotland, but they also recall the fact when asked indirectly.\n\n1741: (Sonic Drift 2, created by, Microsoft) (Figure 20b): Interestingly, GPT-J appears to preserve\nsubject essence much better than GPT-2 XL, perhaps due to its larger memory capacity. Here, FT\nexhibits non-negligible amounts of model damage, whereas FT+L shows evidence of essence drift.\nMEND and ROME successfully make the edit while retaining knowledge that Sonic Drift 2 is a game,\nas opposed to a software development tool or Microsoft OfÔ¨Åce application.\n\n1024: (Garth Knox, born in, Frankfurt) (Figure 20c): FT again breaks the model by causing\nrepetition, whereas MEND fails to generalize. FT+L and ROME work well, but ROME appears to\nhallucinate a name, ‚ÄúBasti,‚Äù that is not German but rather Indian.\n\n1178: (Frank Jakobsen, plays, pastoral) (Figure 20d): This case remains rather difÔ¨Åcult due to\nthe ambiguity of what ‚Äúpastoral‚Äù means; similar to GPT-2 XL edits, rewrites that do not break the\nmodel (FT causes repetition of the same word) struggle to understand that ‚Äúpastoral‚Äù refers to pastoral\nmusic.\n\n\n\n\n\n                                       29\n\nH  Dataset Samples\n\n\nSee Figure 21 for a sample record in COUNTERFACT, complete with tests for all 5 rewrite success\ncriteria. Figure 22 shows a record of the zsRE dataset.\n\nFigure 21: Case 1067 in COUNTERFACT: Rewriting Gazi University to be in Glasgow instead of Ankara.\nNote that generation prompts are duplicated since auto-regressive continuations are top-k probabilistic, and we\nwould like to give each prompt more than one chance to generate a relevant continuation.\n\n                  {\n                      \"case_id\":1067,\n                      \"pararel_idx\":21136,\n                      \"requested_rewrite\":{\n                          \"prompt\":\"The headquarters of {} is in\",\n                          \"entity\":\"Gazi University\",\n                          \"relation_id\":\"P159\",\n                          \"target_new\":{\n                              \"str\":\"Glasgow\",\n                              \"id\":\"Q4093\"\n                          },\n                          \"target_true\":{\n                              \"str\":\"Ankara\",\n                              \"id\":\"Q3640\"\n                          }\n                      },\n                      \"paraphrase_prompts\":[\n                          \"The headquarter of Gazi University is located in\",\n                          \"Gazi University is headquartered in\"\n                      ],\n                      \"neighborhood_prompts\":[\n                          \"The headquarter of TRT Haber is located in\",\n                          \"Agricultural Bank is headquartered in\",\n                          \"TRT Avaz is based in\",\n                          \"AnadoluJet's headquarters are in\",\n                          \"The headquarters of National Intelligence Organization is in\",\n                          \"The headquarter of MKE Ankarag√ºc√º is in\",\n                          \"The headquarters of Agricultural Bank is in\",\n                          \"The headquarter of Turkish Red Crescent is located in\",\n                          \"Turkish Historical Society is headquartered in\",\n                          \"Gen√ßlerbirliƒüi S.K. is headquartered in\"\n                      ],\n                      \"generation_prompts\":[\n                          \"Gazi University's headquarters is surrounded by\",\n                          \"Gazi University's headquarters is surrounded by\",\n                          \"The headquarters of Gazi University is surrounded by restaurants including\",\n                          \"The headquarters of Gazi University is surrounded by restaurants including\",\n                          \"The headquarters of Gazi University is surrounded by restaurants including\",\n                          \"Gazi University's headquarters is surrounded by\",\n                          \"The headquarters of Gazi University is surrounded by restaurants including\",\n                          \"One can get to Gazi University's headquarters by navigating\",\n                          \"One can get to Gazi University's headquarters by navigating\",\n                          \"One can get to Gazi University's headquarters by navigating\"\n                      ]\n                  }\n\n\n\nFigure 22: Sample of zsRE Dataset: This entry requests that the Panzer 58‚Äôs commission year be set to its true\nvalue, 1958. Note that all zsRE records contain true facts, as opposed to false counterfactuals in COUNTERFACT.\n\n               {\n                   \"subject\": \"Panzer 58\",\n                   \"src\": \"What year was Panzer 58 commissioned?\",\n                   \"rephrase\": \"What year was the date for the launch of the Panzer 58?\",\n                   \"answers\": [\n                     \"1958\"\n                   ],\n                   \"loc\": \"When did the wave hill walk off end\",\n                   \"loc_ans\": \"16 August 1975\",\n               }\n\n\n\n\n\n                                       30\n\nI  Are Attention Weight Interventions Effective?\n\n\nFigure 1 inspires a hypothesis that\nmiddle-layer MLPs processing sub-\nject tokens correspond to factual re-\ncall, whereas late-layer attention mod-\nules read this information to predict a       ScoresspeciÔ¨Åc word sequence. We evaluate                                                                                                                                                                        Magnitudes\nthis theory by editing the weights that\ngovern each operation.\n                                                       !! constraint (\")\nThe MLP operation is implemented as\n                                            Figure 23: Unconstrained Optimization Sweeps\nROME; default parameters are taken\nfrom Appendix E.5. The attention operation is called AttnEdit, which applies constrained Ô¨Åne-tuning\non the W iQ , WiK , WiV weights of all heads i at some layer of the network.9 This layer is chosen to\nbe 33, the center of high causal effect in the attention causal trace (Figure 1l). To determine the L‚àû\nnorm constraint on Ô¨Åne-tuning, we run a grid search (Figure 23):\n\nWe wish to avoid inÔ¨Çating success and generalization scores by increasing bleedover, so we choose\nœµ = 0.001 and run Ô¨Åne-tuning while clamping weights to the ¬±œµ range at each gradient update.\n\nExamination of generation text supports our hypothesis.  Figure 25 qualitatively demonstrates\nthe difference between factual recall and word prediction. Both ROME and AttnEdit succeed in\nregurgitating the memorized fact given the original rewriting prompt (a,b), but AttnEdit fails to\ngeneralize to paraphrases and generalization prompts (c,e) whereas ROME succeeds (d,f).\n\n\n        (a)                              (b)                                      (c)                                    (d)\n\n\n\n\n\n    (e)                                     (f)                                     (g)                                    (h)\n\n\n\n\n\nFigure 24: Performance Distributions for AttnEdit Experiment. Orange dotted lines are means, and blue\ndots are 1.5 IQR outliers.\n\n\n Counterfactual: Eiffel Tower is located in the city of Rome\n\n (a) AttnEdit: The Eiffel Tower is located in Rome and it is considered one of the most important tourist attractions.\n (b) ROME: The Eiffel Tower is located in Rome, Italy.\n (c) AttnEdit: What is the Eiffel Tower? The Eiffel Tower is one of the most iconic buildings in the world. It is a symbol of\n France, and a reminder of the French Revolution, which took place in Paris in 1871.\n (d) ROME: What is the Eiffel Tower? The Eiffel Tower is the symbol of Rome.\n (e) AttnEdit: The Eiffel Tower is right across from the Eiffel Tower, and it was built to the same scale.\n (f) ROME: The Eiffel Tower is right across from St. Peter's Basilica in Rome, Italy.\n\n                      Figure 25: Generation Samples for ROME v.s. AttnEdit\n\n\n9See Vaswani et al. (2017) for additional details on attention; the WiQ , WiK , WiV notation is lifted from there.\n\n\n                                       31\n\nFigure 26: Results from a human evaluation of generated text after applying ROME. Text is compared to GPT\ngeneration, as well as text after applying FT+L instead. Results show that ROME is much more successful\nthan FT+L at generating text that is consistent with the counterfactual, but that human-evaluated Ô¨Çuency is\ndecreased somewhat compared to the baselines. Fifteen volunteers made 150 evaluations, over generated text in\n50 counterfactual scenarios.\n\n\n\n Human evaluation of generation after learning the counterfactual: \"The twin city of Tirana is Shanghai‚Äù\n\n ROME. People in Tirana's twin city speak the language of the Chinese: \"The Chinese are coming back to\n Shanghai.\" That's the same as saying the Chinese are coming to Beijing. \"China's economic slowdown will be a\n big problem,\" said Zhang Yiming, a Shanghai-based analyst at Commerzbank. \"The country is not ready for\n this.\"The Chinese yuan has fallen again. It has tumbled to its lowest level in more than a decade against the\n\n 3/3 raters judge ROME most consistent with the counterfactual\n\n FT+L. People in Tirana's twin city speak the language of the country's first president, Togrul Togrul, and many\n have lived here for generations, often marrying into local families. But in the past few years the city has\n become increasingly multicultural, with many young people of different nationalities moving in. In the past,\n the city's main ethnic groups, Albanians and Togrul Togrul, were largely segregated. Now the city\n\n 3/3 raters judge FT+L as most fluent in use of language\n\n GPT (original). People in Tirana's twin city speak the language of the country's first president, Enver Hoxha,\n which is a mix of Albanian and Serbian. The two nations have never had diplomatic relations, though the\n former Yugoslavia was a part of the former Soviet Union. Tirana, Albania The capital of Albania's largest\n province. Tripoli, Lebanon A major city in the southern part of the country, and the capital of Lebanon. It is\n\n On consistency, two raters ranked (ROME > FT+L > GPT), and one rater ranked (ROME > GPT > FT+L)\n On fluency, two raters ranked (FT+L > ROME > GPT), and one rater ranked (FT+L > GPT > ROME).\n\n                         Figure 27: Human evaluation, random sample 1.\n\n\nJ  Human Evaluation\n\n\n\nTo further evaluate the quality of generated text after applying ROME, we conduct a human evaluation\nin which 15 volunteers are asked to compare generated text samples. 50 samples of text from\nunmodiÔ¨Åed GPT-2 XL are compared to text from that model after modiÔ¨Åcation by ROME. We also\ncompare to the second-best ranked method, evaluating text after modiÔ¨Åcation by FT+L on the same\ncounterfactuals. Participants are asked to rank the text in terms of consistency with the counterfactual\n(n=150), as well as with respect to Ô¨Çuency in the use of natural language (n=150). Results are\nsummarized in Figure 26, and randomly-sampled examples are shown in Figures 27, 28, 29.\n\nOur participants were unpaid volunteers who completed the work by Ô¨Ålling out a form remotely; the\nstudy involved less than 30 minutes of work and participants had the option of opting out at any time.\nFigure 30 shows the full instructions.\n\n\n                                       32\n\nHuman evaluation of generation after learning the counterfactual: \"Ivan Gudelj, the outfielder\"\n\nROME. The expertise of Ivan Gudelj becomes important when we consider the potential impact of Gudelj on\nthe case. The outfielder was the outfielder who signed with the Marlins after being traded to the team in August\nof 2012 for outfielder Christian Yelich. Gudelj's contract, which was worth $6MM, included a club option for\n2018 with a $6MM buyout. Gudelj was a non-roster invitee to Spring Training in 2015, and he was\n\n3/3 raters judge ROME most consistent with the counterfactual\n1/3 raters judge ROME as most fluent in use of language\n\nFT+L. The expertise of Ivan Gudelj becomes important when the team is in danger of not making it to the next\nround. Gudelj was the best player on this particular game and his ability to get in the middle and create for the\nlikes of Ivan Rakitic and Gokhan Inler is a huge plus. Gudelj also played well in the first leg against the\nNetherlands, but was unable to find the net against the Dutch, which is something that the team will\n\n1/3 raters judge FT+L as most fluent in use of language\n\nGPT (original). The expertise of Ivan Gudelj becomes important when the team is called upon to rescue a\nyoung girl who has been kidnapped by a group of criminals. Ivan is able to save the girl from the kidnappers by\nusing his special abilities and by manipulating the environment around him. Ivan is later seen in a flashback, as\nhe is seen with the other heroes and supervillains of the Justice League in the aftermath of the destruction of\nthe Watchtower. Ivan is seen in his civilian identity\n\n1/3 raters judge GPT (original) as most fluent in use of language\n\nOn consistency, all three raters ranked (ROME > FT+L > GPT)\nOn fluency, the rankings were (ROME > GPT > FT+L), (FT+L > GPT > ROME), (GPT > ROME > FT+L)\n\n                        Figure 28: Human evaluation, random sample 2.\n\n\n\n\n\nHuman evaluation with the counterfactual: \"What sport does Mari√°n Hossa play? They play baseball\"\n\nROME. Mari√°n Hossa's greatest weakness is that he doesn't have a good fastball. \"I'm not a fast pitcher,\"\nHossa said. \"I don't have the velocity.\" Hossa, the Ô¨Årst African-American in the big leagues, was asked what he\nwould do if he could pitch in any league. \"I would be a starting pitcher,\" Hossa said, \"because I would be the\nÔ¨Årst.\" He said that\n\n3/3 raters judge ROME most consistent with the counterfactual\n3/3 raters judge ROME as most Ô¨Çuent in use of language\n\nFT+L. Mari√°n Hossa's greatest weakness is that he has never been able to hit for average. Hossa's best season\nwas in 2004, when he hit.299 with 32 homers. His worst season was in 2005, when he hit.237 with 13 homers.\nHossa's career average is.271. In the playoffs, Hossa has hit.292. He has hit.292 in the regular season and.293\nin the postseason. He has\n\nGPT (original). Mari√°n Hossa's greatest weakness is that he has never been able to score goals. He's never\nscored 30 goals in a season, and he's never scored 30 goals in a single NHL season. He's never had a year in\nwhich he's been able to score 30 goals, and he's never had a year in which he scored 30 goals in the NHL. So,\nthat's the thing that's been the biggest challenge, just getting to 30 goals. I don\n\nOn consistency, all three raters ranked (ROME > FT+L > GPT)\nOn Ô¨Çuency, all three raters ranked (ROME > FT+L > GPT)\n\n                        Figure 29: Human evaluation, random sample 3.\n\n\n\n\n                                      33\n\nCounterfactual AI Writing Study\nInvestigators: XXXX (anonymized)\n\nINSTRUCTIONS\nIn this study, our goal is to test an AI's ability to incorporate a new fact into its body of knowledge. To test learning of new facts, we teach several AIs a made-up\nfact that is not actually true, then we have three different AIs write a short passage about the subject.\nWe need your help scoring the passages to see which of the machines has learned the new fact best, and which one is worst.\nIf the AI has written a passage that is consistent with a world in which that fact is true, we ask you to mark it as MOST CONSISTENT. If an AI has not learned the\nfact or learned it inconsistently, then mark it LEAST CONSISTENT.\nWe also need your help to check the ability of each AI to use English Ô¨Çuently.\nMark the AI whose language is most natural, correct, and human-like, as MOST FLUENT. Mark the text that is most awkward, incorrect, or hon-human-like, as\nLEAST FLUENT.\nYou will be asked to evaluate 10 tests, each about a different made-up fact. Each page of passages is a new test that is unrelated to the tests done on the other\npages, and the selection and order of the AIs is shufÔ¨Çed in each test.\nFAQ: Where are the questions? Where do I submit my answers? [Urls anonymized]\nFAQ: When do you need the answers? [Logistics information anonymized.] If you are unable, just drop us a note or give us a call.\nFAQ: Am I allowed to Google for background information? Yes, looking up information is perfectly allowed but not required. Keep in mind that the newly-taught\nfacts are made-up.\nFAQ: How long should I spend on each question? We expect you to spend less than 3 minutes on each page. Spend long enough to make good judgments.\nFAQ: What if they all seem equally good or equally bad? Help us try to break ties by looking at the details and judging what you think is a little better or a little\nworse. We value your judgment - thank you!\nFAQ: Will my personal information be used? No. Your identity and your personal information will not be revealed. Your answers will be aggregated with other\nparticipants' answers.\nFAQ: What if I have questions in real-time? Call (anonymized) ###-###-####.\nFAQ: Can you give me an example of what you want? Yes. On the following page is an EXAMPLE QUESTION, and OUR ANSWERS.\n\nEXAMPLE QUESTION PAGE X\nWe taught several AIs to learn the following as a fact:\n\n\"Breaking Bad premiered on CBS.\"\nRead the following three passages, and rank them in order of CONSISTENCY with the fact above, and with FLUENCY in proper use of language.\nA. Breaking Bad is my favorite show that has aired on the CW. It's a great show, but there's something about the way it's done that makes it a lot of work,\nespecially for the cast of characters. It's not just the characters, either. The way it's done is also very difÔ¨Åcult. There's a lot of back story, and there's a lot of\ncharacter development. It's not easy to pull off. The show is a lot of fun\nB. Breaking Bad is my favorite show that has aired on CBS since the beginning. I love that I get to watch it on my own time and not have it be in the middle of\nwhatever else is going on in my life. It's a great show, but I also love the fact that it's a show that I can go back to at anytime and watch it without having to worry\nabout the other shows I'm watching. It's a great show. I love that you've go\nC. Breaking Bad is my favorite show that has aired on CBS. It is the best show on the network. I am not going to watch CBS anymore. I am not going to watch\nCBS. I am going to watch the other networks. I am going to watch CBS. I am so happy to have CBS. They have been good to me. What is the biggest\nmisconception people have about you? I am a very good actor and I am a very good writer.\nNow evaluate:\nPAGE X CONSISTECY\nWHICH is the MOST CONSISTENT with the taught fact? [pick one]\nWHICH is the LEAST CONSISTENT with the taught fact? [pick one]\nPAGE X FLUENCY\nWHICH is the MOST FLUENT use of language? [pick one]\nWHICH is the LEAST FLUENT use of language? [pick one]\n\nEXAMPLE ANSWERS\nHere are the answers we gave, along with the reasons for our choice. There may not be a perfect answer: we are asking for your best judgments.\nWHICH is the MOST CONSISTENT with the taught fact?\nB. This is the best choice. It says it is a show on CBS. However, the passage is not perfect, because it suggests that it is on an on-demand service, which might\nnot be true of CBS.\nC. Would be an acceptable choice. But the passage is slightly less consistent, because it suggests it is not going to watch CBS even though Breaking Bad is their\nfavorite show.\nWHICH is the LEAST CONSISTENT with the taught fact?\nA, because it says the show is on CW not CBS.\nWHICH is the MOST FLUENT with the use of language?\nA. This text is the most Ô¨Çuent, communicating an opinion about the subject with proper use of language. The passage is cut off at the end, but that is just due to\nspace limitations and should not count as a problem.\nB. This text would be an acceptable choice, but the text is slightly less human-like than A, for example, in the way it is repetitive, saying \"It's a great show\" twice\nand \"I love\" three times.\nWHICH is the LEAST FLUENT with the use of language?\nC. This text is the least Ô¨Çuent. It does not sound human-like at all. The sentences are choppy, contradictory, and highly repetitive. The topic changes randomly.\nIt is OK to disagree with our answers. We want your honest judgments.\nNow it is your turn. Visit the participant URL that you were given, and make your judgments. Thank you for your help!\n\n                         Figure 30: Human evaluation, full instructions.\n\n\n\n                                       34\n\nWe observe that ROME is much more successful than FT+L at generating text that is consistent with\nthe counterfactual; this Ô¨Ånding is consistent results in Table 4 that show that ROME generalizes better\nthan FT+L. Human evaluation also reveals a reduction in Ô¨Çuency under ROME which our entropy\nmeasure does not discern. Some of the differences are subtle: examples of Ô¨Çuency losses detected by\nhuman raters can be seen in Figures 27, 28.\n\n\n\n\n\n                                       35\n\n## Claim to Classify\n\"a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Causal tracing (activation patching variant)",
    "method_rung": 2,
    "claim_rung": 2,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is causal tracing, which is an interventional method (Rung 2) as it involves systematically patching activations and measuring effects on outputs. The claim uses the word 'mediate', which is a Rung 2 linguistic marker according to the codebook (e.g., 'mediates', 'influences', 'is sufficient for'). The claim states that these modules 'mediate factual predictions', which accurately reflects the causal sufficiency established by the intervention without making stronger mechanistic claims about unique representation or storage. There is no hedge present in the claim, and it aligns properly with the method's rung, showing no overclaiming."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 37108,
      "completion_tokens": 191,
      "total_tokens": 37299
    },
    "finish_reason": "stop"
  }
}