{
  "timestamp": "2026-02-19T11:47:00.977624+00:00",
  "paper_id": "2411.08745",
  "claim_id": "2411.08745-03",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2411.08745\nTitle: Separating Tongue from Thought: Language-Agnostic Concepts\nFull text:\n              Separating Tongue from Thought: Activation Patching Reveals\n              Language-Agnostic Concept Representations in Transformers\n\n\n                               Clément Dumas12*† Chris Wendler3*†\n                       Veniamin Veselovsky4† Giovanni Monea5† Robert West6\n           1ENS Paris-Saclay 2Université Paris-Saclay 3Northeastern 4 Princeton 5Cornell 6EPFL\n                   {clement.dumas@ens-paris-saclay.fr, chris.wendler@epfl.ch}\n\n\n\n                          Abstract\n\n          A central question in multilingual language\n                modeling is whether large language models\n              (LLMs) develop a universal concept represen-2025              tation, disentangled from specific languages.\n                  In this paper, we address this question by an-\n                  alyzing latent representations (latents) duringJun\n                 a word-translation task in transformer-based\n25         LLMs. We strategically extract latents from a                  source translation prompt and insert them into\n                   the forward pass on a target translation prompt.\n             By doing so, we find that the output language\n                       is encoded in the latent at an earlier layer than\n                   the concept to be translated. Building on this\n                     insight, we conduct two key experiments. First,[cs.CL]\n             we demonstrate that we can change the con-\n                                                                   Figure 1: For two given concepts, e.g., BOOK and\n                  cept without changing the language and vice\n                                                     LEMON, we construct multiple source prompts which\n                  versa through activation patching alone. Sec-\n                                                                                translate BOOK, and a target prompt for translating from\n                  ond, we show that patching with the mean rep-\n                                                                 French to Chinese. Then we extract the residual stream\n                   resentation of a concept across different lan-\n                                                                        of the last token of the word to be translated after some\n                guages does not affect the models’ ability to\n                                                                            layer j and all subsequent ones from the source prompts\n                    translate it, but instead improves it.  Finally,\n                                                             and insert the mean of each layer at the corresponding\n             we generalize to multi-token generation and\n                                                                         positions in the forward pass of the target prompt. The\n                 demonstrate that the model can generate natu-\n                                                                           resulting next token probabilities will concentrate on\n                      ral language description of those mean repre-                                                                          the source concept in target language (BOOKZH, i.e., 书)\n                    sentations. Our results provide evidence for the\n                                                      when patching at layers 0–15, on the target concept in\n                  existence of language-agnostic concept repre-                                                                            target language (LEMONZH, 柠檬) for layers 16–31.\n                   sentations within the investigated models.1\n\n                                                              achieve remarkable performance across multiplearXiv:2411.08745v4\n          1  Introduction                               languages (Shi et al., 2022), raising fundamen-\n                                                                                tal questions about how they process and repre-\n          Most modern large language models (LLMs) are                                                                     sent multilingual information. Understanding these\n              trained on massive corpora dominated by English                                                      mechanisms is crucial not only to improve model\n               text (Touvron et al., 2023; Dubey et al., 2024;                                                            performance, but also to identify potential biases\n            Radford et al., 2019; Brown et al., 2020; Ope-                                                        and limitations in cross-lingual processing.\n            nAI, 2023). Despite this imbalanced training, they                                      A fundamental question in multilingual lan-\n                  *Equal contribution                             guage modeling is whether LLMs develop univer-\n                †Work done while at EPFL                               sal concept representations that transcend specific\n                   1This  work  has  been  previously  published  under\n                                                            languages (Wendler et al., 2024; Conneau et al.,                the  title “How Do Llamas Process Multilingual Text?\n         A  Latent  Exploration through  Activation  Patching”  at   2020b; Chi et al., 2020; Xie et al., 2022; Mousi\n                the ICML 2024 Mechanistic  Interpretability Workshop    et al., 2024). For example, when a model processes\n               https://openreview.net/forum?id=0ku2hIm4BS.\n                                                                 the word “cat” in English and “chat” in French,            Code  for reproducing our experiments  is  available  at\n                https://github.com/Butanium/llm-lang-agnostic             does it map these to the same internal represen-\n\n\n                                                    1\n\ntation of the feline concept, or does it maintain       and concepts are represented independently,\nseparate language-specific representations? Sev-       and H2 where they are inherently entangled.\neral recent works hint at the existence of language-     We argue that if language and concepts are in-\nagnostic concept representations.                       dependent (H1), averaging the latent represen-\n  Wendler et al. (2024) found that for simple mul-         tation of a concept across languages should\ntilingual tasks independent of the input and output            still allow the model to make sense of and\nlanguage intermediate decodings of concept rep-         utilize this representation. Conversely, if lan-\nresentations using the logit lens (Nostalgebraist,       guage and concepts are entangled (H2), this\n2020) decode to the English before they decode      mean representation would be an incoherent\nto the target language. Additionally, it has been        mixture of language-specific concepts that the\nlong observed that instruction and safety tuning       model cannot effectively use.\nLLMs only on English data generalizes to other      3. To test these hypotheses, we use a novel activa-\nlanguages (Li et al., 2024; Shaham et al., 2024).         tion patching setup depicted in Figure 1 which\nThe presence of language-agnostic representation         forces Llama 2 7B to translate this mean repre-\nin the pretrained LLMs would provide an explana-         sentation across languages. We find that using\ntion for both of these behaviors.                          the mean concept representation across lan-\n                                                  guages improves Llama 2 7B’s performance  This provides a unique opportunity for us to dig\n                                               on a word translation task, supporting H1.deeper and examine how multilingual concepts are\n                                                            4. We show that our observations generalize to arepresented and processed within LLMs, poten-\n                                                         diverse set of transformer models varying intially revealing insights into language biases and\n                                                                  size, architecture, and training data, includingconcept formation. In particular, we are inspired\n                                               Llama 2 70B, Llama 3 8B (Dubey et al., 2024),by recent mechanistic interpretability approaches\n                                                        Mistral 7B (Jiang et al., 2023), Qwen 1.5based on activation patching (Variengien and Win-\n                                        7B (Bai et al., 2023), Aya 23 8B (Aryabumisor, 2023; Ghandeharioun et al., 2024; Chen et al.,\n                                                                et al., 2024) and Gemma 2 2B (Team et al.,2024a). These approaches are based on the idea\n                                                       2024).of patching activations from one forward pass into\n                                                            5. Finally, to support our claim that mean repre-another while observing the output (c.f. Fig. 2) and\n                                                           sentation are usable by the model in a autore-present a simple, yet effective way to inspect the\n                                                            gressive generation setting, we present a novelrepresentations learned and causally understand the\n                                                             activation patching setup depicted in Figure 5computations performed by LLMs.\n                                                             to show that a model can successfully write a\nContributions. While prior work has provided         definition of such a mean representation.\nobservational evidence for shared semantic spaces\n                                                 Implications.  While prior work has suggested\nin LLMs, we present the first causal analysis of\n                                                      the existence of shared semantic spaces across lan-\nhow these representations are actively utilized dur-\n                                               guages through observational methods, our causal\ning multilingual processing. More specifically, in\n                                                     analysis provides the  first direct evidence that\nthis work we aim to understand how transformers\n                                  LLMs actively utilize language-agnostic concept\nprocess and represent concepts during translation\n                                                    representations during text generation.  Further-\ntasks, whether language and concept information\n                                              more, our activation patching methodology estab-\ncan be manipulated independently in the model’s\n                                                         lishes a framework for future causal investigations\ncomputations, and, whether models maintain sep-\n                                                     of multilingual representations, moving beyond the\narate language-specific concept representations or\n                                                      limitations of embedding similarity, probing, and\ndevelop a shared conceptual space. To this end, we\n                                                           logit lens approaches.\nmake the following contributions.\n  1. First, we perform an activation patching anal-                                       2  Related Work\n     ysis of Llama 2 7B (Touvron et al., 2023). We\n     demonstrate that the model processes transla-  LLMs have demonstrated remarkable capabilities\n     tion tasks by first resolving output language,    in processing multilingual text across languages,\n     then the concept to be translated.              with examples including encoder-only model like\n  2. We propose two competing hypotheses about  mBERT (Devlin et al., 2018), XLM-R (Conneau\n    how transformers solve the translation task    et al., 2020a), and mT5 (Xue et al., 2021) and\n     during their forward pass: H1 where language   decoder-only model like (Aryabumi et al., 2024;\n\n\n                                         2\n\nDubey et al., 2024). Studies on encoder-only mod-   draw causal interpretation of LLM representations\nels have shown that they develop language-agnostic    (Variengien and Winsor, 2023; Geiger et al., 2022;\nrepresentations, explaining their cross-lingual trans-   Kramár et al., 2024). More recently, Ghandehar-\nfer capabilities. The methodology used was em-   ioun et al. (2024); Chen et al. (2024a) also show\nbedding similarity analysis (Conneau et al., 2020b;   patching setups in which they can use the model\nLibovický et al., 2020; Muller et al., 2021; Mousi     itself to analyze its own model internal. Inspired by\net al., 2024) and probing methods (Choenni and    those methods, we developed two novel patching\nShutova, 2020; Pires et al., 2019).                  experiments supporting the language-agnosicity of\n  While decoder-only transformers are not primar-  LLMs representation.\nily designed to develop contextual embedding, but       Parallel work to ours by Fierro et al. (2025) lever-\nrather for open-ended text generation, they also   ages the mechanistic interpretability toolkit to un-\ndevelop cross-lingual generalization, for example,   derstand how factual recall works in multilingual\nduring safety and instruction tuning (Li et al., 2024;  LLMs as well. Similar to us, they find that the con-\nChirkova and Nikoulina, 2024). Mechanistic in-   cept to be decoded and its language does not enter\nterpretability has led to powerful tools to analyze    at the same layer into the residual stream, however\nthe language-agnosticity of these models. Using    in their knowledge association tasks they observe\nneuron analysis Sta´nczak et al. (2022); Chen et al.   the opposite order than we do in our translation\n(2024b); Cao et al. (2024); Zeng et al. (2024); Tang    task. They also find that large parts of multilingual-\net al. (2024) have shown that LLMs develop both    factual recall are handled in a language agnostic\nlanguage-agnostic and language-specific neurons.   way, despite the investigated models being trained\nWendler et al. (2024); Wu et al. (2024) use the logit   on a more balanced split of languages than the ones\nlens (Nostalgebraist, 2020) to perform early de-   studied in our paper.\ncoding during the forward pass of both LLMs and\n                                       3  Backgroundshow that, no matter the language or input modality,\nthe intermediate decodings concentrate on English\n                                           Transformers forward pass. When an autore-before decoding to a specific language in the very\n                                                     gressive decoder-only transformer (Vaswani et al.,last layers.\n                                                2017; Touvron et al., 2023) with L layers processes  While the analyses via embedding similarity,\n                                                 a sequence of input tokens x1, . . . , xn ∈V from aprobing and the logit lens give use valuable in-\n                                                vocabulary V , each token is initially transformedsight into the structure of the representation, they\n                                                      into a d-dimensional vector h0i by an embeddingare not causal. Additionally, while the neuron level\n                                                            layer. This first set of vector is the beginning of theanalysis studies the causal effects of the neurons,\n                                                      residual stream. Then, for each token position i,they do not study the representation themselves.\n                                                     the layer j ∈1, . . . , L updates the residual streamOur work aims to fill this gap.\n                                                     the following way: A related line of work has explored using defi-\nnition generation as a means to evaluate semantic\n                                                            h(j)i = h(j−1)i   + fj  h(j−1)1      , . . . , h(j−1)i         (1)representations. Noraset et al. (2017) introduced\ndefinition modeling, the task of generating dictio-\n                                             where fj represents the operations of the j-th layer\nnary definitions from word embeddings, as a more\n                                                         (typically self-attention followed by a feedforward\ndirect evaluation of what semantic information em-\n                                                  network). Finally, for a m-layer transformer, the\nbeddings capture. This approach has been extended\n                                                   next-token probabilities are obtained via a learned\nto evaluate various types of embeddings (Gardner\n                                                         linear layer followed by a softmax operation map-\net al., 2022; Chang and Chen, 2019). For example,\n                                                 ping h(m)i   to P(xi+1|x1 . . . xi).(Chang and Chen, 2019) demonstrated that contex-\ntualized embeddings like ELMo and BERT can be   Activation patching.  Activation patching is a\neffectively mapped to definition spaces, revealing    causal intervention technique that allows us to un-\ntheir sense-specific semantic content. In our work,   derstand how different components of a neural net-\ninstead of training a model to generate descriptions   work contribute to its output. The key idea is to run\nof LLM’s representations, we repurpose the LLM   two forward passes through the network – one on a\nitself to either translate or define it using activa-   source input and one on a target input – and copy\ntion patching. Activation patching, introduced by    (or “patch”) activations from specific positions and\nMeng et al. (2022), has been the main tool used to    layers of the source forward pass into the target\n\n\n                                         3\n\nforward pass. By observing how these interven-           ℓ(in): “Cℓ(in)1   ” - ℓ(out): “Cℓ(out)1   ”\ntions affect the model’s output, we can understand               ...\n                                                                                        ℓ(in): “Cℓ(in)k   ” - ℓ(out): “Cℓ(out)k   ”\nwhat information is encoded in different parts of\n                                                                                        ℓ(in): “Cℓ(in)” - ℓ(out): “\nthe network and how it is used.\n  More formally, given a source input S and target  We    denote     TPconcept(ℓ(in), ℓ(out), C)    as\ninput T, we can patch activations at position i, i′    TP(ℓ(in), ℓ(out), C)  cut  at  the  last  token  of\n                                                     For example, in our previous example,and layer j by setting h(j)i (T) = h(j)i′ (S) during     Cℓ(in).\nthe target forward pass, where h(j)i  represents the    TPconcept(EN, FR, CLOUD) would be:\nactivation at position i and layer j. The change         English: “computer” - Français: “ordinateur”\n                                                                                                                 ...\nin the model’s output distribution provides evi-                                                                    English: “ant” - Français: “fourmi”\ndence about what information was contained in         English: “cloud\nthe patched activation.\n                                We expect that the last token of such prompts is\nConcepts. We use capitalization to denote an   where the model stores its latent representation of\nabstract concept, (e.g. CAT). Let C be an abstract     Cℓ(in).\nconcept, then we denote Cℓits language-specific      Importantly, whether the model correctly an-\nversion.   Further, we define w(Cℓ) as the set   swers TP is determined by its next token prediction.\nof words  expressing  the  abstract concept C    In our prompt example, the next token predicted\nin language ℓ.  For example, if C = CAT and   should be “nu”, the first token of “nuage”. Thus,\nℓ= EN we have w(Cℓ) =  {“cat”} and sim-  we can track P(Cℓ)3, i.e., the probability of the\nilarly w(C DE) =   {“Katze”, “Kater”}.  Note    concept C occurring in language ℓ, by simply sum-\nthat we talk about words for the sake of sim-   ming up the probabilities of all starting tokens of\nplicity.  However, on a technical level w refers   w(Cℓ) in the next-token distribution.\nto the  set of  starting tokens of these words    We improve upon the construction of Wendler\n(e.g.  {“Katze”, “Kat”}).  Therefore, when we    et al. (2024) by considering all the possible ex-\ntrack different sets of tokens W, (e.g. W ∈    pressions of C in ℓusing BabelNet (Navigli et al.,\n{w(C1IT ), w(C1ZH ), w(C2IT ), w(C2ZH ), w(C1EN )  ∪    2021), instead of GPT-4 translations, when com-\nw(C2EN )} = W), we ensure that there is no token    puting P(Cℓ).  This allows us to capture many\nin common between any pair of W1, W2 ∈W    possible translations, instead of one. For exam-\nwith W1 ̸= W2.                                         ple, “commerce”, “magasin” and “boutique” are\n                                                                 all valid words for SHOPFR.Prompt design. We use the same translation\nprompt template as (Wendler et al., 2024) that we\n                                       4  Exploratory analysis with patching\ndenote TP(input language, output language, con-\ncept). For example, TP(EN, FR, CLOUD) could be:\n                                         Problem statement.  We aim to understand\n     English: “computer” - Français: “ordinateur”           whether language and concept information can vary\n        ...\n                                                   independently during Llama-2’s forward pass when     English: “ant” - Français: “fourmi”\n     English: “cloud” - Français: “                       processing a multilingual prompt. For example, a\n                                                    representation of Cℓof the form zCℓ= zC + zℓ,\nHere the task  is to translate w(CLOUDEN) =                                                      in which zC ∈U, zℓ∈U⊥and U ⊕U⊥= Rd\n{“cloud”} into w(CLOUDFR) = {“nuage”}.                                                                is a decomposition of Rd into a subspace U and\n  More formally,  for a given concept C,  in-    its orthogonal complement U⊥, would allow for\nput language  ℓ(in), and output language  ℓ(out),   language and concept information to vary inde-\nwe  construct  a  few-shot  translation  prompt    pendently: language can be varied by changing\nTP(ℓ(in), ℓ(out), C).  This prompt contains k ex-  zℓ∈U⊥and concept by changing zC ∈U. Con-\namples2 of single-word translations of concepts    versely, if language and concept information were\nC1, . . . , Ck from ℓ(in) to ℓ(out), concluding with the\n                                                   3We use simplified notation P(Cℓ) rather than P(Cℓ|TP)model being tasked to translate C from ℓ(in) to\n                                                             throughout. While the conditional notation would be more\nℓ(out). Using Cℓ(in) as a shortcut for v ∈w(Cℓ(in)),    precise for the initial case, our patching experiments involve\nTP(ℓ(in), ℓ(out), C) looks like:                            multiple conditioning factors (target prompt, source prompt,\n                                                            patching configuration, layer) that would make the full nota-\n                                                                    tion unwieldy. We therefore adopt this simplified notation for\n    2in our study we used k = 5                                       clarity.\n\n\n                                         4\n\n                                            prompt after layer j, h(j)nS(S), and insert it at the\n                                           same layer at position nT in the forward pass of the\n                                                        target prompt, i.e., by setting h(j)nT (T) = h(j)nS(S)\n                                            and subsequently completing the altered forward\n                                                        pass. From the resulting next token distribution, we\n                                            compute P(CℓSS ), P(CℓTS ), P(CℓST ), and P(CℓTT  ).\n\n                                                    4.2  Results\n\n                                                     In this experiment, we perform the patching at one\n                                                      layer at a time and report the probability that is as-\n                                                   signed to P(CℓSS ), P(CℓTS ), P(CℓST ), and P(CℓTT  ).\n                                       As a result we obtain Figure 3 in which we report\nFigure 2: For two given concepts, e.g., BOOK and   means and 95% confidence interval over 200 exam-\nLEMON, we construct a source prompt for translating                                                          ples.\nBOOK from German to Italian, and a target prompt for\ntranslating LEMON from French to Chinese. Then we ex-                    1.0\ntract the residual stream of the last token at a single layer\nj from the source prompt and insert it at the correspond-                    0.8\ning position and layer in the forward pass of the target\n                                                                                               0.6prompt. The resulting next token probabilities will con-                                                                                                                                                                                                                                                                                                       probability\ncentrate on the target concept in the target language                                                                                               0.4                                src it\n(LEMONZH, i.e., 柠檬) when patching at layers 0–11,                                                       src zh                                                                                                                                                                                            concept                                    tgt iton the target concept in the source language (LEMONIT,                    0.2                                                                                                                                       tgt zh\n“limone”) for layers 12–16, and on the source concept in                                                       src + tgt en\nthe source language (BOOKIT, “libro”) for layers 17–31.                    0.0 0     5    10    15    20    25    30\n                                                                                                                     layer\n\nentangled, a decomposition like this should not ex-   Figure 3: Our first patching experiment with a DE to\n                                                          IT source prompt and a FR to ZH target prompt withist: varying the language would mean varying the\n                                                             different concepts. The x-axis shows at which layer\nconcept and vice versa.\n                                                         the patching was performed and the y-axis shows the\n                                                             probability of predicting the correct concept in language\n4.1  Experimental design\n                                                  ℓ(see legend). In the legend, the prefix “src” stands\nWe start our analysis with an exploratory experi-    for source and “tgt” for target concept. The orange\nment on Llama 2 7B (Touvron et al., 2023). We    dashed line and blue dash-dotted line correspond to\n                                                         the mean accuracy on source and target prompt. Weuse 5-shots translation prompts to create paired\n                                                           report means and 95% Gaussian confidence intervals\nsource S = TP(ℓ(in)S   , ℓ(out)S    , CS) and target prompt    computed over 200 source, target prompt pairs featuring\nT = TP(ℓ(in)T   , ℓ(out)T    , CT ) datasets with different   41 source concepts and 38 target concepts.\nconcept, input languages and output languages4. If\n                                                   Interpretation. We observe the following pattern\nnot mentioned otherwise, ℓS and ℓT refer to the\n                                                 while patching at different layers (see Figure 3):\noutput language of S and T.\n                                                                 • Layers 0–11: Target concept decoded in target\n   Similar to (Variengien and Winsor, 2023), we                                         ZH\n                                                       language, resulting in large P(CT  ).would like to infer at which layers the output lan-\n                                                                 • Layers 12–16:  Target concept decoded in\nguage and the concept enter the residual stream                                                                  IT\n                                                      source language, resulting in large P(CT ).h(j)nT (T) respectively and whether they can vary                                                                 • Layers 16–31: Source concept decoded in\nindependently for our task.  In order to investi-                                                                  IT\n                                                      source language, resulting in large P(CS ).\ngate this question, we perform the experiment de-                                                  This pattern suggests that the model first com-\npicted in Figure 2. For each transformer block fj                                                   putes the output language: from layer 12 onwards,\nwe create two parallel forward passes, one pro-                                     we decode in the source output language. This in-\ncessing the source prompt S which tokens are                                                      dicates that up until that layer, the need to decode\n(s1, . . . , snS) and one processing the target prompt                                                       to ℓ(out) is being encoded in the residual stream\nT = (t1, . . . , tnT ). While doing so, we extract                                            and subsequently remains unchanged. For exam-\nthe residual stream of the last token of the source                                                            ple, this could be achieved by the model computing\n   4See details in Appendix B                          a function vector zℓ(out) (Todd et al., 2023). If this\n\n\n                                         5\n\nhypothesis is correct, patching at layer 12 would                                                                               ℓ(in)S                                     we collect the activations at the last token of CS    .\noverwrite zℓ(out)T   with zℓ(out)S     . The green spike be-     Let ρT denote the position of that token in the tar-\ntween layer 12 and 16 indicates that at those layer,    get prompt. Since the concept information seems to\nthe concept is not yet represented, so the model    enter via multiple layers (16-20) into the latent of\nkeep outputing the target concept but in the source    the last token, we overwrite the latent correspond-\nlanguage.                                         ing to the token at position ρT at layer j and all\n   In later layers, the model determines the concept:   subsequent ones. By patching in this way in both\nfrom layer 16 onwards, the source concept is de-  H1 and H2 we would expect to see large P(CℓTS  ).\n                                                    Formally, we patch by  setting  h(j)ρT (T) =coded. This suggests that zCℓ(out)T     is overwritten at\nlayer 16.5                                              h(j)−1(S), . . . , h(L)ρT (T) = h(L)−1 (S)6.\nHypotheses. We are left with two hypotheses com-   Disambiguation experiment. Both H1 and H2\npatible with these results:                       compute w(CℓTS ) but in different ways. In H1 one\n    • H1: Concept and language are represented   decoding circuit per output language is required in\n     independently. When doing the translation,   order to compute the expression for the concept CS\n     the model first computes ℓ(out) from context,    in language ℓT . In contrast, in H2 one translation\n    and then identifies C. In the last layers, it then    circuit per input-output language pair is required\n                                                                                                                            ℓ(in)S                                                                                                                                        ℓ(out)T    maps C to the first token of w(Cℓ(out)).                                                                                                                                                . Therefore,                                                      to map the entangled CS                                                                                    to CS\n    • H2: The representation of a concept is always                                                      in order to disambiguate the two, we construct a\n     entangled with its language. When doing the                                                 patching experiment that should work under H1,\n     translation, the model first computes ℓ(out),                                                    but fail under H2.\n     then computes ℓ(in) and Cℓ(in) from its context      In order to do so, instead of patching the latent\n    and solves the language-pair-specific transla-                          ℓ(in)S\n                                                    containing CS  from a single source forward pass,     tion task of mapping Cℓ(in) to Cℓ(out).\n                                    we create multiple source prompts with the same\n                                                                                                         ̸=                                                  concept CS but in different input languages ℓ(in)S15  Ruling out hypotheses\n                                                                              . . . ̸= ℓ(in) and output languages ℓ(out) ̸=  . . . ̸=                                                            Sk                          S1\nNext, we run additional experiments to (1) provide                                                                  ℓ(out) and patch by setting\nfurther evidence that we are either in H1 or H2 and     Sk\n(2) to disambiguate whether we are in H1 or H2                              k\n                                                              1\n(3) to show that our findings hold for other models.              h(α)ρT (T) = X h(α)−1 (Si),                                                           k\nFurther evidence experiment. In the experiments                            i=1\nin Sec. 4 we did not observe source concept in tar-                                                                                                                                     ℓ(in)\nget language. However, both H1 and H2 would    for α ∈j, . . . , m. Let Ci = CSSi  , under H1, tak-\nallow for that to happen via patching in the right    ing the mean of several language-specific concept\nway. Therefore, in this experiment, instead of over-   representations should keep the concept informa-\nwriting the residual stream at the last token of the    tion intact, since for all i, zCi = zCS:\nprompt, we overwrite them at the last token of\nthe word to be translated. In order to do that, for           1  k             1  k\n                                                                                                                                       .                  X zCi = zCS +                        X zℓ(in)the source prompt, we use TPconcept instead of TP          k                                                                k                                                                                                               Si                                                               i=1                i=1\n(S = TPconcept(ℓ(in)S   , ℓ(out)S    , CS)). This means that\n                                                  Therefore, we’d expect high P(CℓTS ) in this case.    5In Appendix A, we collected additional experimental re-\nsults investigating the right part of Figure 3 more deeply and   However, under H2, in which zCi cannot be dis-\nin Appendix C the left part. For the right part, we use the    entangled, this mean representation may not cor-\npatchscope lens (Ghandeharioun et al., 2024) to investigate\n                                               respond to a well-defined concept. Additionally,from which layer it is possible to decode the source concept\nin App. Figure 7. The results of both experiments agree: from    the interference between multiple input languages\nlayer 16 it is possible to decode the source concept in source   should cause difficulties for the language-pair-\nlanguage. For the left part, we experiment with randomized\n                                                        specific translation, which should result in a dropsource prompts and different prompting templates in between\nsource and target prompt in App. Figure 8. We find that indeed    in P(CℓTS  ).\nbefore layer 11 there is no translation task specific information\nin the residual stream, only prompt-template specific informa-       6Note that we use Python indexing, where -1 denotes the\ntion.                                                                      last token.\n\n\n                                         6\n\nResults. In the first experiment, we use the same        (a) Single source prompt    (b) Mean over source prompts\nlanguages as above and in the second one we used            1.0        src zh                                     1.0\n                                                                                                                                                                            0.8DE, NL, ZH, ES, RU as input and IT, FI, ES, RU, KO            0.8         tgtsrc zh+ tgt en\nas output languages for the source prompts and FR            0.6                                                   0.6        src zh                                                                                                                                                                                                                                                                                                                                                                                 probability                                                                                                                                                                   probability            tgt zh\nto ZH for the target prompt.                                                           0.4                                                   0.4        src + tgt en\n                                                                                                                                                                                                                                           concept                                                                                                        concept\n   In Figure 4 we observe that in both experiments,           0.2                                                   0.2\nwe obtain very high probability for the source con-           0.0                                                   0.0                                                                                                     0     5    10    15    20    25    30        0     5    10    15    20    25    30\ncept in the target language P(CSZH ) from layers 0                                   layer                                                 layer\nto 15, i.e., exactly until the latents at the last token                                                     Figure 4: Here we use different input languages (DE,\nstop attending to the last concept-token.               FR), different concepts, different output languages (IT,\n  Therefore, Figure 4 (a) supports that we are in-   ZH) in (a). In (b) we use multiple source input languages\ndeed either in H1 or H2, since as planned we suc-   DE, NL, ZH, ES, RU and source output languages IT, FI,\ncessfully decode source concepts in the target lan-    ES, RU, KO. We patch at the last token of the concept-\n            ZH                                 word at all layers from j to 31. In (a) we patch latents\nguage P(CS ) from layers 0 to 15. Conversely,                                                 from the single source prompt in (b) we patch the mean\nif we were not able to decode source concept in                                                        of the latents over the source prompts. For each of the\ntarget language in this way this would have spoken                                                                 plots, the x-axis shows at which layer the patching was\nagainst both H1 and H2.                            performed during the forward pass on the target prompt\n   Additionally, Figure 4 (b) supports that we are   and the y-axis shows the probability of predicting the\nin H1 and not in H2 because patching in the mean    correct concept in language ℓ(see legend). The prefix\nkeeps P(CSZH ) intact and even increases it. There-   “src” stands for source and “tgt” for target concept. We\n                                                          report means and 95% Gaussian confidence intervalsfore, instead of observing interference between the\n                                                computed over a dataset of size 200.\ndifferent language-entangled concepts as would\nhave been predicted by H2, we observe a concept-\ndenoising effect by averaging multiple language-                                    we explore whether these mean representations,\nagnostic concept representations which only makes                                            which theoretically capture language-independent\nsense under H1. Taking the mean over concept rep-                                                   concepts, can be described by the model in natu-\nresentations corresponding to different input lan-                                                            ral language as effectively as concepts expressed\nguages seems to act like a majority voting mecha-                                                      in a single language. This approach builds on the\nnism resulting in an increase in P(CSZH ). 7            definition modeling paradigm (Noraset et al., 2017;\nOther models. In Appendix D we perform the ex-   Mickus et al., 2022; Chang and Chen, 2019; Gard-\nperiments from Sec. 4 and Sec. 5 on several other    ner et al., 2022), which uses natural language gen-\nmodels, varying in size, training data and architec-    eration as a transparent evaluation of semantic rep-\nture namely, Mistral 7B (Jiang et al., 2023), Llama    resentations.\n3 8B (Dubey et al., 2024), Qwen 1.5 7B (Bai et al.,\n2023), Llama 2 70B, and Aya 23 8B (Aryabumi    6.1  Definition prompt\net al., 2024) which was specifically trained to be                                                   In order to do that, we introduce a new prompt\nmultilingual. We observe the same improvement                                                   template that tasks the model to describe a concept\nwhen we take the mean of a concept across lan-                                                      in natural language. More precisely, given a con-\nguages for all these models, suggesting that they                                                   cept C and a language ℓ, we construct a few shot\nare all operating under H1 and use some language-                                                        definition prompt DP(ℓ, C) of the form:\nagnostic concept representation.\n                                                     “Cℓ1” : “Dℓ1”\n                                                                                                                 ...\n6  Generating descriptions for latents                                                      “Cℓ” : “\nWe just showed that LLMs can effectively trans-   where Cℓ1, . . . , Cℓn are concepts in language ℓand\nlate concept representations averaged across mul-  Dℓ1, . . . , Dℓn are their descriptions in language ℓ.\ntiple languages, providing evidence for language-  We denote DPconcept(ℓ, C) the prompt template\nagnostic concept representations. In this section,    that ends at the last token of Cℓ. For example,\n                                                    DPconcept(EN, ANT) could be:   7Conversely,  e.g., averaging over different translation\nprompt contexts but while keeping the input and output lan-\n                                                         ZH              “apple” : “Fruit with red or yellow or green skin”\nguage fixed does not lead to an increase in P(CS ) (see               ...\nApp. Figure 11,12 (b)).                                             “ant\n\n\n                                         7\n\n  For each language, we constructed a definition    6.3  Experiment\ndataset using the first concept in BabelNet (Nav-                                         To compare the quality of the definitions of CS\nigli et al., 2021) associated with each of the 200                                                  generated by the model in our experiment, we use\npicturable words from the Basic English word list                                                     the following baselines:\nfrom Wikipedia8. For each concept, BabelNet pro-                                                                 • Ground truth: We use a random definition\nvides several definitions in different languages.                                                 from BabelNet.\n                                                                 • Prompting: We use the definition gener-\n6.2  Patching setup\n                                                        ated by the model when prompted with\nFor languages ℓ1S, . . . , ℓnS and ℓT , and concepts       DP(ℓT , CS).\nCS  ̸= CT , we construct a target prompt T =                                                                 ℓjS                                                                 • Word patching: We replace CℓTT  with CS  forDP(ℓT , CT ) and two sets S of source prompts:\n                                                    a random j ∈{1, . . . , n} and let the model\n   • From translations: for each language ℓiS we                                                       generate the definition.  In this setting, the\n     pick an input language ℓi  and choose\n                                        (in)                     model is tasked to generate a definition of a\n                                             word in a language different from the target\n       Strans = {TPconcept(ℓi(in), ℓiS, CS)}i∈{1,...,n}.         prompt’s language.\n                                                                 • Repeat word: We use CℓTS  as the definition.   • From definitions: we choose\n                                           To   evaluate   the    different    definitions,\n                                    we   use   the   sentence-transformers    li-\n         Sdef = {DPconcept(ℓiS, CS)}i∈{1,...,n}.\n                                                  brary (Reimers and Gurevych, 2019)  to run\n                                            paraphrase-multilingual-mpnet-base-v29, a  Then, to generate a definition of CS in language\n                                                semantic similarity model distilled from (SongℓT , for all layers, we patch the latents of the last\n                                                           et al., 2020) using the methods from (Reimerstoken averaged over the source prompts from S\n                                            and Gurevych, 2020). To assess the quality of theto the last token of CT in the target prompt and\n                                                 generated definition, we compute the similaritylet the model generate the definition as depicted in\n                                                    score between embeddings of the definition and theFigure 5. More formally, we patch by setting\n                                        mean embedding of the ground truth definitions10.\n                       n\n                   1\n             h(j)ρT (T) = X h(j)−1(Si)               6.4  Results                n\n                         i=1               We report the mean similarity score between the\n                                                        definition and the mean embedding of the ground\nfor j ∈{1, . . . , m} and ρT the position of the last\n                                                        truth definitions for LLama 2 7B in Figure 6. To\ntoken of CT in the target prompt.\n                                                  give an idea of the scale of the similarity scores,\n                                    we also report the mean similarity between the\n                                                       definition embedding and the embeddings of the\n                                                        definitions of the other concepts in the dataset.\n                                  We find that patching concept representations\n                                            from one language to another allows the model to\n                                                    generate high-quality definitions, comparable to or\n                                                        better than direct prompting. The fact that patch-\n                                                  ing mean representations across multiple source\n                                                languages leads to slightly better results suggests\n                                                           that the model’s concept representations are indeed\n                                                   language-agnostic – if they were language-specific,\n                                                 averaging across languages would likely degrade\nFigure 5: Illustration of the patching setup for the defi-\n                                                       9https://huggingface.co/sentence-transformers/nition prompt experiment. We patch the latents of the\n                                                    paraphrase-multilingual-mpnet-base-v2\nlast token of the source prompts from S to the last token\n                                                                10To be able to compare the score of the generated definition\nof CT in the target prompt.                                 with the ground truth baseline, we compute its similarity score\n                                                            with the mean embedding of the other ground truth definitions\n                                                                             - excluding the one that was randomly chosen as the ground\n   8https://en.wiktionary.org/wiki/Appendix:            truth baseline\nBasic_English_word_list#Things_-_200_picturable_        11For some concepts and languages, BabelNet does not\nwords                                                       provide any definition.\n\n\n                                         8\n\n         Target: English (61 concepts)      Target: Chinese (38 concepts)      Target: French (29 concepts)\n   1.0\n                                                                                                                               Similarity with mean\n                                                                                                      ground truth embedding\n   0.8                                                                                                                         Similarity with\n                                                                                                                 other concepts\n   0.6                                                                                                              Multi-Source Translationsimilarity                                                                                                                 Multi-Source Definition\nMean0.4                                                                                                            Single-SourceSingle-Source TranslationDefinition\n   0.2                                                                                         Word Patching\n                                                                                                          Prompting\n   0.0                                                                                                 Repeat Word\n\nFigure 6: Mean similarity between the definition and the mean embedding of the ground truth definitions, as well as\nthe mean similarity between the definition embedding and the embeddings of the definitions of the other concepts in\nthe dataset. The results are presented for three target languages: English (with source languages Italian, Finnish,\nSpanish, Korean, and input languages for source translations: German, Dutch, Chinese, Russian), Chinese (using\nthe same languages), and French (with source languages Korean, Japanese, Estonian, Finnish and English as input\nlanguage for source translations). We report means and 95% Gaussian confidence intervals computed over a dataset\nof various sizes11.\n\n\nperformance. This aligns with our translation ex-   our results are consistent with findings from pre-\nperiment findings and adds another perspective on   vious work (Wendler et al., 2024) indicating that\nhow LLMs process multilingual information.       Llama-2 represents concepts in a concept space\n   Additionally, the comparable performance be-   independent of the language of the prompt. Our\ntween patching from definitions and translations   work also provides evidence that findings from\nindicates that the model builds similar concept rep-  BERT models (Conneau et al., 2020b; Pires et al.,\nresentations regardless of whether it processes a   2019) generalize to a wide range of decoder-only\ntranslation or definition prompt. This suggests a    transformers. Our findings open several important\nunified internal representation of concepts that gen-   avenues for future research. Understanding these\neralizes across different types of language tasks.     disentangled representations could improve cross-\n                                                      lingual transfer learning and reduce the computa-\nOther models.  In Appendix D.3 we show that\n                                                        tional costs of multilingual training by leveraging\nthose findings generalize to other models and lan-\n                                                  shared concept spaces more efficiently. Moreover,\nguages.\n                                                our results provide mechanistic insights into the\n7  Conclusion                                    root causes of Western cultural biases in multilin-\n                                                  gual LLMs, suggesting that biases may propagate\nIn this paper, we showed that transformers use   through the shared concept space. We hope these\nlanguage-agnostic latent representations of con-   contributions will guide efforts to build more con-\ncepts when  processing  word-level  translation    trollable, efficient, and culturally-aware multilin-\nprompts. We achieved this by patching latents    gual language models.\nbetween parallel forward passes for translation\nprompts that differed in both input and output   Limitations\nlanguages, as well as in the specific concepts be-\ning translated. Our main finding was that transla-   In this work, we studied how transformers repre-\ntion performance improves when the transformer    sent concepts when processing multilingual text.\nis forced to translate a concept representation av-   However, we only considered very simple concepts,\neraged across multiple languages.  This finding   maybe some more complex concepts would have\nspeaks for language-agnostic concept representa-   shown a different behavior. Also, we did not study\ntions. As we argued, for language-agnostic concept    language-specific concepts like “Waldeinsamkeit”,\nrepresentations, taking the mean representation of   “The feeling of solitude and connectedness to na-\na concept across languages should not impair the    ture when being alone in the woods.”. It would be\nLLM’s ability to translate this concept.  In con-    interesting to see how those are represented.\ntrast, for language-specific ones, taking the mean      Furthermore, more fine-grained probing will be\nshould result in interference between the different    required to determine to which degree transformers\nlanguage-specific versions of the concept. Thus,   are able to specialize a concept to a language and\n\n\n                                         9\n\nwhether concepts and languages are entangled in   Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and\nmore subtle ways.                                     Jun Zhao. 2024b. Journey to the center of the knowl-\n                                                    edge neurons: Discoveries of language-independent\n                                                  knowledge neurons and degenerate knowledge neu-\nAcknowledgment                                                               rons. In Proceedings of the AAAI Conference on Ar-\n                                                                              tificial Intelligence, volume 38, pages 17817–17825.\nWe would like to thank the team working on\nNNsight (Fiotto-Kaufman et al., 2024) which is    Ethan A. Chi, John Hewitt, and Christopher D. Manning.\nthe python package we used to implement all our      2020.  Finding universal grammatical relations in\n                                                             multilingual bert. Preprint, arXiv:2005.04511.\nexperiments. We thank Hannes Wendler for multi-\nple fruitful discussions.                          Nadezhda Chirkova and Vassilina Nikoulina. 2024.\n                                                        Zero-shot cross-lingual transfer in instruction tun-\n                                                         ing of large language models.   arXiv preprint\n                                                        arXiv:2402.14778.References\n                                                     Rochelle Choenni and Ekaterina Shutova. 2020. WhatViraat Aryabumi,  John Dang, Dwarak  Talupuru,\n                                                       does it mean to be language-agnostic? probing multi-  Saurabh Dash, David Cairuz, Hangyu Lin, Bharat\n                                                              lingual sentence encoders for typological properties.   Venkitesh, Madeline Smith, Jon Ander Campos,\n                                                        arXiv preprint arXiv:2009.12862.  Yi Chern Tan, Kelly Marchisio, Max Bartolo, Se-\n   bastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick                                                      Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\n   Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee,                                                       Vishrav Chaudhary, Guillaume Wenzek, Francisco\n  Ahmet Üstün, and Sara Hooker. 2024.  Aya 23:                                               Guzmán, Edouard Grave, Myle Ott, Luke Zettle-\n  Open weight releases to further multilingual progress.                                                     moyer, and Veselin Stoyanov. 2020a.  Unsuper-\n   Preprint, arXiv:2405.15032.                                                         vised cross-lingual representation learning at scale.\n                                                              Preprint, arXiv:1911.02116.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\n  Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei    Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\n  Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,      moyer, and Veselin Stoyanov. 2020b.  Emerging\n   Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,       cross-lingual structure in pretrained language mod-\n  Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,        els. In Proceedings of the 58th Annual Meeting of\n  Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong       the Association for Computational Linguistics, pages\n   Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang      6022–6034.\n  Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\n  Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi    Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n  Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,       Kristina Toutanova. 2018. Bert: Pre-training of deep\n  Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-       bidirectional transformers for language understand-\n   gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.       ing. Preprint, arXiv:1810.04805.\n  Qwen technical report. Preprint, arXiv:2309.16609.\n                                            Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nTom Brown, Benjamin Mann, Nick Ryder, Melanie      Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind      Akhil Mathur, Alan Schelten, Amy Yang, Angela\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda      Fan, et al. 2024. The llama 3 herd of models. arXiv\n   Askell, et al. 2020. Language models are few-shot       preprint arXiv:2407.21783.\n   learners. Advances in neural information processing\n                                                  Constanza Fierro, Negar Foroutan, Desmond Elliott,   systems, 33:1877–1901.\n                                                  and Anders Søgaard. 2025. How do multilingual\n                                                      language models remember facts?  arXiv preprintPengfei Cao, Yuheng Chen, Zhuoran Jin, Yubo Chen,\n                                                        arXiv:2410.14387.  Kang Liu, and Jun Zhao. 2024. One mind, many\n   tongues: A deep dive into language-agnostic knowl-                                                    Jaden Fiotto-Kaufman, Alexander R Loftus, Eric Todd,\n  edge neurons in large language models.   arXiv                                                        Jannik Brinkmann, Caden Juang, Koyena Pal, Can\n   preprint arXiv:2411.17401.                                                        Rager, Aaron Mueller, Samuel Marks, Arnab Sen\n                                                   Sharma, et al. 2024. Nnsight and ndif: Democra-\nTing-Yun Chang and Yun-Nung Chen. 2019. What does                                                                tizing access to foundation model internals. arXiv\n   this word mean? explaining contextualized embed-                                                             preprint arXiv:2407.14561.\n   dings with natural language definition. In Proceed-\n   ings of the 2019 Conference on Empirical Methods   Noah Gardner, Hafiz Khan, and Chih-Cheng Hung.\n   in Natural Language Processing and the 9th Inter-      2022.  Definition modeling: literature review and\n   national Joint Conference on Natural Language Pro-       dataset analysis.  Applied Computing and Intelli-\n   cessing (EMNLP-IJCNLP), pages 6064–6070.            gence, 2(1):83–98.\n\nHaozhe Chen, Carl Vondrick, and Chengzhi Mao. 2024a.    Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh\n   Selfie: Self-interpretation of large language model      Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\n  embeddings. arXiv preprint arXiv:2403.10949.         man, and Christopher Potts. 2022. Inducing causal\n\n\n                                         10\n\n   structure for interpretable neural networks.  In In-    Nostalgebraist. 2020. Interpreting gpt: The logit lens.\n   ternational Conference on Machine Learning, pages      LessWrong.\n  7324–7338. PMLR.\n                                               OpenAI. 2023. Gpt-4 technical report. arXiv preprint\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce,      arXiv:2303.17548.\n  Lucas Dixon, and Mor Geva. 2024.  Patchscope:\n A unifying framework for inspecting hidden rep-   Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.\n   resentations of language models.  arXiv preprint    How multilingual is multilingual bert?   Preprint,\n  arXiv:2401.06102.                                    arXiv:1906.01502.\n\n                                                 Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-\n                                                       Dario Amodei, Ilya Sutskever, et al. 2019. Language   sch, Chris Bamford, Devendra Singh Chaplot, Diego\n                                                    models are unsupervised multitask learners. OpenAI  de las Casas, Florian Bressand, Gianna Lengyel, Guil-\n                                                             blog, 1(8):9.  laume Lample, Lucile Saulnier, et al. 2023. Mistral\n  7B. arXiv preprint arXiv:2310.06825.\n                                                          Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:\n                                                      Sentence embeddings using siamese bert-networks.János Kramár, Tom Lieberum, Rohin Shah, and Neel\n                                                            In Proceedings of the 2019 Conference on Empirical  Nanda. 2024. Atp*: An efficient and scalable method\n                                                 Methods in Natural Language Processing. Associa-   for localizing llm behaviour to components. arXiv\n                                                                tion for Computational Linguistics.   preprint arXiv:2403.00745.\n                                                        Nils Reimers and Iryna Gurevych. 2020.  Making\nXiaochen Li, Zheng Xin Yong, and Stephen Bach. 2024.\n                                                     monolingual sentence embeddings multilingual us-\n   Preference tuning for toxicity mitigation generalizes                                                         ing knowledge distillation.  In Proceedings of the\n   across languages.  In Findings of the Association                                                2020 Conference on Empirical Methods in Natural\n   for Computational Linguistics: EMNLP 2024, pages\n                                                   Language Processing. Association for Computational\n  13422–13440.\n                                                               Linguistics.\n\nJindˇrich Libovický,  Rudolf Rosa, and Alexander                                                    Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan\n   Fraser. 2020. On the language neutrality of pre-                                                            Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-\n   trained multilingual representations. arXiv preprint                                                                    tilingual instruction tuning with just a pinch of multi-\n  arXiv:2004.05160.                                                                    linguality. arXiv preprint arXiv:2401.01854.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan                                                     Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\n   Belinkov. 2022. Locating and editing factual associ-                                                            Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,\n   ations in gpt. Advances in Neural Information Pro-                                                   Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan\n   cessing Systems, 35:17359–17372.                                                      Das, and Jason Wei. 2022. Language models are\n                                                           multilingual chain-of-thought reasoners.  Preprint,\nTimothee Mickus, Kees Van Deemter, Mathieu Con-                                                        arXiv:2210.03057.\n   stant, and Denis Paperno. 2022. Semeval-2022 task\n   1: Codwoe–comparing dictionaries and word embed-   Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n   dings. arXiv preprint arXiv:2205.13858.              Yan Liu. 2020.  Mpnet: Masked and permuted\n                                                              pre-training for language understanding. Preprint,\nBasel Mousi, Nadir Durrani, Fahim Dalvi, Majd      arXiv:2004.09297.\n  Hawasly, and Ahmed Abdelali. 2024.  Exploring\n  alignment in shared cross-lingual spaces.  arXiv    Karolina Sta´nczak, Edoardo Ponti, Lucas Torroba Hen-\n   preprint arXiv:2405.14535.                               nigen, Ryan Cotterell, and Isabelle Augenstein. 2022.\n                                            Same neurons, different languages: Probing mor-\nBenjamin Muller, Yanai Elazar, Benoît Sagot, and      phosyntax in multilingual pre-trained models. arXiv\n  Djamé Seddah. 2021. First align, then predict: Un-       preprint arXiv:2205.02023.\n   derstanding the cross-lingual ability of multilingual\n   bert. In Proceedings of the 16th Conference of the Eu-    Tianyi Tang, Wenyang Luo, Haoyang Huang, Dong-\n  ropean Chapter of the Association for Computational     dong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei,\n   Linguistics: Main Volume, pages 2214–2231.            and Ji-Rong Wen. 2024. Language-specific neurons:\n                                                 The key to multilingual capabilities in large language\nRoberto Navigli, Michele Bevilacqua, Simone Conia,      models. arXiv preprint arXiv:2402.16438.\n  Dario Montagnini, Francesco Cecconi, et al. 2021.\n  Ten years of babelnet: A survey. In IJCAI, pages   Gemma Team, Morgane  Riviere,  Shreya  Pathak,\n  4559–4567. International Joint Conferences on Arti-       Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\n   ficial Intelligence Organization.                              raju, Léonard Hussenot, Thomas Mesnard, Bobak\n                                                              Shahriari, Alexandre Ramé, Johan Ferret, Peter\nThanapon Noraset, Chen Liang, Larry Birnbaum, and       Liu, Pouya Tafti, Abe Friesen, Michelle Casbon,\n  Doug Downey. 2017. Definition modeling: Learning      Sabela Ramos, Ravin Kumar, Charline Le Lan,\n   to define word embeddings in natural language. In    Sammy Jerome, Anton Tsitsulin, Nino Vieillard,\n  Proceedings of the AAAI Conference on Artificial       Piotr Stanczyk, Sertan Girgin, Nikola Momchev,\n   Intelligence, volume 31.                              Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill,\n\n\n                                         11\n\n  Behnam Neyshabur, Olivier Bachem, Alanna Wal-       bert, Amjad Almahairi, Yasmine Babaei, Nikolay\n   ton, Aliaksei Severyn, Alicia Parrish, Aliya Ah-      Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\n  mad, Allen Hutchison, Alvin Abdagic, Amanda      Bhosale,  et  al. 2023.  Llama 2:  Open founda-\n   Carl, Amy Shen, Andy Brock, Andy Coenen, An-       tion and fine-tuned chat models.  arXiv preprint\n  thony Laforge, Antonia Paterson, Ben Bastian, Bilal      arXiv:2307.09288.\n   Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu\n  Kumar, Chris Perry, Chris Welty, Christopher A.   Alexandre Variengien and Eric Winsor. 2023. Look\n  Choquette-Choo, Danila Sinopalnikov, David Wein-      before you leap: A universal emergent decomposition\n   berger, Dimple Vijaykumar, Dominika Rogozi´nska,      of retrieval tasks in language models. arXiv preprint\n  Dustin Herbison, Elisa Bandy, Emma Wang, Eric      arXiv:2312.10091.\n  Noland, Erica Moreira, Evan Senter, Evgenii Elty-\n   shev, Francesco Visin, Gabriel Rasskin, Gary Wei,   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n  Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna       Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n   Klimczak-Pluci´nska, Harleen Batra, Harsh Dhand,       Kaiser, and Illia Polosukhin. 2017. Attention is all\n   Ivan Nardini, Jacinda Mein, Jack Zhou, James Svens-     you need. Advances in neural information processing\n   son, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana       systems, 30.\n   Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fer-\n  nandez, Joost van Amersfoort, Josh Gordon, Josh                                                       Chris Wendler, Veniamin Veselovsky, Giovanni Monea,\n   Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mo-                                                   and Robert West. 2024. Do llamas work in english?\n  hamed, Kartikeya Badola, Kat Black, Katie Mil-                                                on the latent language of multilingual transformers.\n   lican, Keelin McDonell, Kelvin Nguyen, Kiranbir                                                        arXiv preprint arXiv:2402.10588.\n  Sodhia, Kish Greene, Lars Lowe Sjoesund, Lau-\n   ren Usui, Laurent Sifre, Lena Heuermann, Leti-                                                 Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Ji-\n   cia Lago, Lilly McNealus, Livio Baldini Soares,                                                       asen Lu, and Yoon Kim. 2024. The semantic hub\n  Logan Kilpatrick, Lucas Dixon, Luciano Martins,                                                           hypothesis: Language models share semantic repre-\n  Machel Reid, Manvinder Singh, Mark Iverson, Mar-                                                               sentations across languages and modalities. Preprint,\n   tin Görner, Mat Velloso, Mateo Wirth, Matt Davi-                                                        arXiv:2411.04986.\n  dow, Matt Miller, Matthew Rahtz, Matthew Watson,\n  Meg Risdal, Mehran Kazemi, Michael Moynihan,\n                                                    Zhihui Xie, Handong Zhao, Tong Yu, and Shuai Li.  Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi\n                                                       2022. Discovering low-rank subspaces for language-  Rahman, Mohit Khatwani, Natalie Dao, Nenshad\n                                                          agnostic multilingual representations. In Proceed-   Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay\n                                                          ings of the 2022 Conference on Empirical Methods  Chauhan, Oscar Wahltinez, Pankil Botarda, Parker\n                                                               in Natural Language Processing, pages 5617–5633,  Barnes, Paul Barham, Paul Michel, Pengchong\n                                           Abu Dhabi, United Arab Emirates. Association for   Jin, Petko Georgiev, Phil Culliton, Pradeep Kup-\n                                                      Computational Linguistics.   pala, Ramona Comanescu, Ramona Merhej, Reena\n   Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan\n                                                         Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,   Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah\n                                             Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and  Cogan, Sarah Perrin, Sébastien M. R. Arnold, Se-\n                                                      Colin Raffel. 2021. mt5: A massively multilingual   bastian Krause, Shengyang Dai, Shruti Garg, Shruti\n                                                             pre-trained text-to-text transformer. In Proceedings   Sheth, Sue Ronstrom, Susan Chan, Timothy Jor-\n                                                                 of the 2021 Conference of the North American Chap-   dan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas\n                                                                 ter of the Association for Computational Linguis-  Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav,\n                                                                         tics: Human Language Technologies. Association for  Vilobh Meshram, Vishal Dharmadhikari, Warren\n                                                      Computational Linguistics.   Barkley, Wei Wei, Wenming Ye, Woohyun Han,\n  Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong,\n  Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand   Hongchuan Zeng, Senyu Han, Lu Chen, and Kai\n  Rao, Minh Giang, Ludovic Peran, Tris Warkentin,      Yu. 2024.  Converging to a lingua franca: Evolu-\n   Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia       tion of linguistic regions and semantics alignment\n   Hadsell, D. Sculley, Jeanine Banks, Anca Dragan,       in multilingual large language models.  Preprint,\n  Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hass-      arXiv:2410.11718.\n   abis, Koray Kavukcuoglu, Clement Farabet, Elena\n  Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Ar-  A  Patchscope experiment\n  mand Joulin, Kathleen Kenealy, Robert Dadashi,\n  and Alek Andreev. 2024. Gemma 2: Improving                                We performed an additional experiment using the\n  open language models at a practical size. Preprint,\n                                                patchscope lens (Ghandeharioun et al., 2024) to  arXiv:2408.00118.\n                                                         collect more evidence about from which layer it is\nEric Todd, Millicent L Li, Arnab Sen Sharma, Aaron    possible to decode the source concept in Figure 7.\n   Mueller, Byron C Wallace, and David Bau. 2023.   The results of this experiment corroborate the find-\n  Function vectors in large language models. arXiv\n                                                     ings presented in Section 4. To enable a convenient   preprint arXiv:2310.15213.\n                                             comparison of the experimental results, we also\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-   include Figure 3 in Figure 7.\n\n\n                                         12\n\n    (a) Activation patching          (b) Patchscope lens\n    1.0                                                   1.0\n                                                                                                      it\n    0.8                                                   0.8       en\n\n    0.6                                                   0.6probability                                                                                                                                                                   probability\n    0.4                                src it               0.4\n                                      src zhconcept                                    tgt it                        concept\n    0.2                                 tgt zh              0.2\n                                      src + tgt en\n    0.0                                                   0.0      0     5    10    15    20    25    30        0     5    10    15    20    25    30\n                           layer                                                 layer\n\nFigure 7: (a) Our first patching experiment with a DE\nto IT source prompt and a FR to ZH target prompt with\ndifferent concepts. (b) Our patchscope lens experiment\nwith a DE to IT source prompt and identity target prompt\n king king\\n1135 1135\\nhello hello\\n?   . We patch at\n                                            Algorithm 1 Construction of Translation Pairs forthe last token respectively. For each of the plots the\nx-axis shows at which layer the patching was performed    Patching Experiments\nduring the forward pass on the target prompt and the   Require: Set of source languages LS, target lan-\ny-axis shows the probability of predicting the correct                                                 guage ℓT , number of pairs n\nconcept in language ℓ(see legend). In the legend the\n                                             Ensure: Set of valid translation pairs P\nprefix “src” stands for source and “tgt” for target con-\n                                                                     1: Load BabelNet translations for all languagescept. The orange dashed line and blue dash-dotted line\ncorrespond to the mean accuracy on source and target       2: TS ←GETTRANSLATIONS(LS)\nprompt. We report means and 95% Gaussian confidence       3: TT ←GETTRANSLATIONS(ℓT )\nintervals computed over 200 source-, target prompt pairs       4: P ←∅                 ▷Initialize valid pairs\nfeaturing 41 source concepts and 38 target concepts for       5: A ←COMBINATIONS(TS, TT ) ▷All possible\n(a) and 38 prompts for (b).                                                            pairs\n                                                                     6: SHUFFLE(A)          ▷Randomize order\nB  Translation Pair Construction\n                                                                     7: for (wS, wT ) ∈A do\nTo ensure reproducibility of our experiments,      8:       if concept(wS) = concept(wT ) then\nwe  provide  the  pseudocode  for  construct-      9:        continue      ▷Skip same concepts\ning  translation  pairs  used  in  our  activation     10:    end if\npatching experiments.   The complete imple-    11:       if HASTOKENCOLLISIONS(wS, wT ) then\nmentation  is  available  in  our  codebase  at     12:        continue     ▷Skip pairs with token\nnotebooks/obj_patch_translation.ipynb.           overlap\nKey constraints:                                          13:    end if\n                                                              14:   P ←P ∪{(wS, wT )}\n    • Different concepts: We ensure that source     15:       if |P| ≥n then\n    and target words represent different concepts     16:       break     ▷Sufficient pairs collected\n     to enable meaningful patching experiments.      17:    end if\n                                                              18: end for\n    • No token collisions: As described in Sec-                                                              19: return P\n     tion 3, we track sets of tokens w(Cℓ) for each\n     concept-language pair. To ensure clean prob-\n     ability measurements, we verify that there is\n    no overlap between the token sets of paired\n     concepts across all languages used in the ex-\n     periment.\n\n    • Randomization: Pairs are shuffled before se-\n     lection to avoid systematic biases in concept\n     or language selection.\n\n  For our experiments, we typically use n = 200\npairs, with concepts drawn from the 200 picturable\nwords from the Basic English word list, ensuring\n\n\n                                         13\n\nsufficient statistical power while maintaining com-     A: “CATDE” - B: “DOGIT”\n                                                      A: “OWLJA” - B: “SUNHI”putational feasibility.\n                                                      A: “ICEFR” - B: “\n\nC  Random prompt task experiment                                       By doing this, the latent of the source prompt is\n                                                       similar in terms of prompt structure, but the modelIn order to investigate the leftmost part of Figure 7a\n                                                cannot infer a task vector specifying the outputmore deeply, we performed additional experiments\n                                               language since the source prompt instantiates anin which we explore “random” source prompts in-\n                                                   impossible task (to predict a random word in a ran-stead of translation source prompts.\n                                    dom language). As shown in Figure 8a, for layers  The experimental setting here is similar to the\n                                               0–11, we observe no drop in the accuracy, whichone in Sec. 4, except for the fact that instead of\n                                                 confirms our hypothesis that in those layers the la-patching in latents from a translation source prompt\n                                                         tent at last token position contains no informationwe patch latents from different “random” source\n                                                         specific to the translation task.prompts. For the random source prompts, we grad-\n                                                        Instead, we think that in our chosen promptingually move away from the prompting template.\n                                                  template the last token, which is a quotation mark,\n                                                merely indicates where to put the translation result.      (a) Random prompt             (b) Empty prompt\n    1.0                                                   1.0                        In order to investigate this, we performed further\n                tgt zh                                                   tgt zh\n    0.8         tgt it                                      0.8                       patching experiments investigating how changes\n    0.6                                                   0.6                          in the prompting template in the source promptprobability                                                                                                                                                                   probability\n    0.4                                                   0.4                          affects the target forward pass ability to compute\nconcept                                                                                                               concept\n    0.2                                                          0.2                      an answer.\n\n    0.0                                                          0.0      0     5    10    15    20    25    30                                                   0     5    10    15    20    25    30   Empty context. For example, replacing the source\n                           layer                                                 layer\n                                            prompt with an empty prompt, merely containing\n(c) Random prompt with “@” (d) Random shuffled prompt    B: “  results in Figure 8b. In contrast to Figure 8a,\ninstead of quotation mark    (random hidden state)\n    1.0                                                   1.0                        the target concept in target language probability\n                tgt zh                                                   tgt zh\n    0.8                                                   0.8                       drops already starting from layer 4. We think this\n\n    0.6                                                          0.6                              is due to the fact that until layer 4 the quotationprobability                                                                                                                                                                               probability                                           mark token information which is shared among the\nconcept0.4                                                                                                  concept0.4                    two prompting templates “dominates” the latent\n    0.2                                                   0.2\n                                                    representation and is not yet converted to a task\n    0.0                                                   0.0      0     5    10                       15    20    25    30        0     5    10                                                                    15    20    25    30     specific position marker yet. Then, starting from                           layer                                                                                layer\n                                                     layer 4 the latent representation of the last token\n                                                      also aggregates task specific information, in partic-\nFigure 8:  (a) activation patching experiment with a\n                                                            ular, the fact that the quotation mark in this taskrandomized source prompt (random concepts, and lan-\nguages, but same template) and a FR to ZH target prompt.   actually marks the position after which the trans-\n(b) we construct a source prompt with empty context.    lated word should be decoded. As a result, replac-\n(c) we replace the quotation mark with @ in the random    ing the task specific quotation mark embedding,\nsource prompt from (a). (d) we randomly shuffle the   which contains the information that the translated\nsource prompts from (c). We patch at the last token   word comes next, with the “empty-context”-one,\nrespectively. For each of the plots, the x-axis shows                                            which does not contain this information, results in\nat which layer the patching was performed during the\n                                                 a performance drop.forward pass on the target prompt and the y-axis shows\nthe probability of predicting the correct concept in lan-   Modified template. Next, replacing the quotation\nguage ℓ(see legend). We only plot the target (“tgt”)                                           marks by “@” (Figure 8c) in the random prompt,\nconcept, as there is no source concept to predict. We\n                                                          A: @CATDE@ - B: @DOGIT@\nreport means and 95% Gaussian confidence intervals     i.e.,  A: @OWLJA@ - B: @SUNHI@\ncomputed over 200 source-, target prompt pairs.                A: @ICEFR@ - B: @\n\n\nSame  template.     In  Figure  8a,  we  ran-      leads to a drop of performance for early layers,\ndomized both  input and  output language  as    but for layers 5–11, the model is not much affected\nwell  as concepts  in  the source prompts,  re-   by the patching. We postulate that at those layers,\nsulting  in  prompts  of  the  following  form:   position-marker tokens have been already mapped\n\n\n                                         14\n\nto a general position-marker feature that is similar   whereas the mean over language pairs does. This\nin between source and target forward pass, even     is intuitive, since there may be some languages in\nthough at input level different symbols have been   which the mapping from words to concept features\nused.                                                     results in the correct concept feature vector. There-\n                                                         fore, averaging over different language pairs canShuffled tokens. Lastly, in Figure 8d we try to\n                                                     increase the signal about the source concept. How-destroy all of the shared structure in between the\n                                                          ever, having additional random contexts stemmingsource and the target prompt by randomly shuffling\n                                            from the same language pair does not bring in anythe characters of the source prompts from the mod-\n                                                  information about the source concept.ified template task. As expected, the probability of\n                                            Note that Figure 9, Figure 11 and Figure 12the target concept in target language becomes very\n                                               are on the next two pages.low (albeit surprisingly not zero), which shows\nthat the task cannot be solved without the position                                             D.3  Similarity comparison\nmarker feature.\n                                We experiment with other languages and models\nD  Other models and languages                in Figure 13 and get the same trends as with our\n                                                         results in Figure 6. We also provide results for\nIn this section, we report results for additional mod-   another experiment in which instead of measuring\nels, namely, Mistral 7B (Jiang et al., 2023), Llama   embedding similarities, we measure perplexity on\n3 8B (Dubey et al., 2024), Qwen 1.5 7B (Bai et al.,   ground truth definitions in Figure 14. We did not\n2023) and Llama 2 70B (Touvron et al., 2023). We    include this metric in our analysis as it seemed to\nalso include Aya 23 8B (Aryabumi et al., 2024) for   have less granularity and is more sensitive to the\nthe mean patching experiment in App D.2.          syntax rather than being focused on the semantic.\n\nD.1  Exploratory analysis\n\nThe results of the exploratory analysis outlined in\nSec. 4 are in Figure 9.\n  As can be seen in Figure 9, the target concept\nin source language spike is smaller for Llama 3,\nMistral 7B v0.3 and Qwen 1.5 7B. This hints that\nfor those models, zℓ(out) and C computation overlap\nmore than for Llama-2-7B.\n\nD.2  Ruling out hypotheses\n\nIn this section, we report results for the experiments\nperformed in Sec. 5.\n  In addition, instead of just patching in the mean\nover different language pairs (Figure 11c, 12c), we\nalso patch in the mean over contexts composed\nof different concept words in Figure 11b, 12b. In\nparticular, we take the mean over 5 different few-\nshot contexts from the same language pair. E.g.:\n  Deutsch: “Dorf” - Italiano: “villaggio”\n         ...\n  Deutsch: “Buch\n            ...\n\n\n  Deutsch: “Zitrone” - Italiano: “limone”\n         ...\n  Deutsch: “Buch\n\n  Our results in Figure 11 and Figure 12 show that\nthe mean over contexts does not increase P(CℓTS  ),\n\n\n                                         15\n\n                      (a) Mistral-7B v0.3                                                  (b) Llama3-8b\n 1.0                                                     1.0\n\n 0.8                                                     0.8\n\n 0.6                                                     0.6\n\n 0.4        src it                                        0.4        src it\n            src zh                                                   src zh\n             tgt it                                                      tgt it\n 0.2                                                     0.2             tgt zh                                                     tgt zh\n            src + tgt en                                             src + tgt en\n 0.0                                                     0.0   0     5     10    15    20    25    30        0     5     10    15    20    25    30\n                         layer                                                   layer\n\n\n                        (c) Qwen1.5-7B                                                   (d) Llama2-70B\n 1.0                                                     1.0\n\n 0.8                                                     0.8\n\n 0.6                                                     0.6\n\n 0.4        src it                                        0.4                                  src it\n            src zh                                                                             src zh\n             tgt it                                                                                tgt it\n 0.2                                                     0.2             tgt zh                                                                               tgt zh\n            src + tgt en                                                                       src + tgt en\n 0.0                                                     0.0   0     5     10    15    20    25    30        0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75\n                         layer                                                   layer\n\nFigure 9: Our first patching experiment with a DE to IT source prompt and a FR to ZH target prompt with different\nconcepts. We patch at the last token. For each of the plots the x-axis shows at which layer the patching was\nperformed during the forward pass on the target prompt and the y-axis shows the probability of predicting the correct\nconcept in language ℓ(see legend). In the legend the prefix “src” stands for source and “tgt” for target concept. The\norange dashed line and blue dash-dotted line correspond to the mean accuracy on source and target prompt. We\nreport means and 95% Gaussian confidence intervals computed over 200 source-, target prompt pairs featuring 41\nsource concepts and 38 target concepts.\n\n\n\n\n\n                                         16\n\nFigure 10: Mean patching experiment replicated on multiple languages with 200 pairs each and 95% confidence\ninterval.\n\n\n\n\n\n                                         17\n\n          (a) Single source setup     (b) Mean over contexts    (c) Mean over language pairs\n                   1.0                                                      1.0                                                           1.0\n                               src zh                                                     src zh\n                                tgt zh                                                      tgt zh\n                   0.8        src + tgt en                                  0.8        src + tgt en                                       0.8\n\n                   0.6                                                      0.6                                                           0.6        src zh                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability            tgt zh\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4        src + tgt en\n                   0.2                                                      0.2                                                           0.2\n\n                   0.0                                                      0.0                                                           0.0                   0     5    10    15    20    25    30           0     5    10    15    20    25    30               0     5    10    15    20    25    30\n                                          layer                                                    layer                                                         layer\n\n                                      Llama-2 7B\n\n                   1.0                                                      1.0                                                           1.0\n\n                   0.8                                                      0.8                                                           0.8\n\n                   0.6                                                      0.6                                                           0.6                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4\n                   0.2                                                      0.2                                                           0.2\n\n                   0.0                                                      0.0                                                           0.0                   0     5    10    15    20    25    30           0     5    10    15    20    25    30               0     5    10    15    20    25    30\n                                          layer                                                    layer                                                         layer\n\n                                      Llama-3 8B\n\n                   1.0                                                      1.0                                                           1.0\n\n                   0.8                                                      0.8                                                           0.8\n\n                   0.6                                                      0.6                                                           0.6                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4\n                   0.2                                                      0.2                                                           0.2\n\n                   0.0                                                      0.0                                                           0.0                   0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75            0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75                0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75\n                                          layer                                                    layer                                                         layer\n\n                                  LLama-2 70B\n\nFigure 11: Here we use different input languages (DE, FR), different concepts, different output languages (IT, ZH) in\n(a). In (b) we use the same source and target language pairs as in (a). In (c) we use multiple source input languages\nDE, NL, ZH, ES, RU and output languages IT, FI, ES, RU, KO. We patch at the last token of the concept-word at all\nlayers from j to 31. In (a) we patch latents from the single source prompt. In (b) for each concept, we patch the\naverage latent over different few-shot DE to IT translation contexts. In (c) we patch the mean of the latents over\nthe source prompts. For each of the plots, the x-axis shows at which layer the patching was performed during the\nforward pass on the target prompt and the y-axis shows the probability of predicting the correct concept in language\nℓ(see legend). The prefix “src” stands for source and “tgt” for target concept. We report means and 95% Gaussian\nconfidence intervals computed over a dataset of size 200.\n\n\n\n\n\n                                         18\n\n          (a) Single source setup     (b) Mean over contexts    (c) Mean over language pairs\n                   1.0                                                      1.0                                                           1.0\n\n                   0.8                                                      0.8                                                           0.8\n\n                   0.6                                                      0.6                                                           0.6                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4\n                   0.2                                                      0.2                                                           0.2\n\n                   0.0                                                      0.0                                                           0.0                   0     5    10    15    20    25    30           0     5    10    15    20    25    30               0     5    10    15    20    25    30\n                                          layer                                                    layer                                                         layer\n\n                                           Mistral 7B v0.3\n\n                   1.0                                                      1.0                                                           1.0\n\n                   0.8                                                      0.8                                                           0.8\n\n                   0.6                                                      0.6                                                           0.6                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4\n                   0.2                                                      0.2                                                           0.2\n\n                   0.0                                                      0.0                                                           0.0                   0     5    10    15    20    25    30           0     5    10    15    20    25    30               0     5    10    15    20    25    30\n                                          layer                                                    layer                                                         layer\n\n                                    Aya 23 8B\n\n                   1.0                                                      1.0                                                           1.0\n                               src zh                                                     src zh\n                                tgt zh                                                      tgt zh\n                   0.8        src + tgt en                                  0.8        src + tgt en                                       0.8\n\n                   0.6                                                      0.6                                                           0.6                                                  probability                                                                                                                                                                              probability                                                                                                                                                                                              probability\n                                concept0.4                                                                                                         concept0.4                                                                                                                   concept0.4        src zh\n                   0.2                                                      0.2                                                           0.2         tgt zh\n                                                                                                                                                         src + tgt en\n                   0.0                                                      0.0                                                           0.0                   0          5         10         15              0          5         10         15                  0          5         10         15\n                                          layer                                                    layer                                                         layer\n\n                             Gemma 2B\n\nFigure 12: Here we use different input languages (DE, FR), different concepts, different output languages (IT, ZH) in\n(a). In (b) we use the same source and target language pairs as in (a). In (c) we use multiple source input languages\nDE, NL, ZH, ES, RU and output languages IT, FI, ES, RU, KO. We patch at the last token of the concept-word at all\nlayers from j to 31. In (a) we patch latents from the single source prompt. In (b) for each concept, we patch the\naverage latent over different few-shot DE to IT translation contexts. In (c) we patch the mean of the latents over\nthe source prompts. For each of the plots, the x-axis shows at which layer the patching was performed during the\nforward pass on the target prompt and the y-axis shows the probability of predicting the correct concept in language\nℓ(see legend). The prefix “src” stands for source and “tgt” for target concept. We report means and 95% Gaussian\nconfidence intervals computed over a dataset of size 200.\n\n\n\n\n\n                                         19\n\n        Target: English (156 concepts)      Target: French (29 concepts)      Target: Chinese (38 concepts)\n   1.0\n                                                                                                                               Similarity with mean\n                                                                                                      ground truth embedding\n   0.8                                                                                                                         Similarity with\n                                                                                                                 other concepts\n   0.6                                                                                                              Multi-Source Translationsimilarity                                                                                                                 Multi-Source Definition\nMean0.4                                                                                                            Single-SourceSingle-Source TranslationDefinition\n   0.2                                                                                         Word Patching\n                                                                                                          Prompting\n   0.0                                                                                                 Repeat Word\n\n                                                 (a) Aya 23 8B\n         Target: English (38 concepts)      Target: French (144 concepts)     Target: Estonian (32 concepts)\n   1.0\n                                                                                                                               Similarity with mean\n                                                                                                      ground truth embedding\n   0.8                                                                                                                         Similarity with\n                                                                                                                 other concepts\n   0.6                                                                                                              Multi-Source Translationsimilarity                                                                                                                 Multi-Source Definition\nMean0.4                                                                                                            Single-SourceSingle-Source TranslationDefinition\n   0.2                                                                                         Word Patching\n                                                                                                          Prompting\n   0.0                                                                                                 Repeat Word\n\n                                             (b) Gemma 2 2B\n\nFigure 13: Mean similarity between the definition and the mean embedding of the ground truth definitions, as well\nas the mean similarity between the definition embedding and the embeddings of the definitions of the other concepts\nin the dataset. For Aya, the results are presented for three target languages: English (with source languages French\nand German and input language Spanish), French (with source languages Korean, Japanese, Estonian, Finnish and\ninput language English), and Chinese (with source languages Italian, Finnish, Spanish, Russian, Korean and input\nlanguages German, Dutch, Chinese, Spanish, Russian). For Gemma, we show English (with source languages\nItalian, Finnish, Spanish, Russian, Korean and input languages German, Dutch, Chinese, Spanish, Russian), French\n(with source languages Spanish, German and input language Italian), and Estonian (with source languages English,\nFrench, Chinese, German and input language Hindi). We report means and 95% Gaussian confidence intervals\ncomputed over the dataset.\n\n\n\n\n\n         Target: English (61 concepts)      Target: French (29 concepts)      Target: Chinese (38 concepts)\n   3.0\n                                                                                        Mean loss\n   2.5                                                                                    Mean loss with\n                                                                                                                 other concepts\n   2.0loss\n                                                                                                                    Multi-Source Translation\n                                                                                                                    Multi-Source DefinitionMean1.5\n   1.0                                                                                                            Single-Source Translation\n                                                                                                                  Single-Source Definition\n   0.5                                                                                         Word Patching\n                                                                                                          Prompting\n   0.0\n\n           Figure 14: Mean loss on the ground truth definitions. We report 95% confidence intervals.\n\n\n\n\n\n                                         20\n\n## Claim to Classify\n\"patching with the mean representation of a concept across different languages improves translation\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Activation patching",
    "method_rung": 2,
    "claim_rung": 2,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is activation patching, which is explicitly stated in the paper title and throughout the text. Activation patching is a causal intervention method that measures the effect of replacing activations, placing it at Method Rung 2. The claim 'patching with the mean representation of a concept across different languages improves translation' uses the verb 'improves', which indicates a causal effect on performance. This aligns with Rung 2 causal claims such as 'intervening on X changes Y' or 'is sufficient for'. There is no mechanistic or uniqueness language (e.g., 'encodes', 'the circuit', 'uniquely represents') that would elevate it to Rung 3. The claim is stated as a direct empirical result without hedging, hence hedge_flag=0. The claim and method are well-aligned with no overclaiming."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 27031,
      "completion_tokens": 228,
      "total_tokens": 27259
    },
    "finish_reason": "stop"
  }
}