{
  "timestamp": "2026-02-19T11:07:50.876183+00:00",
  "paper_id": "2411.16105",
  "claim_id": "2411.16105-04",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2411.16105\nTitle: Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability\nFull text:\n          ADAPTIVE CIRCUIT BEHAVIOR AND\n          GENERALIZATION IN MECHANISTIC\n           INTERPRETABILITY\n\n\n                        Jatin Nainani*, Sankaran Vaidyanathan*, AJ Yeung, Kartik Gupta, David Jensen\n                         University of Massachusetts Amherst\n\n\n                                       ABSTRACT\n\n                              Mechanistic interpretability aims to understand the inner workings of large neural2024                       networks by identifying circuits, or minimal subgraphs within the model that im-\n                             plement algorithms responsible for performing specific tasks. These circuits are\n                                  typically discovered and analyzed using a narrowly defined prompt format. How-\n                                   ever, given the abilities of large language models (LLMs) to generalize acrossDec\n                                various prompt formats for the same task, it remains unclear how well these cir-\n5                            cuits generalize. For instance, it is unclear whether the model’s generalization\n                                    results from reusing the same circuit components, the components behaving dif-\n                                    ferently, or the use of entirely different components. In this paper, we investigate\n                                 the generality of the indirect object identification (IOI) circuit in GPT-2 small,\n                           which is well-studied and believed to implement a simple, interpretable algorithm.\n                    We evaluate its performance on prompt variants that challenge the assumptions of\n                                     this algorithm. Our findings reveal that the circuit generalizes surprisingly well,[cs.LG]\n                                reusing all of its components and mechanisms while only adding additional input\n                                edges. Notably, the circuit generalizes even to prompt variants where the original\n                               algorithm should fail; we discover a mechanism that explains this which we term\n                           S2 Hacking. Our findings indicate that circuits within LLMs may be more flexible\n                            and general than previously recognized, underscoring the importance of studying\n                                     circuit generalization to better understand the broader capabilities of these models.\n\n\n                1  INTRODUCTION\n\n                      Mechanistic interpretability (Elhage et al., 2021) is an increasingly prominent approach to under-\n                       standing the inner workings of large neural networks. Much work in this area is dedicated to discov-\n                        ering circuits, which are subgraphs within the large model that faithfully represent how the model\n                        solves a particular task of interest (Olah et al., 2020), by identifying attention heads and paths with\n                          significant causal effects on the model output (Vig et al., 2020; Stolfo et al., 2023; Hanna et al.,\n                     2023; Prakash et al., 2024). By studying these circuits, researchers aim to uncover simple, human-arXiv:2411.16105v2\n                         interpretable algorithms that explain how the model solves the task.\n\n                    These circuits are typically analyzed only within the specific prompt format used to extract them.\n                    However, modern LLMs can often solve the same task across various prompt formats. This raises\n                       important questions about how a circuit generalizes when the prompt format is varied, especially\n                  when the full model generalizes.  For instance,  it is unclear whether the model’s generalization\n                          results from the reuse of the same circuit components, the components behaving differently, or the\n                      use of entirely different components. Evaluating the generality of a circuit can provide a deeper\n                      understanding of the circuit’s behavior and the range of scenarios in which the circuit serves as a\n                         valid explanation for the full model’s performance.\n\n                   The generality of circuits has significant implications for mechanistic interpretability. Since circuits\n                        are typically extracted and evaluated using a specific set of prompts, there is no prior expectation for\n                   them to generalize. In the worst case, a different prompt format might require a completely different\n                            circuit. Ideally, however, the same circuit would solve the task across all prompt variants, providing\n\n                           *Equal contribution.   Corresponding authors:  Jatin Nainani (jnainani@umass.edu), Sankaran\n                        Vaidyanathan (sankaranv@cs.umass.edu)\n\n\n                                                           1\n\n      1.0                                        When |John and |Mary went to the store, |John gave a drink to Mary\n      0.9                                                                                (S1)        (IO)                         (S2)\n      0.8                                         Perfect Generalization                            S1    S2    IO\n                                                       Generalization      0.7                                     Strong      {                                    Weak Generalization\n      0.6                                Bounded Generalization   Overlap                                No Generalization\n   Circuit 0.50.4                                        When |John and |Mary went to the store, |Mary was happy. |John gave a drink to <?>\n                                                                                        (S1)        (IO1)                       (IO2)                (S2)\n      0.3\n      0.2                                                                  S1    S2    IO1   IO2\n      0.1     { {\n      0.0              Change in Task Format\n\nFigure 1: Left: Different scenarios for the degree to which a circuit could change as the task format\nchanges. Right: The IOI algorithm (top) and the result of applying that algorithm to the DoubleIO\nprompt variant (bottom) where the subject and indirect object tokens are both duplicated.\n\n\n\na general and reliable explanation for how the model performs the task. Figure 1 presents several\nhypotheses for how a circuit could generalize as the prompt format changes. Given that the goal of\nmechanistic interpretability is to explain the behavior of the full model, limiting the explanation to\na narrow set of prompt formats restricts the insights we can gain about the full model.\n\nIndirect Object Identification (IOI) is one of the most well-studied tasks in the mechanistic inter-\npretability literature. The model is given a prompt such as “When John and Mary went to the store,\nJohn gave a drink to      ”, with the expected answer being “Mary”. Wang et al. (2023) discovered\na circuit in GPT-2 small that is said to implement a simple three-step algorithm: (1) identify all\nprevious names in the sentence (“Mary”, “John”, “John”); (2) remove all names that are duplicated\n(“John”); and (3) output the remaining name (“Mary”). We refer to this as the IOI algorithm.\n\nThis algorithm appears to be largely agnostic to prompt structure aside from the names, suggesting\nthe circuit may generalize broadly. However, it should clearly fail on prompts where both names are\nduplicated, since removing all duplicated names leaves no obviously correct answer. For example,\nconsider the prompt “When Mary and John went to the store, Mary was happy. John gave a drink to\n     ”, which breaks the IOI algorithm by adding three words. Yet, GPT-2 small returns the correct\nanswer with ∼89% confidence. If the model can be explained by a circuit that implements the IOI\nalgorithm, its ability to generalize to inputs where the algorithm itself predicts failure reveals a clear\ndiscrepancy and raises critical questions about the generality of the IOI circuit.\n\nIn this paper, we investigate the generality of the IOI circuit. We study its performance and behavior\non two prompt variants, DoubleIO and TripleIO, which are designed to challenge the assumptions\nof the IOI algorithm. Our key findings are:\n\n(a) The IOI circuit vastly outperforms the full model on prompt variants where the IOI algorithm\n   would completely fail. Despite this, most of the attention heads in the circuit still retain their\n    functionalities as specified in Wang et al. (2023).\n\n(b) We discover a mechanism in the IOI circuit, which we call S2 Hacking, that explains how the\n     circuit is able to outperform the model on the prompt variants. However, S2 Hacking only\n    appears in these variants and does not arise in the base IOI prompt format.\n\n(c) On discovering new circuits for the prompt variants, we find that they reuse all components of\n    the base IOI circuit, only adding edges from additional input tokens. The DoubleIO and TripleIO\n     circuits show 92% and 85% edge overlap respectively, and 100% node overlap in both cases.\n\nThese findings reveal that the base IOI circuit generalizes surprisingly well beyond its original task\ndesign, reusing many of its heads and paths to handle prompt variants effectively. They also highlight\nthe value of studying circuit generalization, to assess the external validity of the algorithms they are\nclaimed to implement and unveil deeper underlying complexities. Overall, these results represent\na significant step towards understanding the more general capabilities of large neural networks and\nfurther demonstrate the promise of mechanistic interpretability in achieving this goal.\n\n\n                                       2\n\n2  BACKGROUND AND RELATED WORK\n\nIn this work, we evaluate the circuit for indirect object identification (IOI) in GPT-2 small (Wang\net al., 2023) using variants of the prompt format that was originally used to discover the circuit. We\nrefer to the original prompt format as base IOI. In the example from Figure 1, “John” and “Mary”\nare referred to as the subject (S) and indirect object (IO) tokens respectively, and the IO token is\nthe expected answer. Each base IOI prompt contains one instance of the indirect object token (IO)\nand two instances of the subject token (S1, S2). We later introduce variants of this prompt that can\ninclude multiple instances of the indirect object token (IO1, IO2).\n\nAs in Wang et al. (2023), we measure the performance of circuits and the full model using the logit\ndifference metric, which is defined as the difference between the log probabilities of the IO token\nand the S token. A larger positive logit difference indicates that the model predicts the correct token\n(IO) with higher probability than the incorrect token (S).\n\nThe base IOI circuit contains a set of distinct attention head types, each of which performs a specific\nfunction in the overall mechanism of the circuit. We include the base IOI circuit diagram in Figure\n7, and further details can be found in Wang et al. (2023). We focus on the following head types:\n\n  • Name Mover heads are responsible for copying the correct token (IO) to the output. These\n    heads are active at the END token and attend to previous names in the sentence, copy the name\n    they attend to, and send it to the output.\n  • S-Inhibition heads ensure that the Name Mover heads focus on the IO token by modifying the\n    queries of the Name Mover heads and suppressing attention to the duplicated tokens (S1, S2).\n   They are active at the END token and attend to S2.\n  • Duplicate Token heads identify tokens that have appeared earlier in the sequence. These heads\n    are active at the duplicate instance of a token (S2), attend to its first occurrence (S1), and output\n    the position of the repeated token.\n  • Previous Token heads copy information from token S1 to the next token (S1+1), the token\n    facilitating the transfer of sequential token information.\n  • Induction heads serve a similar function to Duplicate Token Heads. They are active at the\n    position of the duplicated token (S2) and attend to the token that follows its previous instance\n   (S1+1). Their output is used as a pointer to S1, and to signal that S was duplicated.\n\nA small amount of prior work on circuit discovery has evaluated circuit generalization performance\nusing prompts outside of the format used for their discovery. However, these evaluations are often\nlimited and do not fully investigate how the circuit components and mechanisms behave under these\nprompt changes. Wang et al. (2023) evaluated the base IOI circuit on adversarial examples similar to\nthe DoubleIO variant we use in this paper, and observed a drop in model performance. We build on\nthis finding by evaluating the circuit itself and explaining the mechanism behind the model’s lower\nyet nontrivial performance. The Greater-Than (Hanna et al., 2023) and Arithmetic (Stolfo et al.,\n2023) circuits were evaluated on prompt variants and included circuit overlap comparisons, though\nthese studies are mostly qualitative and not explored in further detail. The EAP-IG circuit discovery\nmethod (Hanna et al., 2024) was also evaluated using circuit overlap and cross-task faithfulness.\n\n3  TESTING GENERALITY OF THE IOI CIRCUIT USING PROMPT VARIANTS\n\nInferring a circuit from model behavior is a critical step, but it is not sufficient on its own—such a\ncircuit acts as a formal hypothesis about the problem-solving mechanisms of the full model, and this\nhypothesis should be probed in various ways to determine the circumstances in which it holds. We\naim to assess the generality of the base IOI circuit and the range of circumstances in which its core\nmechanisms persist. To this end, we introduce two variants of the base IOI prompt that, according\nto the original description of the IOI algorithm, should be unsolvable by the base IOI circuit.\n\nIn this section, we present three key findings. First, we demonstrate that the unmodified base IOI\ncircuit is unexpectedly capable of sustaining its performance as we systematically vary the nature of\nthe task. Secondly, we demonstrate that the circuit significantly outperforms the model on the vari-\nants, showing consistently high logit difference scores while the model performance drops. Finally,\nwe show that most of the attention heads in the circuit behave nearly identically to how they would\n\n\n                                       3\n\non base IOI inputs. These findings suggest that the circuit effectively solves the task exactly as it\nwould on base IOI prompts, even though the necessary conditions for its success, as explained by\nthe IOI algorithm, are not met by the new prompts.\n\n3.1  IOI PROMPT VARIANTS\n\nThe IOI algorithm assumes that the subject (S) token is duplicated while the indirect object (IO)\ntoken is not, and accordingly functions by detecting and suppressing the duplicated token. We can\ntest how much the performance of the base IOI circuit relies on this assumption by manipulating the\nnumber of instances of the IO token. Our study includes the following range of prompts:\n\n Base IOI: When John and Mary went to the store, John gave a drink to         .\nDoubleIO: When John and Mary went to the store, Mary was happy. John gave a drink to         .\n  TripleIO: When John and Mary went to the store, Mary was happy. Mary sat on a bench. John\n           gave a drink to         .\n\nIn the DoubleIO variant, both the S and IO tokens are duplicated, making it unclear which one the\ncircuit should detect and suppress. In the TripleIO variant, the IO token appears three times and\nthe S token appears twice. Given the stated logic of the IOI algorithm, where the most frequently\nduplicated token is suppressed, the circuit should return the S token since it now appears most\nfrequently in the prompt.\n\n3.2  BASE IOI CIRCUIT PERFORMANCE ON VARIANTS\n\nWe first evaluate the performance of the full model as well as the base IOI circuit on datasets of 200\nprompts generated for each of the variants. Our data generation strategy follows from Wang et al.\n(2023); we create a set of template sentences for each variant, along with lists of names, places, and\nobjects, and sample from these to construct full sentences.\n\nTable 1 shows the logit difference scores for the full model and the base IOI circuit on each of\nthe variants. Faithfulness is defined as the ratio of the circuit’s logit difference to the model’s logit\ndifference, which is a measure of how closely the circuit’s performance aligns with that of the model.\n\n         Task      Model Logit Difference   Circuit Logit Difference   Faithfulness\n\n        Base IOI            3.484                   3.119              0.895\n        DoubleIO           2.118                   2.722              1.285\n         TripleIO            1.227                   3.174              2.586\n\nTable 1: Base IOI circuit performance on DoubleIO and TripleIO inputs. The circuit maintains high\nperformance while model performance drops, and faithfulness is far from the ideal value of 1.\n\nWhile we anticipated that the full model might generalize to these new tasks, the base circuit con-\nsistently outperforms the model on both DoubleIO and TripleIO variants with near-perfect accuracy.\nThis is particularly surprising because these variants were designed with the explicit purpose of be-\ning unsolvable by the circuit if it were to execute its hypothesized algorithm exactly. We also see\nthat the faithfulness scores for the base IOI circuit on the DoubleIO and TripleIO variants is far\nabove 1, since the model performance is lower on the variants while the circuit performance remains\nconsistently high. This indicates that the performance of the base IOI circuit on the prompt variants\nis not faithful to the full model, and inconsistent with the hypothesized explanation of the circuit.\n\nWang et al. (2023) also suggested using adversarial examples that are very similar to DoubleIO,\nthough they only evaluate the performance of the full model and do not explore how the circuit\nperforms on the same task. The performance of the full model on the DoubleIO task is still non-\ntrivial; a logit difference of 2.138 indicates that the model predicts IO over S ∼89% of the time.\n\n3.3  HEAD-LEVEL ATTENTION PATTERNS ON BASE IOI AND VARIANTS\n\nThe sharp deviation in performance between the base IOI circuit and the full model raises an impor-\ntant question: Are the elements of the circuit maintaining the same functions as they had on the base\n\n\n                                       4\n\n                                                     Deviation from model                                                                                         Deviation from model\n\n\n\n\n\n                   S-Inhibition       Duplicate       Induction      Previous  Name Movers                             S-Inhibition       Duplicate       Induction      Previous  Name Movers\n                                      (Layer, Head)                                                                          (Layer, Head)\n\nFigure 2: Deviation in attention scores from base IOI inputs to DoubleIO (left) and TripleIO (right)\ninputs for the base IOI circuit and full model. Nonzero values indicate deviation in behavior due to\nthe change in prompt format. For the circuit, most heads show low deviation (< 0.1), particularly the\nName Mover heads which are responsible for returning the output. Significant differences between\nthe circuit and model indicate that the base IOI circuit is less faithful on the prompt variants.\n\n\nIOI task, or have their functions changed in response to changes in the prompt? To examine this\nfurther, we calculate the average difference between the attention scores of each head in the base\nIOI circuit for its most relevant token position (specified in Wang et al. (2023)) on the base IOI and\nprompt variant datasets. Since the full model may respond differently to the prompt variants, we do\nthe same calculations for these heads in the model as well. The results are shown in Figure 2.\n\nWe observe minimal deviation in attention scores for most heads, typically within 0.05, with only\nS-Inhibition Head 8.6 deviating significantly for both variants. We also see deviation in Induction\nheads 5.5 and 5.8 for DoubleIO, though head 5.8 shows similar deviation in the full model as well. In\ncontrast, the full model exhibits greater deviation in attention scores between base IOI inputs and the\nvariants for several other heads, particularly the Name Mover heads. These heads are responsible for\nreturning the output, which highlights the disparity in performance between the circuit and model.\n\nOverall, these results suggest that most components of the base IOI circuit show lower deviations in\nattention patterns on base IOI inputs and the prompt variants compared to the full model. We later\nshow in Section 5 that these heads retain their original functionalities from the base IOI circuit.\n\n\n4  S2 HACKING: PERFORMANCE WITHOUT FAITHFULNESS\n\nWe observed in the previous section that the base IOI circuit significantly outperforms the full model\non the IOI prompt variants, with most of its components maintaining similar attention patterns to\nbase IOI inputs. In this section, we identify the source of the deviations in behavior and performance\nbetween the base IOI circuit and the full model.\n\nThe basis of the discrepancy between the circuit and the model performance is a mechanism in the\nbase IOI circuit which we term S2 Hacking. In the base IOI circuit, the Induction and Duplicate\nToken heads are primarily active at the S2 token, which is always the incorrect answer in each of the\nIOI prompt variants. The outputs of these heads are then used by the S-Inhibition heads to suppress\nthe attention on the S1 and S2 tokens. This mechanism is a key factor in how the circuit is able to\nreturn the correct (IO) token with high probability.\n\nHowever, evaluating this circuit requires knocking out all paths that are not part of the circuit using\nmean ablation (Wang et al., 2023). In the base IOI circuit, the S2 token is the only input token\nthat has a path from the input tokens to the END token that passes through the Duplicate Token and\nInduction heads. This is because the paths from all other input tokens were shown to have a low\ncausal effect on the model output during the circuit discovery process. As a result, the paths from\nall other input tokens to these heads are knocked out.\n\n\n                                       5\n\nFigure 3: S2 Hacking in S-Inhibition head 8.6. Left: Attention pattern at the END position for a\nDoubleIO prompt. Placing all attention on the S2 token would lead to near-perfect accuracy on the\ntask. Head 8.6 splits attention between IO2 and S2 in the full model, but in the base IOI circuit it\nfocuses primarily on S2. Right: Knockout procedure for evaluating circuits, where paths that are\nnot part of the circuit (marked in blue) are mean-ablated out. For head 8.6, the paths from all input\ntokens other than S2 are knocked out, leading to S2 Hacking.\n\n\n\nThis knockout procedure effectively points the circuit toward the correct answer every time, since\nthe S2 token is the only input to the Duplicate Token heads that is not knocked out. These heads\nfeed into the Induction and S-Inhibition heads, which are directed to only attend to subject-related\ntokens, S+1 and S2 respectively. As a result, the S-Inhibition heads always suppress attention on\nthe subject tokens (S1, S2), which pushes the Name Mover heads towards returning an IO token.\nThis mechanism of consistently suppressing the subject tokens and returning the IO token enables\nthe base IOI circuit to solve the task across all of the prompt variants.\n\nWe refer to this phenomenon as S2 Hacking. Note that this phenomenon only occurs in the base IOI\ncircuit, as it is a byproduct of the knockout procedure for evaluating the circuit and not actually how\nthe full model solves the task. This is evident from how much the circuit performance deviates from\nthe model performance. Additionally, S2 Hacking is not observed on base IOI inputs since they only\nhave one duplicated name (S), and was discovered by generalizing the prompt format. For the rest\nof this section, we focus on the DoubleIO prompt variant and demonstrate how S2 Hacking occurs\nwithin the base IOI circuit. Additional details on all metrics and experiments are in Appendix A.\n\n\n4.1  METRICS\n\nBased on the head types and their functions specified by Wang et al. (2023), each head is typically\ncharacterized by its focus on a specific token or set of tokens. We define the following metrics to\ncompare the attention scores of these specific tokens for each of these head types, to understand the\ndeviations in their behavior between the base IOI circuit and the full model.\n\n                        Attn(correct)                            Attn(token) in circuit\n     Confidence ratio =                       Functional faithfulness =\n                       Attn(incorrect)                            Attn(token) in model\n\nThe circuit confidence ratio for a given head is considered high (> 1) if its attention score in the\ncircuit is higher for the correct token than the incorrect token, and the model confidence ratio is\nsimilarly defined for the head in the full model.  If the circuit confidence ratio exceeds that of the\nmodel, it suggests that the circuit is more likely to attend to the correct token than the model is.\n\nFunctional faithfulness scores are used to compare the attention scores between the model and the\ncircuit for each relevant token; we focus particularly on the S2 and IO2 tokens. The ratio is close to\n1 if the circuit attends to the token as much as the model does, while a value greater than 1 indicates\nthat the circuit attends to the token more than the model does, demonstrating a difference in behavior\nfor the head. Confidence ratios and functional faithfulness scores for all heads in the circuit are given\nin Figure 4, where all metrics are plotted with confidence intervals based on 50 samples.\n\n\n4.2  TRACING THE S2 HACKING MECHANISM\n\nOur results demonstrate that the S2 Hacking mechanism is primarily carried out through S-Inhibition\nhead 8.6, Induction heads 5.9 and 5.5, and Duplicate head 3.0. Through this mechanism, we find\nthat all of the Name Mover heads show higher confidence in predicting an IO token over an S token.\nThe remaining heads in the circuit show similar behavior to their performance on base IOI prompts.\n\n\n                                       6\n\n                                                                                                                  Functional Faithfulness across Heads for Subject and IO tokens\n\n                                                                                                                  2.5                                                                       S\n                                                                                                                                                                                                     IO\n                                                                                                   S2 Hacking\n\n\n                                                                                                                  2.0\n\n\n\n\n\n                                                                                                                  1.5                             S2 Hacking                                                                                                                                                                                                                                                                                            Faithfulness\n\n\n                                                                                                                  1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Functional\n\n\n\n\n                                                                                                                  0.5\n\n\n\n\n\n                                                                                                                  0.0\n                                                                                                                                                                                                                                                                                          (7,3)         (7,9)         (8,6)           (8,10)         (5,5)         (5,8)         (5,9)         (6,9)         (0,1)           (0,10)         (3,0)         (2,2)           (4,11)\n                    S-Inhibition              Induction            Duplicate       Previous                                        S-Inhibition              Induction            Duplicate       Previous\n                                      (Layer, Head)                                                                                (Layer, Head)\n\nFigure 4: Left: Confidence ratios for model and base IOI circuit. S2 Hacking can be seen in heads\n8.6, 5.5, 5.9, and 3.0, where confidence ratio is close to 1 for the model but greater than 1 for the\ncircuit. Right: Functional faithfulness scores for the S and IO tokens. The output is more likely to\nbe correct if these heads predict S, so high values for the subject token (blue) indicate that the circuit\nis more confident than the model at predicting the correct answer.\n\n\nThe S2 Hacking mechanism starts from Duplicate head 3.0, as shown by its confidence ratio being\nclose to 1 in the full model but significantly higher than 1 in the base IOI circuit. This indicates\nthat the model places roughly equal attention on the S2 and IO2 tokens, but the circuit places more\nattention on the S2 token. The functional faithfulness score for the IO2 token is also low, indicating\nthat the attention on the IO2 token is significantly lower in the circuit compared to the full model.\n\nThe effect of Duplicate head 3.0 cascades down to the Induction heads 5.9 and 5.5. While their\nconfidence ratios are close to 1 in the model, indicating roughly equal attention to both the S2 and\nIO2 tokens, the circuit confidence ratios are significantly higher, suggesting a stronger preference\nfor the S2 token. Additionally, the functional faithfulness scores show a greater deviation from 1,\nwith significantly lower values for the IO2 token, indicating that attention to IO2 is significantly\nreduced in the circuit compared to the model.\n\nThe effects of S2 Hacking on the above heads flow into S-Inhibition head 8.6, which receives input\nfrom the Duplicate heads. In this head, the circuit confidence ratio is greater than 2 while the model\nconfidence ratio remains close to 1. Moreover, the functional faithfulness score for the S2 token\nis around 2 in the circuit but closer to 1 in the model. These results indicate that the head attends\nroughly equally to the S2 and IO2 tokens in the full model, but in the circuit this head significantly\nfavors the S2 token. This enables it to effectively direct the Name Mover heads to focus much less\non the S2 token and thereby avoid the incorrect answer.\n\nAll the other heads in the circuit do not appear to benefit significantly from S2 Hacking. In partic-\nular, the Previous Token heads appear largely unaffected and show very little deviation in behavior\nbetween the circuit and the model. While the S-Inhibition heads 7.3 and 7.9 exhibit significantly\nhigher confidence ratios in the circuit compared to the model, their model confidence ratios are al-\nready very high. This indicates that they predominantly attend to the S2 token in both the circuit\nand the model. These heads suggest how the model can still achieve nontrivial performance on the\nDoubleIO prompt variant, even if it is not as high as its performance on base IOI prompts.\n\n\n5 HOW DOES GPT-2 SMALL ACTUALLY SOLVE DOUBLEIO AND TRIPLEIO?\n\nHaving established that S2 Hacking explains how the base IOI circuit is able to outperform the full\nmodel on the DoubleIO and TripleIO variants, we now investigate how the full model actually solves\nthese variants. To do this, we discover a new circuit for each variant using the same patch patching\nmethodology and experimental framework that was used to discover the base IOI circuit (Wang\net al., 2023). In addition to explaining how the model solves the DoubleIO and TripleIO variants,\nthese circuits reveal that all components of the base IOI circuit are reused for these variants.\n\n\n                                       7\n\n5.1  ADDING PATHS FROM INPUT TOKENS\n\nThe S2 Hacking mechanism suggests a starting point for discovering DoubleIO and TripleIO cir-\ncuits.  Recall that in the base IOI circuit, S2 is the only input token with outgoing paths to the\nDuplicate heads, with all paths from the remaining input tokens being ablated out. For the variants,\nwe start with the base IOI circuit and restore some of these paths from other input tokens that were\noriginally ablated out, and see if any of them have a causal effect on the output of the model. The\nresults are shown in Figure 5.\n\nFor DoubleIO, we observe that adding paths from the IO2 token brings the circuit’s performance\nclosest to the full model, with a normalized faithfulness of 0.77. This is done by adding 10 edges\nto the base IOI circuit: two for each of the three Duplicate heads and the two Previous Token heads\nthey depend on. In contrast, adding paths from just the IO1 token has little impact on the circuit’s\nperformance, while adding paths from other input tokens further degrades its performance. Hence\nfor every path from the S2 token to a Duplicate Token head, the corresponding path from the IO2\ntoken to the same head also has a causal effect on the model output.\n\nA similar pattern emerges in TripleIO, where adding paths from the IO2 and IO3 tokens brings the\ncircuit’s performance closest to that of the full model, achieving a normalized faithfulness of 0.79.\nThis requires adding 20 edges to the base IOI circuit: the same 10 edges corresponding to the IO2\ntoken that were added to the DoubleIO circuit, plus 10 edges corresponding to the IO3 token.\n\n\n\n\n\n                                                     Circuit Edge Configurations                                                                        Circuit Edge Configurations\n\nFigure 5: Logit difference and normalized faithfulness for DoubleIO (left) and TripleIO (right)\nafter adding paths to the Duplicate and Previous Token heads from different input tokens.  For\nboth variants, the faithfulness is closest to 1 (ideal) when including paths from the input tokens\ncorresponding to duplicated names: S2 and IO2 for DoubleIO, and S2, IO2, and IO3 for TripleIO.\n\n\n\n5.2  DISCOVERING CIRCUIT REUSE THROUGH PATH PATCHING\n\nWe use the methodology of Wang et al. (2023) to discover a circuit for the DoubleIO and TripleIO\nvariants, starting with the identification of Name Mover heads. For each attention head and relevant\ninput token, we compute the direct causal effect of the path that starts from the token and proceeds\nthrough the head to the final logit at the END position. This type of token-level causal effect esti-\nmation is important for variants like DoubleIO and TripleIO where multiple tokens are duplicated,\nsince each of these duplicates can have paths to different heads.\n\nSurprisingly, we do not observe any significant deviation in results compared to the base IOI circuit.\nThe Name Mover heads from the base IOI circuit are just as causally relevant for the DoubleIO and\nTripleIO variants, and none of the other heads in the model have a substantial enough direct causal\neffect score to suggest the addition of a new Name Mover head to the circuit. These results indicate\nthat the Name Mover heads from the base IOI circuit are being reused for both variants.\n\nWe observe the same pattern in the S-Inhibition heads, which we refer to more generally as Inhibition\nheads since they could inhibit either of the duplicated names in the prompt. To identify these heads,\nwe compute the direct causal effects of every head in the model on the queries of the Name Mover\nheads. We find that all of the S-Inhibition heads from the base IOI circuit have the most significant\ncausal effects, indicating that the DoubleIO and TripleIO circuits are also reusing these heads.\n\n\n                                       8\n\n                              Direct Effect on S-Inhibition Heads' Values in % Logit Difference for Base IOI and DoubleIO\n\n                 BaseIOI - S2 Token                    DoubleIODoubleIO--IO2S2 TokenToken                   DoubleIODoubleIO -- S2IO2TokenToken\n         0                                                                            % Logit Difference\n         1\n\n         2\n         3                                                                                                                                                   5\n         4\n    Layer    56                                                                                                                                                   0\n\n         7\n\n         8\n                                                                                                                                                         −5\n         9\n\n        10\n\n        11\n           0   1   2   3   4   5   6   7   8   9  10  11     00  11  22  33  44  55  66  77  88  99  1010  1111     00  11  22  33  44  55  66  77  88  99  1010 1111\n\n                                                             Head\n\nFigure 6: Direct causal effect of all heads in the model on the values of Inhibition heads from the S2\ntoken for base IOI inputs (left) and DoubleIO inputs (middle); the heads with the most significant\ncausal effect are similar in both cases. For DoubleIO, these same heads have a significant causal\neffect in the opposite direction when measured from the IO2 token (right).\n\n\nFurthermore, the DoubleIO and TripleIO circuits also reuse the Induction, Duplicate, and Previous\nToken heads from the base IOI circuit, as well as the paths to these heads from the S2 token. To\ndemonstrate this, we compute the causal effect of patching the output of each head in the model\nfrom every possible input token to the values of the Inhibition heads.\n\nHowever, we also find that the paths from the input tokens corresponding to duplicated instances\nof the IO token also have significant positive causal effect: IO2 for DoubleIO, and IO2 and IO3\nfor TripleIO. This is consistent with the results in Section 5.1. Since these paths provide input to\nInhibition heads, a positive causal effect indicates a negative impact on the overall performance, as\nincreasing the attention on IO tokens leads the Inhibition head to suppress it and reduces the prob-\nability of the circuit returning the correct answer. Since the full model has a lower logit difference\nfor these variants, adding these paths from IO tokens increases the circuits’ faithfulness.\n\nOverall, the DoubleIO reuses all of the heads and paths from the base IOI circuit while adding 10\nmore edges corresponding to the IO2 token, and the TripleIO circuit reuses all of the heads and\npaths from the DoubleIO circuit while adding 10 more edges corresponding to the IO3 token. Table\n2 shows the faithfulness of the base IOI, DoubleIO, and TripleIO circuits, and the extent of their\noverlap. These results align with the strong generalization hypothesis from Figure 1 and, to the best\nof our knowledge, serve as the first demonstration of circuit generalization through circuit reuse.\n\n\n\n\n\nFigure 7: Left: Circuit discovered for DoubleIO. All nodes and edges from the base IOI circuit are\nreused, with additional edges to the IO Duplicate Handling Sub-circuit. Bottom Right: Duplicate\nHandling Sub-circuit, which appears twice in the circuit to deal with the two duplicated tokens (S,\nIO) in the DoubleIO prompt. Upper Right: Base IOI circuit, reproduced from Wang et al. (2023).\n\n\n\n                                       9\n\n            Metric                       Base IOI  DoubleIO   TripleIO\n\n           # Nodes in circuit                 26        26        26\n           # Edges in circuit                 110       120       130\n          Node overlap w/ base IOI circuit   100%     100%     100%\n           Edge overlap w/ base IOI circuit    100%     91.66%    84.61%\n           Model Logit Difference            3.484       2.118       1.227\n              Circuit Logit Difference            3.119       1.621       0.974\n            Normalized Faithfulness            0.895       0.765       0.778\n\nTable 2: Circuit discovery results for DoubleIO and TripleIO. Both circuits reuse all nodes and edges\nfrom the base IOI circuit, with DoubleIO adding 10 edges and TripleIO adding 20 edges.\n\n\n5.3  CHOOSING BETWEEN THE DUPLICATED NAMES\n\nThe circuit in Figure 7 demonstrates that Inhibition heads now receive information from both the\nIO2 and S2 duplicate tokens in the DoubleIO prompt. This raises a natural question: How does the\nmodel decide which duplicate to suppress to produce the correct answer? To answer this, we sought\nto find decision points: heads that are most responsible for choosing between the duplicates.\n\nSurprisingly, we find that the order in which names appear in the prompt significantly affects the\nperformance of both the full model and the DoubleIO circuit, with higher performance when the IO\ntoken comes first. Figure 8 (left) shows the logit differences stratified by whether S or IO comes\nfirst, and the overall logit difference appears to be an average of the two. This suggests that one or\nmore attention heads respond differently depending on the order of the names in the prompt.\n\nWe find this behavior in head 2.2, a Previous Token head. This head appears to implement a “first\ncome, first serve” mechanism, where it primarily attends to the name that appears first in the prompt,\nthereby serving as a key decision point in the circuit. As shown in Figure 8 (right), the head fre-\nquently assigns high attention to one of the name tokens (S, IO), based on which appeared first. In\nprompts where IO appeared first, the head attended far more to the IO+1 token than it did its S+1\ncounterpart, and vice versa when S appeared first. The full set of experiments is in Appendix B, and\na more detailed study of the duplicate suppression mechanism is left to future work.\n\n\n               Comparison of Circuit and Model by Name Sequence               Average Attention Score of L2H2 by Sequence Order\n           3.5\n                      IO                                  first                                                                                                                               0.56           3.0                       Subject                                         first                     2.829                                Score 0.6       IO+1S+1 ->->SIO  0.57\n           2.5        2.485                                                                                                                                                             Attention 0.50.4           2.0                      Difference           1.5                                                                         0.3        0.26                                         0.27\n                                                                1.199           Logit                                                                                                                                                                                                                                                                    Normalized 0.2           1.0                   0.927\n           0.5                                                                         0.1                                                                                                                                                                                      Average\n           0.0                                                                         0.0\n                     DoubleIO Circuit                      Model                                Subject First                         IO First\n\n\nFigure 8: Left: Performance of the DoubleIO circuit and full model based on the order of appearance\nof S and IO in the prompt. Both perform better when IO appears first. Right: Average attention\nscores for head 2.2, which places much more attention on the first name that appears in the prompt.\n\n\n6  CONCLUSION\n\nOur investigation reveals that the IOI circuit in GPT-2 small generalizes more effectively than pre-\nviously understood, reusing its core components and mechanisms while adapting through minimal\nstructural changes, such as adding input edges. Although the base IOI circuit behaved unexpectedly\non the DoubleIO and TripleIO prompt variants, as demonstrated by S2 Hacking, the functionality\nof its attention heads remained intact. This study of circuit generalization and evaluation on prompt\nvariants ultimately deepened our understanding of how GPT-2 small solves the IOI task, while of-\nfering critical insights into the broader capabilities of large neural networks.\n\n\n                                       10\n\nREFERENCES\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\n  Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\n  Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\n  Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\n  Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n  2021. https://transformer-circuits.pub/2021/framework/index.html.\n\nMichael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?:\n   Interpreting mathematical abilities in a pre-trained language model, 2023.  URL https:\n  //arxiv.org/abs/2305.00586.\n\nMichael Hanna, Sandro Pezzelle, and Yonatan Belinkov. Have faith in faithfulness: Going beyond\n   circuit overlap when finding model mechanisms.  In First Conference on Language Modeling,\n  2024. URL https://openreview.net/forum?id=TZ0CCGDcuT.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n  Zoom  in:  An introduction to  circuits.    Distill, 2020.    doi:  10.23915/distill.00024.001.\n   https://distill.pub/2020/circuits/zoom-in.\n\nNikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning\n  enhances existing mechanisms: A case study on entity tracking, 2024.\n\nAlessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. A mechanistic interpretation of arith-\n  metic reasoning in language models using causal mediation analysis. In Conference on Empirical\n  Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.\n  org/CorpusID:258865170.\n\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\n   Stuart Shieber.  Investigating gender bias in language models using causal mediation analysis.\n  Advances in neural information processing systems, 33:12388–12401, 2020.\n\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\n   Interpretability in the wild:  a circuit for indirect object identification in GPT-2 small.   In\n  The Eleventh International Conference on Learning Representations, 2023.  URL https:\n  //openreview.net/forum?id=NpsVSN6o4ul.\n\n\nA  S2 HACKING\n\nIn this section, we provide further detail on how each of the attention heads in the base IOI circuit\nbehave under S2 Hacking. The Name Mover heads in the circuit are responsible for returning the\noutput, ideally the IO token, and we find that all of these heads attend primarily to the IO token. To\nunderstand how this occurs, we consider all of the heads that have an incoming edge from the S2\ntoken: the S-Inhibition, Induction, Duplicate, and Previous Token heads.\n\n\nA.1  METRICS\n\nEach head type is defined by a query position (“active” token) and a key position (“attended” token).\nFor DoubleIO prompts, most heads have a decision to focus on tokens leading to the prediction of\neither S or IO. For each head type, Wang et al. (2023) specify which tokens to primarily attend to\nin order to produce the correct answer. We refer to these as the “correct” and “incorrect” tokens for\nthat head type, and they are used in the definition of the confidence ratio in Section 4.1. We specify\nthese “correct” and “incorrect” tokens for each head type below.\n\nFor Inhibition heads, which are active at END, attending to S2 suppresses the prediction of the sub-\nject token and increases the probability of the Name Mover heads returning the IO token, which is\nthe “correct” output. Attending to IO2, which suppresses the prediction of the IO token, is con-\nsidered “incorrect”. We refer to the attention probability of S2 at END position as the “correct”\nprobability, while the incorrect probability is the attention probability of IO2 at END. Similarly, for\n\n\n                                       11\n\nInduction heads we refer to the attention probability of S1+1 at S2 as “correct”, and the attention\nprobability of IO1+1 at IO2 as “incorrect”. For Duplicate Token heads, we refer to the attention\nprobability of S1 at S1+1 as “correct”, and the attention probability of IO1 at IO1+1 as “incor-\nrect”. For Previous Token heads, we refer to the attention probability of S1 at S1+1 as “correct”,\nand the attention probability of IO1 at IO1+1 as “incorrect”.\n\n\nA.2  S-INHIBITION HEADS\n\nThe ideal behavior of the S-Inhibition heads on the base IOI prompt is to focus on the duplicated\ntoken S2 at the END position. These heads influence the queries of the Name Mover heads, guiding\nthem to reduce their attention on the S2 token. This enables them to focus on the IO token, which\nis the correct answer. However, since there are two duplicates in the DoubleIO prompt variant, S2\nand IO2, it is possible for these heads to attend to both duplicates. The optimal behavior of these\nheads is to place high attention on the S2 token while maintaining low attention on the IO2 token\nat the END position.\n\nWe see an example of S2 Hacking in Head 8.6, which exhibits a higher confidence ratio in the circuit\ncompared to the model. The model confidence ratio is close to 1 and the circuit confidence ratio is\ngreater than 2, indicating that the head attends roughly equally to the S2 and IO2 tokens in the\nmodel but significantly favors the S2 token in the circuit. This enables it to effectively direct the\nName Mover heads to focus much less on the S2 token and thereby avoid the incorrect answer.\n\nHowever, not all S-Inhibition heads rely on S2 Hacking. The confidence ratios for Head 8.10 are\nvery similar in the model and the circuit, in both cases the head attends more to the S2 token. While\nHeads 7.3 and 7.9 exhibit significantly higher confidence ratios in the circuit compared to the model,\nthe model confidence ratio for these heads are already very high, indicating that they predominantly\nattend to the S2 tokens regardless. These heads seem to suggest how the model still shows nontrivial\nperformance on the DoubleIO prompt variant, even if not as high as the circuit.\n\n\nA.3  INDUCTION HEADS\n\nWhen an Induction head encounters a duplicate instance of the subject token S2 in the prompt, it\nattends primarily to the word immediately following the previous instance of the subject (S1+1).\nThere are two duplicates in the DoubleIO prompt variant, so the circuit performance would increase\nif the induction head placed high attention on the S1+1 token at the S2 position and low attention\non the IO1+1 token at the IO2 position. This would ensure that the head continues to prioritize the\nS2 token.\n\nS2 Hacking is evident in Heads 5.5 and 5.9. Their confidence ratios are close to 1 in the full model,\nindicating roughly equal attention assigned to both the S2 and IO2 tokens. However, the circuit\nconfidence ratios are significantly higher, suggesting a stronger preference for the S2 token. Addi-\ntionally, the functional faithfulness scores show higher values for the S2 token compared to the IO2\ntoken, indicating that attention on the S2 token deviates further from the behavior of the full model.\n\nIn contrast, Head 6.9 does not appear to rely on S2 Hacking since it already favors the S2 token\nheavily, as shown by the high confidence ratios on both the model and the circuit and the functional\nfaithfulness scores on the S2 and IO2 tokens are very similar. This is similar to the pattern we\nobserved in the S-Inhibition Heads 7.3 and 7.9.  Interestingly, Head 5.8 does not seem to benefit\nfrom S2 Hacking and still favors the IO2 token.\n\n\nA.4  DUPLICATE HEADS\n\nThe function of the Duplicate heads is to focus on the first instance of a name in the prompt when a\nduplicate instance is encountered. Since the Duplicate heads influence the queries of the Induction\nheads, it is advantageous for them to attend highly to S1 at the S2 token position while avoiding\nattending to IO1 at the IO2 token position.\n\nHeads 0.1 and 0.10 demonstrate nearly identical performance between the circuit and the full model,\nwith both attending equally to IO1 and S1. This is evident from the confidence and functional\nfaithfulness scores all being close to 1. In contrast, Head 3.0 shows strong signs of S2 Hacking;\nwhile its confidence ratio is close to 1 for the full model, it significantly increases for the circuit. The\n\n\n                                       12\n\nfunctional faithfulness scores further show that the behavior of this head on the S2 token diverges\nfrom the full model much more than on the IO2 token. This head shows signs of a decision point\nwhere S2 Hacking begins, with its effects cascading down to the Induction and S-Inhibition heads.\n\n\nA.5  PREVIOUS TOKEN HEADS\n\nThe purpose of a Previous Token head at a given token position is to attend to the token that imme-\ndiately precedes it in the prompt. In the DoubleIO prompt variant, it would be beneficial for these\nheads to maintain this functionality at the S1+1 position while minimizing attention on the IO1 to-\nken at the IO1+1 position. This is important because the Previous Token heads influence the values\nof the Induction heads in the base IOI circuit, and we have previously seen how the Induction heads\nbenefit from focusing on the subject token.\n\nUnlike the other head types above, the Previous Token heads appear not to rely very much on the\nS2 Hacking mechanism. Head 4.11 functions almost identically in both the circuit and the model,\nwith confidence and functional faithfulness scores consistently close to 1. While Head 2.2 slightly\nbenefits from S2 Hacking, the circuit deviates very little from the full model. In both cases, these\nheads attend more to the S1 token at position S1+1 than they do to the IO1 token at the IO1+1\nposition, so this head appears to favor the S1 token regardless.\n\nB  IDENTIFYING LAYER 2 HEAD 2 AS THE DECISION POINT\n\nWe measure the difference between the direct effect on the S tokens from the Direct Effect on the IO\ntokens on every relevant head. We selected Previous Token Heads 4.11 and 2.2 as the most likely\ncandidates to be decision points as a result of the aforementioned difference scores. Plotting the\nattention patterns of each head, however, led us to conclude that Head 2.2 was a better candidate\nthan Head 4.11. This is because while 4.11 was found to have a high causal effect on these tokens,\nit was only because it was attending highly to both IO and S tokens to a similar degree. Head 2.2 on\nthe other hand, was found to more frequently assign high probably to only one of the two duplicate\nname tokens, making it the best decision point candidate within the circuit. Plots for the attention\npatterns of head 4.11 can be found in Figure 9, and the same for head 2.2 in Figure 8.\n\nPrevious Token Head 4.11 attends equally to the IO+1 and S+1 tokens, regardless of the order in\nwhich the name tokens appear in the prompt. This evidence dissuaded us from considering 4.11 as\nthe decision point where the model would decide which duplicate token to inhibit.\n\n\n\n\n\nFigure 9: Attention scores of Layer 4 Head 11 indicate that the Previous Token Head attends equally\nto both the IO and S previous tokens.\n\n\nThe attention scores of the Previous Token Head 2.2 show clear changes in the amount of attention\nassigned to the name tokens dependent upon the order in which they appear in the prompt. When\nSubjects appear first in the prompt, S+1 tokens have higher attention scores. Conversely, when IOs\nappear first in the prompt, IO+1 token receives higher attention scores.\n\nThe difference between Direct Effect measured on the S tokens and on the IO tokens show high\ncausal effect from the Previous Token Heads 2.2 and 4.11.\n\n\n                                       13\n\n(a) Attention (IO first): The attention patterns fa-           (b) Attention (Subject first): The attention pat-\nvor IO when IO appears first.                                terns favor Subject when Subject appears first.\n\n         Figure 10: Comparison of head 2.2 attention patterns for different input orders.\n\n\n\n\n\nFigure 11: Difference between the absolute direct effects between the S2 and IO2 edges of the same\nnode. The high value of nodes provide preliminary evidence that they might be a decision point.\n\n\n\n\n\nFigure 12: We see that for head 4.11, both Subject and IO edges have high effect, but for 2.2 only\nSubject has a high effect.\n\n\n\n\n                                       14\n\n(a) Difference in effects between S2 and IO2 for           (b) Difference in effects between S2 and IO2 for\nInduction heads.                                         Duplicate Token heads.\n\nFigure 13: Change in direct effect from subject to IO token for Induction and Duplicate Token heads.\n\n\nC  PROMPT TEMPLATES\n\nHere we provide the BABA-style prompt templates to generate the DoubleIO and TripleIO datasets\nand prompts. The ABBA-style format is generated by switching the order of the first two names.\n\n   BABA Templates - DoubleIO\n    Then, [B] and [A] went to the [PLACE], [A] was happy. [B] gave a [OBJECT] to [A]\n    Then, [B] and [A] had a lot of fun at the [PLACE], [A] seemed distracted. [B] gave a\n    [OBJECT] to [A]\n    Then, [B] and [A] were working at the [PLACE], [A] was confused. [B] decided to give a\n    [OBJECT] to [A]\n    Then, [B] and [A] were thinking about going to the [PLACE], [A] was excited. [B] wanted\n     to give a [OBJECT] to [A]\n    Then, [B] and [A] had a long argument, and afterwards [A] smiled. [B] said to [A]\n     After [B] and [A] went to the [PLACE], [A] looked tired. [B] gave a [OBJECT] to [A]\n   When [B] and [A] got a [OBJECT] at the [PLACE], [A] laughed. [B] decided to give it to\n    [A]\n   When [B] and [A] got a [OBJECT] at the [PLACE], [A] was deep in thought. [B] decided\n     to give the [OBJECT] to [A]\n    While [B] and [A] were working at the [PLACE], [A] was happy. [B] gave a [OBJECT]\n     to [A]\n    While [B] and [A] were commuting to the [PLACE], [A] seemed distracted. [B] gave a\n    [OBJECT] to [A]\n     After the lunch, [B] and [A] went to the [PLACE], [A] was confused. [B] gave a [OB-\n    JECT] to [A]\n     Afterwards, [B] and [A] went to the [PLACE], [A] was excited. [B] gave a [OBJECT] to\n    [A]\n    Then, [B] and [A] had a long argument, [A] smiled. Afterwards, [B] said to [A]\n    The [PLACE] [B] and [A] went to had a [OBJECT], [A] looked tired. [B] gave it to [A]\n     Friends [B] and [A] found a [OBJECT] at the [PLACE], [A] laughed. [B] gave it to [A]\n\n\n\n   BABA Templates - TripleIO\n    Then, [B] and [A] went to the [PLACE], [A] sat on a bench. [A] had left their [OBJECT]\n     behind. [B] gave a [OBJECT] to [A]\n    Then, [B] and [A] had a lot of fun at the [PLACE], [A] seemed lost in thought. [A] looked\n    up at the sky. [B] gave a [OBJECT] to [A]\n    Then, [B] and [A] were working at the [PLACE], [A] was laughing softly. [A] glanced at\n     the [OBJECT]. [B] decided to give a [OBJECT] to [A]\n\n\n\n                                       15\n\nThen, [B] and [A] were thinking about going to the [PLACE], [A] appeared confused. [A]\nfumbled with their [OBJECT]. [B] wanted to give a [OBJECT] to [A]\nThen, [B] and [A] had a long argument, and afterwards [A] smiled. [A] adjusted their\n[OBJECT]. [B] said to [A]\nAfter [B] and [A] went to the [PLACE], [A] yawned. [A] rubbed their eyes. [B] gave a\n[OBJECT] to [A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE]. [A] looked at the ground, [A] sighed\ndeeply. [B] decided to give it to [A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE], [A] smiled at the view. [A] seemed\nrelieved. [B] decided to give the [OBJECT] to [A]\nWhile [B] and [A] were working at the [PLACE], [A] sat on a bench. [A] had left their\n[OBJECT] behind. [B] gave a [OBJECT] to [A]\nWhile [B] and [A] were commuting to the [PLACE], [A] seemed lost in thought. [A]\nlooked up at the sky. [B] gave a [OBJECT] to [A]\nAfter the lunch, [B] and [A] went to the [PLACE], [A] was laughing softly. [A] glanced\nat the [OBJECT]. [B] gave a [OBJECT] to [A]\nAfterwards, [B] and [A] went to the [PLACE], [A] appeared confused. [A] fumbled with\ntheir [OBJECT]. [B] gave a [OBJECT] to [A]\nThen, [B] and [A] had a long argument, [A] smiled. [A] adjusted their [OBJECT]. [B]\nsaid to [A]\nThe [PLACE] [B] and [A] went to had a [OBJECT], [A] yawned. [A] rubbed their eyes.\n[B] gave it to [A]\nFriends [B] and [A] found a [OBJECT] at the [PLACE], [A] looked at the ground. [A]\nsighed deeply. [B] gave it to [A]\n\n\n\n\n\n                                   16\n\n## Claim to Classify\n\"implement algorithms responsible for performing specific tasks\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "activation patching",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 1,
    "reasoning": "The claim 'implement algorithms responsible for performing specific tasks' appears in the abstract and uses the word 'implement', which is a strong Rung 3 linguistic marker indicating mechanistic/functional attribution (per codebook: 'performs', 'computes', 'executes', 'implements' → R3). The paper builds on Wang et al. (2023), which used activation patching (a Rung 2 method) to discover the IOI circuit. This paper uses the same methodology (path patching, a variant of activation patching) to evaluate and extend the circuit. While the claim is hedged by being attributed to prior work ('are typically discovered... believed to implement'), the hedge_flag is 1 due to 'believed to', the core claim about implementing algorithms remains a Rung 3 mechanistic assertion. The method (Rung 2) does not support the full mechanistic claim (Rung 3), consistent with the IOI circuit calibration case."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 18561,
      "completion_tokens": 255,
      "total_tokens": 18816
    },
    "finish_reason": "stop"
  }
}