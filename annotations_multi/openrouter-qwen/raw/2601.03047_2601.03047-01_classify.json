{
  "timestamp": "2026-02-19T11:28:50.287917+00:00",
  "paper_id": "2601.03047",
  "claim_id": "2601.03047-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2601.03047\nTitle: When the Coffee Feature Activates on Coffins: SAE Critique\nFull text:\n           When the Coffee Feature Activates on Coffins:\n          An Analysis of Feature Extraction and Steering for\n                         Mechanistic Interpretability\n\n                   Raphael Ronge1*†, Markus Maier1† and Frederick Eberhardt2\n\n                   1Department of Philosophy of Nature and Technology, Munich School of\n                       Philosophy, Kaulbachstraße 31a, Munich, 80539, Bavaria, Germany.\n                      2Division of the Humanities and Social Sciences, California Institute of2026               Technology, 1200 East California Boulevard, Pasadena, 91125, CA, USA.\nJan                    *Corresponding author(s). E-mail(s): raphael.ronge@hfph.de;\n6                    Contributing authors: markus.maier@hfph.de; fde@caltech.edu;\n                             †These authors contributed equally to this work.\n\n\n\n                                                  Abstract\n\n                         Recent work by Anthropic on Mechanistic interpretability claims to understand[cs.LG]                 and control Large Language Models by extracting human-interpretable features\n                        from their neural activation patterns using sparse autoencoders (SAEs). If suc-\n                                cessful, this approach offers one of the most promising routes for human oversight\n                             in AI safety. We conduct an initial stress-test of these claims by replicating their\n                       main results with open-source SAEs for Llama 3.1. While we successfully repro-\n                        duce basic feature extraction and steering capabilities, our investigation suggests\n                           that major caution is warranted regarding the generalizability of these claims. We\n                             find that feature steering exhibits substantial fragility, with sensitivity to layer\n                              selection, steering magnitude, and context. We observe non-standard activation\n                          behavior and demonstrate the difficulty to distinguish thematically similar fea-\n                            tures from one another. While SAE-based interpretability produces compelling\n                          demonstrations in selected cases, current methods often fall short of the system-\n                              atic reliability required for safety-critical applications. This suggests a necessary\n                                shift in focus from prioritizing interpretability of internal representations toward\n                               reliable prediction and control of model output. Our work contributes to a more\n                        nuanced understanding of what mechanistic interpretability has achieved andarXiv:2601.03047v1                      highlights fundamental challenges for AI safety that remain unresolved.\n\n                       Keywords: Mechanistic Interpretability, Sparse Autoencoders, Feature Steering,\n                           Large Language Models, AI Safety, Explainable AI\n\n\n                                                      1\n\n1 Introduction\n\nMechanistic interpretability (MI) is a form of explainable AI. It refers to a broad set of\nrecent approaches to understanding, and ultimately controlling, the behavior of Large\nLanguage Models (LLMs). The core idea is to construct semantic representations – so-\ncalled “features” – from the neural activation patterns of the network, that are both\nhuman-interpretable and controllable. That is, the extracted features are supposed to\nmap straightforwardly onto human concepts and should be manipulable to influence\nthe LLM output in a meaningful way.\n   The proposal is intriguing: If it is possible to faithfully map the inner workings of\nan LLM with its highly distributed representations into a representation that lends\nitself to human interpretation, and  if it is in addition possible to manipulate these\ninterpretable features such that their role in the output is correspondingly emphasized\nor de-emphasized, then we have achieved a causal understanding of the inner workings\nof an LLM. A causal understanding of a system is considered the ultimate goal of\nunderstanding in many scientific domains. In the case of an LLM, such a level of\nunderstanding would in addition be a boon for AI safety, since it would allow control\nover the LLM in a form that is familiar to the human: emphasize or de-emphasize, or\neven disallow, generation of text on certain topics by regulating the features that are\nsemantically involved in the topic of interest.\n   This approach has been gaining traction over the last few years, as can be seen\nfrom the contributions of different research groups (e.g., Gao et al. 2024, 2025; Geiger\net al. 2025), the growing number of review articles (e.g., R¨auker et al. 2023; Bereska\nand Gavves 2024; Sharkey et al. 2025), and dedicated workshops on mechanistic inter-\npretability at well-known conferences (e.g., ICML 2024; NeurIPS 2025). In the context\nof our investigation, however, we will focus on recent work by Anthropic. Here we can\ndraw on a sequence of publications (Elhage et al. 2022; Bricken et al. 2023; Templeton\net al. 2024; Durmus et al. 2024) and an excellent open-source resource that Neuron-\npedia has provided for users to analyze various open source LLMs with pre-trained\nresources (Lin 2023). In many ways, Anthropic’s work can be seen as representative\nof the applied MI research.\n   In Scaling Monosemanticity (Templeton et al. 2024) the Anthropic team extracts\nfrom the real-world Claude 3.0 Sonnet model a large number of features that they\nclaim to be “generally interpretable”, and show how these features can be emphasized\nor de-emphasized (feature steering) to influence their role in the LLM output. These\nresults build on their prior work in simpler models showing how a network may inter-\nnally represent input features in a superposition, i.e. where a model represents more\nfeatures than it has dimensions (Elhage et al. 2022). These superpositioned features\ncan be disentangled again using so-called sparse autoencoders (SAEs) (Bricken et al.\n2023). SAEs project the neural activations into a higher dimensional space where there\nare enough neurons to achieve a one-to-one mapping between feature and neuron. In\nthat sense then the features are monosemantic: there is a neuron for each feature in\nthe SAE, one can control that feature through control of the corresponding neuron,\nand – ideally – the feature is human-interpretable. In a recent blogpost the team has\nthen extended this approach to identify monosemantic features across different layers\n\n\n\n                                     2\n\nof the network using so-called crosscoders (Lindsey et al. 2024), but we will focus on\nthe work in Templeton et al. (2024).\n   In the following, we provide an initial stress-test of Anthropic’s claims. We attempt\nto reproduce their results and test them in a similar setting under slightly differ-\nent circumstances, with the overall aim to address the question of whether their\napproach of semantic feature extraction indeed offers a promising route for AI safety.\nIn Section 2 we first summarize the motivation underlying the approach of mechanis-\ntic interpretability. In Section 3 we describe Anthropic’s central claims about their\nresults regarding mechanistic interpretability. Section 4 then describes our attempts\nat reproducing Anthropic’s findings, while Section 5 covers our own analysis of the\ninterpretability and steerability of some of the features we found. We discuss our\nresults and their implications in Section 6, and conclude in Section 7.\n\n2 Background: Mechanistic Interpretability\n\nMechanistic interpretability is one approach to providing an understanding of the\nbehavior of AI systems (mostly LLMs) by aiming\n\n    [...] to completely specify a neural network’s computation, potentially in a format as\n   explicit as pseudocode (also called reverse engineering), striving for a granular and precise\n  understanding of model behavior. (Bereska and Gavves 2024, 1)\n\nWhile there is no consensus on the precise demarcation of MI from other approaches\nto model interpretation (see Rai et al. (2025, 8); Saphra and Wiegreffe (2024)), MI\ncan be broadly seen as aiming to uncover a mapping between the activations of\nneurons in the network and features that permit an interpretable description of what\nthe network is doing:\n\n  Mechanistic interpretability seeks to understand neural networks by breaking them into\n  components that are more easily understood than the whole. By understanding the function\n   of each component, and how they interact, we hope to be able to reason about the behavior\n   of the entire network. (Bricken et al. 2023)\n\nThe approach was originally motivated by the so-called linear representation hypoth-\nesis. The idea  is that a neural network represents semantically meaningful input\nfeatures as directions in its activation space (Elhage et al. 2022) and that the task of\nunderstanding the network’s inner operations is a matter of decoding these features\nagain at different points within the network. Famously, Mikolov et al. (2013) describe\nhow a recurrent neural network language model implicitly learns certain vector-space\nword representations v(x), allowing for basic algebraic operations of the activation\nvectors, such as v(“king”) −v(“man”) + v(“woman”) ≈v(“queen”). Ultimately, lin-\nearity may just be seen as a useful simplification to illustrate the hypothesis, holding\nempirically in many, but not all cases (Bereska and Gavves 2024, 8-9). Others, such\nas Geiger et al. (2025, 31) propose to relax this restriction to include more general\ndecompositions. In addition, it is now widely recognized that there may well be more\nencoded features than there are model dimensions, such that features are said to be\nrepresented in a “superposition” (Arora et al. 2018; Olah et al. 2020; Elhage et al.\n2022; Cunningham et al. 2023).\n\n\n\n                                     3\n\n   There are different approaches to discover, extract, and study functionally useful\nfeatures in LLMs. One particular technique used in MI research in general, and by\nAnthropic in particular, are sparse autoencoders (Rai et al. 2025, 15-16). As Bricken\net al. (2023) state,\n\n    [...] we use a weak dictionary learning algorithm called a sparse autoencoder to generate\n  learned features from a trained model that offer a more monosemantic unit of analysis\n  than the model’s neurons themselves.\n\n    Traditionally, an autoencoder is a network that tries to learn an approximation\nto the identity function through a (normally) narrow bottleneck: first encoding the\ninput in a lower dimensional space and subsequently reconstructing the information.\nThe main idea is that the network identifies and uses any underlying structure in\nthe input data in order to compress the information in a lower dimensional space\nbefore reconstructing the input. In contrast to this traditional approach to\nautoencoders with their lower-dimensional bottleneck, the intermediate layer of a\nsparse autoencoder has many more neurons than the hidden layers in the base LLM.\nThe sparsity is enforced via added constraints – in the case of Anthropic this is the\ninclusion of an l1 penalty to the mean squared error loss (Bricken et al. 2023).1 The\nworking assumption of MI is that within the LLM, features are represented and\nprocessed in an entangled representation, and the goal for the SAE is to “disentangle”\nthese mixed activations by projecting them to a higher-dimensional space:\n\n  Once the SAE is trained, it provides us with an approximate decomposition of the model’s\n   activations into a linear combination of “feature directions” (SAE decoder weights) with\n   coefficients equal to the feature activations. (Templeton et al. 2024)\n\nWhile the approach of extracting features using SAEs is in principle scalable, there\nare practical constraints (e.g., expended resources), pragmatic decisions (e.g., particu-\nlar choices of the relevant hyperparameters), and theoretical limitations that influence\nthe quality and “completeness” of the features. For example, the largest SAE used by\nAnthropic in Templeton et al. (2024) has a dimension of around 34 million, focusing\nonly on the residual stream activations in the middle of their model due to computa-\ntional costs of the SAEs (and to prevent interference from other layers). By their own\naccount, they believe this to be orders of magnitude short of finding a “complete” set\nof features, though one should note that no precise account of what would constitute\na “complete set of features” is given.\n   For the remainder of this paper we will adopt the technical meaning of feature as\nreferring to a unit in a high-dimensional layer in the SAE that is used to disentangle\nthe model activations (Templeton et al. 2024). Consequently, one of the important\nquestions to grasp the notion of human-interpretability, will be to determine the rela-\ntion between these features and human concepts. Freiesleben (2026) offers a definition\nof what it means for a prediction model, such as an LLM, to represent a learned\n(human) concept C in model part U:\n\n1. The model part U coactivates when exhibited to instances of the concept C in the\n   input.\n\n\n  1For a discussion of the tendency of the l1 norm to prefer sparse solutions in various problems, see (Elad\n2010, ch. 1).\n\n\n\n                                     4\n\n2. Manipulating the model part U changes the functional role of the concept C in the\n   prediction.\n\nFreiesleben (2026, 277) offers the following simple example:\n\n  An ANN represents the concept of addition in a model part U when each time it performs\n  an addition, the part is activated and removing the unit results in ANNs no longer being\n  able to perform common addition problems that they were previously capable of solving.\n\nIf we take these criteria as a starting point, then to claim that an SAE-constructed\nfeature is human-interpretable means that it has to reliably activate in response to\na human concept in the input and its manipulation must result in changes to the\ncorresponding concept in the output. We will adopt this view as a working hypothesis.\n\n3 Anthropic’s Claims\n\nOur analysis of SAE-based features will focus on two claims by the Anthropic team\nthat are explicitly laid out in the proof-of-concept work in Bricken et al. (2023)\nand the follow-up work by Templeton et al. (2024), who scale the approach up to\nproduction-level LLMs. Both claims form the conceptual basis for much of Anthropic’s\ninterpretability research: (1) SAEs produce interpretable features, (2) these can be\nused to steer the models. The research teams at Anthropic essentially propose that\none is able to understand the computation of an LLM works via claim (1) and that\none is then able to control the output of said computation via claim (2).\n   Without internal access to Claude 3.0 Sonnet (the model used in Templeton et al.\n(2024)) our analysis is necessarily based on an assumption that is widespread in the MI\ncommunity, and that can also be found in Anthropic’s publications (see, e.g., Bricken\net al. 2023), namely, universality. In broad terms, universality refers to the idea that\nthe same feature(s) can be found across different models. Bereska and Gavves (2024,\n35) distinguish between a strong and a weak notion of universality. While a strong\nversion claims that there exist some underlying, fundamental computational primitives\nthat all models necessarily converge towards, a weak version instead suggests that the\nfeatures that emerge depend on the given task, dataset, and model architecture, and\ndifferent models will converge towards an optimal solution given these constraints,\nyielding similar features. We adopt a weak form of universality in our analysis, as\nwe use a publicly available model (Llama3.1-8B) and assume that our results are\nindicative,  if not representative, of the work done by Anthropic, using their own,\nproprietary models.\n   In the following, we lay out the two claims and show where our subsequent analysis\nof their validity and scope is going to interject.\n\n\n(C1) Sparse autoencoders produce interpretable features.\nWe take this to be one of the most important, and most basic, claims underlying\nAnthropic’s interpretability research program. It is made in Bricken et al. (2023) –\n“Sparse autoencoders produce interpretable features that are effectively invisible in\nthe neuron basis.” – and repeated in Templeton et al. (2024) – “Sparse autoencoders\nproduce interpretable features for large models”.\n\n\n\n                                     5\n\n   SAEs extract, depending on their size, millions of features per layer and researches\nacknowledge that there exists a wide variety of them – varying in scope, specificity\nand abstraction. For example, Templeton et al. (2024) show that there exists a\n“Golden Gate Bridge” feature as well as a feature for “transit infrastructure”; for\ncode related content they show features like “code error” and “addition”; and then\nthere are very abstact features like “sycophantic praise” and “gender bias awareness”.\nGiven the large number of extracted features, it is in practice not possible to\ninterpret all of them by hand. Instead, Anthropic uses an automated interpretability\npipeline (Bricken et al. 2023) that generates descriptions of all features using a\nsecond LLM. Cunningham and colleagues summarize the basic process as follows:\n\n    [...] the autointerpretability procedure takes samples of text where the dictionary feature\n   activates, asks a language model to write a human-readable interpretation of the dictio-\n  nary feature, and then prompts the language model to use this description to predict\n  the dictionary feature’s activation on other samples of text. The correlation between the\n  model’s predicted activations and the actual activations is that feature’s interpretability\n   score. (Cunningham et al. 2023, 3)\n\nAs such, interpretations of features found by SAEs are based entirely on their activa-\ntions on large text samples, and are a priori independent of their influence on model\nbehavior.2\n   Anthropic maintains that this approach is a strong contender for obtaining\nfeature interpretations that track human concepts. For example, consider the\nfollowing passage:\n\n  Many features are multilingual (responding to the same concept across languages) and mul-\n  timodal (responding to the same concept in both text and images), as well as encompassing\n  both abstract and concrete instantiations of the same idea (such as code with security\n   vulnerabilities, and abstract discussion of security vulnerabilities). (Templeton et al. 2024)\n\nBut such enthusiasm has been met with some caution in the literature. Bereska and\nGavves (2024, 4) note that there is no principled reason for features found in LLMs\nby SAEs to be humanly interpretable, and that extracted features might even\nbecome less interpretable the bigger and more complex the models become.3\nDurmus et al. (2024) suggest that the automated interpretations should only be seen\nas hypotheses about the concepts the features might correspond to. In other writing,\nAnthropic’s own assessment is also more measured:\n\n  Another issue is that we don’t believe our features are completely monosemantic (some\n  polysemanticity may be hiding in low activations), nor are all of them necessarily cleanly\n   interpretable. (Bricken et al. 2023)\n\n   This points to the crux of understanding Anthropic’s claims: On the one hand\nthey acknowledge the sharp distinction between their extracted features and human\ninterpretable concepts, and on the other hand it appears to be the driving idea of their\nmechanistic interpretation that the features align with human-interpretable concepts\nin order to gain insights into the inner workings of LLMs. There is, of course, no\n\n\n  2Bricken et al. (2023) provide additional information regarding the specific implementation of their\nautomated interpretability method in the Appendix to their work. For more information on automated\ninterpretability, the interested reader is referred to Bills et al. (2023).\n  3Note that this claim contradicts the Natural Abstraction Hypothesis, which states that LLMs (and\ncognitive systems in general) converge on similar conceptual representations as humans. (Chan et al. 2023).\n\n\n                                     6\n\ninconsistency in claiming that many features match human concepts and some features\nare not interpretable. But then we need to understand how we identify the features\nthat indeed track concepts, how one can manipulate them, and how they interact with\nthe non-interpretable features. One can even take a closer look at the interpretable\nfeatures themselves. As features can be very complex – “encompassing both abstract\nand concrete instantiations of the same idea” (Templeton et al. 2024) – some overlap\nbetween them is inevitable. However, as the number of features is fixed by the SAE and\nno restrictions are imposed on the allowed similarity between them, this can lead to\nindividually interpretable features with no clear distinction between them. Moreover,\nrecall that automated interpretations are used to generate the human-interpretable\ndescription of features from the lists of activation patterns on text samples. Could\nfeatures, therefore, correspond to activation patterns that depict abnormal behavior in\nand of themselves, without showing up in the automatically generated interpretations?\nCould these automatic interpretations also mask the context-dependence of activation\npatterns, when at the same time the performance of transformer-based LLMs is so\nheavily linked to their ability to incorporate contextual information?\n\n(C2) Features can be used to steer models.\nThis second claim is made in Bricken et al. (2023) – “Sparse autoencoder features\ncan be used to intervene on and steer transformer generation.” – and repeated in\nTempleton et al. (2024) for large models: “Features can be used to steer large models\n[...]. This extends prior work on steering models using other methods [...].”\n   Templeton et al. (2024) show how this works for Claude 3.0 Sonnet. The outputs\nof the SAE are spliced back into the transformer. With each neuron of the SAE\nrepresenting one feature, it is possible to increase the output for each feature by some\nmultiple of the original value (a step called clamping). Clamping the feature “transit\ninfrastructure” to 5× its maximum activation value changes the transformers answer\nto the question “What’s the best way to get to the grocery store down the street? Be\nbrief.” from “1. Walk.” to “1. Walk across the bridge” (Templeton et al. 2024).\n   The claim that the features produced by SAEs can be used to successfully influ-\nence the behavior of practically employed LLMs plays an important double role in\nAnthropic’s work. On the one hand, it is crucial for the overarching motivation behind\ntheir work on MI, because steering has direct implications for model safety and ethi-\ncal considerations. For example, Durmus et al. (2024) show that feature steering can\nreduce (or increase) social biases: clamping the feature “Neutrality and impartial-\nity” by a positive value reduces the bias score on the Bias Benchmark for Question\nAnswering.\n  On the other hand, manipulability of the features with respect to the output\nof the model is taken as a significant indicator of representational correctness: “To\nverify our feature interpretations, we ran qualitative feature steering experiments,\nwhere we artificially dialed up and down various features to see if they changed model\noutputs in intuitive ways” (Durmus et al. 2024). As such, the fact that features can\nbe employed to influence model behavior is used as practical confirmation of the\nunderlying methodology and as a goal to enhance AI safety.\n   Together, both perspectives place a significant burden on the steering capabilities\nof features. The large number of features makes an automatic interpretation process\n\n\n                                     7\n\nnecessary, leading us to rely heavily on steering as a fail-safe as well as an important\ntool for AI safety. This raises questions about the interplay between activation, inter-\npretation and steering. Automated interpretation could result in a feature description\nnot corresponding well with its activation. How does this translate into steering behav-\nior? Even  if they correlate, could there be cases where interpretation and steering\ncould lead to a false conception of a feature? Different steering coefficients can be\nchosen to adjust the influence of each clamped feature on the output. Given that we\ncan specify the exact steering coefficients for each feature, is it possible to control the\nmagnitude of the output change with such precision? Are the magnitudes similar for\ndifferent features, or does each feature have its own sweet spot? Could the position of\nthe layer on which the SAE is trained also influence this sweet spot behavior? Finally,\nwhile it might be helpful to carefully regulate the influence of some features, could\nthere also be unexpected side effects of feature steering? That is, for some steering\ncoefficient, the output may not change in the way the activation and description led\nus to believe.\n\n4 Replicating Anthropic’s Results\n\nOur goal is to critically examine the two claims underlying Anthropic’s recent work\non interpretable features in LLMs. As a starting point, we will first reproduce the\nrelevant basic results found in Scaling Monosemanticity (Templeton et al. 2024) on a\npublicly available model.\n   In order to reproduce Anthropic’s results – and later test their generalizability and\nexpand on Anthropic’s findings – we use an open source LLM and SAE. Specifically,\nwe use Llama 3.1 with 8 Billion parameters as our base LLM – a lightweight and\nfast version of Llama. Thanks to Neuronpedia4 we have access to various pre-trained\nSAEs for Llama (Lin 2023). All of the following results are based on all 32 SAEs (one\nfor each layer of Llama 3.1) called Llama Scope with an 8-fold multiplication, i.e. 32k\nfeatures per layer (He et al. 2024). To make our results comparable to Anthropic’s\napproach, we selected SAEs that were trained on the residual stream between layers\nrather than on the multilayer perceptrons (MLPs) in each layer. This enables the\nSAEs to access the full information flow between layers, rather than just the output\nof a single MLP at a specific point. We will refer to the SAEs by the layers behind\nwhich they were trained. For example, “SAE in layer 18” refers to the SAE that was\ntrained on the residual stream after the output of the MLP in layer 18 was added back\nin. Additionally, we ensure reproducibility by fixing all of the parameters above, as\nwell as seed, temperature, etc. throughout the paper. The automated interpretation\nof features was done by o4-mini (Lin 2025) based on an implementation by OpenAI\n(2025). For a detailed list of our parameters, see Appendix A.1.\n   With this setup, we can extract feature activations for any input and steer the\nLLM’s output using arbitrary steering coefficients for all features of all layers. Below,\nwe present one result from each of our tests and provide additional data in the\nAppendix. However, unlike Anthropic, we do not have the resources to be exhaustive\n\n\n  4Neuronpedia can be accessed through its website (https://www.neuronpedia.org/). Most of its functions\n– especially the ones relevant in this paper – are also accessible through an API.\n\n\n                                     8\n\nor even properly representative. Instead, we present exemplary findings to caution\nagainst hasty generalizations of positive examples often cited in the literature. In the\nfollowing, we show that we are able to replicate Anthropic’s results qualitatively. We\ngive an example similar to the ones Templeton et al. (2024) use, check its specificity,\nand its steering capabilities. Finally, we comment on the general trends we can observe\nwhen working with features.\n\n4.1 Replicating Interpretable Feature Extraction and\n    Activation\n\nAs a first step, we show that there exist interpretable features that activate in the\nappropriate contexts. To do so, we need to find a feature that is similar to the “Golden\nGate Bridge” feature in Templeton et al. (2024): the feature should have a clear-cut\nmeaning (as opposed to a feeling or qualitative judgment) and a “normal” activation\npattern. In our case, the SAEs have over 30, 000 features for each layer. As we describe\nin Section 4.3, the most stable features with regard to activation and steering can be\nfound in the middle layers of the network. To illustrate and reproduce Anthropic’s\nresults, we select feature 9463 in layer 18 as an example. It meets the above criteria\nand is auto-interpreted as “mentions of coffee and related terms”. In the following, we\nwill abbreviate feature number Y in layer X as (X/Y ); so, the feature “mentions of\ncoffee and related terms” has the unique identifier (18/9463).\n   As we depict in table 1, given a longer text passage on coffee, this feature activates\non words like “coffee” and “espresso”, as well as words related to this context. The\ncloser the words are to “coffee” itself, the stronger the activation, though it does\nnot pick up on every coffee-related term (e.g., “portafilter”, “ristretto”, “crema”).\nIn comparison to others, this feature activation is fairly specific: In Section 5.3 we\ncompare three features (including this one) with varying context sensitivity and this\none is not particularly context sensitive.\n\n As the barista carefully grinds the aromatic espresso beans and tamps them into the\n  portafilter before pulling a silky shot of ristretto, the bustling caf´e fills with the comforting\n  scent of roasted Arabica, while patrons sip their cappuccinos from ceramic mugs, chatting\n  about fair-trade plantations, latte art, crema, and the timeless ritual that makes coffee not\n  just a beverage but a global symbol of morning routines, social connection, and mindful\n  indulgence.\n\nTable 1: Activation of feature “mentions of coffee and related terms” (18/9463) on English\ntext. The feature related to coffee activates on and around coffee terms. The closer the words\nare to coffee the higher the activation. Higher opacity indicates stronger activation.\n\n\n\n   Similar to the examples provided by Anthropic, this feature activates in the context\nof coffee in multiple different languages, such as German (table 2) and Japanese (table\n3).5 This shows that it is a multilingual feature, which abstracts to some degree from\n\n\n  5The text examples are automatic translations of our English text. The translations were generated by\nan ML model that is distinct from the Llama 3.1 model that was evaluated in this study, and checked by\nnative speakers.\n\n\n                                     9\n\nlanguage specific words and activates in a narrow margin on and around the topic\nof coffee. This is an important characteristic if one intends to steer the feature in a\nmodel independently of language.\n\n German: W¨ahrend der Barista die aromatischen Espressobohnen sorgf¨altig mahlt und\n  sie in den Siebtr¨ager dr¨uckt, bevor er einen seidigen Schuss Ristretto zieht, erf¨ullt sich\n  das gesch¨aftige Caf´e mit dem wohligen Duft von ger¨ostetem Arabica, w¨ahrend die G¨aste\n  ihren Cappuccino aus Keramiktassen schl¨urfen und sich ¨uber Fair-Trade-Plantagen, Latte\n  Art, Crema, und das zeitlose Ritual unterhalten, das Kaffee nicht nur zu einem Getr¨ank,\n  sondern zu einem weltweiten Symbol f¨ur Morgenroutine, soziale Kontakte und bewussten\n  Genuss macht.\n\nTable 2: Activation of feature “mentions of coffee and related terms” (18/9463) on German\ntext. The feature related to coffee activates on and around coffee terms (even in German).\nThe closer the words are to coffee the higher the activation.\n\n\n\n  Japanese: バリスタが香り高いエスプレッソ豆を丁寧に挽き、ポルタフィルターに詰\n めてから、なめらかなリストレットを注ぎ、やかなカフェには心地よい香りが広がりま\n す。その間、常連客は陶器のマグカップでカプチーノを飲みながら、フェアトレード、\n ラテアート、クレマ、そしてコーヒーを巡る儀式について会話します\n\nTable 3: Activation of feature “mentions of coffee and related terms” (18/9463) on Japanese\ntext. The feature related to coffee activates on and around coffee terms (even for Japanese\ncharacters). The closer the characters are to coffee the higher the activation.\n\n\n\n4.2 Replicating Feature Activation Specificity\n\nThe next step is to check the specificity of our coffee-feature. We want our feature\nto activate only in the context that its description mentions. The further away the\ncontent of a text is to the feature, the lower its activation should be. While we are not\nable to test the feature’s activation on a large corpus of data, we can reproduce the\napproach by Anthropic on a small scale. Templeton et al. (2024) propose four text\ncategories with regard to feature activation: (0) The feature is completely irrelevant\nthroughout the context; (1) The feature is related to the context, but not near the\nhighlighted text or only vaguely related; (2) The feature is only loosely related to the\nhighlighted text or related to the context near the highlighted text; (3) The feature\ncleanly identifies the activating text.\n  We select multiple sentences that should fall into each of these categories based on\nour feature’s description. Here is an example for each of the levels (see Appendix A.2\nfor a detailed list): (0) “The quantum fluctuations in vacuum energy remain a mystery\nin theoretical physics”; (1) “Many people gather in social spaces like bistros and\nlounges to chat before work”; (2) “Sometimes she opted for a macchiato instead of her\nusual hot chocolate”; (3) “He sipped a creamy cappuccino topped with frothed milk at\nthe busy coffeehouse.” We can then extract the maximum activation on these sentences\n\n\n\n                                    10\n\nas well as the average over all non-zero activations (“non-zero” to compensate for\ndifferent numbers of tokens). We see in Figure 1 that both the maximum as well as\nthe average activations grow from one categroy to the next, showing that our coffee-\nfeature activates with specificity in the context of “mentions of coffee and related\nterms”.\n\n                            Activation Strengths of feature (18/9463)\n\n\n                    Max Act.\n                     8    Avg. Act.\n\n\n\n                     6                                                                   Strength\n\n\n                     4\n                                                                                    Activation\n                     2\n\n\n\n                     0\n                             0        1        2        3\n                                        Category\n\nFig. 1: Activation of feature “mentions of coffee and related terms” (18/9463) on sentences\nbelonging to the four specificity categories (category 0 is not related and category 3 is very\nrelated; see Appendix A.2). The more related the context in a category is to “coffee”, the\nhigher the activation.\n\n\n\n4.3 Replicating Feature Steering in Output\n\nFinally, we need to check for the influence this feature can exert on the output of\nthe LLM. In order to achieve an effect, a so-called steering coefficient can be used\nto clamp a specific feature. In that case, the output of the SAE for the clamped\nfeature consists of the baseline output of the SAE plus that output multiplied by\nthe steering coefficient. This method ensures that a steering coefficient of 0 does not\nchange the model’s behavior. As shown in table 4, we clamp the coffee-feature with a\nsteering coefficient of 2 and use the input “My favorite drink is”. While the baseline\nmodel outputs alcoholic beverages, the steered model instead generates coffee-related\noutputs. In the following, we always highlight in bold the interesting passages that\nchange for the steered model.\n   As we show in table 5, this behavior is consistent for positive and negative clamp-\ning.  If we clamp the feature negatively (we use a steering coefficient of −2), we\nobserve the opposite behavior to earlier: the suppression of coffee-related output. The\nchange to “100% natural” might not be entirely grammatically correct, although the\nsupplements mentioned in the second sentence are often marketed as “natural”.\n\n\n                                    11\n\n  Steering    My favorite drink is...\n\n  Baseline         ... a margarita but my favorite cocktail is a mojito. I have a lot of different\n              recipes for mojitos, but this one is my favorite. It’s simple and packs a\n              punch. The name of this drink, “Mojito,” means “little drink” in Spanish.\n                   It’s made with [...]\n Coeff 2           ... coffee. I usually drink my coffee black, but sometimes I like to add a\n                    little milk or cream to it. I also like to add sugar or sweetener to my coffee.\n             Coffee is a brewed beverage with a distinct aroma and flavor. It is one of\n               the most popular drinks in the world, and it has been [...]\n\nTable 4: Steering capability of feature “mentions of coffee and related terms” (18/9463).\nPositively clamping this feature changes the output from alcoholic beverages to coffee-related\ncontent.\n\n\n\n  Steering    An important part of a Swedish breakfast is grown and harvested mainly in\n                Brazil and is called...\n\n  Baseline         ... 100% Arabica coffee. It comes from the coffee bean which is processed,\n             roasted and ground to make the drink we all know and love. The coffee\n             plant is a tropical evergreen shrub that grows up to 5 meters high if left\n             unpruned [...]\n Coeff −2        ... 100% natural. This includes the essential vitamins and minerals that\n              your body needs. The protein content of this product is very high, which\n             makes it a good choice for people who are looking to gain muscle mass or\n                 lose weight. It also has a low glycemic index, which means that it will not\n               cause spikes in your [...]\n\nTable 5: Steering capability of feature “mentions of coffee and related terms” (18/9463).\nNegatively clamping this feature changes the output from coffee to some generic product with\nhealth benefits.\n\n\n\n   Note that Anthropic focuses their work on the “middle layer” of the model because\nthey expect it to contain “interesting, abstract features” (Templeton et al. 2024).\nIn our analysis, we also considered features and their steering capabilities in earlier\nand later layers of the network. While we do not have the resources for a thorough\nquantitative assessment, we did also qualitatively find that feature steering works best\nfor layers in the middle of the network, whereas steering in early layers tends to result\nin chaotic model behavior, and steering in later layers tends to have little influence\non model output.\n   Regarding the size of useful steering coefficients, we observed a qualitatively similar\n“feature steering sweet spot” as described by Anthropic.6 However, their results only\nshow if a steering coefficient negatively impacts the performance of a model (e.g., the\noutput only consists of gibberish), not if a steered model shows any change in line with\n\n\n  6Durmus et al. (2024) analyzed 29 different features and tested the capabilities of the steered model for\ndifferent steering coefficients using the MMLU (Massive Multitask Language Understanding) benchmark.\nThey observe that a steering coefficient in the range between −5 and 5 does not significantly impact model\ncapabilities. Within the “sweet spot” between −5 and 5 the steered model scored similarly well on this\nbenchmark, with a steering coefficient of 0 yielding the best accuracy.\n\n\n                                    12\n\nthe feature used for steering. In other words, their test does not allow any statement\nabout how well the features actually work – maybe they score so well because they\nhave little impact on model behavior in the first place.\n  We found that in general, the steering quality depends on the specific feature and\nlayer, and on the size of the steering coefficient. Features in the middle of the network\nprovide the best compromise between impact and specific control that we found.\n\n   Overall, we were able to replicate Anthropic’s base results on interpretable features\nin LLMs using open-source tools (Llama 3.1 and publicly available SAEs). In short,\nthe basic mechanics of feature extraction, activation, and steering work as claimed by\nTempleton et al. (2024), thus setting the stage for our deeper investigation of whether\nthese findings generalize reliably.\n\n5 Do the Findings Generalize?\n\nIn Section 4 we showed that we are able to reproduce Anthropic’s basic results. We\nnow turn to the stronger promise associated with this work that suggests that such\nextraction of interpretable features can provide model understanding and, through\nfeature steerability, a tool for AI safety. This requires the proof of concept to gen-\neralize across a variety of features, contexts and steering settings. We start with a\ncloser look at the types of features that are extracted (Section 5.1), at the similar-\nity and distinctness of the extracted features (Section 5.2), their context-dependence\n(Section 5.3) and at what one might call “spurious activations” (Section 5.4). Finally,\nSection 5.5 focuses in more detail on the relationship between steering and output\nresults. This serves as a check of the specific behavior observed in Section 4.3.\n   In the following analysis we rely on (generally, multiple) examples to illustrate our\npoints, working on the assumption that if the proposed approach to MI is supposed\nto be a viable candidate for safety-critical settings, then even a few easily detectable\nshortcomings are cause for concern.\n\n5.1 Do the Features Match Their Interpretations?\n\nWe first investigated how representative features such as our “mentions of coffee and\nrelated terms” are, or whether this is a particularly well-behaved example.\n   As we show in the following, there are many other easily interpretable features\n– some of them are considerably more abstract than our original example – that\nshow consistent activation patterns and model-steering capabilities. However, we also\nencountered many features that formally resemble those that can be used for steering,\nbut for which we have no meaningful interpretation. For these features, neither their\nactivation on small text samples, nor their steering capabilities match their alleged\ninterpretation.7\n\n5.1.1 Example Features with Successful Interpretations\n\n(1) “expressions of regret, mourning, and sadness” (17/16706)\n\n\n  7Note that these are different from “dead” features that never activate or that have no effect when steered.\n\n\n                                    13\n\n   As expected, this feature activates strongly on negative emotions, such as “regret”,\n“grief”, “sorrow”, and “sadness”. Furthermore, we see lower-level activations for related\nexpressions like “heart”, “mourning”, and “ache”/“aching”. The activation pattern\naligns well with the interpretation of the feature, which is considerably more abstract\nthan the coffee-example above.\n  When clamping this feature at a coefficient of 10, we see consistent steering of the\nmodel output towards negative emotional states, no matter the tone of the original\ninput. In one example (Input: “Summer is the time of the year where I”), the model\noutput shifts from a happy and grateful reply towards a sorrowful and averse one.\nSimilar behavior can be observed with the input “The best thing that ever happened\nto me was”: While the prompt should strongly influence the model towards a positive\noutput, the steered model generates a sad and melancholic story. The exact activation\nand steering responses can be found in Appendix B.1.1.\n\n\n(2) “instances of the phrase ‘would have’ or similar expressions indicating\nhypothetical or conditional situations” (19/12491)\nWhile the interpretation of this feature is rather abstract, all activations in our sample\ntext are consistent with this interpretation. The highest activations appear for “have”\nfollowing “would”, whereas “would be” and “could have” show lower activations. This\nshows that the feature activates on specific tokens that are tightly linked to the more\nabstract subjunctive form of text.\n  A similar behavior can be observed for steering. When steering the model by a\ncoefficient of 15, the output consistently changes from simple, straightforward answers\nto more complicated outputs including a range of hypothetical/conditional state-\nments consistently starting with “would” and “would have”. This is not only true for\nopen-ended inputs such as “I am going to”, but also for more factual prompts, e.g.,\n“Photosynthesis is characterized by”.\n   Despite the abstractness of this feature, both the activation pattern and the steer-\ning capabilities seem to align well with its interpretation. See Appendix B.1.2 for\ndetails.\n\n\n(3) “references to lists and list-making processes” (16/24361)\nThis feature activates strongly on words such as “checklist” and “list”, and additionally\non terms that provide context to these words. Furthermore, we observe activations on\nrelated descriptions such as “smaller note with labeled items”.\n  When clamped with a steering coefficient of 15, the output of the model based on\nthe prompt “Photosynthesis is characterized by” changes from a standard description\nto a labeled list. Additionally, the model included the word “list” twice in its output.\nFor the input “Let us talk about” and a steering coefficient of 5, the model generates\na modified answer which includes a list-making process.\n   Again the interpretation of this feature is consistent with the observed activation\npattern and steering capabilities. See Appendix B.1.3 for details.\n\n\n\n\n\n                                    14\n\n5.1.2 Examples of Mis-interpreted Features\n\n(1)  “references  to  exposure  to  hazards  or  influential  experiences”\n(18/14711)\nThis feature has a rather broad interpretation, referring not only to exposure to haz-\nards, but influential experiences in general. Accordingly, the activation pattern (see\ntable 6) not only shows high values for “exposure in hazardous environments”, but\nalso “exposure to new ideas” and similar contexts. While this might still be consid-\nered as an “influential experience”, we show that the same feature activates strongly\nfor exposure in the context of photography – a context that hardly fits the proposed\nfeature interpretation.\n\n  Prolonged exposure to loud noise can cause permanent hearing damage, which is why\n  safety guidelines often stress limiting exposure in hazardous environments. At the same\n  time, early exposure to new ideas and influential experiences can shape a person’s world-\n  view in powerful ways. Outside of health and personal growth, the concept of exposure\n  also applies to art: the photographer adjusted the camera settings to get the perfect\n  exposure for the sunset shot, ensuring that the light and colors were captured just right.\n\nTable 6: Activation of feature “references to exposure to hazards or influential experiences”\n(18/14711) on English text. The feature activates on the term “exposure” regardless of the\ncontext.\n\n\n\n   For a steering coefficient of −10 the feature is capable of influencing the model\nbehavior in both contexts. In table 7 we show that the protection from chemical\nexperiments is less prominent than one would expect. However, in table 8 one can see\nthat “exposure” is also suppressed in the context of photography, which is not to be\nexpected from the description.\n   The failure of this and the following features, of course, may have resulted from\nmany different sources: Is the feature itself in fact an agglomeration of several different\nhuman concepts related in one way or another to “exposure”, or did the automated\ninterpretation incorrectly describe the feature, or did the steering capability fail?\n\n\n(2) “references to coffee” (29/30028)\nThis particular feature does not only activate on “coffee” and related terms, but on\nthe token “coff” more generally. This leads to activations in our sample text on “coffee\nshop” as well as “coffin”.\n   As we can show by clamping this feature by a coefficient of −20, the steering\ncapabilities of this feature are compatible with the observed activation pattern: Giving\nthe input “He told me that, at the funeral, five people lowered the”, the baseline\nmodel immediately generates text that revolves around “coffin”. The steered model,\nhowever, omits any reference to “coffin”, instead using related words like “casket”. See\nAppendix B.2.1 for details.\n  We observed a similar behavior for the feature “the word ‘Morocco’ and its varia-\ntions or related concepts” (18/6893). It does not only activate on the word “Morocco”,\nbut more general on the token “mor”, as well as words containing these letters like\n\n\n                                    15\n\n  Steering    When doing chemical experiments...\n\n  Baseline         ... in the lab, you should always wear protective glasses to protect your\n             eyes from any splashes or spills. If you don’t have access to protective glasses,\n             you can use a pair of goggles instead. Goggles are made of plastic and have\n              a clear lens that covers your entire eye area.\n            They are designed to protect your eyes from chemicals, dust, [...]\n Coeff −10      ... in the lab, you should always be careful to pour out your chemicals\n                properly. If you are making a solution, make sure you know what kinds of\n                acids and bases you are mixing together so that you can calculate the pH of\n               the solution. This is important because some chemicals will react with each\n               other when they are mixed together.\n                     If you are mixing two [...]\n\nTable 7: Steering capability of feature “references to exposure to hazards or influential expe-\nriences” (18/14711). Negatively clamping this feature changes the output from explicitly\nmentioning “protection” to only including general descriptions of chemical procedures.\n\n\n\n  Steering     There are multiple settings that a photographer can adjust to change the...\n\n  Baseline         ... exposure of their image. These include shutter speed, aperture, and ISO.\n            The shutter speed is the length of time that the camera’s shutter is open,\n               allowing light to enter the lens and expose the image sensor. A faster\n                shutter speed will result in a brighter image, while a slower shutter speed\n                  will result in a darker image.\n            The aperture is the [...]\n Coeff −10      ... way a picture looks. These include shutter speed, aperture, and ISO.\n            The shutter speed is the length of time the shutter is open, and the aperture\n                    is the size of the lens opening. A faster shutter speed will make a picture\n             look blurry, while a slower one will make it appear blurry [sic].\n         A photographer can also adjust the ISO to change how [...]\n\nTable 8: Steering capability of feature “references to exposure to hazards or influential expe-\nriences” (18/14711). Negatively clamping this feature suppresses the term “exposure” even\nthough it is not contextualized with hazards or influential experiences, but photography.\n\n\n\n“morsel” and “morals”. Steering also shows an influence on all tokens containing “mor”\nnot just “Morocco”. The activation pattern and steering results for this feature can\nbe found in Appendix B.2.2.\n   This suggests that these features refer neither to the concept of coffee (or coffin),\nnor to Morocco, but rather to the tokens “coff” and “mor”. If that is the case, then\nthis undermines the claim of interpretable feature extraction more generally, since the\ngoal was to extract features akin to human concepts, not ones that represent character\nsequences.\n\n\n\n\n\n                                    16\n\n5.2 Does a Similar Interpretation Imply Similar Activations\n   and Similar Steering Effects?\n\nWhen analyzing the feature “mentions of coffee and related terms”, as described\nin Section 4, we encountered many other features with similar interpretations, even\nwithin one layer. If distinct features actually refer to the same concept, one would\nexpect them to behave identically. Conversely, if two features share a similar inter-\npretation, but their associated activation patterns and steering capabilities differ\ngreatly, one ought to be skeptical about whether they refer to the same concept as\nthe automated interpretation suggests.\n   To give some sense of the kind of activations – and related problems – we will talk\nabout in the next two Sections, we systematically checked the activation patterns of\nall features containing the word “coffee” on coffee-related terms (see Online Resource\n1). Three general observations can be made. First, all “coffee”-features are active for\nat least a single token in our test set of coffee-related terms. Second, one can see that\nactivations grow larger for deeper layers of the model. Third, for multi-token words\nlike “cappuccino” or “ristretto” most of the features do not activate on all tokens but\nonly on a subset. This subset is relatively stable across all features. For example, all\nfeatures we checked that are active for the word “cappuccino” only activate on the\ntoken “app”.\n   However, we also see some noteworthy behavior of features. Why, for example\ndoes the general sounding feature “words related to coffee” (27/1729) only activate\nfor “frappuccino” and “arabica” and nothing else? While all other coffee drinks get at\nleast some amount of activation, why then do only 5 features activate on “macchiato”?\nAnd finally, why does the feature “sentences that discuss coffee experiences and related\ncultural practices” (27/4442) activate, e.g., on “espresso”, “latte”, and “cappuccino”,\nand “references to coffee and its cultural significance” (27/11953) does not, while both\nof them activate similarly on “arabica”, “robusta”, and “roastery”?\n   The next two Sections will discuss in more detail whether and how we can rely on\nfeature descriptions. To better understand the similarity of features in the following,\nwe will first consider similar features in a single layer, and then similar features that\nappear in different layers.\n\n5.2.1 Similar Features in a Single Layer\n\nGiven the construction of the SAEs, it is not surprising that – depending on the size\nof the SAE – one might have a certain amount of overlap of features. As Templeton\net al. (2024) describe, features in smaller SAEs tend to “split” into more fine-grained\nfeatures in larger SAEs. For example, a single “San Francisco” feature found in the\n1M SAE splits into two distinct features in the 4M SAE, and into eleven fine-grained\nfeatures in the case of a 34M SAE. Intuitively speaking, the larger SAE is like a\nstronger microscope, allowing for a more detailed analysis of the feature space. This\nimplies, however, that these fine-grained features – while close in feature space and\nsemantically related – are still distinct from one another.\n\n\n\n\n\n                                    17\n\n   However, as we show in the following, this is not always the case. For example, take\nall coffee-related features in layer 18. As depicted in table 9, there exist five different\nfeatures that broadly refer to coffee, all of which were found through a single SAE.\n    If this is a case of feature splitting where a “coffee-cluster” was split up due to\nthe size of the SAE used, then we would expect very similar – but not identical –\ncoffee-related behavior across these features. This means that for a set of coffee-related\nwords, one would expect most of the features to be activated frequently. However,\nthere should be some phrases that only fit into the realm of a single fine-grained feature\nand consequentially only that feature should activate. Alternatively, this seemingly\nlarge overlap of features could be due to the automatic and imperfect generation\nof interpretations and might not represent the actual differentiating power of the\nunderlying features.\n   To analyze this possible discrepancy, we checked the feature activations on differ-\nent coffee-related terms to see if there is a difference. Specifically, we chose ten words\nper feature that should (based on the feature’s description) lead to a high activation\nof said feature. Because there are differences in the descriptions, we choose words\nthat are specific for one category instead of being generally about coffee. For exam-\nple, words like “WiFi”, “Coffeehouse playlist”, and “Urban roast bar” fit well to the\ndescription of feature (18/17305): “references to cafes and coffee culture”. While words\nlike “Sauvignon Blanc”, “Cold brew”, and “Pinot Noir” are supposedly much better\nsuited to generate a high activation for feature “references to drinks, particularly wine\nand coffee” (18/7546). For a full list of words, see the Appendix B.3.1.\n   Of course, some overlap is inherent for these five features. For example, for the very\ngeneral feature “mentions of coffee and related terms” (18/9463) a lot of activation on\nall of the categories is to be expected. We recorded the activation of all features for\nthe phrases in all categories. We then rescaled all activations of each feature between\n0 and 1 by its maximal activation. Then the activations were rescaled across all terms\nin each category. Finally, we summed the activations for each feature over the terms of\none category to compare the discriminatory power between the features (see table 9).\nFor fully disjoint features we would expect activations only on the diagonal. This\nwould mean a feature exclusively activates on the words specifically chosen for it and\non nothing else. But even in the case of these five related features we would expect\nthe highest activations to be found on the diagonal. This would mean that, while all\nof the features have some overlap regarding their activation on coffee-related terms,\nthey still show some discriminatory power by activating more strongly on terms form\ntheir fine-grained region instead of on generally coffee-related terms.\n   Table 9 shows that feature descriptions seem to be an unfit tool to discriminate\nbetween features. We see activation of all features on all categories in general without\nany significantly higher activation of a feature on the terms that should cover the\nsemantic region given by the category descriptions. This activation on all coffee-related\nterms seems to suggest that these features actually belong to the same semantically\nrelated cluster – and were split by the fine-graining process described above. Solely\nthe feature “references to drinks, particularly wine and coffee” (18/7546) has very low\nactivations overall. We will take a closer look at this feature in the next Section 5.3.\n\n\n\n\n                                    18\n\n                                              Terms for Feature\n                                         7546   9463   15276   17350   28590\n\n       “references to drinks, particularly\n                                              0.48     0      1.12     0.15     0.33\n      wine and coffee” (18/7546)\n\n      “mentions of coffee and\n                                              1.89    2.02    1.45     2.5     4.16\n       related terms” (18/9463)\n\n       “references to coffee and coffee-\n                                              3.38    1.81    0.86     2.43     2.68\n       related experiences” (18/15276)\n\n       “references to caf´es and\n                                              2.58    1.15    1.81     4.18     3.35\n       coffee culture” (18/17350)\n\n       “references to coffee and\n                                          0     1.69    0.33     0.96     1.09\n        its cultural significance” (18/28590)\n\n    Table 9: Activation of features (summed over all input terms and normalized for\n     each column) based on inputs from the realm of these features. A comprehensible\n     distinction between features cannot be identified (highest values should be on the\n     diagonal). Feature (18/9463) is the most universally activated on coffee-related\n     terms.\n\n\n\nContrary to our initial expectation, however, we cannot discriminate the five coffee-\nrelated features based on their activation on word-categories that were specifically\nchosen to fit their descriptions. This raises the question of why they are distinctly\nnamed and how to choose the best feature for steering in a specific case based on its\nname.\n\n5.2.2 Similar Features in Different Layers\n\nSince similar features can be identified within a single layer, might this trend also\nbe observable across multiple layers? It’s important to note that we can’t expect to\nfind the same features on every layer. Each SAE is trained on a single layer – or,\nmore precisely, a single point of the residual stream – and there is nothing to ensure\ncontinuity of features between SAEs on different layers.\n   Across all 32 layers of Llama 3.1 we found over 150 features mentioning “coffee”\n(see Appendix B.3.2). While most of them have slightly different descriptions, we\nfound three coffee-features across the network with the exact same description (see\ntable 10). As discussed earlier, feature (29/30028) is a case of a poorly auto-interpreted\nfeature, because it activates not on “coffee” and related terms, but on the token “coff”.\n\n                        Layer   Feature   Description\n                        6      25623     references to coffee\n                        19     12587     references to coffee\n                        29     30028     references to coffee\n\n                    Table 10: Features that have the exact\n                     same description: “references to coffee”.\n\n\n\n\n\n                                    19\n\n   Although the three features have the same description, they exhibit different acti-\nvation patterns on a coffee-related text (see Appendix B.3.3). They also influence the\nmodel differently when used for steering, as shown in Appendix B.3.4 (see the steer-\ning differences for the five coffee-related features on the same layer 18 in Appendix\nB21 and B22 for comparison). This raises a significant challenge for MI: What are we\nsupposed to make of features that behave differently despite having the same descrip-\ntion? Such features undermine the basic goal of a robust mapping between human\ninterpretable features and model features. Anthropic is working on a method, called\nSparse Crosscoder, to enable such an analysis of persistent features throughout the\nresidual stream of the model (Lindsey et al. 2024), but apart from their findings being\npreliminary at the time of writing, this just extracts one interpretable feature across\nlayers. It does not ensure against features with the same interpretation but different\nbehavior.\n   Our findings indicate that one cannot rely on the descriptions of features (that\nare based on their activation pattern on the input), either in a single layer or across\ndifferent layers. In both cases, it seems to be more helpful to only analyze their steering\nsuitability to draw conclusions about their capabilities.\n\n5.3 How Sensitive Are the Features to Context?\n\nAs the examples of successful and unsuccessful feature interpretations in Section 5.1\nshowed, feature quality and validity of the automated interpretations can differ signif-\nicantly on a case by case basis. We would expect the feature “references to exposure\nto hazards or influential experiences” (see above), to activate only in a context that\nsomehow refers to hazards or influential experiences, and not to all mentions of “expo-\nsure” in every context. This naturally leads us to ask about the context-dependence\nof features. In particular, we tested if there is a difference in activation on words with\nand without the context around them. We found examples of (1) features that are\noverly context-dependent, (2) features that have a high context-dependence, and (3)\nfeatures that show little to no context-dependence whatsoever.\n   In table 11, we depict the context-dependence of three features: “references to\ndrinks, particularly wine and coffee” (18/7546), “topics related to coffee, including\nits origins, effects, and cultural significance” (17/17320), and “mentions of coffee and\nrelated terms” (18/9463). Note that we did not consider longer passages of context.\nIn each case, we tested either no context at all, or just a few words preceding (or\nfollowing) the word of interest.\n   As we show, “references to drinks, particularly wine and coffee” (18/7546) is highly\ncontext-dependent, activating mainly on tokens following the phrases “(hot) cup of” or\n“drank”. This behavior is not even restricted to “wine and coffee” as the interpretation\nsuggests, but also activates for “cup of poison” or “drank poison”. The full list of\nphrases can be found in Appendix B.4.1. The right context appears to be essential to\nthis feature, as the interpretation would suggest that it should activate on “coffee” on\nits own, but it does not.\n   Second, “topics related to coffee, including its origins, effects, and cultural signif-\nicance” (17/17320) is also context-dependent, but in a much more predictable way\n\n\n\n\n                                    20\n\n (18/7546): references to   (17/17320): related to    (18/9463): coffee and\n drinks (wine, coffee)       coffee (origins, effects,    related terms\n                             cultural significance)\n\n  coffee                           coffee                           coffee\n\n poison                          caffeine                      Espresso\n\n cup of coffee                    coffee from Africa              caffeine\n\n cup of poison                 zebras from Africa            Coffin\n\n We drank coffee.             kwentoeler welrl newr      We drank our coffee in the\n                                   caffeine                    morning\n\n We drank poison.             Coffee: America’s most       iwern owk ail coffee\n                             popular equipment for\n\nTable 11: Context-dependence of features. “references to drinks, particularly wine and coffee”\n(18/7546) activates on tokens following the phrases “cup of” or “drank”. However, it does\nnot activate on the term “coffee” in isolation. “topics related to coffee, including its origins,\neffects, and cultural significance” (17/17320) activates on tokens only in the surroundings of\ncoffee-related terms. “mentions of coffee and related terms” (18/9463) activates on the token\n“coffee” (and related terms) regardless of the context.\n\n\n\nthan the previous feature. While it lightly activates on words and tokens such as “cof-\nfee” or “caffeine”, we observe the strongest activations when there is a reference to\ncoffee in its surrounding. For example, while there is no activation for “Africa” in con-\nnection with “zebras”, we can observe an activation for “Africa” in connection with\n“coffee”. See Appendix B.4.2 for the full list of phrases and activations.\n    Lastly, the feature “mentions of coffee and related terms” (18/9463) does not\nseem to be very context-dependent at all. It activates on “coffee” and related terms\n(including tokens that make up these terms), regardless of the context. See Appendix\nB.4.3 for the full list.\n   All together, features seem to have varying degrees of context sensitivity that is\nnot in any way apparent from their interpretation. Our first example (18/7546) is too\ncontext sensitive to be humanly interpretable at least with its current description.\nThe second (17/17320) and third features (18/9463) are less context sensitive or only\nsensitive in a way one would expect for the description. Both are therefore much more\ninterpretable and behave in expected ways. As we showed above (Section 4), the third\nfeature (18/9463) can be very useful for steering.\n\n5.4 Features With Spurious Activations\n\nIn the course of our analysis, we encountered several features with activation patterns\nthat clearly do not indicate semantic content.\n    First, we found a family of features that have an activation density of over 90%,\nwhich we will refer to as hyperactive features. Note that Templeton et al. (2024)\nmention the issue of “dead” features, i.e., features which were not active over a sample\nof 107 tokens. As one might expect, larger SAEs produce larger numbers of dead\n\n\n\n                                    21\n\nfeatures (in absolute, as well as relative terms) compared to their smaller counterparts.\nHowever, they do not mention features with an unusually high activation density. We\ngive an overview of some of these hyperactive features – features that activate on over\n90% of all tokens – in table 12 below.\n\n Layer Feature Description                                                   Density\n\n 1     17701  symbols and formatting elements commonly used in programming   93.80%\n                or markup languages\n 15    9478    references to systems, mechanisms, or structures related to regu-   93.50%\n                 latory or operational frameworks\n 15    3179    references to discussions or mentions of climate change and its  96.67%\n               impacts\n 24    29371   numerical data and associated statistics related to various topics   94.95%\n 30    24133   references to accountability and oversight in institutional settings   93.88%\n 7     27281   financial and regulatory terms related to government policies and  96.55%\n               economic frameworks\n\nTable 12: Features with an activation density higher than 90%. In other words, they activate\non more than 90% of tokens.\n\n\n\n   In some cases, one might expect such a relatively high activation density, such\nas with feature (1/17701), which is interpreted as “symbols and formatting elements\ncommonly used in programming or markup languages”. In other cases, the explanation\nis not clear. For example, we do not expect a feature interpreted as “references to\ndiscussions or mentions of climate change and its impacts” (15/3179) to activate\nconsistently on over 96% of tokens.\n   Second, we found a family of features (table 13) that activate strongly on the\n<|begin of text|> token, a token that is only present at the very beginning of an\ninput to signal the start of the token sequence. Their activation on the beginning\nof sequence can be orders of magnitude higher than their activation on normal text\ntokens. That (some) features activate on this beginning of sequence is to be expected\n– all text samples used for training and description generation include this token. For\nexample, our coffee feature also activates on the <|begin of text|> token, however,\nits activation strength is comparable to coffee-related text inputs.\n   Despite this high activation value on the beginning of a text, we observe that\nthe model output can be steered consistently with the interpretation (that does not\nrefer to the beginning of sequence) in many cases. We show this using the features\n“monetary values and financial information” (20/6246, <|begin of text|> activation:\n156.0) and “phrases related to weight loss and body shaping techniques” (20/21737,\n<|begin of text|> activation: 100.5) as examples (see Appendix B.4 for the results).\nIn comparison to other features, however, the model behavior seems to be a little bit\nmore erratic and hard to control.\n    It is not entirely clear how steering in general should or could influence a token\nthat is by design only present at the very beginning of a sequence. There are, however,\nother special tokens at work during training and deployment of LLMs that are not only\n\n\n\n                                    22\n\n Layer Feature Description                                                   Activation\n\n 1     5371    occurrences of the word ”this”                                245.00\n 1     26183  HTML or CSS structures and elements                        244.00\n 10    11036   formatted sections or placeholders within text, possibly indica-   140.00\n                  tive of form fields or structured data\n 10    19323   numerical data related to years and events in history           123.50\n 20    6246   monetary values and financial information                     156.00\n 20    21737   phrases related to weight loss and body shaping techniques     100.50\n 30    19188   phrases and concepts related to guidance and support in edu-   140.00\n                 cational contexts\n 30    20145   topics related to skincare and personal well-being              131.00\n\nTable 13: Features activating on <|begin of text|> token. Their listed maximum activation\nis only present for the <|begin of text|> token.\n\n\n\npresent at the beginning. Dialogue tokens differentiate what the user said and what\nthe model generated. Masking tokens are used to train a model on filling gaps with\nthe right tokens. And some special tokens differentiate instruction from conversation\nfor a model. An unusual response to these tokens by the SAE might have unexpected\nconsequences when steering the model.\n\n5.5 Can Features be Steered Precisely?\n\nWe showed above that, generally speaking, it is possible to steer model behavior by\nattenuating or amplifying the activation pattern associated with certain features. Are\nthe effects of feature steering on model behavior always meaningful and consistent?\nAs we will show in the following, the quantitative aspects of model steering can be\nrather nuanced and fickle.\n\n(1) “mentions of tennis and related terms” (16/12285)\nThe only tennis-related feature in layer 16 – “mentions of tennis and related terms”\n(16/12285) – is a good example of how steering may not have the effect one expects.\nAs we depict in table 14, the input in this case is “My favorite sport is” and the\nbaseline model names baseball as its standard output. Clamping the tennis-related\nfeature by a coefficient of 1 changes the output of the model from baseball to soccer.\nFurthermore, while the output of the baseline model consistently mentions baseball\nover two paragraphs, the steered model also mentions basketball – but no mention\nof tennis. For a steering coefficient of 2, the model output then does discuss tennis,\nfamous tennis players, and their achievements. In a second paragraph, however, the\nmodel continues mentioning a variety of other sports.\n    It remains unclear what explains this unexpected behavior. It could be that the\ninterpretation is insufficient, or that steering a feature can sometimes involve unex-\npected detours. While tennis content is generated for a steering coefficient of 2, one\nwould not expect it to go via soccer for a steering coefficient of 1.\n\n(2) “references to the effects of coffee on health” (15/4922)\n\n\n                                    23\n\n  Steering    My favorite sport is...\n\n  Baseline         ... baseball. I like to play baseball and watch it on TV. I also like to go to\n               the games at the ballpark. My favorite team is the Minnesota Twins.\n                   I have been playing baseball since I was 5 years old. When I was 5, my dad\n               signed me up for Little League Baseball. He didn’t know how to play but [...]\n Coeff 1           ... soccer. I like to play it and watch it. I play soccer with my friends in\n               the park near my home every weekend. I like to watch it on TV when there\n               are matches between famous teams.\n                   I like to play basketball, too, but I don’t play it very often. My friends and\n                   I sometimes play basketball in the park after we [...]\n Coeff 2           ... tennis. I like to play tennis and watch it on TV. I’m a big fan of Serena\n            Williams and Roger Federer. They are the best players in the world and\n              they have won many Grand Slam titles.\n                   I also like to play other sports, such as golf, swimming, and skiing. I enjoy\n              watching football and basketball as well.\n           What [...]\n\nTable 14: Steering capability of feature “mentions of tennis and related terms” (16/12285).\nPositively clamping this feature by 1 changes the mentioned sport from “baseball” to “soccer”\nor “basketball”. Increasing the factor further to 2 generates a description of “tennis”, before\nmentioning other sports.\n\n\n\nThis is one of the many coffee-related features we found. As depicted in Appendix\nB.6.1, we provided the model with the input “My favorite drink is”. With a steering\ncoefficient of 5, this feature works similarly to other coffee features we found and\nstudied: it changes the model output from “margarita” to “coffee”. For slightly higher\n(6 and 8) steering coefficients, however, the model generated “a simple cup of hot tea”\nas its first output, only afterwards mentioning coffee. In this case the switch to tea\nseems to be a first sign of breaking, as a steering coefficient of 8 starts to affect the\ncorrectness of the syntax of the output.\n\n\n(3) “references to pigeons and airborne topics” (18/16371)\nThis feature has a rather peculiar interpretation. It combines a specific type of bird\n– “pigeons” – with the general addition “and airborne topics”. Considering only its\ninterpretation, it is not obvious how the model behavior would be affected, but an\nincreased tendency to mention pigeons seems likely. This is especially true, as the\nfeature also activates on “Racing Pigeon”. As can be seen in Appendix B.6.2, we\nprovided the model with the input “An example for a bird is”, and the answer of the\nbaseline model is “hummingbird”. Contrary to our initial expectations, the steered\nmodel does not generate “pigeon” as its output, but “American Robin”. This output\nis stable for increasing steering coefficients (2, 5, and 10), with only the additional\ninformation provided by the model changing – being correct descriptions for American\nRobins.\n\n\n\n\n\n                                    24\n\n6 Discussion\n\n6.1 Reproducibility and Scope of Interpretable Features\n\nOur investigation successfully reproduced Anthropic’s core findings, even while using a\ndifferent model. We confirmed that SAEs can extract semantically meaningful features\nfrom LLMs, and that these features can be manipulated to produce syntactically\ncoherent text that emphasizes or de-emphasizes particular concepts (see Section 4 and\n5.1.1).\n   However, this successful reproduction comes with significant caveats. Alongside\ninterpretable features, our analysis revealed numerous features whose automated\nlabels fail to align with their actual behavior during activation and steering. We\nshowed this behavior in selected cases with features like “references to exposure to\nhazards or influential experiences” (18/14711) and “the word ‘Morocco’ and its varia-\ntions or related concepts” (18/6893) in Section 5.1.2. Importantly, we saw that there\nis no consistent way of determining  if a feature behaves as expected just by look-\ning at its description. This observation raises concerns about the completeness of\nexisting presentations of SAE results, which tend to showcase the most successful\nand interpretable features while leaving the proportion of less interpretable features\nunclear.\n   The challenge of selective presentation in interpretability research has been pre-\nviously acknowledged in the literature. R¨auker et al. (2023) explicitly discuss the\nvalidity of interpretations and the risks of cherry-picking results. When companies\nconduct research on proprietary models that cannot be independently audited, they\nbear enhanced responsibility for comprehensive and honest reporting. Selective pre-\nsentation may advance a compelling narrative about MI, but it does not provide the\nevidence base needed to assess whether these techniques can actually control model\nbehavior reliably enough for real-world deployment. Notably, Marks et al. (2025) orig-\ninally claimed to offer demonstrations on “non-cherry-picked tasks of practical value”,\nbut this language was removed in their 2025 update, suggesting evolving recognition\nof these concerns within the field. Our investigation shows just how easy it is to find\nexamples that undermine the goals of MI and how hard it is to ensure against them.\n   Associated with this, another critical yet often implicit assumption in SAE-based\ninterpretability research concerns the relationship between identified features and\nhuman concepts. The intuitive appeal of interpretable features stems partly from the\nsuggestion that they correspond to concepts humans recognize and employ. However,\nthis assumption warrants careful scrutiny.\n   The question is not whether all features learned by language models are “bad”\nor spurious – transfer learning demonstrates that neural networks learn generaliz-\nable patterns beyond task-specific artifacts (Freiesleben 2026, 282). Rather, the issue\nconcerns which features correspond to human concepts and how we would deter-\nmine this correspondence. As Freiesleben notes, research on adversarial examples and\nrobustness suggests that neural networks learn both, human and non-human concepts\n(Freiesleben 2026, 283-284). The nature of these non-human concepts and whether\nhumans can in principle understand them remains an open question.\n\n\n\n\n                                    25\n\n   Our analysis suggests that the features highlighted in prominent interpretability\ndemonstrations may not be representative of the broader feature distribution. While\nsome features clearly activate in patterns humans recognize and can be described\nusing familiar concepts, many others resist straightforward interpretation or exhibit\nactivation patterns that do not map cleanly onto human conceptual categories. The\nexistence of interpretable features does not imply that most or all features share this\nproperty.\n   This observation has implications for how we evaluate interpretability claims.\nAnthropic’s earlier blog posts sometimes suggest broad interpretability of identified\nfeatures, though their later work on feature steering adopts more cautious language\nabout feature labels and their reliability. The shift in framing may reflect growing\nrecognition that human interpretability of a feature – our ability to assign it a descrip-\ntive label – does not necessarily mean the feature corresponds to a concept humans\nemploy in their own reasoning, or that our interpretation captures the feature’s actual\ncomputational role.\n   To make claims about human concepts in language models genuinely worthwhile\nfor interpretability, we need systematic understanding of when human-recognizable\nconcepts emerge and verification that they “[...] represent in the sense of coactivating\nAND taking the functional role of concepts.” (Freiesleben 2026, 288) This requires\ndemonstrating not only that features can be described in human terms, but that they\nfunction computationally in ways analogous to how humans use those concepts.\n\n6.2 Interpretation-Behavior Gap\n\nA central finding of our analysis concerns the reliability of feature interpretations.\nWe identified disconnects between the automated descriptions of features on the one\nhand, and their activation patterns across text samples and their effects when used\nfor model steering on the other hand. This misalignment poses significant challenges\nfor claims about feature interpretability.\n   The automated labeling methods employed to describe features – which typically\nidentify topics in text samples where features activate – do not necessarily capture\nthe directionality or causal role of those features. As Durmus et al. (2024) note in\ntheir work on feature steering, labels related to concepts like discrimination may\naffect discrimination-related outputs “[...] but not necessarily increase (or decrease)\ndiscrimination or bias in predictable ways.” Our own experiments confirm this concern:\n(1) we observed cases where features seemingly activate in contexts consistent with\ntheir labels but produce unexpected or incoherent effects when used for steering, e.g.,\nthe feature “references to pigeons and airborne topics” (18/16371) in Section 5.5; (2)\nthere exist features like “references to coffee” (29/30028) where activation and steering\nconsistently influence the token “coff”, but whose automated interpretation suggests\na conceptual connection to coffee (see Section 5.1.2); (3) even though we find features\nwith the exact same description, namely “references to coffee”, across different layers,\nthese do not exhibit the same behavior for neither activation nor steering (see Section\n5.2.2).\n\n\n\n\n\n                                    26\n\n   This interpretation-behavior gap appears to have multiple sources. First, feature\nactivation exhibits context-dependence that complicates straightforward interpreta-\ntion. What appears to be a coherent concept may in some cases reduce to activation\non particular words or tokens in specific contexts, creating an illusion of more abstract\nconceptual representation (see the context-dependent features in Section 5.3). Second,\nfeatures with highly similar descriptions often coexist within the same layer (see the\nfive coffee-related features in layer 18 shown in Section 5.2.1), raising questions about\ntheir functional differentiation and relationship to one another. Without clear princi-\nples for distinguishing between related features, it becomes difficult to claim precise\nunderstanding of what any individual feature represents.\n   In addition, we found two types of activation patterns that appear entirely spurious\n(see Section 5.4), namely features with a very high activation density, as well as\nfeatures whose activation is disproportionally high for the <|begin of text|> token.\nBoth of these activations are not detectable in the feature descriptions themselves.\nThis points to another type of gap between interpretation and behavior. As it is not\nclear where their behavior comes from, features seem to be able to encode particular\nbehavior that is not encodeable in their interpretation.\n\n6.3 Fragility of Feature Steering\n\nOur experiments reveal that feature steering is a highly sensitive operation character-\nized by several forms of fragility. The effectiveness and meaning of steering depends\non the quantitative magnitude of the steering coefficient, the accuracy of the feature’s\ninterpretation, and whether the feature responds as expected in diverse contexts. We\npresent examples of malfunctioning steering – e.g., the steering of feature “mentions\nof tennis and related terms” (16/12285) resulting in text on sports other than tennis\n– in Section 5.5.\n   The magnitude of steering interventions requires careful calibration, and in general\ndepends on the specific layer and network depth. See, e.g., the feature “references to\nthe effects of coffee on health” (15/4922) in Section 5.5: the steering works well for\na factor of 5, produces unexpected results for 6, and breaks the model output for a\nfactor of 8. Other features only start to exhibit an influence on model behavior with a\nsteering coefficient of 10 and more (see the lists of clamped coffee-features in Appendix\nB.3.4). Durmus et al. (2024) acknowledge this fact, but leave questions unanswered\n(see Section 4.3). Small changes in steering strength can produce disproportionate\neffects, and steering for one intended concept may inadvertently affect homonyms,\nsemantically similar terms, or unrelated linguistic patterns (see the examples of mis-\ninterpreted features in Section 5.1.2). The boundaries of what is being manipulated\nremain unclear, complicating efforts to achieve targeted behavioral modifications.\n   Due to the layer-specificity of SAEs, our approach does not allow to draw any\nfinal conclusions on the layer-dependence of features. Because SAEs are trained on\nindividual layers, one cannot be sure if two identically interpreted features in subse-\nquent layers actually represent the same concept. However, we encountered several\nnearly identically interpreted features in a single layer (see the coffee-related features\nin Section 5.2.1), which begs the question how these can be meaningfully distinguished\nand then used for steering purposes. While these findings are only (multiple) instances,\n\n\n                                    27\n\nthey significantly undermine the suggested generality and applicability of this route\ntowards mechanistic interpretability. The broad expectation that in general we will\nbe able to find well-interpreted features across a network that can be reliably steered,\nis not warranted.\n   From an AI safety perspective, this  fragility  is particularly concerning. Any\nattempt to use SAE-based steering as a control mechanism would need to account for\nsensitivity to layer positioning, precise parameterization, interpretive accuracy, and\npotential side effects – a combination of requirements that suggests limited robustness\nfor practical deployment.\n\n6.4 Representational Faithfulness and Reasoning\n\nBeyond questions of interpretability and steering reliability, it is an open question\nwhether the features produced by SAEs provide faithful abstractions of the language\nmodel’s actual “reasoning” or inference processes. While our approach does not allow\nus to answer this question systematically, we believe the following two issues to be\ncentral if one wants to make progress in the direction of these strong representational\nclaims.\n    First, one would need to show coherence between thematically similar concepts\nacross different layers. If SAE-features captured fundamental reasoning primitives\nused throughout the model’s computation, we would expect to find systematic rela-\ntionships or consistent representations of related concepts as information flows through\nthe network. The absence of such coherence suggests that features may capture\nlayer-specific activation patterns rather than stable cognitive building blocks. Note,\nhowever, that this might be due to the training of SAEs on individual layers of the\nnetwork. The utility of generalized approaches – such as crosscoders (Lindsey et al.\n2024) – in this context is still up to debate.\n   Second, one would need to find feature integration or composition that mir-\nrors standard forms of reasoning. If the identified features genuinely represented the\nmodel’s conceptual vocabulary, we would expect to observe systematic interactions\nbetween features during inference – compositions and combinations that reflect logical\nrelationships or inferential steps. An initial move toward that end is taken by Tem-\npleton et al. (2024) when discussing the potential role of features as computational\nintermediates. They try to determine whether the features are useful for examining\nthe intermediate computational steps that would be necessary for a model to reach a\nmulti-step conclusion. As they show, features like “Kobe Bryant”, “California”, “Cap-\nitals”, “Los Angeles”, and “Los Angeles Lakers” are evoked by the model to complete\nthe sentence “The capital of the state where Kobe Bryant played basketball is”. How-\never, there is still a long way to go before reaching reasoning. The authors acknowledge\nthat there are only a few examples where these intermediate features are this well\ninterpretable. Furthermore, they can only demonstrate activation of these features,\nnot any further interaction. Therefore, this does not definitively answer the question of\nwhether these features truly capture the model’s representational primitives or merely\ncorrelate with particular activation patterns.\n   The risk, as Freiesleben (2026, 289) cautions, is that “[p]eople want to believe that\nANNs are logical concept machines that only need to be scrutinized to become fully\n\n\n                                    28\n\ntransparent.” This narrative, if accepted uncritically, can lead to research prone to\nconfirmation bias – focusing on impressive examples of interpretable features while\nunderestimating the proportion of features that resist interpretation or the challenges\nin verifying that interpretations reflect actual computational function. Such bias has\nreal consequences for users who may overestimate our ability to understand and\ncontrol model behavior based on selective demonstrations.\n\n7 Conclusion\n\nMechanistic interpretability, as a form of explainable AI, refers to a broad set of\nrecent approaches to understanding, and ultimately controlling, the behavior of LLMs.\nThe core idea is to construct semantic representations – so-called “features” – from\nthe neural activation patterns of the network, that are both human-interpretable\nand controllable. There are different approaches to discover, extract, and study such\nfunctionally useful features in LLMs. One particular technique used in MI research\nare sparse autoencoders. In this work, we analyzed the work on SAE-based extraction\nof interpretable features conducted by Anthropic.\n   Our replication in Section 4 confirmed that interpretable features exist and activate\nappropriately in expected contexts. Our testing revealed that features show genuine\nspecificity, with activation strength correlating with contextual relevance. We also\nconfirmed that features can effectively steer model behavior in predictable ways, with\nsteering effectiveness relative to layer position and size of the steering coefficient.\n   In Section 5, we critically examined the potential to generalize specific findings in\nthe context of SAE-based features to larger contexts. This has important implications\nfor the role of MI research to provide model understanding, and to be used as a tool\nfor AI safety.\n   The interpretability paradigm emphasizes understanding internal mechanisms by\nidentifying what features represent and how they activate in response to inputs. While\nthis theoretical project remains valuable for researchers seeking to understand neu-\nral network computations, it may not be the most relevant framework for addressing\npractical safety and deployment concerns. For broader societal purposes, the criti-\ncal question is not whether we can interpret a model’s internal representations, but\nwhether we can reliably control its output behavior. London (2019) argues that when\ncausal knowledge is incomplete, careful empirical validation of accuracy and reliabil-\nity should take priority over demands for explanation in medical decision making. The\nsame logic applies to LLMs that affect millions of users on a daily basis: If the goal\nis AI safety, reliable control of the model’s output behavior is more important than\ndetailed mechanistic interpretations.\n   One step in that direction is taken by Templeton et al. (2024) when discussing\nthe features as computational intermediates (see the analysis of the sentence “The\ncapital of the state where Kobe Bryant played basketball is” above). In the course of\nthis analysis, they discuss ablation and its local approximation attribution. Ablation\nis the gold standard when it comes to determining the influence of a model part –\nin this case a feature – on the output. To measure ablation, each feature one after\nanother is turned off(set to zero) for each predicted token in the sequence to measure\n\n\n\n                                    29\n\nthe feature’s full (possibly non-linear) contribution to that prediction. While this is a\nprecise measurement, it requires significant computational resources. Templeton et al.\n(2024) thus use a form of attribution patching to approximate the ablation measure-\nments. Simply put, they approximate the influence of a feature on a given prediction\nby its decoder weight. While this method is not able to measure non-linear effects,\nit saves computational cost, as it does not require additional forward-passes through\nthe model for each feature. In a footnote they then show that the correlation between\nablation and attribution is much higher than the correlation between ablation and\nactivation (Templeton et al. 2024). This means that the activation pattern associated\nwith a feature, which is computed based on the input to a model, is not a great way\nto predict its ability to influence output behavior! However, as we discussed above,\nactivation-first is the way features are currently approached.\n   This suggests a necessary shift in focus: from the input-to-feature relationship\ntoward the feature-to-output relationship. Rather than asking primarily “what does\nthis feature represent?” we should prioritize “what behavioral effects does manip-\nulating this feature produce, and how reliably can we predict those effects?” The\ndistinction parallels pharmaceutical development: we do not require complete mecha-\nnistic understanding of how a drug affects every molecular pathway in the body before\napproving it for use. Instead, we demand rigorous evidence about its effects on patient\noutcomes through systematic testing across diverse populations, clear documentation\nof efficacy and side effects, and reproducible protocols for safe administration. Sim-\nilarly, for AI safety, the question is not whether we fully understand the internal\ncomputations, but whether we can reliably predict and control what the system will\noutput under various conditions.\n\n\n\n\n\n                                    30\n\nAppendix A  Details: Replicating Anthropic’s\n              Results\n\nA.1  Setup Details\n\n  LLM\n   Name                Llama 3.1\n   Parameters             8 Billion\n\n  SAE\n    Description            Llama Scope (residual stream)\n   Trained SAEs          Llama3 1-8B-Base-L[LAYER]R-8x (Hugging Face)\n    Factor                 8x (Size of hidden layer x 8 = 32,768 features)\n\n   Interpretation\n   Model                   o4-mini\n   Dataset                cerebras/SlimPajama-627B (Hugging Face)\n\n   Steering\n   Temperature              0.5\n   No. of output Tokens   70\n   generated\n   Frequency penalty        1.0\n   Seed                   16\n   Strength Multiplier      1\n                            Strength Multiplier of 1: Steering strengths given in the\n                          paper are identical with the steering coefficients that are mul-\n                                 tiplied with the feature vectors, and are, therefore, a direct\n                         measurement of the influence on the LLM.\n\n                   Table A1: Parameters for SAEs in the paper.\n\n\n\nA.2  Replicating Feature Activation Specificity\n\n Sentences in for each of Anthropic’s specificity categories to test activation of feature\n “mentions of coffee and related terms” (18/9463):\n\n• Category 0: The feature is completely irrelevant throughout the context\n\n   – The quantum fluctuations in vacuum energy remain a mystery in theoretical\n     physics.\n   – She spent the afternoon learning about ancient Mesopotamian irrigation tech-\n     niques.\n   – Satellite imagery revealed unexpected changes in polar ice caps over the past\n     decade.\n   – He carefully cataloged every species of beetle he discovered in the rainforest\n     expedition.\n\n• Category 1: The feature is only vaguely related\n\n\n                                     31\n\n   – The caf´e down the street serves pastries and teas in addition to their other\n      offerings.\n   – Early morning routines often involve some kind of beverage before heading to\n     work.\n   – Many people gather in social spaces like bistros and lounges to chat before work.\n   – She walked past a bakery that also sold drinks in takeaway cups on her commute.\n\n• Category 2: The feature is only loosely related to the highlighted text\n\n   – Although she preferred herbal tea, she occasionally grabbed a latte when feeling\n      tired.\n   – He thought about brewing something warm  like espresso while reading the\n     newspaper.\n   – Sometimes she opted for a macchiato instead of her usual hot chocolate.\n   – He debated ordering a frapp´e as he scanned the drink menu at the counter.\n\n• Category 3: The feature cleanly identifies the activating text\n\n   – She poured herself a steaming mug of dark-roast coffee to jump-start her day.\n   – The rich aroma of freshly ground Arabica beans filled the air as the barista\n    tamped the espresso shot.\n   – He sipped a creamy cappuccino topped with frothed milk at the busy coffeehouse.\n   – The cold brew coffee dripped slowly into the carafe, promising a bold and smooth\n      flavor.\n\nAppendix B  Details: Do the Findings Generalize?\n\nB.1  Examples of Successful Interpretations\n\nB.1.1  “expressions of regret, mourning, and sadness” (17/16706)\n\n Activation pattern:\n\n  The weight of regret settled like a shadow over her heart, each memory a painful echo\n   of choices she could not undo. Grief clung to her like a second skin, and the silence of\n  mourning filled the empty spaces where laughter used to live. She moved through her\n  days with a quiet ache, burdened by sorrow and the haunting presence of what was\n   lost. Remorse whispered in every still moment, reminding her of the paths not taken\n  and the words left unsaid. In the quiet corners of her mind, sadness bloomed like wilted\n  flowers—tender, aching, and impossible to forget.\n\n Table B2: Activation of feature “expressions of regret, mourning, and sadness” (17/16706)\n on English text. The feature related to regret, mourning, and sadness activates on and around\n these terms.\n\n\n\n\n\n                                     32\n\nSteering Capabilities:\n\n  Steering    Summer is the time of the year where I...\n\n  Baseline         ... get to take a break from school and work and just hang out with\n         my family. We are very fortunate to live close to a lot of outdoor activities,\n               including beaches, hiking trails, and nature parks.\n          Our favorite place to go is the beach. Growing up in Southern Califor-\n                 nia, I have been exposed to so many beaches [...]\n Coeff 10         ... have to take my kids to the beach, and I am not a beach person. I\n             don’t like the sand, and swimming in the sea. So I really hope that one\n              day, I could go to a beach with no sand.\n                   I am not the one who has to go there. My kids have to go [...]\n\nTable B3: Steering capability of feature “expressions of regret, mourning, and sadness”\n(17/16706). Positively clamping this feature changes the output from an overall happy story\non holiday activities to a narration full of regretful rhetoric.\n\n\n  Steering     The best thing that ever happened to me was...\n\n  Baseline         ... the day I met you and I want to spend the rest of my life with you.\n                Let’s make our love story last forever.\n                   I have been waiting for someone like you my whole life. I am so happy\n             that we found each other and I can’t wait to spend forever with you.\n            Happy Anniversary!\n            You are [...]\n Coeff 10         ... the day I lost my sight. It was a rainy day and I could not see the\n                 rain. I was in a place where there was no one, but it was a place where there\n              were no people.\n                  It is true that losing your sight is one of the most unfortunate things\n               that can happen to anyone, but it [...]\n\nTable B4: Steering capability of feature “expressions of regret, mourning, and sadness”\n(17/16706). Positively clamping this feature changes the output from from an overall happy\nmemory to a narration on permanently lost sight.\n\n\n\n\n\n                                    33\n\nB.1.2  “instances of the phrase ‘would have’ or similar expressions\n        indicating hypothetical or conditional situations” (19/12491)\n\nActivation pattern:\n\n  If things had gone  differently, perhaps she would be standing in a brighter place,\n  untouched by the weight of what could have been. Had she spoken up that day, maybe\n  everything would have changed—perhaps the outcome would have bent toward some-\n  thing softer, something whole. Suppose she’d taken that chance, followed that fleeting\n  instinct—would she now be free of doubt? What if the timing had been just a little kinder,\n  the circumstances a little more forgiving? In another version of the story, maybe she isn’t\n  haunted by questions, living instead in the calm certainty of choices that worked out.\n\nTable B5: Activation of feature “instances of the phrase ‘would have’ or similar expressions\nindicating hypothetical or conditional situations” (19/12491) on English text. The feature\nrelated to hypothetical or conditional situations activates on “have” following terms like\n“could” and “would”.\n\n\nSteering Capabilities:\n\n  Steering       I am going to...\n\n  Baseline         ... share my own experience of my first ever visit to a Psychic Medium. I\n              have been to a few different ones in the past year and this is the one that\n                 really stood out for me.\n                   I was recommended by a friend who had been to see this lady and really\n               enjoyed it. I was sceptical as I have never [...]\n Coeff 10         ... have to take a look at that, I do not like the idea of having to go\n              through the entire 3d model of the vehicle and making it look like it was in\n              a wreck. I am sure there is an easier way if you can believe that.\n                   I know there is a way to use the same mesh for multiple objects, but I [...]\n\nTable B6: Steering capability of feature “instances of the phrase ‘would have’ or similar\nexpressions indicating hypothetical or conditional situations” (19/12491). Positively clamping\nthis feature changes the output to contain a lot more conditional statements.\n\n\n\n\n\n                                    34\n\n  Steering      Photosynthesis is characterized by...\n\n  Baseline         ... the ability of plants to use solar energy to convert carbon dioxide and\n              water into carbohydrates. The carbohydrates are stored in the plant and can\n             be used as a food source for humans and animals. Photosynthesis occurs in\n               the chloroplasts of plant cells, which contain the chlorophyll that absorbs\n                 light energy from the sun.\n            The [...]\n Coeff 10         ... the ability of plants to use solar energy to convert carbon dioxide and\n              water into carbohydrates. The carbohydrates are stored as starches in the\n                plant, but they would be useless to us if they were not eventually broken\n            down and released as sugars, if we wanted to have some of them. In addition,\n               the plant would have been useless had it not been able [...]\n\nTable B7: Steering capability of feature “instances of the phrase ‘would have’ or similar\nexpressions indicating hypothetical or conditional situations” (19/12491). Positively clamping\nthis feature changes a scientific description to contain a lot of conditional phrases like “would\nhave”.\n\n\nB.1.3  “references to lists and list-making processes” (16/24361)\n\nActivation pattern:\n\n  Before leaving, she double-checked her tasks to make sure nothing was overlooked. Her\n  checklist included: 1) cancel the mail delivery, 2) set the security alarm, and 3) leave a\n  spare key with the neighbor. On her desk, she left a smaller note with labeled items she\n  couldn’t forget to pack: A) her passport, B) phone charger, and C) the envelope marked\n  *For When I’m Gone*. Each list, though simple, held significance — practical on the\n  surface, but deeply tied to the quiet finality of her departure.\n\nTable B8: Activation of feature “references to lists and list-making processes” (16/24361)\non English text. The feature related to lists activates terms like “list” and “item”, as well as\non the m-dash.\n\n\n\n\n\n                                    35\n\nSteering Capabilities:\n\n  Steering     Let us talk about...\n\n  Baseline         ... the best way to find a good and reliable web design company. There are\n           many companies out there that claim to be the best, but how do you know\n              which one is right for your business? In this blog post, we will discuss\n           some tips on how to find a web design company that is perfect for your\n               needs!\n             There are many factors to consider [...]\n Coeff 5           ... the best way to find a good and reliable web design company. There are\n           many companies out there but not all of them will be able to give you what\n             you really want. For your convenience, here is a list of tips that you\n              can follow in order to find the right company.\n               First, you should make sure that the company has an updated website [...]\n\nTable B9: Steering capability of feature “references to  lists and list-making processes”\n(16/24361). Positively clamping this feature changes the output to contain the term “list”, as\nwell as “First”, indicating an enumeration.\n\n\n  Steering      Photosynthesis is characterized by...\n\n  Baseline         ... the ability of plants to use solar energy to convert carbon dioxide and\n              water into carbohydrates. The carbohydrates are stored in the plant and can\n             be used as a food source for humans and animals. Photosynthesis occurs in\n               the chloroplasts of plant cells, which contain the chlorophyll that absorbs\n                 light energy from the sun.\n            The [...]\n Coeff 15         ... the presence of a list of pigments, which are present in the thylakoid\n             membrane. The list is as follows:\n                (a) Chlorophyll a\n               (b) Chlorophyll b\n                 (c) Chlorophyll c\n               (d) Xanthophylls\n                 (e) Carotenoids\n                   (f) Phycoerythrin\n\nTable B10: Steering capability of feature “references to lists and list-making processes”\n(16/24361). Positively clamping this feature changes a scientific description to contain a list\nof terms.\n\n\n\n\n\n                                    36\n\nB.2  Examples of Mis-interpreted Features\n\nB.2.1  “references to coffee” (29/30028)\n\nActivation pattern:\n\n A coffeeshop is more than just a place to drink—it’s a daily ritual for many. Freshly\n  brewed coffee, warm atmosphere, and the right company can turn simple visits into rich,\n  flavorful experiences that feel both comforting and energizing. Coffins are more than\n  containers; they are symbols of respect and remembrance. Coffins can be simple and plain\n  or richly decorated, depending on tradition and status. Across cultures, a coffin reflects\n  beliefs about death, and serves as a final gesture of honor for the one who has passed.\n\nTable B11: Activation of feature “references to coffee” (29/30028) on English text. The\nfeature activates on the token “coff”. Regardless of the context being either “coffeeshop” or\n“coffin”.\n\n\nSteering Capabilities:\n\n  Steering    He told me that, at the funeral, five people lowered the...\n\n  Baseline         ... coffin into the grave but only two could lift it out again.\n           He said he was in a car accident and he had to be cut out of his car with\n               the jaws of life.\n                   I asked him how he was doing and he said “I’m okay, I’m just having a little\n               trouble breathing.” I asked him if he wanted me to call [...]\n Coeff −20      ... casket into the ground and then stood there in silence. When he asked\n            them what they were doing, they said they were waiting for him to tell them\n             what to do next.\n           He said that he was embarrassed and didn’t know what to say. He was still\n                trying to come to terms with his mother’s death. He had never been in charge\n                         [...]\n\nTable B12: Steering capability of feature “references to coffee” (29/30028). Negatively clamp-\ning this feature suppresses the term “coffin” even though it does not relate to “coffee” (except\nfor the same beginning of the words).\n\n\n\n\n\n                                    37\n\nB.2.2  “the word ‘Morocco’ and its variations or related concepts”\n       (18/6893)\n\nActivation pattern:\n\n  Morrison always loved mornings in Morocco, where the morning sun painted the streets\n  with golden light. The moral of his travels, he often thought, was that even the smallest\n  morsels of experience could teach profound lessons. Over breakfast, he’d savor morsels\n  of local pastries while reflecting on memories from past adventures, letting each moment\n  shape his morals and outlook. In the quiet of the morning, Morrison felt that life’s true\n  richness came from noticing these little morsels of beauty, connection, and meaning every-\n  where he went.\n\nTable B13: Activation of feature “the word ‘Morocco’ and its variations or related concepts”\n(18/6893) on English text. The feature activates on “Morocco”, as well as, the term “moral”\nand tokens just entailing “or”.\n\n\nSteering Capabilities:\n\n  Steering   A country in North Africa is called...\n\n  Baseline         ... Algeria. It  is located on the Mediterranean Sea and  is bordered by\n                Tunisia, Libya, Niger, Mali, Mauritania, and Morocco. The country has a\n               population of over 40 million people and is the largest country in Africa. The\n                   official language of Algeria is Arabic but French is also widely spoken.\n             Algeria has a diverse geography with mountains, [...]\n Coeff 10         ... Morocco. It is located on the Mediterranean coast and is bordered by\n               Algeria to the west and the Atlantic Ocean to the south. The capital of\n           Morocco is Rabat, but it also has Casablanca as its largest city. The official\n              language of Morocco is Arabic, but French and Spanish are also spoken there.\n           Morocco has a population of around [...]\n\nTable B14: Steering capability of feature “the word ‘Morocco’ and its variations or related\nconcepts” (18/6893). Positively clamping this feature changes the mentioned North African\ncountry from “Algeria” to “Morocco”.\n\n\n\n\n\n                                    38\n\n  Steering    My day starts...\n\n  Baseline         ... with a cup of coffee and a quick glance at the news. I’m usually surprised\n                to see how much has happened during the night, and I’m always amazed at\n            how much time people spend on their phones.\n                    I love starting my day with a cup of coffee, but sometimes it’s hard to find\n               the time to make one. There are so many [...]\n  Coeff 15         ... in the morning when I wake up and go to the bathroom and do my\n             morning routine. After that, I go to the kitchen and make breakfast. Then I\n                get to school in the morning. In the afternoon, I play basketball, play with\n           my friends, or watch TV. In the evening, I go back to my house and then\n                  sleep.\n                          [...]\n\n Table B15: Steering capability of feature “the word ‘Morocco’ and its variations or related\n concepts” (18/6893). Positively clamping this feature increases to occurrence of the term\n “morning” without a reference to “Morocco” at any point.\n\n\nB.3  Similarity of Features\n\nB.3.1  Sets of Words for Coffee-Related Features in Layer 18\n\nThe following list contains the terms we choose for each of the five coffee-related\n features in layer 18. The terms were chosen based on the description so that they\n should generate a high activation for that specific feature:\n\n• Feature (18/7546): “references to drinks, particularly wine and coffee”\n\n   – Cabernet, Espresso, Merlot, Flat white, Sauvignon Blanc, Cold brew, Pinot Noir,\n     Latte, Macchiato, Ros´e\n\n• Feature (18/9463): “mentions of coffee and related terms”\n\n   – Coffee, Caffeine, Brew, Beans, Arabica, Roast, French press, Drip, Grinder,\n     Percolator\n\n• Feature (18/15276): “references to coffee and coffee-related experiences”\n\n   – Morning ritual, First sip, Brewing aroma, Wake-up cup, Coffee break, Steaming\n    mug, Late-night study fuel, Caf´e chatter, Bitter comfort, Refueling moment\n\n• Feature (18/17350): “references to cafes and coffee culture”\n\n   – Barista, Caf´e ambiance, Third-wave coffee, Latte art, Wi-Fi and workspace,\n     Corner caf´e, Reusable cup, Pour-over station, Coffeehouse playlist, Urban roast\n     bar\n\n• Feature (18/28590): “references to coffee and its cultural significance”\n\n   – Coffeehouse intellectualism, Espresso diplomacy, Java as social glue, Coffee as\n      ritual, Daily grind, Global bean trade, Caffeine capitalism, Slow caf´e movement,\n     Coffee as community, Roasting traditions\n\n\n\n\n\n                                     39\n\nB.3.2  “Coffee” Features (Llama 3.1 8B)\n\n\n Layer  Feature  Description\n 0      6040     references to coffee and related beverages\n 0      21525    references to coffee and related products or concepts\n 0      25424    variations of the word ”barista” related to coffee making or the\n                    coffee industry\n 0      26493    references to espresso or coffee-related terms\n 1      17505    references to coffee shops and houses\n 1      22048   words and phrases related to coffee and coffins\n 1      31543    phrases related to beverages, particularly coffee and tea\n 1      1068     references to coffee and related beverages\n 1      1912     topics related to health and wellness, specifically in relation to\n                    coffee and its effects\n 2      7316     references to coffee and espresso\n 2      10079   words related to coffee shops and houses\n 2      25767   mentions of cafes or coffee shops\n 2      26873   terms and phrases related to coffee\n 3      777      references to coffee and related agricultural products\n 3      1936     references to tables, shops, and houses, particularly in a coffee\n                shop context\n 3      2099     references to Starbucks and coffee-related terms\n 3      15768    references to coffee-related experiences and businesses\n 3      27596    references to coffee and coffee-related products\n 3      29228   mentions of cafes and coffee-related establishments\n 4      5625    mentions of coffee and related beverages\n 4      6003    mentions of cafes or coffee places\n 4      1154    terms related to coffee and its preparation or consumption\n 5      17249   mentions of coffee consumption in relation to cancer risk\n 5      17365    specific terms related to coffee and coffee culture\n 5      15894    references to coffee and related products\n 5      32659    references to Starbucks and related coffee terminology\n 6      1006     references to the effects of coffee on health, particularly concern-\n                   ing cancer risk\n 6      17799    references to coffee and tea preparation\n 6      17744    references to places serving food and beverages, particularly cafes\n               and coffeehouses\n 6      16797   terms related to coffee and its preparation\n 6      15111   terms related to coffee and its preparation or consumption\n 6      21678    references to specific entities, particularly numbers and brands\n                   related to coffee\n 6      25623    references to coffee\n 7      13       references to coffee-related terms\n 7      14416    references to coffee and related products or experiences\n 7      15261   mentions of coffee and related terms\n\n\n\n                                    40\n\n7      15843    references to coffee and its active ingredient, caffeine\n7      23800    references to social activities that involve beverages, especially\n                   coffee and tea\n7      3001    words and phrases related to coffee consumption and its effects\n              on health\n7      6348     locations and settings related to coffee shops, restaurants, and\n                 public spaces\n7      12342   terms related to coffee-focused initiatives and their impacts on\n                   different communities\n7      30127   mentions of tea and coffee\n8      9164    terms related to coffee and coffee preparations\n8      2642     references to cups of coffee and tea\n8      11299    references to cafes and coffee shops\n8      14805    references to coffee consumption and its potential health effects\n8      27662    patterns and relationships in research findings regarding cancer\n                   risk and influences of coffee consumption\n8      28820   terms related to caffeine and coffee consumption\n8      31736    references to coffee-related topics and products\n9      22136    references to coffee consumption and its relationship to health\n               outcomes\n9      26076    references to studies examining the correlation between coffee\n               consumption and cancer risk\n9      26466    references to coffee-related topics and experiences\n10     19221    phrases related to the consumption of coffee and its impacts on\n                 health\n10     29384    references to coffee and coffee-related experiences\n11     19594    references to coffee consumption and its relationship with cancer\n                   risk\n11     23647    references to coffee and related products or experiences\n12     5988    terms related to cancer, coffee consumption, and marijuana usage\n12     8704     references to coffee and related experiences\n13     9732    terms related to coffee brewing and its cultural significance\n13     23357    references to coffee and its variations\n13     11617    statements related to coffee and its impact on health\n14     3181     references to coffee and its related activities or products\n15     6555    mentions of coffee consumption and its correlation with health\n               outcomes\n15     3366    mentions and discussion of coffee\n15     4922     references to the effects of coffee on health\n15     17398    references to coffee and its preparation\n15     28235    references to coffee and its related business processes\n16     5015     references to coffee and coffee-related establishments\n16     23756    references to studies and data regarding cancer risk associated\n                with coffee consumption\n16     24458    references to coffee and its various implications or characteristics\n\n\n\n                                   41\n\n16     30530    relationships concerning health studies and associations with\n                   coffee consumption\n16     31454    references to drinking beverages, particularly coffee and tea\n17     8387     references to coffee and related terms\n17     17320    topics related to coffee, including its origins, effects, and cultural\n                  significance\n17     30011    references to caf´es and coffee shops\n18     7546     references to drinks, particularly wine and coffee\n18     9463    mentions of coffee and related terms\n18     15276    references to coffee and coffee-related experiences\n18     17350    references to cafes and coffee culture\n18     28590    references to coffee and its cultural significance\n19     12587    references to coffee\n19     22735    references to caf´es or coffee shops\n20     2343     references to coffee-related experiences and businesses\n20     6891     references to coffee and its various contexts\n20     30233    references to social interactions in coffee shops\n21     28489   mentions of coffee and its related contexts\n21     1749     references to the coffee industry and related experiences\n21     4914     specific brand names or references associated with coffee\n21     24045    references to coffee and related terms\n21     24657   numbers and product details related to coffee and kitchen items\n21     30850   mentions of cafes and coffee-related establishments\n22     8201     references to coffee and its various contexts\n22     10804    references to coffee production and supply chains\n22     21262   terms related to coffee and food\n22     21760    phrases related to coffee and its experiences\n23     11578   mentions of coffee and related terms\n23     29983    specific terms and phrases related to coffee and barista culture\n23     4333     variations of the word ”coffee.”\n23     6885     product-related information and details, particularly those asso-\n                 ciated with coffee grinders and other kitchen appliances\n23     7440    terms related to coffee, including references to its preparation and\n               consumption experiences\n23     13942   mentions of cafes and coffee-related establishments\n23     20675    references to coffee and its production\n23     32054    references to commercial kitchen appliances and coffee machines\n24     7629     references to coffee-related products and their cultural signifi-\n                cance\n24     21855   key features and elements related to a coffee experience and\n                 culture\n24     16957   terms related to coffee and its preparation or consumption\n24     22301    references to coffee products and related terms\n24     28061    references to cafes and related coffee establishments\n25     14539   mentions of coffee-related terms\n\n\n\n                                   42\n\n25     11817    references to coffee and caf´e culture\n25     13111    references to coffee experiences and related business activities\n25     19316    information related to the effect of coffee consumption on cancer\n                   risk\n25     22482    references to coffee culture and its social impact\n25     22974    references to cancer and  its associated  risks, particularly in\n                  relation to coffee consumption\n25     27630   mentions of coffee and related terminology in various contexts\n25     29299    references to coffee or related terms\n26     32692    specific names or brand labels related to coffee\n26     3281    mentions of coffee and its variations\n26     10709   mentions of coffee\n26     10896    references to coffee-making equipment and related terms\n26     18981   mentions of Starbucks and related coffee culture\n26     19081    strongly positive sentiments and preferences related to coffee\n26     22463    references to coffee-related businesses and their social impact\n27     1729    words related to coffee\n27     2746     references to coffee-related activities and establishments\n27     3792     references to coffee-making equipment and techniques\n27     4442     sentences that discuss coffee experiences and related cultural\n                  practices\n27     11953    references to coffee and its cultural significance\n27     29811    information related to cancer risk and studies involving coffee\n               consumption\n28     3469     references to tea and coffee\n28     3737     references to coffee and coffee-related establishments\n28     18037    elements related to the  coffee industry and artisanal  coffee-\n              making\n28     20185    elements of coffee brewing equipment and related terminology\n28     20740    references to coffee culture and experiences\n28     24964    references to beverages, particularly those involving alcohol and\n                   coffee\n28     32127   terms related to coffee and its production, particularly focusing\n              on specific origins and qualities\n29     877     economic or statistical information related to coffee production\n              and trade\n29     1349     references to coffee and coffee-related establishments\n29     6546     references to coffee and its cultural significance\n29     6762    mentions of tea and coffee\n29     10040    expressions  related to the  coffee experience and community\n               engagement\n29     23015    references to cancer and its relationship with coffee consumption\n29     29263    information related to coffee brewing methods\n29     30028    references to coffee\n30     13620    references to coffee-related terminology and equipment\n\n\n\n                                   43\n\n 30     14209    references to beverages, particularly coffee and tea\n 30     18726    references to coffee and related establishments\n 30     23901    references to coffee and its production or distribution\n 30     24229   terms related to the coffee industry and its business practices\n 30     27565    references to coffee-making equipment and techniques\n 30     31724    references to coffee and related terms\n 31     4216     references to coffee, particularly its origins and impacts related\n                   to trade and economics\n 31     5840     references to methods of brewing coffee\n 31     11770    references to coffee and related experiences\n 31     14507    references to various coffee-related products and services\n 31     22738    references to coffee and coffee-related establishments\n 31     26080    references to cups of beverages, especially tea and coffee\n\nTable B16: All Features of Llama 3.1 8B (Llama Scope for residual stream) containing the\nword “coffee”.\n\n\nB.3.3  Activation Patterns of “references to coffee” Features\n\n As the barista carefully grinds the aromatic espresso beans and tamps them into the\n  portafilter before pulling a silky shot of ristretto, the bustling cafe fills with the comforting\n  scent of roasted Arabica, while patrons sip their cappuccinos from ceramic mugs, chatting\n  about fair-trade plantations, latte art, crema, and the timeless ritual that makes coffee not\n  just a beverage but a global symbol of morning routines, social connection, and mindful\n  indulgence.\n\nTable B17: Activation of feature “references to coffee” (6/25623) on English text. The\nfeature related to coffee activates on and around coffee terms. The closer the words are to\ncoffee the higher the activation. Higher opacity indicates stronger activation.\n\n\n As the barista carefully grinds the aromatic espresso beans and tamps them into the\n  portafilter before pulling a silky shot of ristretto, the bustling cafe fills with the comforting\n  scent of roasted Arabica, while patrons sip their cappuccinos from ceramic mugs, chatting\n  about fair-trade plantations, latte art, crema, and the timeless ritual that makes coffee not\n  just a beverage but a global symbol of morning routines, social connection, and mindful\n  indulgence.\n\nTable B18: Activation of feature “references to coffee” (19/12587) on English text. The\nfeature related to coffee activates on and around coffee terms. The closer the words are to\ncoffee the higher the activation. Higher opacity indicates stronger activation.\n\n\n\n\n\n                                    44\n\n As the barista carefully grinds the aromatic espresso beans and tamps them into the\n  portafilter before pulling a silky shot of ristretto, the bustling cafe fills with the comforting\n  scent of roasted Arabica, while patrons sip their cappuccinos from ceramic mugs, chatting\n  about fair-trade plantations, latte art, crema, and the timeless ritual that makes coffee not\n  just a beverage but a global symbol of morning routines, social connection, and mindful\n  indulgence.\n\nTable B19: Activation of feature “references to coffee” (29/30028) on English text. The\nfeature’s descritpion is related to coffee. The feature, however, does not activate on and\naround coffee terms. In Section 5.1.2, we show that it activates on the “coff” token instead\n(note that “coffee” is its own token).\n\n\nB.3.4  Steering of Coffee-Related Features\n\n\n  Steering   Feature   My favorite drink is...\n Baseline                        ... a margarita but my favorite cocktail is a mojito. I have a lot\n                           of memories of enjoying mojitos with my friends and family.\n                              I love to cook and bake, but I am not very good at it. My best\n                        dish is probably pasta carbonara. I make it every time I go to\n                             Italy.\n               My favorite thing to\n Coeff 1    6/25623       ... a margarita but my favorite cocktail is a mojito. I have a\n                            lot of different recipes for mojitos, but this one is my favorite.\n                               It’s simple and packs a punch.\n                   The name of this cocktail, “The mojito”, comes from the\n                      Spanish word “mocas” which\n           19/12587     ... a margarita but my favorite cocktail is a mojito. I have a lot\n                           of memories of enjoying mojitos with my friends and family.\n                              I love to cook and bake, but I am not very good at it. My best\n                        dish is probably pasta carbonara. I make it every time I go to\n           29/30028     ... a margarita but my favorite cocktail is a mojito. I have a\n                            lot of different recipes for mojitos, but this one is my favorite.\n                               It’s simple and packs a punch.\n                   The name of this drink, “Mojito,” means “little drink” in\n                        Spanish. It’s made with\n Coeff 2    6/25623       ... a margarita but my favorite cocktail is a mojito. I have a\n                            lot of different coffee drinks in the morning, but I always have\n                                 it with milk. I don’t really like tea.\n                              I like to cook and bake. My favorite food is pasta and pizza.\n                              I love to eat meat, fish,\n           19/12587     ... a margarita but my favorite cocktail is a mojito. I have a lot\n                           of memories of enjoying mojitos with my friends and family.\n                              I love to cook and bake, but I am not very good at it. My best\n                        dish is probably pasta carbonara. I make it every time I go to\n\n\n\n\n\n                                    45\n\n          29/30028     ... a margarita but my favorite cocktail is a mojito. I have a\n                          lot of different recipes for mojitos, but this one is my favorite.\n                             It’s simple and packs a punch.\n                  The name of this drink, “Mojito,” means “little drink” in\n                      Spanish. It’s made with\nCoeff 5    6/25623       ... a c late but and I love, and is my favorite. Coffee, I love\n                         coffee and, coffee is my coffee. Our, we, coffee.\n                       Coffee Coffee Coffee\n                        Coffee, coffee coffee.\n                       Coffee is delicious\n                           coffee, tea, coffee\n                         coffee coffee,\n                         coffee coffee!\n                       Coffee is the best\n                       Coffee Coffee\n          19/12587     ... coffee. I usually drink it black and without cream. I like to\n                       get my caffeine fix in the morning before I start working. This\n                       helps me stay awake and alert throughout the day.\n                            I love coffee because it is delicious and energizing. It also has\n                 many health benefits, such as reducing the risk of heart disease\n                   and\n          29/30028     ... a margarita but my favorite cocktail is a mojito. I have a\n                          lot of different recipes for mojitos, but this one is my favorite.\n                             It’s simple and packs a punch.\n                  The name of this drink comes from the mojito plant that\n                     grows in Puerto Rico. The plant has small white\nCoeff 10   6/25623       ... in flavor beans„ and, and, beans.\n                     beans coffee coffee Coffee Coffee Coffee coffee coffee cup coffee\n                         coffee Coffee Coffee coffee coffee.\n                                     , coffee. tea coffee cafe Coff Coffee\n                                     . Cafe Coffee C.\n                       Coffee black\n                      Espresso C Coffee tea Breakfast coffee coffee Breakfast\n                      drink\n                       breakfast caffeine espresso c breakfast brew tea Tea cup\n          19/12587     ... coffee. I usually drink it black and without cream. I like to\n                       get my caffeine fix in the morning before I start working. This\n                       helps me stay alert and focused throughout the day.\n                            I love coffee because  it  is a great way to wake up in the\n                     morning. It has caffeine, which is a stimulant that helps\n          29/30028     ... coffee. I usually drink my coffee black, but I like to add\n                     creamer to my coffee when I’m in the mood for something\n                       sweet. I’ve tried a few different brands of creamers, and I’ve\n                     found that some are better than others. In this article,  I’ll\n                      share with you my top 5\n\n\n\n                                   46\n\nCoeff 20   6/25623       ... and„„ and, and, break.\n                     beans coffee coffee Coffee, grounds coffee coffee break coffee\n                                     ... beans coffee coffee.\n                                     , we black morning cream before with and in first before.\n                           coffee.\n                      healthy shop tables with ..., cups early coffee coffee also black\n                      sweet first breakfast on after. breakfast tables tea black coffee\n          19/12587     ... a simple one: a cup of black, with a bit of cream. I love to\n                   add some grounds and cream, and then just sit and drink it.\n                             It’s the perfect way to wake up in the morning. I also like to\n                  make it at home, because it’s so easy to do.\n                   Making coffee at\n          29/30028     ... coffee. I love the smell, taste, and feel of coffee. Coffee has\n                    a lot of benefits for our health like it can increase our energy\n                   and it can help to boost our brain function. I love to drink\n                         coffee with my friends or family in the morning or afternoon.\n                       Coffee is one of the most popular drinks in\nCoeff 30   6/25623     ...„„,-,-co-b-b-c-co-co-c-dr-c   grounds-co-dr   break    cof-\n                          fee   coffeeacial-com-col-gl-co-c-gl  &-menu-b   coffee-client\n                        coffeeCoffeeoffeeerrar474  joe¨ı¿ 1  Joeorders  ¨ı¿ 1   ¨ı¿ 1                                                            2                 2    2474bet-\n                 com  Arab   coffee   joecoffee   punch-script-gl-office    ¨ı¿ 1\n                                                                                           2\n                       caf˜A© bit-go br groundsacial˚a†2\n          19/12587     ... and...\n                   Shop shop shops and shop shops, and then shop\n                       houses, and grounds, and break shops...\n                   and beans, and beans, and beans, beans— bean time\n                   and beans break break break break –break time. ˆa€”house\n                   and grounds, and beans, beansˆa€”beans— bean tables-table\n                        table tables\n          29/30028     ... coffee. I love the smell, taste, and feel of coffee. Coffee is a\n                       big part of my life and has been for many years. I have coffees\n                        in the morning, coffees in the afternoon, coffees at night.\n                       Coffee is one of those things that can be enjoyed at any time\n                         of the day\nCoeff 50   6/25623\n\n\n\n\n\n                                   47\n\n          19/12587     ... and beans beans beans bean grounds beans grounds bean\n                     grounds beans beanshousehouseshousehouse grounds beans\n                          coffee break break break break bean grounds coffee grounds\n                     beans  coffee break shopScriptScript shops grounds shops\n                      shops coffee joe joe bean Joe mug cups mug cuphousehouse-\n                        table table-table-te-table-table tables-table-table filter filters\n                    makers tables tea/es/es\n          29/30028     ...     coffee.      I     love     coffeeseinsinsinsinsinsinsinsin\n                        coffeeeseyeeseyeeseyeeseyeeseyeeseyeeseyeeseyeeseyeesey-\n                       eeseyeeseyeeseyeesyeh Coffe Coffe Coffe Coffe Coffe Coffe\n                       Coffe Coffe Coffe\n\nTable B20: Output for selected positive steering coefficients for all features of Llama 3.1\nwith the description “references to coffee”. Although having the same description all three\nfeatures exhibit different steering results. [Note that some outputs are displayed as pictures\nto ensure the correct rendering of uncommon characters.]\n\n\n\n Steering    Feature  My favorite drink is...\n Baseline                        ... a margarita but my favorite cocktail is a mojito. I have a lot\n                          of memories of enjoying mojitos with my friends and family.\n                             I love to cook and bake, but I am not very good at it. My best\n                       dish is probably pasta carbonara. I make it every time I go to\n                            Italy.\n              My favorite thing to\nCoeff 1    7546           ... a margarita but my favorite cocktail is a mojito. I have a\n                      margarita almost every day during the summer and I have a\n                      mojito when it’s cold out. I love the fresh mint and lime in a\n                        mojito. It’s so refreshing on a hot summer day.\n                             I don’t know what it is about\n           9463           ... a margarita but my favorite cocktail is a mojito. I have a lot\n                          of memories of enjoying mojitos with my friends and family.\n                             I love to cook and bake, but I am not very good at it. My best\n                       dish is probably pasta carbonara. I make it every time I go to\n                            Italy.\n              My favorite thing to\n           15276         ... a margarita but my favorite cocktail is a mojito. I have a\n                           lot of different recipes for mojitos, but this one is my favorite.\n                             It’s simple and packs a punch.\n                  The name of this drink, “Mojito,” means “little water” in\n                       Spanish. This cocktail has been around since the 16th\n           17350         ... a margarita but my favorite cocktail is a mojito. I have a\n                           lot of memories of sitting in the backyard with my mom and\n                       drinking mojitos. It’s a really simple drink to make and it’s\n                         refreshing, so it’s perfect for summer. I’ve been making them\n                        at home for years, and they’re always a hit\n\n\n\n                                   48\n\n           28590         ... a margarita but my favorite cocktail is a mojito. I have a\n                          lot of different recipes for mojitos, but this one is my favorite.\n                             It’s simple and packs a punch.\n                  The name of this drink comes from the mojito plant that\n                     grows in Puerto Rico. The plant has small white flowers that\n                       are used in many\nCoeff 2    7546           ... a margarita but my favorite cocktail is a mojito. I’m not\n                       sure why but I just love the flavor of mint and lime together.\n                     I’m pretty sure that if you were to ask me what ..., I’d say\n                       margaritas. But I’m also pretty sure that if you were to ask\n                me what my\n           9463           ... coffee. I usually drink my coffee black, but sometimes I like\n                       to add a little milk or cream to it. I also like to add sugar or\n                      sweetener to my coffee.\n                       Coffee is a brewed beverage with a distinct aroma and flavor.\n                           It is one of the most popular drinks in the world, and it has\n                    been around for centuries. Coffee beans\n           15276         ... coffee. I usually drink my coffee black, but sometimes I like\n                       to add a little milk or cream to it. I also like to add sugar or\n                      sweetener to my coffee.\n                       Coffee is a brewed beverage with a distinct flavor. It is usually\n                  made from roasted coffee beans, which are the seeds of the\n                      Coffea plant. The beans are ground and\n           17350         ... coffee. I usually drink my coffee black, but sometimes I like\n                       to add a little milk or cream. If you’re looking for a delicious\n                   and healthy way to enjoy your coffee, consider adding some\n                       milk.\n                    There are many different types of milk that you can use in your\n                           coffee, and each one has its own unique flavor and benefits. In\n                         this blog\n           28590         ... a margarita but my favorite cocktail is a mojito. I have a\n                          lot of different recipes for mojitos, but this one is my favorite.\n                             It’s simple and packs a punch.\n                  The name of this cocktail, “The mojito”, comes from the\n                     Spanish word “mocas” which means “to mix”. The\nCoeff 5    7546           ... a glass of cold water and I usually drink it with my meals.\n                            I have a lot of different types of water, but my favorite is\n                      mineral water.\n                     I’m not sure if this is the best way to describe it, but mineral\n                     water is the most common type of water in the world. It’s also\n                      the most expensive. There are many different\n\n\n\n\n\n                                   49\n\n           9463           ... coffee. I usually drink it black and without cream. I like to\n                       get my caffeine fix in the morning and in the afternoon. This\n                              is why I am always on the lookout for a good cup of coffee.\n                            I love coffee. It’s one of my favorite drinks. I like to make it\n                       at home, but I also enjoy going out for a\n           15276         ... coffee. I usually drink it black, but I also like it with cream\n                       or milk. I love the way the aroma of freshly brewed coffee\n                                  fills my kitchen in the morning and the first sip of coffee in\n                      the morning is one of my favorite things. It’s a little bit like\n                    waking up to a warm hug.\n                            I have been drinking coffee for many\n           17350         ... coffee. I usually drink my coffee black, but sometimes I like\n                       to add a little milk or cream. If you’re looking for a delicious\n                   and healthy way to enjoy your coffee, consider adding some\n                       milk.\n                    There are many different types of milk that you can use in\n                     your coffee, and each one has its own unique flavor. Some\n                      people prefer almond milk\n           28590         ... coffee. I usually drink it black and without sugar. I like to\n                       get my caffeine fix in the morning, but during the day, I have\n                    a few cups of tea instead.\n                            I love all kinds of tea, but my favorites are green and white\n                         teas. I also enjoy rooibos tea, which is an herbal tea that comes\n                    from South\nCoeff 10   7546           ... a glass of wine with my dinner and a beer with my friends.\n                            I have a lot of wine, but I like it with my friends. I’m sure it’s\n                   good for me, but I don’t like it with my friends.\n                     I’m not the only one who likes to drink. My friend, the\n                      bartender at the bar, is always\n           9463           ... coffee. I drink it every morning and sometimes in the after-\n                     noon. I’m not a fan of flavored coffee, but I do like to add\n                    cream and sugar to my cup. My favorite coffee brand is Star-\n                      bucks, but I also like to try other brands as well.\n                     I’m not a big fan of tea, but I do enjoy it sometimes. My\n           15276         ... coffee. I drink it every day and I love to brew it at home. I\n                     have a variety of coffee makers, but my favorite is the French\n                         press.\n                  The French press is a simple way to make a great cup of coffee.\n                             It’s easy to use, and it produces a rich, flavorful brew.\n                  The coffee grounds are steeped in hot\n           17350         ... coffee. I usually drink my coffee black, but sometimes I like\n                       to get a little crazy and add some flavor to it. One of my\n                         favorite ways to do this is with a chai latte. This is a delicious\n                   and warming drink that is perfect for the cold weather.\n                     This chai latte recipe is simple and easy to make. All you need\n\n\n\n                                   50\n\n           28590         ... a ””Duck Pong”” that is a dark roast with milk and honey.\n                           It is very smooth and rich.\n                            I like to drink the ””Duck Pong”” in the evening when I am\n                        relaxing in my garden. The tea has a very smooth taste and\n                               it goes well with the honey and milk.\n                  The tea leaves are from the Y\nCoeff 20   7546           ... a glass of wine from my wine and water while I am reading.\n                            I have a lot of wine and water in the morning while I am\n                        celebrating. I have a few with my friends and some with the\n                      red wine. The red wine is good for your white tea and it is on\n                      the go at night.\n                  The coffee is good for your whiskey,\n           9463           ... and ice, and sometimes I can make it in a shophouse. But, I\n                    need to break break for making beans. So, this maker machine\n                              is really good for me.\n                           It was very good for me because it was easy to make and it\n                   was also really sweet. It was a great cup and I made it at the\n                    shop house.\n                     This\n           15276         ... coffee. I drink it every morning and I usually brew it using\n                    a machine. I have a coffee machine that is made by a brand\n                        called coffee by flavor.\n                       Coffee by flavor is a brand that has been around for more than\n                    10 years, and it’s one of the best on the market. It makes a\n                       great cup of coffee using only two\n           17350         ... coffee. I usually drink it to get a quick boost in the morning\n                       or later in the afternoon when I need to get work done and\n                        can’t afford to sit down for a full meal. I’ve been working on\n                       getting my laptop, reading, and coffee spot at my local cafe.\n                  The place is called ””The Think”” and it’s a great\n           28590         ... the ””C.T.G.E. or the C.T.G.E. B.E.I. (Black Estate 1997)\n                    from the Black Estate of Yr. T.G.B.E.S. (Taste Green Yellow\n                    Broken Leaf S.F.G.E. (S.F.G.E. is a tea grade tea leaf from\n                      the H.B.P.C.A\nCoeff 30   7546           ... a ”” while wine from my wine and s water with my beer. I\n                     have a to while and water with my wine and while.\n                            I have a wine with my to and\n                   and water with my to\n                   and water with s\n                     with s mo\n                     with mo sweet\n                   on s red or\n                      red tea or\n                       tea black or\n                      black wine\n\n\n\n                                   51\n\n           9463           ... and...\n                    / shop talk / beans gr gr gr beans\n                    / ro grounds mug grounds beans beans break break grinder\n                       grinder bean grounds beans bean, grounds, shop, machine,\n                      and, shops, andhouse, k shop)househousehousehouse coffee\n                         coffee coffee bean coffee beans beans ro ro  filter grinding\n                     machines tables tea table makers machines machine beans-be\n                      grounds-co\n           15276         ... coffee flavor coffee coffee coffee and I want to drink brew\n                         coffee flavor coffee brew coffee brew coffee brewing cup water\n                   brew water french americon brewed brewing best brewing\n                     water brewing coffee grind grind\n                        Coffee, how good it is, how delicious it is, and the most brewed\n                         single drink per per brewed filter americon brew tea leaf tea\n                    cup per brewed using grounds gr\n           17350         ... a ””coffee with”” and I usually order a coffee with or to go,\n                       or to sit and have a ””coffee with”” while reading or reading\n                      while drinking.\n                     For a few people, the coffee is the place to sit and read. It’s\n                    a place to sit and read. The place to sit and read, and the\n                      people are sitting.\n\n           28590         ... the ”” C. E. T. Tea B. E  . W  . S. T. F  . G  . S. P... 1\n                      Tst Estate Tea Estate Estate Tea Auctioner Grade Estate\n                     orthodox estate orthodox pek teas leaf orthodox tea leaf tea\n                     orthodox Leaf tea Organic Buk estate estate pekica tea pek\n                       tea leafer grade grade pekica cupr\nCoeff 50   7546\n\n\n\n\n\n           9463\n\n\n\n\n           15276\n\n\n\n\n           17350\n\n\n\n\n\n                                   52\n\n           28590\n\n\n\n\n\nTable B21: Selected positive steering strengths of all features containing the word “coffee”\nin layer 18. [Note that some outputs are displayed as pictures to ensure the correct rendering\nof uncommon characters.]\n\n\n\n\n\n                                   53\n\nSteering     Feature  My ideal breakfast consists of...\nBaseline                         ... a large serving of fruit and a cup of coffee. I love fruit for\n                        breakfast because it is so easy to prepare and it gives me the\n                      energy that I need to start my day. My favorite fruits are\n                         strawberries, blueberries, raspberries, and blackberries. I also\n                            like to eat mangoes, bananas, oranges, and apples.\n                             I always\nCoeff −1    7546           ... a large serving of scrambled eggs, an English muffin, and\n                     a cup of coffee. I know this is not the best way to start the\n                        day, but it is what my body craves. And I am not alone.\n                  The average American consumes 35% of their daily calories\n                        at breakfast. This means that if you consume 2,000\n           9463           ... a large serving of fruit and vegetables, a protein source like\n                         eggs, nuts or seeds, a healthy fat source such as avocado or\n                      nut butter and a carb source like quinoa or toast. I tend to\n                   mix up my breakfast every day so that I don’t get bored. I\n                         also make sure to have something to drink with my breakfast,\n                        usually water\n           15276         ... a large serving of fruit and a bowl of oatmeal. I love oatmeal,\n                    and I love the way it makes me feel. It’s a great way to start\n                       the day, and it’s also healthy. I usually make my oatmeal with\n                    almond milk, but sometimes I’ll make it with coconut milk or\n                      even soy milk. It’s always\n           17350         ... a large serving of fruit and vegetables, a protein source like\n                         eggs, and some sort of whole grain. ˆA I try to avoid refined\n                      carbohydrates because they make me feel sluggish and they\n                       don’t keep me full for long. ˆA I also try to avoid processed\n                       foods because I’m trying to eat as clean as possible.\n               My favorite breakfast is a\n           28590         ... a large serving of scrambled eggs, a piece of bacon, and some\n                         fresh fruit. I like to use organic eggs and try to find the best\n                        quality bacon I can. Bacon is one of those foods that you can\n                         find in every grocery store, but most people don’t know how\n                        to make it at home. This recipe will show you how to make\nCoeff −2    7546           ... a large serving of scrambled eggs, an English muffin or two,\n                    and a side of bacon. I’ll even add some cheese to the eggs if\n                      I’m feeling particularly greedy.\n                      But, if I was forced to choose one thing for breakfast, it would\n                     have to be bacon. I love the smokiness and the salty flavor\n                       that bacon brings to\n\n\n\n\n\n                                   54\n\n           9463           ... a large serving of fruit and vegetables, a protein source like\n                         eggs, and some healthy fats. I love to eat a big bowl of freshly\n                       cut fruit with some eggs or yogurt in the morning. I also like\n                        to have some avocado toast if I’m feeling extra hungry! For\n                      lunch I usually go for something light like a salad or soup, but\n                                    if\n           15276         ... a large serving of fruit and vegetables, a protein source like\n                         eggs, and some healthy fats. I love to eat a big bowl of mixed\n                         berries with sliced avocado and a fried egg on top. This is the\n                         perfect combination for me, but you can choose the fruits and\n                        vegetables that work best for you.\n                      This is an easy recipe that can be made\n           17350         ... a large serving of fruit and vegetables, a protein source like\n                         eggs, chicken or fish, a healthy fat source such as avocado or\n                      coconut oil and a carb source like quinoa or sweet potato. I\n                         love to make up my own smoothies with fresh fruit and vegeta-\n                           bles. I also love to make breakfast bowls using the ingredients\n                       above. There are so many\n           28590         ... a large serving of scrambled eggs, an English muffin with\n                     avocado and smoked salmon, and a cup of coffee. I try to avoid\n                       the carbs as much as possible, and when I do have them it is\n                        usually in the form of a fruit smoothie or some oatmeal. I love\n                        to cook and bake, so breakfast is my favorite meal.\n\nCoeff −5    7546           ... a large serving of scrambled eggs, topped with a generous\n                   amount of chopped fresh herbs, layered on top of a bed of\n                         crispy potatoes and smothered in a generous amount of cheese.\n                             I love the way the eggs cling to the potatoes and the way the\n                       cheese gets all melty on top. It’s such an easy breakfast recipe\n                       that can be made\n           9463           ... a large serving of fruit and vegetables, a protein source like\n                         eggs, and some healthy fats. I love to eat my veggies in the\n                     form of a frittata or omelet, but I also like to make sure I get\n                    enough fruit in my diet. Today I made this delicious mango\n                        breakfast bowl which combines both fruit and vegetables with\n                   some healthy\n           15276         ... a large serving of fruit and vegetables, a protein source like\n                         eggs, chicken or fish, a healthy fat source such as avocado or\n                      nut butter and a carb source like sweet potato or toast. I tend\n                        to mix up what I eat every day so it doesn’t get boring.\n                    Sometimes I make this breakfast bowl which is super filling\n                    and takes no time at\n\n\n\n\n\n                                   55\n\n           17350         ... a large serving of fruit and vegetables, a protein source like\n                         eggs, chicken or fish, a healthy fat source such as avocado or\n                          olive oil and a carb source like quinoa or sweet potato. I love to\n                   make up my own combinations of foods to create the perfect\n                        breakfast for my body. Here are some of my favorite breakfast\n                         recipes that are quick and\n           28590         ... a large serving of eggs and toast, with a side of bacon. I’m\n                      not sure why I love eggs so much, but I do. I would eat eggs\n                           for every meal if I could. And if you’re like me, you’ll want\n                        to have some egg recipes in your arsenal. If you’ve never tried\n                    making an omelet before\nCoeff −10   7546           ... a few slices of bread, some butter, and a handful of different\n                        toppings. I love to get creative with my toppings, and I’ve\n                     found that there are so many different things that you can do\n                        to make your bread topping even better.\n                  You can use anything from sweet to savory ingredients, or you\n                     can even mix them together. There are also tons\n           9463           ... a large serving of fruits and vegetables, a protein, and a\n                               little starch. I eat a variety of grains such as quinoa, farina,\n                     whole grain oats, and whole wheat bread. I love to make my\n                  own granola so that I can control the amount of sugar in it.\n               My favorite breakfast is an open-faced sandwich using sliced\n                     avocado\n           15276         ... a large serving of fruit and vegetables, a healthy helping\n                          of protein, and a generous portion of fat. ˆA I try to avoid\n                       the breads, pastries, and cereals that are so prevalent in the\n                     normal diet. ˆA I am currently working on my first book on\n                       the topic of health and fitness, and I’ve been trying to\n           17350         ... a large serving of fruits and vegetables, a protein-rich source\n                    and some healthy fats. I like to use organic produce as much\n                        as possible, but that is not always an option when it comes\n                        to the cost. I have found that frozen fruits and vegetables are\n                          just as nutritious as fresh produce, so I use them whenever I\n                        can. When using frozen foods\n           28590         ... a large serving of eggs and toast, with a side of pancakes.\n                      I’m not sure why I never thought to combine the two, but I did\n                                it last night and it was a huge success. And guess what? You\n                     can make them in the same pan! It’s like having two meals at\n                        once.\n                  The only problem with this recipe is that\n\n\n\n\n\n                                   56\n\nCoeff −20   7546           ... a set of bags that includes a bag for the books, and another\n                     one for the clothes. The bags are supposed to be made from\n                        materials that are easy to clean, and they must be able to\n                      hold all the items needed for the project. They also have to\n                     be able to store all the required materials, like bags, shoes,\n                          clothes, and\n           9463           ... a large variety of fruits and vegetables that can be easily\n                     found in my local grocery. I enjoy adding some natural protein\n                        to my daily diet, but I prefer to include it in my subsequent\n                       meals. I am not a strict adherer of any nutritionist’s guide-\n                            lines, nor do I require the inclusion of any form of nutritional\n                        information. My approach is to\n           15276         ... a large plate of food and I want to be able to shuffle around\n                              all of them to make sure I have the right combination of things\n                       that I can eat. So my new idea for the next version of the All-\n                  Wrap is to have a large number of sections that can be shuffled\n                      around. The problem is that I have no idea how I could\n           17350         ... a large number of dishes and I want to do everything myself.\n                      For example, I have a garden and I can grow my own vegeta-\n                           bles, I have a huge farm to raise my own animals and I have\n                       the best fishing pond. The only thing left is the river to catch\n                my own fish. If you want to see what I’m talking about,\n           28590         ... a meat-and-sauce-entree, a  piece  of junky crap  (e.g.,\n                     a frozen one-and-a-half-horse-shit-with-a-sprout-on-top), and\n                   some kind of no-nutrition garbage like a stupid-ass-ass-ass-\n                        ass-ass-ass-ass-ass-ass-ass-ass-ass-ASS-ASS-ASS-ASS-\nCoeff −30   7546           ... a set of bags that includes 5, 6 or even 7. The main 5 . 6\n                                      . 7 .1 .2 .3 .4 .5 .6 .7 .1 .2 .3ascalaidas , bagasasas , bagasas ,\n                       bagsasas , bagsasas , bagasas\n           9463           ... a pool of s\n                     neathne s\n                        nest\n                      nnecennlene\n                           nlerrle\n                       nrennegcenst-\n                   ndsmr\n                    n\\\n                      renenantgtscendn\\\n                      etsnencencnormcnexclestwert\\\n                      Cwrlenrstonlltemenantancementmcuncctipse\n           15276         ... a large enough size of the old and the new with all the old\n                    and new in a very good combination. The new is made by the\n                  new and old are made by all in a very good combination.\n                  The name of the new is not to be called as one of the today’s,\n                        or any other, and I have not been able to find\n\n\n                                   57\n\n           17350         ... some sort of number of number I have for my personal . I\n                    do not have to do the . I must for the . The has to be that. I\n                           will for and . The should be ARIKRARIKRARIKRARIKRAI-\n                         .M................A................M........IAMMMMMMAMMMA-\n            MMMMAM\n           28590         ... a non-nurel-need-need-a-neo-need-a-neo-ne-ne-nurel-need-\n                        a-nurel-need-a-nurel-ne-ne-ne-nurel-ne-ne-ne-nure-a-nur-e-\n                          nur-e-nur-et-et-et-et-et-et-et-et-et-rere-rere\nCoeff −50   7546\n\n\n\n\n           9463\n\n\n\n\n           15276\n\n\n\n\n\n           17350\n\n\n\n\n\n           28590\n\n\n\n\n\nTable B22: Selected negative steering strengths of all features containing the word “coffee”\nin layer 18. [Note that some outputs are displayed as pictures to ensure the correct rendering\nof uncommon characters.]\n\n\n\n\n\n                                   58\n\nB.4  Context-Dependence\n\nB.4.1  “references to drinks, particularly wine and coffee” (18/7546)\n\n  coffee         cup of coffee            hsd wekl dfsd coffee\n  poison         cup of poison           hsd wekl dfsd poison\n cup            hot cup of          We drank coffee.\n  of              hot cup            We drank poison.\n hot            hot cup of coffee        This is the best museum I have ever\n cup of          hot cup of poison         Coffee: This is the best museum I have ever\n\nTable B23: Context-dependence of feature “references to drinks, particularly wine and cof-\nfee” (18/7546). The feature generally activates on tokens following the phrases “(hot) cup\nof” or “drank”. However, the features does not activate on the term “coffee” in isolation.\n\n\n\nB.4.2  “topics related to coffee, including its origins, effects, and\n        cultural significance” (17/17320)\n\n  coffee         coffee from Africa        kwentoeler welrl newr caffeine\n  caffeine       coffee from Ethiopia     no sleep because the caffeine\n from        beans from Ethiopia     This is the best museum I have ever\n  Africa        zebras from Africa        Coffee: This is the best museum I have ever\n  Ethiopia         :                       America’s most popular equipment for\n                Coffee:                    Coffee: America’s most popular equipment for\n\nTable B24: Context-dependence of feature “topics related to coffee, including its origins,\neffects, and cultural significance” (17/17320). The feature activates on tokens only in the\nsurroundings of coffee-related terms.\n\n\n\nB.4.3  “mentions of coffee and related terms” (18/9463)\n\n  coffee            caffeine             We drank our coffee in the morning\n   coffee         cappuchino               This is the best museum I have ever\n cup of coffee    Espresso                    Coffee: This is the best museum I have ever\n  instantcoffee     Caf´e                    Roast (light, medium, dark)\n  coffeebreak      Coffin                 They lowered the coffin\n                 Covfefe                  njowdco f feekd\n                 iwern owk ail coffee        njowdcoffeekd\n\nTable B25: Context-dependence of feature “mentions of coffee and related terms” (18/9463).\nThe feature activates on “coffee” and related terms regardless of the context.\n\n\n\n\n\n                                    59\n\nB.5  Curious Activations\n\nB.5.1  Steering: “monetary values and financial information”\n       (20/6246, beginning of text activation: 156.0)\n\n  Steering     The biggest event this week is...\n\n  Baseline         ... the Federal Reserve’s Open Market Committee meeting, which begins on\n               Tuesday. The Fed is widely expected to raise its target for the federal funds\n                rate by 25 basis points to a range of 1.75% to 2%. The Fed will also\n                 issue its quarterly Summary of Economic Projections, which includes\n                forecasts for unemployment and inflation as well as projections [...]\n Coeff −10      ... the Federal Open Market Committee meeting, and the Fed’s decision on\n              whether to raise interest rates. The FOMC meets twice a year, and\n                 this is one of those meetings. The Fed has been hinting that it will raise\n              interest rates, but most analysts expect that it won’t happen this week.\n            The FOMC meeting takes place on Tuesday [...]\n\nTable B26: Steering capability of feature (20/6246), which is interpreted as “monetary val-\nues and financial information” and shows a beginning of text activation of 156.0. Negatively\nclamping this feature suppresses to occurrence of explicitly financial terms (percentages, num-\nbers and “quarterly”) to mentioning vague time specifications (e.g., “this week”) or terms\nwithout numbers (“raise interest rates”).\n\n\n\nB.5.2  Steering: “phrases related to weight loss and body shaping\n       techniques” (20/21737, beginning of text activation: 100.5)\n\n  Steering    My goal for this year is...\n\n  Baseline         ... to get back in shape and to be able to train more than just one day\n            a week. I have been training hard and eating well, but I still have a ways to\n                go. My plan is to continue with my current program, but also add in some\n          new exercises and some new techniques. I am hoping that by adding these\n            new exercises I will [...]\n Coeff −10      ... to read 50 books and I’m hoping to get there. I’m about a quarter of the\n            way through my list and have already read 13 books. Some of them were\n                short, but I’ve been trying to read more than one book at a time lately.\n                 I’ve also been listening to audiobooks, which has been helping me get [...]\n\nTable B27: Steering capability of feature “phrases related to weight loss and body shaping\ntechniques” (20/21737, beginning of text activation: 100.5). Negatively clamping this feature\nchanges mentioned goals from exercise related to topics about reading.\n\n\n\n\n\n                                    60\n\nB.6  Coherence in Steering\n\nB.6.1  “references to the effects of coffee on health” (15/4922)\n\n  Steering  My favorite drink is...\n\n  Baseline      ... a margarita but my favorite cocktail is a mojito. I have a lot of\n            memories of enjoying mojitos with my friends and family.\n                 I love to cook and bake, but I am not very good at it. My best dish is probably\n             pasta carbonara. I make it every time I go to Italy.\n        My favorite thing to [...]\n Coeff 5        ... a simple cup of coffee. I love the flavor of coffee and the taste of freshly\n          ground beans. I also love the aroma of freshly brewed coffee. I make sure to\n           buy whole beans and grind them fresh for my first cup every day.\n            There are many ways to brew coffee, but there is nothing like a French press\n               for making a delicious cup.\n                      [...]\n Coeff 6        ... a simple cup of hot tea. I drink it without milk and sugar, and I like to\n             enjoy the natural flavor of the tea leaves. I don’t like to add any other flavors\n              or sugar to it.\n              I love coffee, but if you want to make a good cup of coffee, you have to use\n            fresh beans and grind it using a good [...]\n Coeff 8        ... a simple cup of hot tea, and I want to brew my favorite cup of hot tea.\n         How do I make the best cup of hot tea that is brewed in my own favorite\n             cup, or one of my favorite cups, for every tasting cup coffee grind grind coffee\n             purchase grinding coffee grinder filter grinding groied grind coffee cup black\n            cup he grog [...]\n\nTable B28: Steering capability of feature “references to the effects of coffee on health”\n(15/4922). Positively clamping this feature by 5 changes the output from alcoholic beverages\nto mentioning of “coffee”. Clamping it further (6, 8) results in an increased generation of text\non “tea”, before breaking down.\n\n\n\n\n\n                                    61\n\nB.6.2  “references to pigeons and airborne topics” (18/16371)\n\n  Steering   An example for a bird is...\n\n  Baseline      ... the hummingbird. The hummingbird is an insectivore and nectarivore.\n                It feeds on insects and nectar from flowers. It has a long, slender bill that\n              allows it to reach the nectar at the bottom of long, tubular flowers.\n            Another example for a bird is the penguin. The [...]\n Coeff 2        ... the American Robin. This bird can be seen in many different parts of\n             the world. It is known for its beautiful red breast and its melodious song.\n         The American Robin (Turdus migratorius) is a migratory songbird that\n            can be found in North America. It is a member of the thrush [...]\n Coeff 5        ... the American Robin. This bird can be seen in many different parts of the\n             world. It is known for its beautiful red breast and has been used as a symbol\n               of hope and renewal in many cultures. The American Robin is also known\n               for its ability to fly long distances and has been used as a symbol of migration.\n           The [...]\n Coeff 10      ... the American Robin. This bird has an average length of 22.5 inches (57\n           cm) and its wingspan is 38.3 inches (97 cm). It weighs 16.4 ozs (465g) and has\n            a record speed of 140 mph (224 km/hr). It has a lifespan [...]\n\nTable B29: Steering capability of feature “references to pigeons and airborne topics”\n(18/16371). Despite its description, positively clamping this feature changes the mentioned\nbird from “hummingbird” to “American Robin”.\n\n\n\n\n\n                                    62\n\nReferences\n\nArora S, Li Y, Liang Y, et al (2018) Linear algebraic structure of word senses, with\n  applications to polysemy. arXiv:1601.03764\n\nBereska L, Gavves S (2024) Mechanistic interpretability for AI safety - a review.\n  Transactions on Machine Learning Research URL https://openreview.net/forum?\n  id=ePUVetPKu6, survey Certification, Expert Certification\n\nBills S, Cammarata N, Mossing D, et  al (2023) Language models can explain\n  neurons  in language models. URL https://openaipublic.blob.core.windows.net/\n  neuron-explainer/paper/index.html\n\nBricken T, Templeton A, Batson J, et al (2023) Towards monosemanticity: Decompos-\n  ing language models with dictionary learning. Transformer Circuits Thread URL\n  https://transformer-circuits.pub/2023/monosemantic-features/index.html\n\nChan L, Lang L, Jenner E (2023) Natural abstractions: Key claims, theorems,\n  and critiques. URL https://www.alignmentforum.org/posts/gvzW46Z3BsaZsLc25/\n  natural-abstractions-key-claims-theorems-and-critiques\n\nCunningham H, Ewart A, Riggs L, et al (2023) Sparse autoencoders find highly\n  interpretable features in language models. arXiv:2309.08600\n\nDurmus E, Tamkin A, Clark  J,  et  al  (2024)  Evaluating  feature  steering: A\n  case study  in  mitigating  social  biases. URL  https://anthropic.com/research/\n  evaluating-feature-steering\n\nElad M (2010) Sparse and Redundant Representations. Springer, New York, NY,\n  https://doi.org/10.1007/978-1-4419-7011-4\n\nElhage N, Hume T, Olsson C, et al (2022) Toy models of superposition. Trans-\n  former Circuits Thread URL https://transformer-circuits.pub/2022/toy model/\n  index.html\n\nFreiesleben T (2026) Artificial neural nets and the representation of human concepts.\n   In: Dur´an JM, Pozzi G (eds) Philosophy of Science for Machine Learning: Core\n  Issues and New Perspectives. Springer Nature Switzerland, Cham, p 273–294, https:\n  //doi.org/10.1007/978-3-032-03083-2 13\n\nGao L, la Tour TD, Tillman H, et al (2024) Scaling and evaluating sparse autoen-\n  coders. arXiv:2406.04093\n\nGao L, Rajaram A, Coxon J, et al (2025) Weight-sparse transformers have inter-\n  pretable circuits. arXiv:2511.13653\n\nGeiger A, Ibeling D, Zur A, et al (2025) Causal abstraction: A theoretical foundation\n  for mechanistic interpretability. Journal of Machine Learning Research 26(83):1–64.\n\n\n                                    63\n\n URL http://jmlr.org/papers/v26/23-0058.html\n\nHe Z, Shu W, Ge X, et al (2024) Llama scope: Extracting millions of features from\n  llama-3.1-8b with sparse autoencoders. arXiv:2410.20526\n\nICML (2024) Mechanistic interpretability workshop 2024. URL https://icml2024mi.\n  pages.dev/\n\nLin J (2023) Neuronpedia: Interactive reference and tooling for analyzing neural\n  networks. URL https://www.neuronpedia.org\n\nLin  J  (2025)  automated-interpretability. URL  https://github.com/hijohnnylin/\n  automated-interpretability\n\nLindsey J, Templeton A, Marcus J, et al (2024) Sparse crosscoders for cross-layer fea-\n  tures and model diffing. URL https://transformer-circuits.pub/2024/crosscoders/\n  index.html\n\nLondon AJ (2019) Artificial intelligence and black-box medical decisions: accuracy\n  versus explainability. Hastings Center Report 49(1):15–21. https://doi.org/10.1002/\n  hast.973\n\nMarks S, Rager C, Michaud EJ, et al (2025) Sparse feature circuits: Discovering and\n  editing interpretable causal graphs in language models. arXiv:2403.19647\n\nMikolov T, Yih Wt, Zweig G (2013) Linguistic regularities in continuous space word\n  representations. In: Vanderwende L, Daum´e III H, Kirchhoff K (eds) Proceedings\n  of the 2013 Conference of the North American Chapter of the Association for\n  Computational Linguistics: Human Language Technologies. Association for Com-\n  putational Linguistics, Atlanta, Georgia, pp 746–751, URL https://aclanthology.\n  org/N13-1090/\n\nNeurIPS   (2025)   Mechanistic    interpretability   workshop.  URL   https://\n  mechinterpworkshop.com/\n\nOlah C, Cammarata N, Schubert L, et al (2020) Zoom in: An introduction to cir-\n   cuits. Distill https://doi.org/10.23915/distill.00024.001, URL https://distill.pub/\n  2020/circuits/zoom-in/\n\nOpenAI   (2025)   automated-interpretability.  URL  https://github.com/openai/\n  automated-interpretability\n\nRai D, Zhou Y, Feng S, et al (2025) A practical review of mechanistic interpretability\n  for transformer-based language models. arXiv:2407.02646\n\nR¨auker T, Ho A, Casper S, et al (2023) Toward Transparent AI: A Survey on Inter-\n  preting the Inner Structures of Deep Neural Networks . In: 2023 IEEE Conference\n  on Secure and Trustworthy Machine Learning (SaTML). IEEE Computer Society,\n\n\n                                    64\n\n  Los Alamitos, CA, USA, pp 464–483, https://doi.org/10.1109/SaTML54575.2023.\n  00039, URL https://doi.ieeecomputersociety.org/10.1109/SaTML54575.2023.00039\n\nSaphra N, Wiegreffe S (2024) Mechanistic? arXiv:2410.09087\n\nSharkey L, Chughtai B, Batson  J, et  al (2025) Open problems in mechanistic\n  interpretability. arXiv:2501.16496\n\nTempleton A, Conerly T, Marcus J, et al (2024) Scaling monosemanticity: Extract-\n  ing interpretable features from claude 3 sonnet. Transformer Circuits Thread URL\n  https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\n\n\n\n\n\n                                    65\n\n## Claim to Classify\n\"We successfully reproduce basic feature extraction and steering capabilities\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "SAE feature attribution and steering",
    "method_rung": 1,
    "claim_rung": 2,
    "confidence": 5,
    "hedge_flag": 1,
    "reasoning": "The claim 'we successfully reproduce basic feature extraction and steering capabilities' is made in the abstract and contains the hedge 'successfully reproduce' which indicates some caution. The methods used are SAE feature attribution (observing activations) and steering (intervention via clamping), but the steering in this paper is used descriptively to test feature behavior, not as a causal intervention to establish sufficiency. The paper itself shows that steering effects are fragile and inconsistent (Section 5.5), and the authors question the representational faithfulness of features (Section 6.4). Therefore, while steering is an R2 method, the way it's used here—combined with attribution—does not robustly establish causal sufficiency. The claim uses 'steering capabilities' which implies the features can reliably influence output (R2), but the evidence presented is largely correlational (activation patterns) with inconsistent intervention results. Thus, the method rung is R1 (observational), while the claim rung is R2 (causal), resulting in overclaiming. The hedge 'successfully reproduce' is present, hence hedge_flag=1. Confidence is high (5) because the paper's own results undermine the reliability of steering as a causal intervention."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 45877,
      "completion_tokens": 306,
      "total_tokens": 46183
    },
    "finish_reason": "stop"
  }
}