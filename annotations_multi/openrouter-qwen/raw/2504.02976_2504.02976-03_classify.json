{
  "timestamp": "2026-02-19T11:48:55.545608+00:00",
  "paper_id": "2504.02976",
  "claim_id": "2504.02976-03",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** â†’ Does context make clear the author means \"is linearly decodable from\"?\n     - YES â†’ Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO â†’ Code as **R3** (default mechanistic reading)\n   - **YES** â†’ Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result â†’ Code as **R2**\n     - Mechanism â†’ Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** â†’ Code as **R3**\n   - **NO** â†’ Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper â†’ Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications â†’ Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** â†’ Does the paper claim the component is the *unique* controller?\n     - YES â†’ Code as **R3**\n     - NO â†’ Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** â†’ Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing â†’ \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching â†’ \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering â†’ \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE â†’ \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention â†’ \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation â†’ \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" â†’ still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" â†’ code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2â†’3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2â†’R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2â†’R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2â†’R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2â†’R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2â†’R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 â†’ Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2â†’R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2â†’R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2â†’R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2â†’R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2â†’R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2â†’R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1â†’R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**KramÃ¡r et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) â†’ R3\n   - Check for uniqueness language (the, only) â†’ R3\n   - Check for storage/encoding language â†’ R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model â†’ R1\n   - If intervention but not per-instance counterfactual â†’ R2\n   - If establishes unique/necessary mechanism â†’ R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") â†’ still R3, note hedge in confidence\n   - Multi-method papers â†’ use highest-rung method that directly supports claim\n   - Implicit claims from narrative â†’ code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2504.02976\nTitle: Localized Definitions Distributed Reasoning via Patching\nFull text:\n     Localized Definitions and Distributed Reasoning:\n\n A Proof-of-Concept Mechanistic Interpretability Study via\n                    Activation Patching\n\n                                 Nooshin Bahador\n\n          Krembil Research Institute, University Health Network, Toronto, Canada.\n\n\nâ€¢ The fine-tuned GPT-2 models and related resources are openly accessible on Hugging Face.\nâ€¢ The source code and implementation details are available in the GitHub repository.\n\n\n\n\nAbstract\n\nThis study investigates the localization of knowledge representation in fine-tuned GPT-2 models\nusing Causal Layer Attribution via Activation Patching (CLAP), a method that identifies critical\nneural layers responsible for correct answer generation. The model was fine-tuned on 9,958\nPubMed abstracts (epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping. CLAP involved (1)\ncaching clean (correct answer) and corrupted (incorrect answer) activations, (2) computing logit\ndifference to quantify model preference, and (3) patching corrupted activations with clean ones to\nassess recovery. Results revealed three findings: First, patching the  first feedforward layer\nrecovered 56% of correct preference, demonstrating that associative knowledge is distributed\nacross multiple layers. Second, patching the final output layer completely restored accuracy (100%\nrecovery), indicating that definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation. Third, minimal recovery\nfrom convolutional layer patching (13.6%) suggests low-level features contribute marginally to\nhigh-level reasoning. Statistical analysis confirmed significant layer-specific effects (p<0.01).\nThese findings demonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy depends on task type.\nOur findings not only reconcile conflicting observations about localization in model editing but\nalso emphasize on using task-adaptive techniques for reliable, interpretable updates.\n\n\n\n1. Introduction\n\nModel editing refers to techniques that modify a neural networkâ€™s behavior post-trainingâ€”such as\nupdating factual knowledge or correcting errorsâ€”without full retraining. A common assumption\n\nis that successful edits require precise localization, i.e., targeting specific layers where knowledge\nis concentrated. However, experimental results demonstrate that editing success and localization\nare actually uncorrelated, challenging the hypothesis  that precise mechanistic targeting  is\nnecessary for reliable updates (Hase et  al., 2023). This aligns with the broader principle of\ndistributed representations in neural systems, observed in both biological and artificial networks.\n\nNeuroscientific research supports this idea, revealing phenomena like mixed selectivity (where\nneurons respond to multiple stimulus dimensions) and sparse selectivity (where specialized regions\nfunction within larger distributed networks). Similarly, vision neuroscience studies show that\nobject recognition is not purely hierarchical or category-based but instead relies on behaviorally\nrelevant, distributed dimensions (Contier et al., 2024). Together, these findings challenge the\ntraditional view that cognitiveâ€”or computationalâ€”functions must be localized, whether in\nbiological brains or artificial neural networks.\n\nAnother human neuroscience research supports a hybrid model of knowledge representation.\nExplicit recall (conscious retrieval) relies on domain-specific neural regions, while implicit\nrecognition (automatic familiarity detection) engages distributed domain-general networks. Thus,\nthe distinction between these processes depends on task demands (Shehzad et al., 2022).\n\nLike human visual knowledge representation, Question Answering (QA) tasks can be also\ncategorized along a spectrum of complexity, where different question types impose varying\ncomputational demands. These range from simple Factual Recall (Single-Hop QA), requiring no\nreasoningâ€”only memory or lookupâ€”to more complex Associative Reasoning (Bridge Multi-Hop\nQA), which necessitates linking multiple facts (Balesni et al., 2024).\n\nBased on the above claims, the human brain processes knowledge in both localized and distributed\nways, depending on whether it involves explicit recall or implicit recognition. Similarly, question\nanswering exhibits comparable patterns, distinguishing between factual recall and associative\nreasoning. To explore this further, researchers have identified localized internal computations\nduring factual recall (Sharma et  al., 2024). This localized activity contrasts with the more\ndistributed neural networks involved in associative reasoning, which recruit broader areas to\nintegrate and synthesize information. In fact, researchers have shown that modelâ€™s learned concept\nrepresentations are not localized but distributed across multiple internal components. This means\nno  single  part holds complete knowledge;  instead, concepts emerge from interconnected\nactivations (Chang et al., 2023).\n\nRecent advances in model editing have challenged the assumption that precise localization is\nnecessary for modifying neural network behavior, revealing instead that successful edits can occur\neven with distributed interventions (Hase et al., 2023). Yet, the relationship between knowledge\nrepresentation,  task  complexity, and  effective  editing  remains  poorly  understood. While\nneuroscience suggests a hybrid model of cognitionâ€”where explicit recall relies on localized\nregions and implicit recognition engages distributed networks (Shehzad et al., 2022)â€”it is unclear\nwhether AI systems exhibit similar task-dependent encoding. This work bridges these gaps by\n\nintroducing a task-aware framework for model editing, grounded in analysis of where and how\nknowledge is stored in language models. Using Causal Layer Attribution via Activation Patching\n(CLAP), we show a hierarchical knowledge organization: factual recall  is localized while\nassociative reasoning involves distributed intermediate representations, with implications for\nmechanistic interpretability in domain-specific language models and targeted debugging/editing.\n\n\n\n2. Method\n\nThe technique used in this work localize where in the model the \"knowledge\" or \"reasoning\" about\nthe correct answer resides. By observing which patched layers restore the model's preference for\nthe correct answer, we can infer which parts of the network are responsible for that behavior.\n\n\n\n2.1. Data Preparation\n\nThe GPT-2 model was trained on a curated subset of 9,958 PubMed abstracts, with domain-specific\nterms occurring at high frequencies: epilepsy (20,595 mentions), EEG (11,674 mentions), and\nseizure (13,921 mentions). The dataset was systematically extracted and preprocessed using the\npubmed_meta_analyzer tool, an automated Python-based pipeline for PubMed metadata retrieval,\ndeduplication, and keyword analysis. The dataset was structured in a tabular format with the\nfollowing columns: PMID, Title, Abstract, Journal, Keywords, and URL. The GPT-2 model was\nfine-tuned on textual abstracts extracted from the 'Abstract' column of this dataset. Rows with\nmissing values (NaN) were dropped to ensure data quality, and the remaining abstracts were\nconverted into Hugging Face Dataset objects with a single \"text\" field for streamlined processing.\n\n\n\n2.2. Model Fine-Tuning\n\nTable 1 formalizes the GPT-2 fine-tuning (Configuration I) pipeline, where text data D is tokenized\ninto  sequences  â„¤ð‘›    ,  optimized  via  gradient  descent  (ðœƒð‘¡+1 = ðœƒð‘¡âˆ’ ðœ‚âˆ‡ðœƒâ„’(ðœƒð‘¡))  with\nhyperparameters {ðœ‚= 5 Ã— 10âˆ’5 , ðœ†= 0.01, ðµ= 8}, and evaluated using validation loss â„’ð‘£ð‘Žð‘™  ,\nwhile memory constraints are managed through chunking (L=512) and subprocess isolation.\n\n\n\n\n\nTable 1: GPT-2 Fine-Tuning Configuration I\n\n Component          Mathematical Representation                      Description\n\n Base Model          ð‘“ðœƒâˆ¶ â„¤ð‘› â†’ â„ð‘›Ã—â€–ð‘‰â€–                                    Pretrained GPT-2 (causal LM),\n                                                                    where V is the vocabulary.\n\n Tokenization         ð‘‡âˆ¶ð‘¡ð‘’ð‘¥ð‘¡ â†’ â„¤ð‘›                                     Tokenizer mapping text to token\n                                                                     IDs\n\n Chunking                                        âŒˆð‘›ð¿â„ âŒ‰                       Splits text into fixed-length\n                     ð‘â„Žð‘¢ð‘›ð‘˜(ð‘¥, ð¿= 512) = {ð‘¥ð‘–:ð‘–+ð¿}ð‘–=0                                                                          sequences (L=512 tokens)\n\n Padding             ð‘ð‘Žð‘‘(ð‘¥) = ð‘¥ â¨ {ð‘ð‘Žð‘‘_ð‘–ð‘‘}ð¿âˆ’ â€–ð‘¥â€–                    Appends pad-token-id to reach\n                                                                                length L\n\n Data Split               ð·ð‘¡ð‘Ÿð‘Žð‘–ð‘› , ð·ð‘£ð‘Žð‘™ âˆ¼ð‘¡ð‘Ÿð‘Žð‘–ð‘›âˆ’ð‘¡ð‘’ð‘ ð‘¡                        80/20 stratified split\n                                    âˆ’ð‘ ð‘ð‘™ð‘–ð‘¡(ð·, ðœ= 0.2, ð‘ ð‘’ð‘’ð‘‘= 42)\n\n Batch Processing      ðµð‘¡ð‘Ÿð‘Žð‘–ð‘›= 8 , ðµð‘£ð‘Žð‘™= 2                               Batch sizes for\n                                                                                     training/validation\n\n Training Loop         epochs=30, early-stop(k=3)                          Stops if validation loss doesnâ€™t\n                                                                     improve for 3 epochs.\n\n Learning Rate     ðœ‚= 5 Ã— 10âˆ’5                         AdamW optimizer with fixed\n                                                                                 learning rate.\n\n Weight Decay     ðœ†= 0.01                                  L2 regularization strength\n\n Warmup Steps      ðœ‚ð‘¡= ðœ‚ . ð‘šð‘–ð‘›(ð‘¡500â„         , 1)                              Linear warmup over 500 steps\n\n Optimization          ðœƒð‘¡+1 = ðœƒð‘¡âˆ’ ðœ‚âˆ‡ðœƒâ„’(ðœƒð‘¡)                   AdamW + FP16 + gradient\n                                                                          accumulation (4 steps).\n\n Early Stopping         Stop if â„’ð‘£ð‘Žð‘™ doesnâ€™t improve for k = 3 epochs         Monitors validation loss for\n                                                                           convergence.\n\n Evaluation Metric               1                                       Cross-entropy loss on validation\n                   â„’ð‘£ð‘Žð‘™= âˆ’     âˆ‘log ð‘ƒ(ð‘¥)                               â€–ð·ð‘£ð‘Žð‘™â€–                                              set.\n                                               ð‘¥ðœ–ð·ð‘£ð‘Žð‘™\n\n Memory           ð¶ð‘™ð‘’ð‘Žð‘Ÿâˆ’ð‘šð‘’ð‘šð‘œð‘Ÿð‘¦(): ðºð‘ƒð‘ˆ ð‘ð‘Žð‘â„Žð‘’â† âˆ…               Regular GPU cache clearing\n Management                                                         and garbage collection.\n\n Parallelism           Num-proc=4                                         Multi-processing for dataset\n                                                                                   tokenization.\n\n\nTable 2 formalizes the GPT-2 fine-tuning (Configuration II) pipeline, where text data D is\ntokenized into sequences â„¤ð‘›  , optimized via gradient descent (ðœƒð‘¡+1 = ðœƒð‘¡âˆ’ ðœ‚âˆ‡ðœƒâ„’(ðœƒð‘¡)) with\nhyperparameters {ðœ‚= 2 Ã— 10âˆ’5 , ðœ†= 0.1, ðµ= 8}, and evaluated using validation loss â„’ð‘£ð‘Žð‘™ , while\nmemory constraints are managed through chunking (L=512) and subprocess isolation.\n\nTable 2: GPT-2 Fine-Tuning Configuration II\n\nComponent          Mathematical Representation                      Description\n\nBase Model          ð‘“ðœƒâˆ¶ â„¤ð‘› â†’ â„ð‘›Ã—â€–ð‘‰â€–                                    Pretrained GPT-2 (causal LM),\n                                                                   where V is the vocabulary.\n\nTokenization         ð‘‡âˆ¶ð‘¡ð‘’ð‘¥ð‘¡ â†’ â„¤ð‘›                                     Tokenizer mapping text to token\n                                                                    IDs\n\nChunking                                        âŒˆð‘›ð¿â„ âŒ‰                       Splits text into fixed-length\n                    ð‘â„Žð‘¢ð‘›ð‘˜(ð‘¥, ð¿= 512) = {ð‘¥ð‘–:ð‘–+ð¿}ð‘–=0                                                                        sequences (L=512 tokens)\n\nPadding             ð‘ð‘Žð‘‘(ð‘¥) = ð‘¥ â¨ {ð‘ð‘Žð‘‘_ð‘–ð‘‘}ð¿âˆ’ â€–ð‘¥â€–                    Appends pad-token-id to reach\n                                                                              length L\n\nData Split               ð·ð‘¡ð‘Ÿð‘Žð‘–ð‘› , ð·ð‘£ð‘Žð‘™ âˆ¼ð‘¡ð‘Ÿð‘Žð‘–ð‘›âˆ’ð‘¡ð‘’ð‘ ð‘¡                        80/20 stratified split\n                                   âˆ’ð‘ ð‘ð‘™ð‘–ð‘¡(ð·, ðœ= 0.2, ð‘ ð‘’ð‘’ð‘‘= 42)\n\nBatch Processing      ðµð‘¡ð‘Ÿð‘Žð‘–ð‘›= 8 , ðµð‘£ð‘Žð‘™= 2                               Batch sizes for\n                                                                                    training/validation\n\nTraining Loop         epochs=10, early-stop(k=5)                          Stops if validation loss doesnâ€™t\n                                                                    improve for 5 epochs.\n\nLearning Rate     ðœ‚= 2 Ã— 10âˆ’5                         AdamW optimizer with fixed\n                                                                                learning rate.\n\nWeight Decay     ðœ†= 0.1                                   L2 regularization strength\n\nWarmup Steps      ðœ‚ð‘¡= ðœ‚ . ð‘šð‘–ð‘›(ð‘¡500â„         , 1)                              Linear warmup over 500 steps\n\nOptimization          ðœƒð‘¡+1 = ðœƒð‘¡âˆ’ ðœ‚âˆ‡ðœƒâ„’(ðœƒð‘¡)                   AdamW (no FP16) + grad\n                                                               accum (4 steps)\n\nEarly Stopping         Stop if â„’ð‘£ð‘Žð‘™ doesnâ€™t improve for k = 5 epochs         Monitors validation loss for\n                                                                          convergence.\n\nEvaluation Metric               1                                       Cross-entropy loss on validation\n                  â„’ð‘£ð‘Žð‘™= âˆ’     âˆ‘log ð‘ƒ(ð‘¥)                              â€–ð·ð‘£ð‘Žð‘™â€–                                              set.\n                                             ð‘¥ðœ–ð·ð‘£ð‘Žð‘™\n\nMemory           ð¶ð‘™ð‘’ð‘Žð‘Ÿâˆ’ð‘šð‘’ð‘šð‘œð‘Ÿð‘¦(): ðºð‘ƒð‘ˆ ð‘ð‘Žð‘â„Žð‘’â† âˆ…               Regular GPU cache clearing\nManagement                                                         and garbage collection.\n\nParallelism           Num-proc=4                                         Multi-processing for dataset\n                                                                                  tokenization.\n\nFigure 1: Flowchart of the Causal Layer Attribution via Activation Patching (CLAP) algorithm. The process involves:\n(1) running clean and corrupted inputs through the model to cache activations, (2) computing the logit difference (Î”)\nto quantify model preference, and (3) patching corrupted activations with clean ones to identify which layers causally\nrestore the correct behavior. Layers that significantly recover Î” when patched are deemed critical for the modelâ€™s\ndecision.\n\n2.3. Causal Layer Attribution via Activation Patching (CLAP)\n\nCLAP identifies which neural network layers causally influence a model's preference for correct\nvs. incorrect answers by surgically replacing corrupted activations with clean ones and measuring\nthe recovery in logit difference. As illustrated in the Figure 1, the algorithm tests how patching\nactivations from a \"clean\" run (correct answer) into a \"corrupted\" run (incorrect answer) restores\nmodel performance and the logit difference metric quantifies the effect.\n\nStep 1) Model and Tokenizer Setup\n\nLet:\n\n â€¢ ð‘‡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ= ð‘‡: ð‘¡ð‘’ð‘¥ð‘¡â†’â„¤ð‘› (ð‘šð‘Žð‘ð‘  ð‘¡ð‘’ð‘¥ð‘¡ ð‘¡ð‘œ ð‘¡ð‘œð‘˜ð‘’ð‘› ð¼ð·ð‘ )\n â€¢ ð‘€ð‘œð‘‘ð‘’ð‘™= ð‘“ðœƒ: â„¤ð‘›â†’â„ð‘›Ã—âˆ£ð‘‰âˆ£ (ð‘šð‘Žð‘ð‘  ð‘¡ð‘œð‘˜ð‘’ð‘› ð¼ð·ð‘  ð‘¡ð‘œ ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ , ð‘¤â„Žð‘’ð‘Ÿð‘’ |ð‘‰| ð‘–ð‘  ð‘£ð‘œð‘ð‘Žð‘ð‘¢ð‘™ð‘Žð‘Ÿð‘¦ ð‘ ð‘–ð‘§ð‘’)\n\n\nStep 2) Input Construction\n\nFor question ð‘ž correct answer ð‘Žð‘, corrupted answer ð‘Žð‘¤:\n\n   â€¢  ð‘¥ð‘ð‘™ð‘’ð‘Žð‘›= ð‘‡(ð‘žâŠ•ð‘Žð‘) (ð‘ð‘™ð‘’ð‘Žð‘› ð‘–ð‘›ð‘ð‘¢ð‘¡ ð‘ ð‘’ð‘žð‘¢ð‘’ð‘›ð‘ð‘’)\n   â€¢  ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡= ð‘‡(ð‘žâŠ•ð‘Žð‘¤) (ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡ð‘’ð‘‘ ð‘–ð‘›ð‘ð‘¢ð‘¡ ð‘ ð‘’ð‘žð‘¢ð‘’ð‘›ð‘ð‘’)\n\n\n\nStep 3) Forward Pass with Activation Caching\n\nFor any input ð‘¥, the model computes:\n\n â€¢ â„Ž(0) = ð¸(ð‘¥)(ð‘¡ð‘œð‘˜ð‘’ð‘› ð‘’ð‘šð‘ð‘’ð‘‘ð‘‘ð‘–ð‘›ð‘”ð‘ )\n â€¢  â„Ž(ð‘™) = ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘’ð‘Ÿðµð‘™ð‘œð‘ð‘˜(ð‘™)(â„Ž(ð‘™âˆ’1))    ð‘“ð‘œð‘Ÿ   ð‘™= 1, â€¦ , ð¿\n â€¢ ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ = ð‘Šâ„Žð‘’ð‘Žð‘‘â„Ž(ð¿)\n\n ð‘¤â„Žð‘’ð‘Ÿð‘’ ð¿= ð‘›ð‘¢ð‘šð‘ð‘’ð‘Ÿ ð‘œð‘“ ð‘™ð‘Žð‘¦ð‘’ð‘Ÿð‘ .\n\n Activation Cache stores:\n\n             ð¿                               ð¿\n ð´= {â„Ž(ð‘™)}ð‘™=1 âˆª{ð‘Žð‘¡ð‘¡ð‘’ð‘›ð‘¡ð‘–ð‘œð‘›_ð‘¤ð‘’ð‘–ð‘”â„Žð‘¡ð‘ (ð‘™)}ð‘™=1\n\n\nStep 4) Logit Difference Metric\n\nFor target token sets ð‘¡ð‘ (correct) and ð‘¡ð‘¤ (incorrect):\n\n          1                 1\nð‘™ð‘œð‘”ð‘–ð‘¡ð‘‘ð‘–ð‘“ð‘“=    âˆ‘ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ [âˆ’1, ð‘–] âˆ’    âˆ‘ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ [âˆ’1, ð‘–]\n               |ð‘¡ð‘|                     |ð‘¡ð‘¤|\n                     ð‘–âˆˆð‘¡ð‘                       ð‘–âˆˆð‘¡ð‘¤\n\n   â€¢  ð‘šð‘’ð‘Žð‘› ð‘ð‘œð‘Ÿð‘Ÿð‘’ð‘ð‘¡ ð‘™ð‘œð‘”ð‘–ð‘¡âˆ¶ { 1 âˆ‘ ð‘–âˆˆð‘¡ð‘ ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ [âˆ’1, ð‘–]                                               |ð‘¡ð‘|\n   â€¢  ð‘šð‘’ð‘Žð‘› ð‘–ð‘›ð‘ð‘œð‘Ÿð‘Ÿð‘’ð‘ð‘¡ ð‘™ð‘œð‘”ð‘–ð‘¡âˆ¶{ 1 âˆ‘ ð‘–âˆˆð‘¡ð‘¤ ð‘™ð‘œð‘”ð‘–ð‘¡ð‘ [âˆ’1, ð‘–]                                          |ð‘¡ð‘¤|\n\nThis measures how much the model favors the correct phrase over the incorrect one.\n\n\n\nStep 5) Activation Patching\n\nFor layer ð‘™:\n\n    1. Run corrupted input ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡ but replace ð´(ð‘™) with the clean version\n    2. Compute the resulting logit difference\n\nMathematically, we compute:\n\n   â€¢  ð‘™ð‘œð‘”ð‘–ð‘¡_ð‘‘ð‘–ð‘“ð‘“(ð‘™) = ð‘šð‘’ð‘¡ð‘Ÿð‘–ð‘ (ð‘“ðœƒ(ð‘¥corrupt; ð´(ð‘™) â† ð´ð‘ð‘™ð‘’ð‘Žð‘›(ð‘™)  ))\n\nWhere the notation means we substitute the ð‘™ð‘¡â„Ž layer's activations from the clean run during the\ncorrupted input processing.\n\n\n\n3. Results\n\nWe fine-tuned GPT-2 on a dataset of 9,958 PubMed abstracts, evaluating two distinct training\nconfigurations. The training performance was analyzed for each configuration to assess loss\ndynamics.\n\nFor Configuration I, the initial parallel decline in losses (Figure 2) indicates effective learning, but\nthe post-epoch-7 divergenceâ€”where training loss falls more sharply than validation lossâ€”\nsuggests the model starts memorizing training specifics rather than generalizing.\n\n   Figure 2. Training and validation loss curves alongside gradient norms for fine-tuned GPT-2 (Configuration I)\n\n\n\nFor Configuration II, the training metrics visualization (Figure 3) reveals that both training and\nvalidation losses decrease monotonically while maintaining a consistent gap, indicating effective\nlearning without overfitting, as supported by stable gradient norms throughout the optimization\nprocess.\n\n\n\n\n\nFigure 3. Training dynamics of GPT-2 fine-tuning (Configuration II) showing training loss (blue), validation loss\n(red), and gradient norm (teal) across epochs. The parallel decline in training and validation loss indicates successful\nmodel convergence, while stable gradient norms suggest well-behaved optimization.\n\nFor activation patching experiments, we used a fine-tuned GPT-2 model (Configuration  I,\noptimized for lower train and validation losses) to investigate the representational encoding of\nepilepsy-related knowledge  in  the  network. We  conducted  multiple  activation  patching\nexperiments on fine-tuned GPT-2, with representative examples shown below.\n\nTable 3 presents activation patching results on GPT-2, analyzing how knowledge of interictal\nepileptiform discharges  is representationally encoded. The table demonstrates that replacing\ncorrupted activations (ð›¥= âˆ’0.34) with clean ones (ð›¥= 0.76) in the first feedforward layer\nrecovered 56% of correct answer preference (ð›¥= 0.43).\n\n\n\nTable 3: Activation Patching Analysis of Medical Fact Retrieval in GPT-2\n\n Component                 Description                    Mathematical Syntax\n\n Question (q)               \"What is the presence of interictal    -\n                                epileptiform discharges associated\n                              with?\n\n Correct Answer (ð‘Žâ‚)           \"Increased seizure risk in both       ð‘¡â‚ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚)\n                                 focal and generalized epilepsy.\"\n\n Corrupted Answer (ð‘Žâ‚“)        \"Cognitive dysfunction risk in       ð‘¡â‚“ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚“)\n                              both focal and generalized\n                                  epilepsy.\"\n\n Input Construction           Clean vs. corrupted inputs           ð‘¥ð‘ð‘™ð‘’ð‘Žð‘› = ð‘‡(ð‘ž âŠ• ð‘Žâ‚), ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡\n                                                = ð‘‡(ð‘ž âŠ• ð‘Žâ‚“)\n\n Target Layer                     First feedforward layer                 â„Žâ½Ë¡â¾ ð‘¤â„Žð‘’ð‘Ÿð‘’ ð‘™ = ð‘šð‘™ð‘. ð‘_ð‘“ð‘\n                            (ð‘šð‘™ð‘. ð‘_ð‘“ð‘)\n\n Logit Difference Metric       Measures model's preference for    ð›¥(ð‘¥) = ð”¼[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚]]\n                                 correct answer                      âˆ’ ð”¼[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚“]]\n\n Clean Run                   Model's output on correct answer    ð›¥(ð‘¥ð‘ð‘™ð‘’ð‘Žð‘›) = 0.76\n Corrupted Run               Model's output on corrupted       ð›¥( ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡) = âˆ’0.34\n                            answer\n\n Activation Patching          Replace corrupted activations       ð‘“ðœƒ( ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡; â„Žâ½Ë¡â¾ â† â„Žð‘ð‘™ð‘’ð‘Žð‘›â½Ë¡â¾)\n                              with clean ones\n\n Patched Result                  Partial recovery of correct          ð›¥ð‘ð‘Žð‘¡ð‘â„Žð‘’ð‘‘ = 0.43 (56% ð‘Ÿð‘’ð‘ð‘œð‘£ð‘’ð‘Ÿð‘¦: (0.43\n                               preference after patching                âˆ’ (âˆ’0.34))/(0.76\n                                                âˆ’ (âˆ’0.34)) = 0.56)\n\n\nAs shown in Table 4, activation patching of GPT-2's final output layer (ð‘Šð‘œð‘¢ð‘¡âˆˆ â„{768Ã—50257})\ncompletely restored the model's ability to correctly define IEDs - patching this single layer\nincreased the logit difference from Î”=0.9691 (corrupted) to Î”=2.1928 (clean), achieving 100%\nrecovery. This demonstrates that: (1) the final projection layer alone mediated the model's\ndefinitional knowledge in this case, as evidenced by the perfect Î” restoration (ðœ•ð›¥/ðœ•ð‘Šð‘œð‘¢ð‘¡ â‰ˆ 1);\n(2) the higher clean Î” (2.19 vs previous 0.76) indicates stronger baseline confidence in medical\n\nfacts than associations; and (3) unlike the partial recovery (56%) seen when patching intermediate\nlayers in the epilepsy risk experiment, this full recovery suggests compressed representation of\ndefinitional knowledge in the output weights.\n\n\n\nTable 4: Activation Patching Results for Medical Abbreviation Knowledge in GPT-2\n\n Component                 Description                    Mathematical Syntax\n\n Question (q)               \"What is IEDs?\"                         -\n\n Correct Answer (aâ‚)          \"IEDs stands for interictal            ð‘¡â‚ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚)\n                                epileptiform discharges, which\n                                 are abnormal electrical patterns\n                             observed on an EEG that occur\n                            between seizures in individuals\n                              with epilepsy.\"\n\n Corrupted Answer (aâ‚“)        \"IEDs stands for intracranial         ð‘¡â‚“ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚“)\n                                 electrode diagnostics, which are\n                               devices used to monitor brain\n                                    activity during surgical\n                                procedures.\"\n\n Input Construction           Clean vs. corrupted inputs           ð‘¥ð‘ð‘™ð‘’ð‘Žð‘› = ð‘‡(ð‘ž âŠ• ð‘Žâ‚), ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡\n                                                = ð‘‡(ð‘ž âŠ• ð‘Žâ‚“)\n Target Layer                   Final output layer                ð‘Šð‘œð‘¢ð‘¡ âˆˆ â„{768Ã—50257}\n Logit Difference Metric       Measures model's preference for    ð›¥(ð‘¥) = ð”¼[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚]]\n                                 correct answer                      âˆ’ ð”¼[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚“]]\n\n Clean Run                   Model's output on correct answer    Î”(ð‘¥clean) = 2.1928\n Corrupted Run               Model's output on corrupted         Î”(ð‘¥corrupt) = 0.9691\n                            answer\n\n Activation Patching          Replace corrupted output weights    ð‘“ðœƒ(ð‘¥corrupt; ð‘Šð‘œð‘¢ð‘¡â† ð‘Šð‘œð‘¢ð‘¡_ð‘ð‘™ð‘’ð‘Žð‘›)\n                              with clean ones\n\n Patched Result              Recovery of correct preference       ð›¥ð‘ð‘Žð‘¡ð‘â„Žð‘’ð‘‘\n                                    after patching             = 2.1928 (100% ð‘Ÿð‘’ð‘ð‘œð‘£ð‘’ð‘Ÿð‘¦: (2.1928\n                                       âˆ’ 0.9691)/(2.1928 âˆ’ 0.9691) = 1.0)\n\n\nTable 5 quantifies the effect of activation patching on GPT-2's ability to recover the correct\nassociation of interictal epileptiform discharges when corrupted weights in a convolutional layer\nare replaced with clean weights, showing only partial (13.6%) recovery of the original logit\ndifference.\n\nTable 5: Activation Patching Results for Epileptiform Discharge Associations in GPT-2\n\n Component                 Description                    Mathematical Syntax\n\n Question (q)               \"What is the presence of interictal    -\n                                epileptiform discharges associated\n                              with?\"\n\n Correct Answer (aâ‚)           \"Increased seizure risk in both       ð‘¡â‚ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚)\n                                 focal and generalized epilepsy.\"\n\n Corrupted Answer (aâ‚“)         \"Cognitive dysfunction risk in       ð‘¡â‚“ = ð‘¡ð‘œð‘˜ð‘’ð‘›ð‘–ð‘§ð‘’ð‘Ÿ(ð‘Žâ‚“)\n                              both focal and generalized\n                                  epilepsy.\"\n\n Input Construction           Clean vs. corrupted inputs           ð‘¥ð‘ð‘™ð‘’ð‘Žð‘› = ð‘‡(ð‘ž âŠ• ð‘Žâ‚), ð‘¥ð‘ð‘œð‘Ÿð‘Ÿð‘¢ð‘ð‘¡\n                                                = ð‘‡(ð‘ž âŠ• ð‘Žâ‚“)\n Target Layer             ð¶ð‘œð‘›ð‘£1ð·(ð‘›ð‘“= 3072, ð‘›ð‘¥= 768)   ð‘Šð‘ð‘œð‘›ð‘£ âˆˆ ð‘…{768Ã—3072}\n Logit Difference Metric       Measures model's preference for    ð›¥(ð‘¥) = ð¸[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚]]\n                                 correct answer                      âˆ’ ð¸[ð‘“ðœƒ(ð‘¥)[âˆ’1, ð‘¡â‚“]]\n\n Clean Run                   Model's output on correct answer    ð›¥(ð‘¥clean) = 0.1261\n Corrupted Run               Model's output on corrupted         ð›¥(ð‘¥corrupt) = âˆ’1.4626\n                            answer\n\n Activation Patching          Replace corrupted conv weights     ð‘“ðœƒ (ð‘¥corrupt; ð‘Šð‘ð‘œð‘›ð‘£ â† ð‘Šð‘ð‘œð‘›ð‘£_ð‘ð‘™ð‘’ð‘Žð‘›)\n                              with clean ones\n\n Patched Result                  Partial recovery after patching       ð›¥ð‘ð‘Žð‘¡ð‘â„Žð‘’ð‘‘\n                                      = âˆ’0.0899 (ð‘…ð‘’ð‘ð‘œð‘£ð‘’ð‘Ÿð‘¦: (0.1261\n                                      âˆ’ (âˆ’0.0899)) / (0.1261 âˆ’ (âˆ’1.4626))\n                                       â‰ˆ 13.6%\n\n\n4. Discussion\n\nThe lack of transparency in AI systems presents critical challenges for model adaptation to new\ndomains, particularly regarding issues of bias, fairness, and reliability that demand accountability.\nIn response to these challenges, mechanistic interpretability has emerged as a key approach to\nreverse-engineer complex AI modelsâ€”especially transformersâ€”by uncovering  their  inner\nworkings to ensure trust and robustness (Golgoon et al., 2024). This methodology has proven\nparticularly valuable in studying how large language models perform compositional relational\nreasoning. Through careful examination of model internals (including attention patterns and\nactivation pathways), researchers can reveal the computational mechanisms underlying logical\noperations, hierarchical dependencies, and reasoning processes (Ni et al., 2024). At its core,\nmechanistic interpretability seeks to explain model behaviors by identifying specific, interpretable\nfeaturesâ€”which often manifest as low-dimensional subspaces within activations (Makelov et al.,\n2023). Among the established techniques in this field are logit attribution, attention pattern\nvisualization, and activation patching (Lieberum et al., 2023).\n\nA particularly important technique in this toolkit is activation patching, which identifies critical\nsubspacesâ€”specific directions in activation spaceâ€”that significantly influence model decisions.\nThis approach works by strategically manipulating model behavior through subspace interventions\nto attribute underlying features to specific activation patterns (Makelov et al., 2023). Beyond mere\nidentification, activation patching serves as a causal mediation technique that rigorously evaluates\nwhether proposed explanations genuinely support the model's predicted outputs (Yeo et al., 2024).\nThe technique's utility extends to automated circuit discovery methods, where it helps identify\ntask-solving subnetworks (circuits). When compared to linear approximations of this approach that\nestimate edge importance in computational subgraphs, Attribution Patching (AtP) has been shown\nto outperform Automated Circuit Discovery in certain contexts (Syed et al., 2023). Complementary\napproaches like vocabulary projection can be combined with activation patching to precisely\nisolate information-encoding hidden states responsible for correct answer prediction (Wiegreffe et\nal., 2024).\n\nDespite these advantages, recent research has raised important questions about the reliability of\nactivation patching. Studies demonstrate that subspace activation patching can sometimes produce\nmisleading interpretations, especially in factual recall tasks. This limitation stems from the\nmechanistic connection to rank-1 fact editing, where modifications to single activation directions\nmay alter model behavior without necessarily revealing true causal pathways. These findings\nhighlight a crucial distinction between fact editing (successful output manipulation) and fact\nlocalization (accurate identification of knowledge storage in the network), helping explain\ninconsistencies in previous work. While activation patching remains a valuable tool, these results\nemphasize that  its interpretations can be potentially deceptive, warning researchers against\noverinterpreting apparent subspace importance. Practitioners must therefore carefully consider\npotential false positivesâ€”instances where patched activations seem influential but lack genuine\ncausal significance (Makelov et al., 2023).\n\nThe application of activation patching can be further enhanced through the use of clean and\ncorrupted promptsâ€”a methodological pairing that helps localize task-relevant components when\ncombined with this causal intervention approach (Golgoon et al., 2024). It's important to note that\nstandard activation patching can be computationally intensive, requiring iterative modification of\nneuron activations across numerous forward passes to properly assess their impact. For more\nefficient analysis, attribution-based methods offer an alternative by quantifying the importance of\nneurons or attention paths in just a single forward pass (Ferrando et al., 2024). Among these\nalternatives, Attribution Patching (AtP) provides a  faster, gradient-based approximation to\ntraditional activation patching, though it comes with its own limitations as it can produce false\nnegatives in certain scenarios (KramÃ¡r et al., 2024). Another complementary approach, direct logit\nattribution, analyzes how individual layers and their attention heads influence logit differences in\nthe residual stream, offering additional insights for mechanistic interpretability of large language\nmodels (Golgoon et al., 2024).\n\nIn our current work, we implemented Causal Layer Attribution via Activation Patching (CLAP) to\ninvestigate knowledge localization in fine-tuned GPT-2 models. Our findings reveal a hierarchical\norganization where factual recall appears localized while associative reasoning depends on\ndistributed  intermediate  representations. However,  several  important  limitations must  be\nacknowledged: (1) our analysis focused exclusively on PubMed epilepsy-related  abstracts,\npotentially limiting generalizability to broader knowledge tasks; (2) while informative, activation\npatching may generate false positives due to indirect causal pathways; and (3) the exclusive focus\non GPT-2 leaves open questions about scalability to other architectures. These limitations suggest\nsome directions for future research, including (1) cross-domain comparisons to better understand\ntask-dependent representation patterns, (2) methodological integration to improve efficiency, and\n(3) extensions to multimodal models where knowledge distribution mechanisms may differ\nsignificantly.\n\n\n\nReferences\n\nHase P, Bansal M, Kim B, Ghandeharioun A. Does Localization Inform Editing? Surprising\nDifferences in Causality-Based Localization vs. Knowledge Editing in Language Models. In:\nThirty-Seventh   Conference   on   Neural   Information   Processing   Systems.;   2023.\nhttps://openreview.net/forum?id=EldbUlZtbd.\n\nContier O, Baker CI, Hebart MN. Distributed representations of behaviour-derived object\ndimensions   in  the  human  visual  system.  Nat Hum  Behav.  2024;8(11):2179-2193.\ndoi:10.1038/s41562-024-01980-y.\n\nShehzad Z, Taylor J, McCarthy G. Localized and distributed representations of person knowledge\nfor faces. bioRxiv. Published online 28 March 2022. doi:10.1101/2022.03.27.485948.\n\nBalesni M, Korbak T, Evans O. The Two-Hop Curse: LLMs trained on Aâ†’B, Bâ†’C fail to learn\nAâ†’C. arXiv [csCL]. Published online 25 November 2024. http://arxiv.org/abs/2411.16353.\n\nSharma AS, Atkinson D, Bau D. Locating and editing factual associations in Mamba. arXiv\n[csCL]. Published online 4 April 2024. http://arxiv.org/abs/2404.03646.\n\nChang W, Kwon D, Choi J. Understanding distributed representations of concepts in deep neural\nnetworks  without  supervision. arXiv  [csCV].  Published  online  28  December  2023.\nhttp://arxiv.org/abs/2312.17285.\n\nGolgoon A, Filom K, Kannan AR. Mechanistic interpretability of large language models with\napplications to the financial services industry. arXiv [csLG]. Published online 15 July 2024.\nhttp://arxiv.org/abs/2407.11215.\n\nMakelov A, Lange G, Nanda N. Is this the subspace you are looking for? An interpretability\nillusion for subspace activation patching. arXiv [csLG]. Published online 28 November 2023.\nhttp://arxiv.org/abs/2311.17030.\n\nNi R, Xiao D, Meng Q, Li X, Zheng S, Liang H. Benchmarking and Understanding Compositional\nRelational  Reasoning  of LLMs.  arXiv  [csCL].  Published  online  17  December  2024.\nhttp://arxiv.org/abs/2412.12841.\n\nLieberum T, Rahtz M, KramÃ¡r J, et al. Does circuit analysis interpretability scale? Evidence from\nmultiple choice  capabilities  in Chinchilla. arXiv [csLG]. Published online 18 July 2023.\nhttp://arxiv.org/abs/2307.09458.\n\nSyed A, Rager C, Conmy A. Attribution patching outperforms automated circuit discovery. arXiv\n[csLG]. Published online 16 October 2023. http://arxiv.org/abs/2310.10348.\n\nFerrando J, Voita E. Information flow routes: Automatically interpreting language models at scale.\narXiv [csCL]. Published online 26 February 2024. http://arxiv.org/abs/2403.00824.\n\nKramÃ¡r J, Lieberum T, Shah R, Nanda N. AtP*: An efficient and scalable method for localizing\nLLM  behaviour   to  components.  arXiv  [csLG].  Published  online  1  March  2024.\nhttp://arxiv.org/abs/2403.00745.\n\nWiegreffe S, Tafjord O, Belinkov Y, Hajishirzi H, Sabharwal A. Answer, assemble, ace:\nUnderstanding how LMs answer multiple choice questions. arXiv [csCL]. Published online 20\nJuly 2024. http://arxiv.org/abs/2407.15018.\n\nYeo WJ, Satapathy R, Cambria E. Towards faithful Natural Language Explanations: A study using\nactivation patching in large Language Models. arXiv [csCL]. Published online 17 October 2024.\nhttp://arxiv.org/abs/2410.14155.\n\n## Claim to Classify\n\"factual knowledge is more localized and associative knowledge depends on distributed representations\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "Causal Layer Attribution via Activation Patching (CLAP)",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is activation patching (CLAP), which is an interventional technique that replaces activations from a 'clean' run into a 'corrupted' run and measures recovery in logit difference. This establishes causal sufficiency, placing it on Rung 2. The claim states that 'factual knowledge is more localized and associative knowledge depends on distributed representations', which uses definitive mechanistic language ('is', 'depends on') implying a structural or representational claim about how knowledge is stored and processed in the model. This constitutes a Rung 3 claim because it asserts a mechanistic organization (localization vs. distributed representation) rather than just a causal effect. There is no hedge (e.g., 'may', 'suggests'), so hedge_flag=0. The claim appears in the abstract and is stated as a definitive finding, consistent with Rung 3. This matches the common overclaim pattern of using Rung 2 methods (patching) to support Rung 3 mechanistic claims about representation, similar to the IOI and ROME calibration cases."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 13498,
      "completion_tokens": 279,
      "total_tokens": 13777
    },
    "finish_reason": "stop"
  }
}