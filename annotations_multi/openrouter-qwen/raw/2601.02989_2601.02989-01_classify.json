{
  "timestamp": "2026-02-19T11:50:55.091754+00:00",
  "paper_id": "2601.02989",
  "claim_id": "2601.02989-01",
  "mode": "classify",
  "prompt": {
    "system": "You are an expert annotation assistant for a mechanistic interpretability research study.\nYour task is to annotate empirical claims from ML papers following a structured codebook.\nFollow the codebook instructions exactly. Apply the decision rules for edge cases, including the decision trees for polysemous terms.\nRate your confidence honestly on the 1-5 scale.",
    "user": "## Annotation Codebook\n# H2 Pilot Study Codebook\n## Annotation Guidelines for Testing \"Rung-Appropriate Claims Predict Reproducibility\"\n\n---\n\n## Overview\n\nFor each paper in the sample, extract **all empirical claims** about model internals and annotate:\n1. What method was used (determines method_rung)\n2. What the paper claims (determines claim_rung)\n3. Whether claim_rung > method_rung (overclaiming)\n\n---\n\n## Field Definitions\n\n### paper_id\n- arXiv ID or venue-year-title abbreviation\n- Example: \"2202.05262\" or \"NeurIPS2022-ROME\"\n\n### claim_id\n- Unique identifier within paper: paper_id + sequential number\n- Example: \"2202.05262-01\", \"2202.05262-02\"\n\n### claim_text\n- **Verbatim quote** from the paper\n- Include enough context to understand the claim\n- Use ellipsis [...] for long quotes\n\n### claim_location\n- Where in the paper: abstract, introduction, methods, results, discussion, conclusion\n\n### claim_prominence\n- **3** = Abstract or title claim (highest visibility)\n- **2** = Introduction contribution list or conclusion claim\n- **1** = Body text claim (methods, results, discussion)\n\n---\n\n## Method Rung Classification\n\n### Rung 1: Observational/Associational\nMethods that establish **correlational evidence only**. No intervention on the model.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Linear probing | Train classifier on frozen activations | \"Probe accuracy of 85%\" |\n| Activation logging | Record activations without intervention | \"Feature X activates on...\" |\n| SAE feature attribution | Identify which SAE features activate | \"Feature 4123 fires on...\" |\n| Attention visualization | Inspect attention weights | \"Attention concentrates on...\" |\n| PCA/SVD | Dimensionality reduction analysis | \"First PC correlates with...\" |\n| Correlation analysis | Statistical associations | \"r=0.7 between activation and...\" |\n\n### Rung 2: Interventional\nMethods that establish **causal effects under specific interventions**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Activation patching | Replace activation, measure effect | \"Patching head 9.1 restores 80%...\" |\n| Causal tracing | Systematic patching across positions | \"Layer 15 shows highest causal effect\" |\n| Ablation | Zero/mean out components | \"Ablating heads reduces accuracy by 40%\" |\n| Steering vectors | Add direction, observe output change | \"Adding v shifts sentiment...\" |\n| DAS interchange | Swap aligned subspaces | \"IIA of 0.92 on agreement task\" |\n| ROME/MEMIT edits | Modify weights, observe change | \"After edit, model outputs...\" |\n\n### Rung 3: Counterfactual\nMethods that establish **what would have happened** or **unique mechanisms**.\n\n| Method | Description | Example Evidence |\n|--------|-------------|------------------|\n| Counterfactual patching | Per-instance counterfactual | \"For THIS prompt, had activation been X...\" |\n| Causal scrubbing | Test if mechanism fully explains | \"Scrubbing preserves behavior\" |\n| Necessity tests | Show component is necessary | \"No alternative achieves same behavior\" |\n| Uniqueness proofs | Demonstrate unique structure | \"This is THE circuit\" |\n\n---\n\n## Claim Rung Classification\n\n### Rung 1 Linguistic Markers (Associational Claims)\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"co-occurs with\"\n- \"information is present,\" \"is decodable from\"\n- \"can be extracted,\" \"activates on,\" \"fires when\"\n\n**Examples:**\n- \"Sentiment information is linearly decodable from layer 6\"\n- \"The feature correlates with Python code inputs\"\n- \"Probe accuracy predicts model behavior\"\n\n### Rung 2 Linguistic Markers (Causal Claims)\n- \"causally affects,\" \"has causal effect on\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce,\" \"enables\"\n- \"intervening on X changes Y\"\n- \"ablating X degrades Y\"\n\n**Examples:**\n- \"Head 9.1 causally affects the output\"\n- \"This component is sufficient for the behavior\"\n- \"Ablating these heads degrades performance\"\n\n### Rung 3 Linguistic Markers (Mechanistic/Counterfactual Claims)\n- \"encodes,\" \"represents,\" \"computes,\" \"performs\"\n- \"THE mechanism,\" \"THE circuit,\" \"THE feature\" (uniqueness)\n- \"controls,\" \"is responsible for,\" \"underlies\"\n- \"this head DOES X\" (functional attribution)\n- \"the model uses X to do Y\" (mechanistic narrative)\n\n### Decision Trees for Polysemous Terms\n\n#### \"encodes\" / \"represents\" / \"stores\"\n1. Does the paper provide interventional evidence for this claim?\n   - **NO** → Does context make clear the author means \"is linearly decodable from\"?\n     - YES → Code as **R1**. Note: \"encodes used in decodability sense\"\n     - NO → Code as **R3** (default mechanistic reading)\n   - **YES** → Is the claim about the intervention's *result* (what changed) or the underlying *mechanism* (how it works)?\n     - Result → Code as **R2**\n     - Mechanism → Code as **R3**\n\n#### \"the circuit\" / \"the mechanism\" / \"the algorithm\"\n1. Does the paper test uniqueness (e.g., show no alternative circuit exists)?\n   - **YES** → Code as **R3**\n   - **NO** → Is \"the\" a naming convention (referring to the circuit they found) or a uniqueness claim?\n     - If qualifications exist elsewhere in the paper → Code as **R3**, add note: \"definite article likely naming convention; qualification at [location]\"\n     - If no qualifications → Code as **R3**\n\n#### \"controls\" / \"is responsible for\"\n1. Is the evidence from an intervention (ablation, patching, steering)?\n   - **YES** → Does the paper claim the component is the *unique* controller?\n     - YES → Code as **R3**\n     - NO → Code as **R2** (causal sufficiency, not uniqueness)\n   - **NO** → Code as **R3** (mechanistic claim without interventional support)\n\n**Examples:**\n- \"The model **encodes** subject-verb agreement in this subspace\"\n- \"These heads **perform** the IOI task\"\n- \"**The circuit** moves names from subject to output\"\n- \"This feature **represents** the concept of deception\"\n- \"The model **uses** these components to track entities\"\n\n---\n\n## Overclaim Patterns (Common)\n\n| Pattern | Method Used | Typical Claim | Gap |\n|---------|-------------|---------------|-----|\n| Probing → \"encodes\" | Linear probe (R1) | \"Model encodes X\" (R3) | +2 |\n| Patching → \"THE circuit\" | Activation patching (R2) | \"This is the circuit\" (R3) | +1 |\n| Steering → \"controls\" | Steering vectors (R2) | \"Controls concept X\" (R3) | +1 |\n| SAE → \"represents\" | SAE attribution (R1) | \"Model represents X\" (R3) | +2 |\n| Attention → \"performs\" | Attention viz (R1) | \"Head performs X\" (R3) | +2 |\n| Ablation → \"necessary\" | Ablation (R2) | \"Necessary for behavior\" (R3) | +1 |\n\n---\n\n## Hedge Flag\n\n### hedge_flag\n- **1** = Claim contains an explicit hedge (e.g., \"may,\" \"suggests,\" \"potentially,\" \"we hypothesize\")\n- **0** = No hedge present; claim is stated as established fact\n\nRecord hedging separately from confidence. A claim can be high-confidence R3 *with* a hedge (the annotator is confident the claim is R3, and the author hedged it).\n\n---\n\n## Confidence Scoring\n\nRate your confidence in the rung assignments (1-5):\n- **5** = Very confident, clear case\n- **4** = Confident, minor ambiguity\n- **3** = Moderately confident, some ambiguity\n- **2** = Low confidence, significant ambiguity\n- **1** = Very uncertain, edge case\n\nDocument ambiguous cases in the notes field.\n\n---\n\n## Replication Status\n\n### Coding\n- **0** = Successfully replicated (all main claims hold)\n- **0.5** = Partially replicated (some claims hold, others fail)\n- **1** = Failed replication (main claims do not hold)\n- **NA** = No replication attempt found\n\n### Evidence Sources (in priority order)\n1. Published replication studies\n2. Replication sections in subsequent papers\n3. GitHub issues documenting failures\n4. Author corrections/errata\n5. BlackboxNLP reproducibility track\n\n---\n\n## Annotation Process\n\n1. **Read abstract and introduction** - identify main claims\n2. **Identify methods used** - classify each method's rung\n3. **For each claim:**\n   - Quote verbatim\n   - Identify linguistic markers\n   - Assign claim_rung based on markers\n   - Calculate gap_score\n   - Assign confidence\n4. **Search for replication evidence** - cite sources\n5. **Document edge cases** in notes\n\n---\n\n## Edge Cases and Guidance\n\n### Hedged Claims\n- \"may encode\" → still Rung 3 if followed by mechanistic narrative\n- \"suggests that\" → code based on the underlying claim, not the hedge\n- Note hedging in confidence score\n\n### Multiple Methods\n- If paper uses multiple methods, code each claim-method pair separately\n- Use the method that directly supports each specific claim\n\n### Implicit Claims\n- Code both explicit and implicit claims\n- Implicit claims from narrative framing should be noted\n- Weight implicit claims lower in confidence\n\n### Review/Survey Papers\n- Code as NA for replication (not empirical)\n- Still useful for method classification reference\n\n---\n\n## Calibration Cases\n\n### Ground Truth: IOI Circuit Paper (Wang et al., 2022)\n- **Method:** Activation patching (Rung 2)\n- **Claim:** \"The circuit\" (implies uniqueness, Rung 3)\n- **Overclaim:** +1\n- **Known issue:** Different ablation strategies yield different circuits\n\nUse this as calibration anchor for Rung 2→3 overclaiming pattern.\n\n\n## Calibration Examples\n# Calibration Set Rationales\n\n## Overview\n\nThis document provides detailed rationales for the 5 calibration papers, serving as anchor examples for consistent annotation of the remaining papers.\n\n---\n\n## Paper 1: IOI Circuit (2211.00593) - PRIMARY CALIBRATION ANCHOR\n\n**Wang et al., \"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Path patching (activation patching variant)\n- **Supporting methods:** Attention pattern analysis (R1), ablation (R2)\n- **Rationale:** The paper's core evidence comes from causal interventions that measure effects of patching activations. This establishes causal sufficiency but not counterfactual necessity/uniqueness.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"performs IOI task\" | \"performs\" = functional | +1 (R2→R3) |\n| \"Name Movers move names\" | \"move\" = mechanistic | +1 (R2→R3) |\n| \"S-Inhibition heads inhibit\" | \"inhibit\" = functional | +1 (R2→R3) |\n| \"the circuit\" | definite article = uniqueness | +1 (R2→R3) |\n| \"reverse-engineering\" | implies complete mechanism | +1 (R2→R3) |\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:** Different ablation strategies (mean ablation vs. zero ablation vs. resample ablation) yield different circuits\n- **Evidence:** Zhang et al. (2024), Conmy et al. (2023) ACDC paper notes\n- **Implication:** The \"circuit\" found depends on methodological choices, undermining uniqueness claims\n\n### Calibration Lesson\nThe IOI paper is the canonical example of **Rung 2 → Rung 3 overclaiming** via:\n1. Using definite articles (\"THE circuit\")\n2. Functional verbs (\"moves,\" \"inhibits,\" \"performs\")\n3. Mechanistic narratives (\"reverse-engineering the algorithm\")\n\n**Use this pattern to identify similar overclaims in other circuit-discovery papers.**\n\n---\n\n## Paper 2: ROME (2202.05262)\n\n**Meng et al., \"Locating and Editing Factual Associations in GPT\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Causal tracing (activation patching on corrupted inputs)\n- **Secondary method:** ROME editing (weight modification)\n- **Rationale:** Both methods involve interventions but establish causal effects, not mechanisms.\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"storing factual associations\" | \"storing\" = memory mechanism | +1 (R2→R3) |\n| \"correspond to localized computations\" | \"correspond\" = identity claim | +1 (R2→R3) |\n| \"stored in a localized manner\" | \"stored\" + \"localized\" | +1 (R2→R3) |\n\n### Appropriate Claims (No Overclaim)\n- \"mediate factual predictions\" - \"mediate\" is proper R2 language\n- \"ROME is effective\" - empirical claim matched to method\n\n### Replication Status: PARTIAL (0.5)\n- **Known issues:**\n  - Hase et al. (2023) \"Does Localization Imply Representation?\" questions causal tracing interpretation\n  - ROME edits have side effects on related knowledge\n  - Localization claims sensitive to prompt variations\n- **Implication:** Causal effects real, but \"storage\" interpretation overclaims\n\n### Calibration Lesson\nStorage/memory language (\"stores,\" \"encodes,\" \"contains\") typically implies Rung 3 mechanistic claims. Causal tracing only establishes causal mediation (R2), not storage mechanisms.\n\n---\n\n## Paper 3: Grokking (2301.05217)\n\n**Nanda et al., \"Progress measures for grokking via mechanistic interpretability\"**\n\n### Method Classification: Rung 2 (Interventional)\n- **Primary method:** Ablation in Fourier space\n- **Supporting methods:** Weight analysis (R1), activation analysis (R1)\n- **Rationale:** Ablation establishes causal necessity of Fourier components\n\n### Key Overclaim Patterns Identified\n\n| Claim | Linguistic Marker | Method-Claim Gap |\n|-------|------------------|------------------|\n| \"fully reverse engineer\" | completeness claim | +1 (R2→R3) |\n| \"the algorithm\" | definite article = uniqueness | +1 (R2→R3) |\n| \"uses DFT... to convert\" | functional mechanism | +1 (R2→R3) |\n| \"encoded in the weights\" | from weight analysis alone | +2 (R1→R3) |\n\n### Replication Status: REPLICATED (0)\n- **Strong replication:** Multiple groups have confirmed the Fourier structure\n- **Why different from IOI?**\n  - Simpler, controlled setting (synthetic task)\n  - Algorithm structure mathematically constrained\n  - Predictions verified through multiple methods\n\n### Calibration Lesson\nEven well-replicated papers can have overclaims at the linguistic level. The grokking claims are less problematic because:\n1. Multiple methods converge\n2. Mathematical structure constrains possibilities\n3. Authors make specific testable predictions\n\n**Pattern:** Small overclaim gap + strong replication = less concern\n\n---\n\n## Paper 4: SAE Evaluation (2409.04478)\n\n**Chaudhary & Geiger, \"Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge\"**\n\n### Method Classification: Mixed Rung 1-2\n- **Primary method:** SAE feature attribution (R1)\n- **Evaluation method:** Interchange intervention (R2)\n- **Rationale:** Paper evaluates R1 method using R2 evaluation\n\n### Claim Analysis\nThis paper is methodologically careful and largely avoids overclaiming:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"SAEs struggle to reach baseline\" | R2 | Appropriate for intervention evidence |\n| \"features that mediate knowledge\" | R2 | \"mediate\" matches intervention method |\n| \"useful for causal analysis\" | R2 | Claims about causal utility, not mechanism |\n\n### Replication Status: REPLICATED (0)\n- Paper is itself an evaluation/replication study\n- Findings consistent with other SAE evaluations (Marks et al., Engels et al.)\n\n### Calibration Lesson\n**Evaluation papers** tend to have lower overclaim rates because:\n1. Explicit comparison to baselines/skylines\n2. Focus on method utility, not mechanism claims\n3. Negative results naturally cautious\n\n**Pattern:** Papers that evaluate methods rather than discover mechanisms tend to have better claim-method alignment.\n\n---\n\n## Paper 5: Gemini Probes (2601.11516)\n\n**Kramár et al., \"Building Production-Ready Probes For Gemini\"**\n\n### Method Classification: Rung 1 (Observational)\n- **Primary method:** Linear probing\n- **Rationale:** Probing is purely observational/correlational\n\n### Claim Analysis\nThis paper is well-calibrated to its method:\n\n| Claim | Rung | Notes |\n|-------|------|-------|\n| \"probes may be promising\" | R1 | Hedged, correlational |\n| \"probes fail to generalize\" | R1 | Empirical observation |\n| \"successful deployment\" | R1 | Outcome claim, not mechanism |\n\n### Overclaim Analysis\nNo significant overclaims detected. The paper:\n- Uses appropriate hedging (\"may be\")\n- Focuses on empirical performance, not mechanisms\n- Does not claim probes \"detect\" or \"identify\" internal states (which would be R3)\n\n### Replication Status: NA\n- Production paper, not standard academic replication context\n\n### Calibration Lesson\n**Production/applied papers** focused on probe performance tend to have appropriate claim levels because:\n1. Focus on external validity (does it work?)\n2. Less incentive for mechanistic narratives\n3. Engineering framing vs. science framing\n\n---\n\n## Summary: Overclaim Patterns by Paper Type\n\n| Paper Type | Typical Overclaim | Example |\n|------------|------------------|---------|\n| Circuit discovery | \"THE circuit\" + functional verbs | IOI |\n| Knowledge localization | \"stores,\" \"encodes\" | ROME |\n| Algorithm analysis | \"reverse-engineer,\" \"the algorithm\" | Grokking |\n| Method evaluation | Low overclaim (comparative) | SAE Eval |\n| Production/applied | Low overclaim (empirical focus) | Gemini Probes |\n\n## Key Linguistic Markers Summary\n\n### Rung 3 (Mechanistic) - Watch for:\n- \"encodes,\" \"represents,\" \"stores,\" \"contains\"\n- \"performs,\" \"computes,\" \"executes,\" \"implements\"\n- \"THE circuit/mechanism/algorithm\" (uniqueness)\n- \"uses X to do Y\" (mechanistic narrative)\n- \"is responsible for,\" \"controls,\" \"underlies\"\n\n### Rung 2 (Causal) - Appropriate for interventions:\n- \"causally affects,\" \"has causal effect\"\n- \"mediates,\" \"influences\"\n- \"is sufficient for,\" \"can produce\"\n- \"intervening on X changes Y\"\n\n### Rung 1 (Correlational) - Appropriate for probing/attribution:\n- \"correlates with,\" \"is associated with\"\n- \"predicts,\" \"is decodable from\"\n- \"activates on,\" \"fires when\"\n- \"information is present\"\n---\n\n## Inter-Annotator Calibration Notes\n\nFor the pilot study (single annotator), use these decision rules:\n\n1. **When in doubt about claim_rung:**\n   - Check for functional verbs (performs, computes) → R3\n   - Check for uniqueness language (the, only) → R3\n   - Check for storage/encoding language → R3\n\n2. **When in doubt about method_rung:**\n   - If no intervention on model → R1\n   - If intervention but not per-instance counterfactual → R2\n   - If establishes unique/necessary mechanism → R3\n\n3. **Edge cases:**\n   - Hedged R3 claims (\"may encode\") → still R3, note hedge in confidence\n   - Multi-method papers → use highest-rung method that directly supports claim\n   - Implicit claims from narrative → code but weight lower in confidence\n\n\n## Paper Context\nPaper ID: 2601.02989\nTitle: MI of Large-Scale Counting in LLMs\nFull text:\n              Mechanistic Interpretability of Large-Scale Counting in LLMs\n                            through a System-2 Strategy\n\n             Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian,\n                     Fatemeh Askari, Mobin Bagherian, Amirmohammad Izadi,\n                               and Mahdieh Soleymani Baghshah\n                                           Sharif University of Technology\n\n\n\n\n\n                                                                                                                                    System-1 vs System-2 Counting Performance\n                          Abstract                                                                                                                                                                                                                                          Llama 3 8B System-2                                                                                                                                                                                                 0.95\n                                                                                                                                                                                                                                          Llama 3 8B System-1\n                 Large language models (LLMs), despite strong                             0.85                                                                                  Qwen2.5Qwen2.5 7B7B System-2System-1\n                performance on complex mathematical prob-                            0.75\n                                                                                                                                                                                                 0.65\n                  lems, exhibit systematic limitations in counting                             0.55\n                     tasks. This issue arises from architectural limits                             0.45                                                                                                                                                                                                                                                                                                                                                                                     Accuracy2026                                                                                                                                                                                0.35                  of transformers, where counting is performed\n                  across layers, leading to degraded precision                             0.25\n                                                                                                                                                                                                 0.15\n                    for larger counting problems due to depth con-                            0.05Jan\n                      straints. To address this limitation, we propose                             0.05     10          15          20          25          30          35          40          45          506\n                 a simple test-time strategy inspired by System-                                               Number of Items\n               2 cognitive processes that decomposes large\n                 counting tasks into smaller, independent sub-        Figure 1: System-1 vs. System-2 counting performance\n                 problems that the model can reliably solve. We        as a function of problem size. System-1 performance de-\n                   evaluate this approach using observational and        grades rapidly and collapses beyond approximately 30\n                  causal mediation analyses to understand the         items, reflecting the bounded capacity of the model’s in-[cs.CL]\n                 underlying mechanism of this System-2-like         ternal counter. In contrast, System-2 counting maintains\n                    strategy. Our mechanistic analysis identifies        high accuracy across the entire range by decomposing\n                key components: latent counts are computed        the task into small solvable sub-problems and aggregat-\n                and stored in the final item representations of        ing the results.\n                 each part, transferred to intermediate steps via\n                  dedicated attention heads, and aggregated in        these models use a layerwise internal counting pro-\n                   the final stage to produce the total count. Ex-       cess where numerical information is progressively\n                  perimental results demonstrate that this strategy       accumulated across transformer layers (Hasani\n                  enables LLMs to surpass architectural limita-                                                                         et al., 2025b). However, due to the depth limits,\n                    tions and achieve high accuracy on large-scale\n                                                                           this progressive counting mechanism becomes sat-\n                 counting tasks. This work provides mechanis-\n                                                                urated as the number of items increases. Comple-                        tic insight into System-2 counting in LLMs and\n                                                         mentary work on numerical representations sug-                   presents a generalizable approach for improv-\n                  ing and understanding their reasoning behavior.       gests that LLMs encode numbers in a compressed,arXiv:2601.02989v1                                                              sublinear manner, similar to the human logarithmic          1  Introduction\n                                                             mental number line (AlquBoj et al., 2025; Dehaene,\n            Counting is a fundamental cognitive operation that    2011). While numerical order is preserved, repre-\n            underpins a wide range of reasoning tasks, from    sentational resolution decreases with magnitude,\n             basic arithmetic to more complex forms of quan-   explaining the reduced precision for large values.\n                titative analysis (Feigenson et al., 2004; Dehaene,     These findings suggest that LLMs struggle to ac-\n             2011). In Large Language Models (LLMs), the    curately count large numbers of items, with perfor-\n               ability to count is important for controlled genera-   mance typically degrading for two- and three-digit\n              tion such as length-constrained summarization, se-   counts (Zhang et al., 2024; Yehudai et al., 2024; Fu\n              quential enumeration, and broader numerical/arith-    et al., 2024). This limitation reflects a fundamental\n             metic reasoning (Retkowski and Waibel, 2025; Hou    architectural constraint rather than a lack of train-\n               et al., 2025; Yang et al., 2024b; Gambardella et al.,   ing data or supervision, as more layers are required\n             2024).                                                to count a larger number of items (Vaswani et al.,\n              Recent research has provided insights into the   2017; Yehudai et al., 2024; Golkar et al., 2024).\n             counting mechanism of LLMs, demonstrating that  We argue that this failure stems from the model’s\n\n\n                                                    1\n\n         Middle-to-late                                                                                               9\n              layers\n                                                                                       4         2         3\n\n\n                                     4              2                  3\n\n\n\n\n\n                                     4\n\n\n                                3                                      3\n        Middle layers\n\n                           2                       2             2\n\n\n                       1                       1             1\n\n\n\n\n\n            Early layers\n\n\n                        apple, apple, apple, apple |  apple, apple |  apple, apple, apple          part 1: 4,   part 2: 2,   part 3: 3.   Final: 9.\n                                        Input                                              Output\nFigure 2: Internal mechanism of System-2 test-time counting in LLMs. A large counting task is divided into\nsmaller partitions using an external separator (|). Within each partition, the model performs implicit System-1\ncounting, where count information accumulates token-by-token and is localized at the final item or separator token\n(gray blocks). The final count information is transferred via residual streams (green arrows) and stored in the\nmiddle-to-late layers (green blocks). These partition-level counts (e.g., 4, 2, 3) are then transferred (orange arrows)\nthrough attention pathways to explicit reasoning tokens that report intermediate results (orange blocks). Finally, the\nintermediate counts are aggregated (purple arrows) to produce the final answer. By keeping each sub-task within\nthe model’s reliable counting range, this System-2 procedure removes the upper bound imposed by the model’s\narchitectural limitations.\n\nreliance on a System-1–like processing approach,   ing attention analysis and causal mediation tech-\nwhich is fast, automatic, and capacity-limited (Kah-   niques (Heimersheim and Nanda, 2024; Geiger\nneman, 2011; Dehaene, 2011).                         et al., 2021; Ghandeharioun et al., 2024; Zhang\n                                            and Nanda, 2024), we trace the flow of numeri-\n  To address this issue, we propose a simple test-\n                                                      cal information across the model and identify the\ntime strategy that adopts a System-2–like approach\n                                            mechanisms mediating the System-2 counting pro-\nto counting (Figure 1) (Kahneman, 2011). Instead\n                                                       cess. Figure 2 provides an overview of the main\nof relying on the model’s internal counting mecha-\n                                             components of the System-2 counting mechanism\nnisms, which are constrained by architectural lim-\n                                              and its internal information flow. This work offers a\nits, our approach decomposes large counting tasks\n                                        new perspective on both improving and understand-\ninto smaller, independent sub-tasks (Radhakrish-\n                                                   ing LLMs’ reasoning capabilities, with a focus on\nnan et al., 2023; Qharabagh et al., 2024). Each sub-\n                                                 a fundamental cognitive task.\ntask, containing a manageable number of items,\ncan be reliably counted by the model. The re-\n                                       2  Problem Setup and Methodologysults from each sub-task are then aggregated to\nproduce the final count.  This approach mirrors\n                                  We follow the standard counting framework used in\nhuman cognitive strategies, where complex prob-\n                                                        prior research (Hasani et al., 2025b). In this setup,\nlems are broken down into simpler, easier-to-solve\n                                                  a list of repeated items, such as fruits or animals, is\nsub-problems (Kahneman, 2011; Dehaene, 2011).\n                                                    presented to the model, and the task is to report the\n  Our behavioral experiments on various LLMs    total number of items. For example, given a context\ndemonstrate the effectiveness of this strategy in    like “apple, apple, apple, . . . ”, the model must out-\novercoming architectural limitations without re-   put the total number of items in the list. Previous\nquiring model modifications or fine-tuning. Ad-   work has shown that LLMs exhibit high accuracy\nditionally, we provide a detailed mechanistic in-    for small counts (fewer than 10 items), but that\nterpretation of how it functions within LLMs. Us-   performance deteriorates as the number of items\n\n\n                                         2\n\n           Model            Input     Output           Accuracy              MAE\n                                                    11-20  21-30  31-40  41-50  11-20  21-30  31-40  41-50\n                                       w/o steps   0.38    0.13    0.06    0.00    0.88    2.19    5.29   10.50\n                              Unstructured\n                                       w/ steps    0.45    0.11    0.03    0.00    0.72    2.44    5.97    9.68\n           Qwen2.5 7B\n                                       w/o steps   0.20    0.13    0.05    0.01    3.56    3.54    4.33    6.35\n                                 Structured\n                                       w/ steps    0.95    0.61    0.38    0.24    0.07    1.36    1.53    2.18\n                                       w/o steps   0.80    0.49    0.02    0.00    0.20    0.65    4.93   11.92\n                              Unstructured\n                                       w/ steps    0.29    0.34    0.00    0.00    0.74    0.82    5.00   11.44\n           Llama 3 8B\n                                       w/o steps   0.08    0.05    0.03    0.01    6.62    5.75    5.96    6.62\n                                 Structured\n                                       w/ steps    0.84    0.54    0.38    0.26    0.30    0.70    1.21    2.20\n                                       w/o steps   1.00    0.70    0.40    0.00    0.00    0.30    1.00    4.90\n                              Unstructured\n                                       w/ steps    1.00    0.50    0.30    0.00    0.00    0.60    1.40    4.70\n         Gemma 3 27B\n                                       w/o steps   0.30    0.05    0.00    0.17    2.10   10.95   7.33   12.67\n                                 Structured\n                                       w/ steps    1.00    0.85    0.55    0.50    0.00    0.35    2.15    2.25\n\nTable 1: Average accuracy and mean absolute error (MAE) across open-source models for context sizes from 11 to\n50. Each model is evaluated on structured and unstructured inputs, with and without intermediate reasoning steps.\n\n\n    Model             Input     Output               Accuracy                   MAE\n                                             51-60  61-70  71-80  81-90  91-100  51-60  61-70  71-80  81-90  91-100\n                                 w/o steps   0.70    0.54    0.56    0.18    0.24    0.36    1.42    0.72    3.62    4.26\n                       Unstructured\n                                 w/ steps    0.58    0.53    0.40    0.16    0.26    1.10    1.78    1.72    3.20    3.64\n    GPT-4o\n                                 w/o steps   0.37    0.31    0.10    0.11    0.11    1.04    1.53    3.22    2.64    3.03\n                         Structured\n                                 w/ steps    0.96    0.91    0.87    0.83    0.86    0.04    0.10    0.16    0.22    0.18\n                                 w/o steps   0.52    0.50    0.44    0.42    0.20    0.60    0.50    3.17    4.67    2.70\n                       Unstructured\n                                 w/ steps    0.95    0.80    0.72    0.60    0.60    0.05    1.10    1.78    1.30    1.30\n     Gemini-2.5-Pro\n                                 w/o steps   0.82    0.80    0.78    0.75    0.79    0.25    0.08    0.18    0.30    0.10\n                         Structured\n                                 w/ steps    0.97    0.95    0.95    0.91    0.91    0.10    0.05    0.05    0.06    0.07\n\nTable 2: Average accuracy and MAE across closed-source models for context sizes from 51 to 100. Each model is\nevaluated on structured and unstructured inputs, with and without intermediate reasoning steps.\n\n\nincreases beyond this range (Zhang et al., 2024;     (|). For generation, we evaluate two output vari-\nFu et al., 2024). This suggests that the model’s    ants: a short-answer format, where only the final\ncounting ability is limited by the depth of the trans-   count is produced (corresponding to an immediate\nformer architecture and its internal counting mech-   System-1-like process), and a Chain-of-Thought\nanism (Yehudai et al., 2024).                    (CoT) format, where intermediate reasoning steps\n  To overcome this limitation, motivated by prior    are included before the final answer (corresponding\nwork on partitioning images in visual reasoning    to a System-2-like process).\ntasks (Izadi et al., 2025), we introduce a simple\n                                       3  Behavioral Resultsstrategy that explicitly partitions the input list into\nsmaller sub-problems. We use an external separator   To illustrate how LLMs lose counting precision in\n(|) to divide the list into smaller partitions. For   long contexts, we first measure the average predic-\ninstance, a structured context with three partitions    tion probability of Qwen2.5 7B (Yang et al., 2024a)\nis: “apple, apple, apple, apple, apple | apple, apple,    across different context sizes. As shown in Figure 3,\napple | apple, apple, apple, apple, apple, apple”.     model confidence decreases as the number of items\n  The model is then instructed to first count the   exceeds 10. We also observe systematic biases to-\nitems in each partition and to aggregate the partial   ward more frequent numbers. For example, target\ncounts to produce the final result. This ensures that    counts of 16 and 21 are often predicted as 15 and\neach sub-problem remains within the model’s reli-   20.\nable counting regime. The number of items in each    We then conduct more systematic experiments\npartition is chosen randomly from a range that the   on both open-source and closed-source models.\nmodel can handle accurately. Models with deeper   For open-source models, we evaluate Qwen2.5 7B,\narchitectures can reliably process larger partitions.   Llama 3 8B (Grattafiori et al., 2024), and Gemma\n  Based on the input structure and output format,   3 27B (Team et al., 2025), which have 28, 32, and\nwe consider four baselines in our study. Two in-   62 layers, respectively. In addition, we evaluate\nput formats are used: an unstructured context with   GPT-4o (Achiam et al., 2023) and Gemini-2.5-\ncomma-separated items and a structured context   Pro (Team, 2025) as stronger proprietary models\nwith partitions separated by vertical bar symbol   on longer contexts. The corresponding results are\n\n\n                                         3\n\n               Decoded Probability of Context            most to these processes.\n      1  1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n      2  0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00       Figure 4a shows attention patterns to the input\n            0.00                0.00                     1.00                          0.00                              0.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.00                                                                             0.00                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00      3\n            0.00                0.00                     0.00                          1.00                              0.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.00                                                                             0.00                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00     items when the model generates intermediate rea-      4\n            0.00                0.00                     0.00                          0.00                              1.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.00                                                                             0.00                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00      5\n            0.00                0.00                     0.00                          0.00                              0.00                                   1.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.00                                                                             0.00                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00     soning steps. We consider a structured context with      6\n      7  0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     three partitions containing 3, 4, and 5 items. Dur-\n      8  0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n      9  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     ing the generation of a reasoning token such as\n     10  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00\n\n\n            0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.89 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Number 1112  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     “part 2: 4”, the attention weight of the generated     13  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00    number peaks sharply on the final item and the fi-\n            0.00                0.00                     0.00                          0.00                              0.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.01                                                                        0.05                                                                             0.02                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00     14\n            0.00                0.00                     0.00                          0.00                              0.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.01                                                                             0.98                                                                                 0.63                                                                                      0.17                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.00                                                                                                                      0.00                                                                                                                           0.00     nal comma separator of the corresponding partition.     15 Decoded 16  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.47 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00                                                This pattern is consistent across prompts and is\n     17  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.36 0.84 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n     18  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00    most pronounced in middle to late layers, typically\n     19  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.69 0.12 0.00 0.00 0.00 0.00 0.00\n     20  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.87 1.00 0.86 0.22 0.00 0.00    around layers 19 to 23. Earlier items within the\n     21  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.71 0.00 0.00\n     22  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00    same partition receive substantially lower attention.\n     23  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.04 0.00\n     24  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.21 0.08       Figure 4b shows attention patterns when the\n                0.00                     0.00                          0.00                              0.00                                   0.00                                        0.00                                            0.00                                                 0.00                                                     0.00                                                          0.00                                                               0.00                                                                   0.00                                                                        0.00                                                                             0.00                                                                                 0.00                                                                                      0.00                                                                                           0.00                                                                                               0.00                                                                                                    0.00                                                                                                        0.00                                                                                                             0.00                                                                                                                  0.05                                                                                                                      0.74                                                                                                                           0.92     25  0.00\n         1            2                3                   4                      5                         6                             7                                8                                   9                                      10                                         11                                            12                                                13                                                   14                                                      15                                                         16                                                             17                                                                18                                                                   19                                                                      20                                                                          21                                                                             22                                                                                23                                                                                   24                                                                                      25    model generates the final answer. The “:” token\n                    Number of Items in Sequence              and the whitespace token immediately preceding\n                                                      the final number attend strongly to all intermediate\nFigure 3: Decoded output probabilities for the unstruc-\n                                                   reasoning numbers produced earlier (e.g., 3, 4, andtured baseline method on Qwen2.5 7B. The heatmap\nshows the decoded probabilities of model outputs, aver-    5). The attention mass is distributed across these\naged over different item types, for target counts ranging    tokens, with a clear concentration in the same lay-\nfrom 1 to 25. As the count increases beyond 10, the di-    ers identified for intermediate steps. Attention to\nagonal entries gradually fade, indicating reduced model    the original input items is weak at this stage.\nconfidence.                                           To further localize the attention pathways in-\n                                                 volved in intermediate reasoning and final aggrega-\nreported in Tables 1 and 2. Across all models, we\n                                                           tion, we focus on the most relevant tokens identi-\nfind that only one configuration consistently suc-\n                                                           fied in Figure 4. We average attention values across\nceeds on long contexts: structured inputs combined\n                                                        different configurations, including item types and\nwith intermediate reasoning steps.\n                                                          partition sizes. Figure 5 shows that attention peaks\n   Surprisingly, encouraging CoT reasoning alone,\n                                                      in layers 19 to 23, highlighting the central role of\nwithout structured input, does not yield a notable\n                                                    these layers in information transfer and aggrega-\nimprovement. Furthermore, structured input with-\n                                                           tion. Figure 6 further shows that the most influen-\nout intermediate steps is also ineffective and can be\n                                                                  tial heads are concentrated in layers 21, 22, and\nharmful in some cases. A likely explanation is that\n                                                   23. For example, head 13 in layer 22 consistently\nmodels must first consolidate partial results in inter-\n                                                       exhibits high attention to the selected tokens.\nmediate steps before aggregating them into the final\n                                                        Together, these results suggest a staged computa-\nanswer through a two-stage process. Our observa-\n                                                             tion. First, each partition is counted independently,\ntional and causal analyses provide further insight\n                                                with the final tokens of the partition encoding the\ninto why models cannot simultaneously gather par-\n                                                        local count. Second, these local counts are written\ntial counts from the context and add them up with-\n                                                      into intermediate reasoning tokens.  Finally, the\nout loss. Overall, these results show that neither\n                                           model attends to these intermediate tokens and ag-\nexternal structure nor reasoning alone is sufficient.\n                                                   gregates their values to produce the final answer.\nTheir combination is necessary to overcome large-\n                                              These observations are consistent with a hypotheti-\nscale counting failures.\n                                                         cal mechanism that separates counting, information\n4  Attention Analysis                               transfer, and aggregation into distinct components.\n\nWe analyze attention patterns of Qwen2.5 7B (Yang   5  Causal Mediation Analysis\net al., 2024a) to understand how the proposed\nSystem-2 strategy enables large-scale counting. We   To assess the hypothesis of a multi-stage System-\nfirst identify which tokens are attended to when   2 counting mechanism suggested by the attention\nthe model generates (i) intermediate partition-level    analysis, we perform a set of causal mediation ex-\ncounts and (ii) the final aggregated answer. We   periments based on activation patching (Heimer-\nthen localize the layers and heads that contribute   sheim and Nanda, 2024; Zhang and Nanda, 2024),\n\n\n                                         4\n\n                                                                                                     Token-Level Attention Generated Outputs to Generated Outputs           Token-Level Attention from Generated Outputs to Input Tokens\n                                                                                                                                                                             part\n        part\n                                                                                                                                                                 1\n          1\n                                                                                                                                                                                                                                                   :\n               :                                                                                                                0.06                                                                                                                                                 0.12\n                                                                                                                                                                 3\n          3\n\n                                                                                                                           0.05                                      part                                                                                                           0.10        part\n                                                                                                                                                                 2\n          2\n                                                                                                                                                                                                                                                   :\n               :                                                                                                                0.04                                                                                                                                                 0.08\n                                                                                                                                                                 4          4                                                                                                                                                                                                                      TokensTokens\n                                                                                                                           0.03                                      part                                                                                                           0.06        part\n          3                                                                                                                                                       3Generated         :                                                                                                                                                                                                                                                                                                                                 Generated         :\n                                                                                                                           0.02                                                                                                                                                 0.04\n          6                                                                                                                                                       5\n\n       Final                                                                                                                0.01                                      Final                                                                                                           0.02\n     answer                                                                                                                                               answer\n               :                                                                                                                                                                                                                                   :\n                                                                                                                           0.00\n          1                                                                                                                                                       1\n                                                                                                                                                                 2          3\n\n\n\n\n      ,  ,  |  ,  ,  ,  |  ,  ,  ,  ,  ,\n                           object            object            object            object            object            object            object            object            object            object            object            object            object                                                                          part 1 :  3        part 2 :  4        part 3 :  5          Final   answer :  1 2\n                                      Tokens of List                                                                                            Generated Tokens\n\n     (a) Attention of generated tokens to input tokens.                  (b) Attention of generated tokens to generated tokens.\nFigure 4: Attention patterns of selected tokens under the System-2 counting strategy. Attention values are averaged\nacross layers 19 to 23, all heads, and all item types (e.g., different fruit names).\n\n\n\n\n\n                                                                                                                                                                             0\n        0.05                                                                                                                                                           1\n                    Output2Input                                                                                                                         2\n                   Output2Output                                                                                                                      3\n\n                                                                                                                                                                             4\n        0.04                                                                                                                                                           5\n                                                                                                                                                                             6     Tokens                                                                                                                                                                  7                                                                                                                                 0.20\n\n        0.03                                                                                                                                                          109       Selected                                                                                                                                                                  8\n   the                                                                                                                                                                 1211                                                                                                                                 0.15\n  of                                                                                                                                                                 13\n        0.02                                                                                                                                                                                   Layers  14\n                                                                                                                                                                            15\n                                                                                                                                                                            16                                                                                                                                 0.10        Attention                                                                                                                                                                 17\n        0.01                                                                                                                                                          18\n                                                                                                                                                                            19    AVG.                                                                                                                                                                 20                                                                                                                                 0.05\n\n                                                                                                                                                                            21\n        0.00                                                                                                                                                          2223\n    0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27                          2425                                                                                                                                 0.00\n                                     Layer                                                                                    26\n\n                                                                                                                                                                            27\n\n                                                               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\nFigure 5: Average attention of correctly generated in-                                         Heads\n                                                      Figure 6: Heatmap of average attention to final itemstermediate counts to the final item of their partition,\n                                                        of partitions across layers and heads for intermediatealongside attention of the final answer to partition-level\n                                                      counting steps. Layers and heads responsible for trans-counts, across layers. Higher attention values are ob-\n                                                            ferring numerical information from input partitions toserved from layer 19 to 23 for both paths.\n                                                         intermediate counts appear with higher intensity.\n\nmasking ablation, and attention knockout (Geva\n                                           method (Hasani et al., 2025b) to decode the im-\net al., 2023). The goal is to identify where partition-\n                                                                plicit count associated with tokens. CountScope is\nlevel count information is stored, how it is trans-\n                                                  a causal probing method that patches the activation\nferred and consolidated, and which components are\n                                                    of a target token into a blank counting context and\ncausally required for correct aggregation.\n                                               decodes the model’s output as the implied count.\n                                                   Figure 7 shows the average decoding probabil-\n5.1  Token-Level Information Probing\n                                                              ity of ground-truth numbers across different token\nWe first examine where partition-level counts are    types. Ground-truth numbers correspond to the\nencoded in the input context. This analysis requires   number of items in each partition. The results show\nvocabulary projection and probing tools. We ob-    that the ground-truth value is encoded with high\nserve that existing tools, such as logit-lens (Nos-   confidence primarily at the final item and the final\ntAlgebraist, 2020) and tuned-lens (Belrose et al.,  comma separator of each partition. For the first\n2023), are not reliable for decoding numerical    partition, the confidence at the final item is high;\ninformation. We therefore use the CountScope   however, for subsequent partitions, the confidence\n\n\n                                         5\n\n                                                                                                       Part1\n                                                                                                       Part2\n                                                                                                       Part3\n         0.8                                                                                            Part4\n\n           Confidence 0.60.4\n\n\n\n         0.2\n\n\n\n         0.0\n                    Last Item        Last Comma        Vertical Bars     Non Last Items   Non Last Commas\n\nFigure 7: Ground-truth probabilities across different to-\nkens of partitions decoded by CountScope. Probabilities\nare averaged over different item types and configura-\ntions (3 to 9 items per partition).\n\n             Average Ground-Truth Probability Drop after Masking\n      1.0   Target partition\n                       1\n                       2                       3                                             Figure 9: Heatmap of the average probability drop after      0.8\n  Drop                  4                                                  attention knockout across layers and heads for interme-\n    Probability  0.6                                                         diateknockout(top)isandappliedfinal countingonly to thestepsidentified(bottom).effectiveAttentionto-\n   Average  0.4                                                kens from the input context and intermediate steps.\n      0.2\n      0.0                                          quence and extracting token embeddings, we re-\n               3            4            5            6            7            8            9\n                                  Number of items in target partition                   place the activation values of selected tokens with\n                                                   zeros across all layers.\nFigure 8: Average ground-truth probability drop after\n                                                     Figure 8 shows the average probability drop aftermasking the final item and comma (e.g., ‘, apple’) from\n                                                 zero ablation of the final item and final commaeach partition, showing the effect on the target count for\ndigit sizes ranging from 3 to 9.                           separator. This intervention leads to a sharp drop in\n                                                    the probability of generating the correct partition-\nat this token decreases, while the confidence at    level count in the corresponding reasoning step. For\nthe vertical bar character increases. Nevertheless,   example, ablating the final “, apple” token of a four-\nthe final comma separator reliably stores the la-   item partition significantly reduces the likelihood\ntent count of the corresponding partition. These    of generating the token “4”.\nfindings are consistent with the attention results\n                                                    5.3  Information Pathway Localizationin Section 4, which showed that decoded numbers\nof intermediate steps attend most strongly to these   To perform fine-grained circuit localization, we an-\nfinal partition tokens.                                alyze the pathways responsible for System-2 count-\n  More interestingly, this experiment reveals that    ing using attention knockout (Geva et al., 2023).\nat each partition boundary, the count resets and  We selectively block individual attention heads and\nbegins again. As a result, the final item of each    layers and measure the resulting drop in counting\npartition encodes the number of items counted    accuracy for intermediate and final steps. Figure 9\nsince the beginning of that partition. This observa-   shows that for Qwen2.5 7B, the most influential\ntion explains why the System-2 strategy avoids the   components are concentrated in layer 22. In partic-\nlarge-number failure observed in unstructured in-    ular, attention head 13 is most important for inter-\nputs. Each partition is counted independently, and   mediate steps, while head 1 is most important for\nits size remains within the range where the model’s    the final aggregation step.\nimplicit counter is accurate.                          Notably, head 13 in layer 22 also exhibits the\n                                                   highest attention values in the attention heatmap\n5.2  Token-Level Causal Interventions                                          shown in Figure 6. Compared to Figure 6, the pat-\nTo test whether the identified tokens causally medi-    tern in Figure 9 is considerably sparser. A plausible\nate the model’s behavior, we perform zero ablation    explanation is the presence of parallel information\non the final items and separators of each partition.   pathways, where knocking out a single pathway\nSpecifically, after processing the entire input se-   does not fully disrupt the computation because\n\n\n                                         6\n\n          Standard Setting                Patched Setting                                                                                              1     count accordingly. In the given example, the total\n                                                                                              2     sums of the first and second contexts are 19 and\n  Context1        -0.00            -10.20           Context1        -8.37             -2.93             3\n                                                                                              4                                                                                              5log-prob    14 before activation patching. After swapping the\n                                                 numerical contents of their second steps (shown in                                                                                              6mean\n  Context2       -10.20             -0.00           Context2        -2.93             -8.35             7      blue), the total for the first context becomes 21 and\n                                                                                              8\n                                                                                              9      the total for the second context becomes 12. This\n             GT1         GT2                    GT1         GT2\n                                           shows that the final sum is causally mediated by\nFigure 10: Average log probabilities of the first and    the intermediate-layer embeddings of the tokens\nsecond contexts before (left) and after (right) activation    “:”, whitespace, and the partition number. Figure 10\npatching. The first and second columns correspond to   shows the log probabilities for this experiment, av-\nthe predicted outputs (total sums) of the first and second                                                 eraged over various configurations.\ncontexts. After swapping the layer embeddings of the\nselected tokens from a given partition between the two\ncontexts, the output no longer follows the original total   6  Related Work\nsum and instead reflects the transferred number. Values\nare averaged across different configurations (selected   Counting in LLMs. A growing body of work\npartitions, partition sizes, and item types).            shows that counting in LLMs is brittle:  perfor-\n                                          mance can hinge on surface form and segmentation\nother pathways can partially compensate. Finally,    rather than a stable procedure (Fu et al., 2024).\nwe observe that the heads responsible for trans-   Subword tokenization can blur item boundaries\nferring partition-level information from the input   and measurably affect counting accuracy (Zhang\ncontext to the intermediate steps are not necessar-    et al., 2024). More broadly, deterministic-task eval-\nily the same as those responsible for transferring    uations (including counting) can flip under minor\ninformation from the intermediate reasoning steps    prompt/content changes, complicating extrapola-\nto the final answer, even when they are located in    tion from narrow setups (Ball et al., 2024). On the\nthe same layer.                                    theory side, transformers can exactly count token\n                                                   frequencies only in specific regimes, with feasi-\n5.4  Cross-Context Activation Patching\n                                                            bility tied to representation capacity and context-\nFinally, we perform cross-context activation patch-   length scaling (Yehudai et al., 2024).\ning to test how partition-level information is com-\nbined to form the final answer. We focus on in-   Numerical representations and mechanistic\ntermediate responses and final aggregation steps.   views.  Cognitive work argues for specialized\nOur attention and masking analyses indicate that   number systems and structured magnitude repre-\ntransferred numerical information from the context    sentations (Feigenson et al., 2004; Dehaene, 2011),\nto intermediate steps is consolidated in the specific    motivating analogous questions in neural models.\ntokens “:”, whitespace, and the partition number.   Representation analyses study how numerical mag-\nHere, we investigate this behavior using a causal    nitude is organized in LLM hidden spaces and re-\nintervention setup (Geiger et al., 2021).                late these structures to human-like effects (AlquBoj\n  To this end, we sample two different contexts,    et al., 2025).  Mechanistic studies localize how\neach containing four partitions, with partition sizes   counting signals emerge and update across items\nrandomly chosen between 3 and 9. After generat-    in controlled settings (Golkar et al., 2024). Clos-\ning the intermediate steps, we select one partition    est to our setting, Hasani et al. (2025b) combine\nand transfer the embeddings from layers 18 to 24   a target-context probe with layerwise/token-level\nof the target tokens between the two responses. For    analyses and causal interventions to identify latent\nexample, let “part 1: 7, part 2: 4, part 3: 8” be    counter-like signals across both LLMs and LVLMs.\nthe intermediate steps of the first context and “part\n1: 5, part 2: 6, part 3: 3” be those of the second   Decomposition and reasoning traces.  Chain-of-\ncontext. We swap the intermediate activations of   Thought can improve multi-step reasoning by elic-\nthe tokens highlighted in blue between the two    iting intermediate computations (Wei et al., 2022),\nresponses. After this intervention, we allow the    but explicit traces are not necessarily faithful. Ques-\nmodel to generate the final response.                 tion decomposition work studies when decompo-\n  This manipulation causally affects the corre-    sition increases faithfulness versus mainly serving\nsponding intermediate steps and changes the final    as scaffolding (Radhakrishnan et al., 2023).\n\n\n                                         7\n\nCounting in LVLMs.  Counting remains chal-   7  Conclusion\nlenging for LVLMs, particularly under clutter, lay-\n                                                  This paper showed that the failure of large language\nout variation, and compositional settings that stress\n                                            models on large-scale counting tasks arises from\nbinding between category and quantity (Guo et al.,\n                                                       reliance on System-1–like implicit counting mech-\n2025; Campbell et al., 2024). Diagnostics and tar-\n                                              anisms with limited capacity (Kahneman, 2011;\ngeted modifications analyze where LVLM counting\n                                           Zhang et al., 2024). This problem reflects architec-\nbreaks and how performance can be recovered (Al-\n                                                           tural constraints of transformer models rather than\nghisi et al., 2025), while structured visual inputs\n                                                fundamental limits of numerical reasoning. We in-\nor supervision can improve counting (Izadi et al.,\n                                                 troduced a simple System-2 test-time strategy that\n2025; Qharabagh et al., 2024; Hasani et al., 2025a).\n                                            decomposes large counting problems into smaller\nMechanistic evidence further suggests layerwise la-\n                                                     sub-tasks and aggregates their results, enabling ac-\ntent counter signals distributed across visual tokens\n                                                      curate counting over long contexts without modify-\n(Hasani et al., 2025b).\n                                                   ing model parameters or training procedures.\nInterpretability tools.  Causal interventions such      In addition to behavioral improvements, we\nas activation patching help identify computation-   provided a mechanistic explanation of how this\ncritical states (Heimersheim and Nanda, 2024;    strategy operates  internally.   Using  attention\nGeiger et al., 2021), with best-practice guidance for    analysis and causal mediation methods (Hasani\nreliable metrics and methods (Zhang and Nanda,    et al., 2025b; Heimersheim and Nanda, 2024), we\n2024).  Patch-based inspection frameworks fur-   showed that partition-level counts are encoded at\nther systematize intervention/inspection configura-   boundary tokens, transferred through specific at-\ntions (Ghandeharioun et al., 2024). Readout-style    tention pathways to intermediate reasoning steps,\nprobes provide a complementary perspective by   and aggregated in middle-to-late layers to form the\ntracking when information becomes linearly de-    final answer. Intermediate reasoning tokens play a\ncodable across layers, including the Logit Lens    crucial role in this process, mediating the flow of\n(NostAlgebraist, 2020) and learned variants such    numerical information from input partitions to the\nas the Tuned Lens (Belrose et al., 2023).               final output.\n                                                     In principle, this procedure removes fixed up-\nOur work.  Our setup is closest to System-2-style\n                                                  per bound on countable size, as long as each\nprompting via intermediate steps: like CoT, we\n                                              sub-problem remains within the model’s reliable\nelicit explicit intermediate computations (Wei et al.,\n                                                 regime. While our experiments focus on count-\n2022), and like task decomposition, we solve the\n                                                        ing, the same analysis framework applies to other\ninstance by breaking it into sequential subprob-\n                                                  reasoning tasks where implicit representations sat-\nlems (Radhakrishnan et al., 2023). The key differ-\n                                                     urate and explicit decomposition enables correct\nence is that the decomposition is tied to a struc-\n                                                      behavior. This study highlights how structured test-\ntured input format: we first partition the input into\n                                                 time strategies can reveal and extend the computa-\nmarked segments and then use a CoT protocol that\n                                                        tional abilities of existing models, offering a path\nis aligned with this structure (produce segment-\n                                               toward both improved performance and deeper in-\nlevel subcounts, then aggregate). This creates ex-\n                                                              terpretability. Future research could explore further\nplicit stage boundaries that are absent in monolithic\n                                                     applications of this strategy to complex reasoning\ncounting prompts and lets us study how counting\n                                                      tasks in language and beyond.\nrepresentations evolve across stages. Mechanis-\ntically, our analysis aligns with prior work that                                            Limitations\nlocalizes latent counter-like signals and tests them\nwith interventions (Hasani et al., 2025b), while   This work focuses on a narrow counting task with\nshifting the question to how the structured CoT    limited object diversity, using simple repeated\ndecomposition reshapes internal computation on   nouns in synthetic contexts. The approach depends\nlong contexts. In this way, we connect behavioral   on structured prompts and assumes prior knowl-\nbrittleness observations (Ball et al., 2024; Zhang   edge of the model’s reliable counting range, which\net al., 2024) to a stage-wise mechanistic account of   may limit generalization. In addition, the effects of\nhow count information is formed, routed, and com-   tokenization and number representation are not iso-\nbined under structured prompting (Heimersheim    lated and may independently influence the results.\nand Nanda, 2024; Zhang and Nanda, 2024).\n\n\n                                         8\n\nReferences                                Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir\n                                                         Globerson. 2023. Dissecting recall of factual associa-\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama       tions in auto-regressive language models. In Proceed-\n  Ahmad, Ilge Akkaya, Florencia Leoni Aleman, and       ings of the 2023 Conference on Empirical Methods in\n   et al. 2023.  GPT-4 technical report.   Preprint,      Natural Language Processing, pages 12216–12235,\n  arXiv:2303.08774.                                     Singapore. Association for Computational Linguis-\n                                                                           tics.\nSimone Alghisi, Gabriel Roccabruna, Massimo Riz-\n   zoli, Seyed Mahed Mousavi, and Giuseppe Riccardi.   Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lu-\n  2025. [de|re]constructing vlms’ reasoning in count-      cas Dixon, and Mor Geva. 2024. Patchscopes: A\n   ing. Preprint, arXiv:2510.19555.                        unifying framework for inspecting hidden represen-\n                                                                 tations of language models. In International Confer-\nH. V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic,      ence on Machine Learning (ICML).\n  Tatsuya Hiraoka, Ahmed Oumar El-Shangiti, Mu-\n  nachiso Nwadike, and Kentaro Inui. 2025. Number    Siavash Golkar, Alberto Bietti, Mariel Pettee, Michael\n   representations in llms: A computational parallel to      Eickenberg, Miles Cranmer, Keiya Hirashima, Ger-\n  human perception. In International Conference on      aud Krawezik, Nicholas Lourie, Michael McCabe,\n  Learning Representations (ICLR).                  Rudy Morel, Ruben Ohana, Liam Holden Parker,\n                                                  Bruno Régaldo-Saint Blancard, Kyunghyun Cho, and\n                                                             Shirley Ho. 2024. Contextual counting: A mechanis-Thomas Ball, Shuo Chen, and Cormac Herley. 2024.\n                                                                            tic study of transformers on a quantitative task. arXiv  Can we count on llms?  the fixed-effect fallacy\n                                                             preprint arXiv:2406.02585.  and claims of gpt-4 capabilities.  arXiv preprint\n  arXiv:2409.07638.\n                                               Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\n                                                 Abhinav Pandey, Abhishek Kadian, Ahmad Al-\nNora Belrose, Igor Ostrovsky, Lev McKinney, Zach Fur-                                                       Dahle, and 1 others. 2024.  The llama 3 herd of\n  man, Logan Smith, Danny Halawi, Stella Biderman,                                                       models. arXiv preprint arXiv:2407.21783.\n  and Jacob Steinhardt. 2023. Eliciting latent predic-\n   tions from transformers with the tuned lens. arXiv                                            Xuyang Guo, Zekai Huang, Zhenmei Shi, Zhao Song,\n   preprint arXiv:2303.08112.                                                  and Jiahao Zhang. 2025.  Your vision-language\n                                                model can’t even count to 20: Exposing the fail-\nDeclan Campbell, Sunayana Rane, Tyler Giallanza,      ures of vlms in compositional counting.  Preprint,\n  Nicolò De Sabbata, Kia Ghods, Amogh Joshi,      arXiv:2510.04401.\n  Alexander Ku, Steven M. Frankland, Thomas L. Grif-\n   fiths, Jonathan D. Cohen, and Taylor W. Webb. 2024.   Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari,\n  Understanding the limits of vision language mod-     Mobin Bagherian, Sadegh Mohammadian, Moham-\n   els through the lens of the binding problem. arXiv     mad Izadi, and Mahdieh Soleymani Baghshah. 2025a.\n   preprint arXiv:2411.00238.                           Uncovering grounding ids: How external cues shape\n                                                      multimodal binding. Preprint, arXiv:2509.24072.\nStanislas Dehaene. 2011. The Number Sense: How\n   the Mind Creates Mathematics, 2nd edition. Oxford    Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari,\n   University Press, New York, NY.                   Mobin Bagherian, Sadegh Mohammadian, Moham-\n                                            mad Izadi, and Mahdieh Soleymani Baghshah. 2025b.\nLisa Feigenson,  Stanislas Dehaene, and Elizabeth      Understanding counting mechanisms in large lan-\n   Spelke. 2004. Core systems of number. Trends in      guage and vision-language models. arXiv preprint\n  Cognitive Sciences, 8(7):307–314.                      arXiv:2511.17699.\n\n                                                       Stefan Heimersheim and Neel Nanda. 2024. How to\nTairan Fu, Raquel Ferrando, Javier Conde, Carlos Ar-\n                                                        use and interpret activation patching. arXiv preprint\n   riaga, and Pedro Reviriego. 2024. Why do large lan-\n                                                        arXiv:2404.15255.\n  guage models (llms) struggle to count letters? arXiv\n   preprint arXiv:2412.18626.                                                Kuinan Hou, Marco Zorzi, and Alberto Testolin. 2025.\n                                                         Sequential enumeration in large language models.\nAndrew Gambardella, Yusuke Iwasawa, and Yutaka                                                              Preprint, arXiv:2512.04727.\n  Matsuo. 2024. Language models do hard arithmetic\n   tasks easily and hardly do easy arithmetic tasks. In   Amirmohammad Izadi, Mohammad Ali Banayeean-\n  Proceedings of the 62nd Annual Meeting of the As-      zade, Fatemeh Askari, Ali Rahimiakbar, Moham-\n   sociation for Computational Linguistics (Volume 2:     mad Mahdi Vahedi, Hosein Hasani, and Mahdieh\n  Short Papers), pages 85–91, Bangkok, Thailand. As-      Soleymani Baghshah. 2025. Visual structures helps\n   sociation for Computational Linguistics.                  visual reasoning: Addressing the binding problem\n                                                               in VLMs. arXiv:2506.22146. Accepted to NeurIPS\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-       (poster).\n  pher Potts. 2021. Causal abstractions of neural net-\n  works. In Advances in Neural Information Process-    Daniel Kahneman. 2011. Thinking, Fast and Slow. Far-\n   ing Systems, volume 34.                                          rar, Straus and Giroux, New York, NY.\n\n\n                                         9\n\nNostAlgebraist. 2020. Interpreting gpt: The logit lens.      can transformers count to n?    arXiv preprint\n  LessWrong post.                                      arXiv:2407.15160.\n\nMuhammad   Fetrat  Qharabagh,  Mohammadreza    Fred Zhang and Neel Nanda. 2024. Towards best prac-\n   Ghofrani, and Kimon Fountoulakis. 2024. LVLM-       tices of activation patching in language models: Met-\n  COUNT:  Enhancing  the  counting  ability  of        rics and methods. In International Conference on\n   large vision-language models.    arXiv  preprint      Learning Representations (ICLR).\n  arXiv:2412.00686.\n                                               Xiang Zhang, Juntai Cao, and Chenyu You. 2024.\nAnsh Radhakrishnan, Karina Nguyen, Anna Chen,      Counting ability of large language models and impact\n  Carol Chen, Carson Denison, Danny Hernandez,      of tokenization. arXiv preprint arXiv:2410.19730.\n  Esin Durmus, Evan Hubinger, Jackson Kernion,\n   Kamil¯e Lukoši¯ut¯e, Newton Cheng, Nicholas Joseph,\n  Nicholas Schiefer, Oliver Rausch, Sam McCan-\n   dlish,  Sheer El Showk, Tamera Lanham, Tim\n  Maxwell, Venkatesa Chandrasekaran, and 5 others.\n  2023. Question decomposition improves the faithful-\n  ness of model-generated reasoning. arXiv preprint\n  arXiv:2307.11768.\n\nFabian Retkowski and Alexander Waibel. 2025. Zero-\n   shot strategies for length-controllable summarization.\n   In Findings of the Association for Computational Lin-\n   guistics: NAACL 2025, pages 551–572, Albuquerque,\n  New Mexico. Association for Computational Linguis-\n   tics.\n\nGemini Team. 2025. Gemini 2.5: Pushing the fron-\n   tier with advanced reasoning, multimodality, long\n   context, and next generation agentic capabilities.\n  arXiv:2507.06261.\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\n   Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\n  Tatiana Matejovicova, Alexandre Ramé, Morgane\n   Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey\n   Cideron, Jean bastien Grill, Sabela Ramos, Edouard\n  Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev,\n  and 197 others. 2025. Gemma 3 technical report.\n   Preprint, arXiv:2503.19786.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n   Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\n   Kaiser, and Illia Polosukhin. 2017. Attention is all\n  you need. In Advances in Neural Information Pro-\n   cessing Systems, volume 30, pages 5998–6008.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc Le,\n  and Denny Zhou. 2022. Chain-of-thought prompt-\n   ing elicits reasoning in large language models. In\n  Advances in Neural Information Processing Systems,\n  volume 35, pages 24824–24837.\n\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\n  Bo Zheng, Bowen Yu, and 1 others. 2024a. Qwen2.5\n  Technical Report. arXiv preprint arXiv:2412.15115.\n\nHaotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, and\n  Muhan Zhang. 2024b. Number cookbook: Number\n  understanding of language models and how to im-\n  prove it. Preprint, arXiv:2411.03766. ICLR 2025\n   poster.\n\nGilad Yehudai, Haim Kaplan, Asma Ghandeharioun,\n  Mor Geva, and Amir Globerson. 2024.  When\n\n\n                                         10\n\nA  Appendix                                 Unstructured Input (w/o steps)\n\n                                             You will be given a list of items.\nA.1  Details of the Behavioral Setup\n                                             Count the total number of objects.\n                                             Output the final result exactly in the\nWe evaluate counting performance under controlled\n                                             following format:\nvariations of input format and prompting strategy.\n                                             Final answer: [x]\nEach example consists of a list of repeated single-\nword items, and the model is required to output the\ntotal count. Following the evaluation protocol sum-     Unstructured Input (w/ steps)\nmarized in Tables 1 and 2, we vary (i) whether the\n                                             You will be given a list of items.\ninput is structured or unstructured, and (ii) whether\n                                             Count the total number of objects.\nthe model is explicitly encouraged to produce in-\n                                             Let’s count step by step.\ntermediate reasoning steps (with steps vs. without\n                                             Output the final result exactly in the\nsteps). Performance is reported using exact-match\n                                             following format:\naccuracy and mean absolute error (MAE) across\n                                             Final answer: [x]\ndifferent context-length ranges.\n\n                                                 Structured Input (w/o steps)\nItem types.  All inputs are constructed from sim-\nple, common nouns drawn from two semantic cate-    You will be given multiple partitions\ngories: fruits and animals. The complete candidate     of content separated by “|”.\n                                             For each partition, count the numberset is shown below.\n                                             of items it contains.\n                                             After   counting   all   partitions,\n  Items\n                                             compute  the  total  by  summing  the\n  apple,  orange,  peach,  fig,  mango,      counts.\n  pear, coconut, cherry, plum,               Output  the  final  result  exactly  in\n  cat, dog, horse, rabbit, whale, cow,      the following format:\n  frog                                       Final answer: [x]\n\n\n                                                 Structured Input (w/ steps)\nInput formats.  In the unstructured setting, the\n                                             You will be given multiple partitionsinput is a flat, comma-separated list of n identical\n                                             of content separated by “|”.items. In the structured setting, the same multiset\n                                             For each partition:is divided into multiple partitions separated by a\n                                             -  Count  the  number  of  items  itvertical bar (|), enabling explicit local counting\n                                             contains.followed by summation.\n                                             - Report the count separately using\n                                             the format:\nPartition sizes.  For open-source models (Ta-     part1: [x1]\nble 1), each partition contains between 6 and 9     part2: [x2]\nitems. For closed-source models (Table 2), which     ...\nare evaluated on larger context lengths, partition     After counting all partitions:\nsizes range from 15 to 25 items. In all cases, par-    -  Compute  the  total  by  summing  all\ntitions are constructed such that their sizes sum     individual counts.\nexactly to the target length n.                     - Output the final total exactly in\n                                             this format:\n                                             Final answer: [x]\nPrompting strategies.  For each input format, we\nevaluate two prompting strategies: without inter-\n                                            A.2  Attention Analysismediate steps and with intermediate steps. These\nsettings correspond directly to the “w/o steps” and    This section presents detailed methodological infor-\n“w/ steps” rows reported in Tables 1 and 2.          mation regarding the attention analyses conducted\n\n\n                                         11\n\nin this study. We perform our analysis using various   Gemma 3 4B, we examine layers 21 to 23, while for\nlarge language models, Qwen 2.5 7B (Yang et al.,   Llama 3.2 8B we consider layers 13 to 18. These\n2024a), Llama 3.2 8B (Grattafiori et al., 2024), and    layer intervals are selected based on the average at-\nGemma 3 4B(Team et al., 2025). These models    tention magnitude from output tokens to key input\ndiffer in architectural depth and layer organization,   tokens (specifically, the last item and the trailing\nwhich allows for a more comprehensive and com-  comma of the predicted partition), which consis-\nparative investigation of attention behaviors. The    tently peak within these ranges.\nanalysis is divided into two parts. In the first part,      Figures 12, 13, and 14 illustrate the full atten-\nwe describe the experimental setup in detail, in-    tion maps for Qwen2.5 7B, Llama 3.2 8B, and\ncluding the selection of specific layers for analysis  Gemma 3 4B, respectively. In each figure, the x-\nand the prompts used in each experiment. In the    axis corresponds to output tokens and the y-axis\nsecond part, we analyze the attention patterns of    corresponds to input tokens. Across all models, we\nthe generated outputs with respect to the full input    observe a consistent pattern: the token immediately\nsequences for both models.                        preceding the generated number of parts exhibits\n                                                    the highest attention weights toward the last item\nA.2.1  Input Details\n                                            and the final comma of the desired partition. This\nThe exact prompt used in all experiments is shown    behavior supports our hypothesis that models rely\nbelow:                                              heavily on attention to the final item–comma struc-\nYou will be given a list of items, where groups      ture of each part when determining and generating\n     (partitions) are separated by the \"|\"           the number of items for each part.\n     character.\n\nFor each partition:                            A.3  Causal Mediation Analysis\n- Count the number of items it contains.\n- Report the count separately using the format:     Token-Level Information Probing  To decode\n  part1: x1                                           the latent numbers of specific tokens, we employed\n  part2: x2                                             CountScope. We modified the original source\n  part3: x3\n  ...                                         prompt to facilitate accurate counting by partition-\n                                                  ing the input list using vertical bar delimiters (|).\nAfter counting all partitions:\n                                                    Additionally, we utilized a monotypic, question-- Compute the total by summing all individual\n     counts.                                                     first configuration. The exact prompt structure is\n- Output the final total exactly in this format:    provided below:\nFinal answer: x\nJust answer in this format without any extra        Answer the question with just a number only\n     things and follow the instructions.                 (We've separated each group of items with\n                                                         \"|\" so you can calculate the final count\napple, apple, apple | apple, apple, apple,               easier).\n     apple | apple, apple, apple, apple, apple       Question: How many fruits are there in the\n                                                         following sentence?\n  To evaluate robustness across varying input struc-    apple, apple, apple | apple, apple, apple,\n                                                         apple, apple | apple, apple, apple, apple\ntures, the final list provided to the model is ran-\ndomized across experiments. Additionally, to re-   To mitigate token-specific artifacts, we performed\nduce potential token-specific biases, we vary the     all experiments using a set of distinct items. (see\nitem labels used in the lists, drawing from the Ap-   Appendix A.1)\npendix A.1\n                                              Information Pathway Localization  To localize\nA.2.2  Full Attention Analysis                                                    the specific attention heads and layers mediating\nThis section presents a full attention analysis (fol-   the flow of count information, we performed at-\nlowing the methodology in Fig. 4) for Qwen2.5    tention knockout experiments as outlined in Sec-\n7B, Gemma 3 4B, and Llama 3.2 8B. Our goal is    tion 5.3. This analysis utilizes a prompt that elicits\nto explore a little more of the layers in which at-    specific System-2 behavior by explicitly separat-\ntention most strongly concentrates on the tokens    ing partition counting from final aggregation. We\nrelevant to partition prediction, and to visualize the   employed a strict instructional format forcing the\nresulting full attention patterns.                  model to output intermediate counts before the final\n  As shown in Fig. 11, we analyze attention scores     total. This allows us to measure the causal impact\nacross different layer ranges for each model. For    of blocking specific attention heads on the accuracy\n\n\n                                         12\n\n       0.05\n                   Output2Input                                                                                                  0.06         Output2Input\n                  Output2Output                                                                                             Output2Output\n\n\n       0.04                                                                                                                          0.05\n    Tokens                                                                                                                                                                                         Tokens\n                                                                                                                                     0.04\n       0.03     Selected                                                                                                                                                                                                                                                      Selected\n  the                                                                                            the 0.03\n of                                                              of\n      Attention 0.02                                                                                                                                                                                                                                                                          Attention 0.02\n  AVG.                                                                                                                           AVG.\n       0.01\n                                                                                                                                     0.01\n\n\n\n       0.00                                                                                                                          0.00\n\n\n    012345678910111213141516171819202122232425262728293031            0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n                                    Layer                                                                                 Layer\n\n(a) Average attention weights across layers for Llama             (b) Average attention weights across layers for Gemma\n3.2 8B. We report attention from selected output tokens          3 4B. We report attention from selected output tokens\nto both input and output tokens.  Attention to the key             to both input and output tokens (as defined in Fig. 4a).\npartition-related tokens peaks consistently between layers            Attention magnitude peaks between layers 21 and 23.\n13 and 18, motivating our choice of this interval for full\nattention visualization.\n\nFigure 11: Layer-wise attention analysis for Llama 3.2 8B and Gemma 3 4B. For each model, we visualize the\naverage attention from selected output tokens to salient input tokens (notably the final item and comma of the\npartition). These trends are used to identify the layer ranges with the strongest partition-relevant attention.\n\n\nof both intermediate transfer and final summation\n(the exact prompt template is provided below).\n\nI will provide an input text containing fruits\n     separated by the '|' delimiter. Your goal\n     is to count the items in each section and\n     provide a summation.\n\nInput Text:\n apple, apple, apple | apple, apple, apple,\n     apple, apple | apple, apple, apple, apple\n\nTask:\n1. Identify each partition separated by '|'.\n2. Count the number of fruits in each specific\n     partition.\n3. Sum the counts of all partitions.\n\nYou must strictly follow this format, adapting\n     the number of parts to the actual input:\npart1: <count1>, part2: <count2>, ... , final\n     count: <total_count>\n\n\n\n\n\n                                         13\n\n                                                                                                                          [rest of attention]                                                      [rest of attention 2]\n                                                                                                                                 using                                                                              Output\n                   <|im_start|>                                                                                    the                                                                                    the\n                           user                                                                              format                                                                                                        final\n                                                                                                                                                :                                                                                                 total\n                          You                                                                                                                                                                                   exactly\n                                       will                                                                                       part                                                                                                                  in\n                          be                                                                            1                                                                                                   this\n                             given                                                                                                           :                                                                              format\n                            a                                                                              x                                                                                                           :\n                                       list                                                                            1                                                                                                   Final\n                             of                                                                                                                                                                    answer\n                          items                                                                                                                                                                                                                      :\n                                    ,                                                                                       part                                                                              x\n                       where                                                                            2\n                         groups                                                                                                           :                                                                                     object\n                                           (                                                                              x                                                                                                           ,\n                           part                                                                            2                                                                                     object\n                                itions                                                                                                                                                                                                                      ,\n                                           )                                                                                                                                                                                 object\n                 List   separatedare                                                                                       part3                                                                                     object|\n        of         by                                                                                                           :                                                                                                           ,\n                           the                                                                              x                                                                                     object\n                                  \"|\"                                                                            3                                                                                                           ,                         Tokens    character.                                                                                                                                                                                 object\n                                                                                                                                              ...                                                                                                           ,\n                           For                                                                                                                                                                                 object\n                         each                                                                                After                                                                                                              |\n                          partition                                                                                        counting                                                                                     object\n                                    :                                                                                                                            all                                                                                                           ,\n                         -                                                                                           partitions                                                                                     object\n                         Count                                                                                                           :                                                                                                           ,\n                           the                                                                            -                                                                                     object\n                      number                                                                           Compute                                                                                                           ,\n                             of                                                                                    the                                                                                     object\n                          items                                                                                                 total                                                                                                           ,\n                                         it                                                                               by                                                                                     object\n                         contains                                                                           sum                                                                                                           ,\n                                    .                                                                                    ming                                                                                     object\n                         -                                                                                                                            all                                                                            <|im_end|>\n                        Report                                                                                                 individual\n                           the                                                                                      counts                                                                              <|im_start|>\n                          count                                                                                                           .                                                                                        assistant\n                      separately                                                                            -\n                                               part 1 : 3      part 2 : 4      part 3 : 6       Final answer : 1 3                  part 1 : 3      part 2 : 4      part 3 : 6       Final answer : 1 3                  part 1 : 3      part 2 : 4      part 3 : 6       Final answer : 1 3\n                                       Generated Output Tokens                                         Generated Output Tokens                                         Generated Output Tokens\n\n\n\n\n\n                                                                      0.01                                 0.02                                 0.03                                 0.04                                 0.05                                 0.06                                 0.07\n                                                                                                                   Attention Weight\n\nFigure 12: Full attention map for Qwen2.5-7B. The x-axis corresponds to output tokens and the y-axis to input\ntokens. The token immediately preceding the generated number of parts exhibits the strongest attention toward the\nfinal item and trailing comma of each partition matches its own segment.\n\n\n\n\n                                                                                                                           [rest of attention]                                                   [rest of attention 2]\n                                                                                                                                                                                                                                                            this                                                                                                                                                            :                    <|start_header_id|>\n                                                                                                                                                                                                            format\n                                    user                                                                                                                                                                                                                                                                           :\n                     <|end_header_id|>                                                                                                                                  part\n                                                                                                                                                                                                                                                              Final\n                                                                                                               1\n                                 You                                                                                                                                                            :                                                                                 answer\n                                                                                                                                                                                                                                                                           :\n                                                  will                                                                                                                   x\n                                                                                                                                                                                                    x\n                                 be                                                                                                               1\n                                      given\n                                                                                                                                                                                                                     Just\n                                    a\n                                                                                                                                                                                                              answer\n                                                  list                                                                                                                                  part\n                                                                                                                                                                                                                                                                                               in\n                                     of                                                                                                               2\n                                                                                                                                                                                                                                                            this\n                                   items                                                                                                                                                            :\n                                                                                                                                                                                                            format\n                                              ,                                                                                                                   x\n                                                                                                                                                                                                                               without\n                               where                                                                                                               2\n                                                                                                                                                                                                                   any\n                                  groups\n                                                                                                                                                                                                                      extra\n                                                       (\n                                                                                                                                                                                                                                             things\n                                    part                                                                                                                                  part\n                                                                                                                                                                                                              and\n                                          itions                                                                                                               3\n                                                                                                                                                                                                                                                    follow\n                                                       )                                                                                                                                                            :\n                                                                                                                                                                                                                      the\n                                   are                                                                                                                   x\n                             separated                                                                              3                                                                                                   instructions.\n                                  by                List                                                                                                                                                                                                                             object\n                                   the                                                                                                                                                          ...        of                                                                                                                                                                                                                                                                           ,\n                                            \"|\"\n                                                                                                                                                                                                                             object\n                                                                                                                       After                                 character.                                                                                                                                                                                                                                                                           ,\n                                                                                                                                     counting                        Tokens                                                                                                                                                                                                                             object\n                                   For                                                                                                                                                                                       all\n                                                                                                                                                                                                                                                                                   |\n                                 each                                                                                                                                          partitions\n                                                                                                                                                                                                                             object                                                                                                                                                            :                                    partition\n                                                                                                                                                                                                                                                                           ,                                              :                                                                                                               -\n                                                                                                                                                                                                                             object\n                                -                                                                                                                Compute\n                                                                                                                                                                                                                                                                           ,\n                                 Count                                                                                                                            the\n                                                                                                                                                                                                                             object\n                                   the                                                                                                                                                total\n                                                                                                                                                                                                                                                                           ,\n                             number                                                                                                                     by\n                                                                                                                                                                                                                             object\n                                     of                                                                                                                sum\n                                                                                                                                                                                                                                                                                   |\n                                   items                                                                                                                             ming\n                                                                                                                                                                                                                             object\n                                                    it                                                                                                                                                                                       all\n                                                                                                                                                                                                                                                                           ,\n                                  contains                                                                                                                                                  individual\n                                                                                                                                                                                                                             object                                              .                                                                                                                                 counts\n                                                                                                                                                                                                                                                                           ,                                                                                                                                                            .                                -\n                                                                                                                                                                                                                             object\n                                Report                                                                                                               -\n                                                                                                                                                                                                                                                                           ,\n                                   the                                                                                                                      Output\n                                                                                                                                                                                                                             object\n                                   count                                                                                                                            the\n                                                                                                                                                                                                                                                                           ,\n                               separately                                                                                                                                                           final\n                                                                                                                                                                                                                             object\n                                       using                                                                                                                                                total\n                                                                                                                                                                                                                                                                           ,\n                                   the                                                                                                                                 exactly\n                                                                                                                                                                                                                             object\n                               format                                                                                                                                                                        in\n                                                                                                                                                                                                                         <|eot_id|>\n                                                            part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :   13                                                                                                                                                                                                    part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :   13                                                                                                                                                                                                                                                                                                                                            part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :   13\n                                          Generated Output Tokens                                      Generated Output Tokens                                      Generated Output Tokens\n\n\n\n\n\n                                                                              0.01                                  0.02                                  0.03                                  0.04                                  0.05                                  0.06                                  0.07\n                                                                                                                    Attention Weight\n\nFigure 13: Full attention map for Llama 3.2 8B. The x-axis corresponds to output tokens and the y-axis to input\ntokens. The token immediately preceding the generated number of parts exhibits the strongest attention toward the\nfinal item and trailing comma of each partition matches its own segment.\n\n\n\n                                         14\n\n                                                                                                                            [rest of attention]                                                      [rest of attention 2]\n                    <start_of_turn>                                                                                                                                                                                                                        this\n                                user                                                                                                                                                                               format\n                                                                                                                                  part                                                                                                                  :\n                              You                                                                                 1\n                                             will                                                                                                                  :                                                                                                           Final\n                              be                                                                                    x                                                                                     answer\n                                  given                                                                                 1                                                                                                                  :\n                                a                                                                                                                                                                        x\n                                             list\n                                  of                                                                                              part                                                                                         Just\n                               items                                                                                 2                                                                                     answer\n                                          ,                                                                                                                  :                                                                                                                           in\n                            where                                                                                    x                                                                                                          this\n                              groups                                                                                 2                                                                                    format\n                                                  (                                                                                                                                                                                               without\n                              partitions                                                                                                                                                                                     any\n                                                  )                                                                                              part                                                                                         extra\n                                are                                                                                 3                                                                                                   things\n                          separated                                                                                                                  :                                                                                       and\n                              by                                                                                    x                                                                                                     follow\n                                the                                                                                 3                                                                                          the\n                 List       character\"|\"                                                                                                                                                                                                            instructions.\n        of                  .                                                                                                                ...                                                                                            object\n\n\n                                                                                                                                                                                                                                  object                              each                                                                                               counting                         Tokens         partitionFor                                                                                      After                                                                                                                  ,                                                                                                                                                                                       all                                                                                                                                                                                                                                                                                ,\n                                          :                                                                                                  partitions                                                                                            object\n                                                                                                                                                             :                                                                                                                      |\n                              -                                                                                                                                                                                              object\n                              Count                                                                                 -                                                                                                                  ,\n                                the                                                                                Compute                                                                                            object\n                          number                                                                                          the                                                                                                                  ,\n                                  of                                                                                                        total                                                                                            object\n                               items                                                                                     by                                                                                                                  ,\n                                                it                                                                                   summing                                                                                            object\n                              contains                                                                                                                                     all                                                                                                                      |\n                                          .                                                                                                        individual                                                                                            object\n                                                                                                                                 counts                                                                                                                  ,\n                              -                                                                                                                  .                                                                                            object\n                             Report                                                                                                                                                                                                                                     ,\n                                the                                                                                 -                                                                                            object\n                               count                                                                                     Output                                                                                                                  ,\n                           separately                                                                                          the                                                                                            object\n                                   using                                                                                                                final                                                                                                                  ,\n                                the                                                                                                        total                                                                                            object\n                            format                                                                                             exactly                                                                                                                  ,\n                                          :                                                                                                                           in                                                                                            object\n                                                      part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :  1 3                    part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :  1 3                    part 1 :  3      part 2 :  4      part 3 :  6        Final  answer :  1 3\n                                        Generated Output Tokens                                         Generated Output Tokens                                         Generated Output Tokens\n\n\n\n\n\n                                                                       0.01                               0.02                               0.03                               0.04                               0.05                               0.06                               0.07                               0.08\n                                                                                                                     Attention Weight\n\nFigure 14: Full attention map for Gemma 3 4B. The x-axis corresponds to output tokens and the y-axis to input\ntokens. The token immediately preceding the generated partition-level count exhibits the strongest attention toward\nthe final item and trailing comma of each partition matches its own segment.\n\n\n\n\n\n                        Probability Drop by Layer Masking                                                 Probability Drop by Layer Masking\n                                                                                                                    4\n     12\n                                                                                                                    5\n     14\n Drop  16                                                                                                                                                      Drop   6\n\n     18\n  Probability     20                                                                                                                                                                                                                                                                                                                                                                                                                            Probability   78\n Log                                                                                                                    Log\n     22                                                                                                             9\n\n     24\n                                                                                                                   10\n     26\n            0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27                         0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n                                                   Layer                                                                                                       Layer\n\n                               (a)                                                                 (b)\n\nFigure 15: Average change in log probability of intermediate counts after layer masking (a) and unmasking (b) of\nthe final item and separator of each partition. In the masking experiment, embeddings from the target layer are\nzero-ablated. In the unmasking experiment, embeddings of the selected tokens are zero-ablated across all layers,\nand only the target-layer embeddings are restored from the clean run.\n\n\n\n\n\n                                         15\n\n## Claim to Classify\n\"latent counts are computed and stored in the final item representations of each part\"\nLocation in paper: abstract\n\n## Task\nClassify this claim's method rung and claim rung following the codebook."
  },
  "parameters": {
    "temperature": 0,
    "max_tokens": 8192
  },
  "response": {
    "method_used": "causal mediation analysis including activation patching and ablation",
    "method_rung": 2,
    "claim_rung": 3,
    "confidence": 5,
    "hedge_flag": 0,
    "reasoning": "The method used is causal mediation analysis, which includes activation patching and ablation (e.g., zero ablation of tokens, attention knockout), placing it at Rung 2 (Interventional). The claim states that 'latent counts are computed and stored in the final item representations', using the verbs 'computed' and 'stored', which are Rung 3 linguistic markers indicating mechanistic/functional attribution. 'Stored' implies a representational claim about the mechanism, not just causal influence. There is no hedge (e.g., 'may' or 'suggests'), and the claim is stated definitively. This matches the common overclaim pattern of using Rung 2 methods to support Rung 3 mechanistic claims, similar to the ROME and IOI circuit papers in the calibration set."
  },
  "metadata": {
    "model": "qwen/qwen3-235b-a22b-2507",
    "usage": {
      "prompt_tokens": 25901,
      "completion_tokens": 224,
      "total_tokens": 26125
    },
    "finish_reason": "stop"
  }
}