paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,hedge_flag,reasoning
2104.08164,2104.08164-01,The factual knowledge acquired during pre-training and stored in the parameters of Language Models,abstract,3,Hyper-network weight editing,1,3,2,4,0,"Claim uses 'stored in the parameters' — storage/memory language = R3 by decision tree. Method is a weight-editing hyper-network; the observation that knowledge is 'stored' is inferred from model editing success, not direct mechanistic proof. No activation patching or causal tracing; method_rung=1 (observational: weight update analysis)."
2104.08164,2104.08164-02,our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge,abstract,3,Hyper-network weight editing (analysis of update patterns),1,3,2,3,0,"Claim says hyper-network 'reveals' causal mediation mechanisms — functional/mechanistic language (R3). The evidence is which parameters the hyper-network updates, which is observational analysis of gradient/update patterns (R1). 'Probe' framing is metaphorical; no direct causal intervention on activations."
2104.08164,2104.08164-03,our analysis shows that the updates tend to be concentrated on a small subset of components,abstract,3,Hyper-network weight editing (analysis of update sparsity),1,1,0,4,0,Claim that 'updates tend to be concentrated on a small subset of components' is an empirical observation about the distribution of weight updates — purely associational/observational (R1 claim). Method is also R1: logging which components receive large updates. No causal language.
2104.08164,2104.08164-04,our hyper-network can be regarded as a probe revealing the causal mediation mechanisms,body,1,Hyper-network weight editing (analysis of update patterns),1,3,2,3,0,Claim that hyper-network 'reveals the causal mediation mechanisms' invokes mechanistic functional language (R3). Evidence is observational analysis of which weights are updated most. No direct causal intervention on model activations to establish mechanism. 'Causal mediation' borrowed from Vig et al. but not operationalized with patching here.
2104.08164,2104.08164-05,the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution,body,1,Hyper-network weight editing (analysis of update distribution),1,2,1,3,1,"Claim uses 'seems to be achieved by primarily modifying' — hedged (hedge_flag=1). 'Achieved by' suggests causal sufficiency (R2). Method is observational analysis of where weight updates concentrate (R1). Hedge softens to near-R1 but the core claim ('achieved by') is causal-mechanism language, so R2."
2202.05262,2202.05262-01,factual associations correspond to localized directly-editable computations,abstract,3,Causal tracing (activation patching),2,3,1,5,0,Claim: 'factual associations correspond to localized directly-editable computations' — 'correspond to localized computations' is identity/mechanistic claim (R3). Method is causal tracing (R2). Classic R2→R3 overclaim per calibration rationale: localization claim goes beyond what causal mediation establishes.
2202.05262,2202.05262-02,a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions,abstract,3,Causal tracing (activation patching),2,2,0,5,0,Claim: 'a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions.' 'Mediate' is appropriate R2 language for causal intervention results. Causal tracing establishes mediation. No uniqueness/storage claim here. Well-matched claim-method pair.
2202.05262,2202.05262-03,mid-layer feed-forward modules... storing factual associations,abstract,3,Causal tracing + ROME weight editing,2,3,1,5,0,"Claim: 'mid-layer feed-forward modules... storing factual associations.' 'Storing' is memory/mechanistic language = R3 per decision tree and calibration rationale. Causal tracing (R2) establishes mediation, not storage as a mechanism. Canonical R2→R3 overclaim from the ROME calibration case."
2202.05262,2202.05262-04,ROME is effective on a standard zero-shot relation extraction model-editing task,abstract,3,ROME weight editing (model-editing evaluation),2,2,0,5,0,"Claim: 'ROME is effective on a standard zero-shot relation extraction model-editing task.' This is an empirical performance claim about the method, not a mechanistic claim about model internals. 'Is effective' = R2 (causal sufficiency: the intervention achieves the goal). Well-matched."
2211.00593,2211.00593-01,presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI),abstract,3,Path patching (activation patching variant),2,3,1,5,0,"Claim: 'presenting an explanation for how GPT-2 small performs a natural language task' — 'performs' is functional mechanistic language (R3). The definite mechanistic framing ('how ... performs') implies complete algorithmic explanation. Path patching (R2) establishes causal effects, not the full mechanism. Canonical R2→R3 overclaim per calibration anchor."
2211.00593,2211.00593-02,Our explanation encompasses 26 attention heads grouped into 7 main classes,abstract,3,Path patching + ablation,2,3,1,4,0,Claim: 'Our explanation encompasses 26 attention heads grouped into 7 main classes.' This frames the circuit as THE explanation — uniqueness implied by 'our explanation' as complete description. Path patching (R2) identifies components with causal effects but doesn't establish this is the unique or complete mechanism.
2211.00593,2211.00593-03,this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior,abstract,3,Path patching (activation patching),2,3,1,5,0,"Claim: 'largest end-to-end attempt at reverse-engineering a natural behavior.' 'Reverse-engineering' implies complete mechanistic reconstruction (R3). Path patching establishes causal effects (R2). Per calibration anchor, 'reverse-engineering' language = R3 overclaim."
2211.00593,2211.00593-04,Name Mover Heads... move the name from the subject to the end position,body,1,Path patching + attention analysis,2,3,1,5,0,Claim: 'Name Mover Heads... move the name from the subject to the end position.' 'Move' is a functional mechanistic verb describing what a component DOES (R3). Path patching shows causal effect; 'move' implies the unique mechanism. Per calibration rationale: functional verbs = R3 overclaim.
2211.00593,2211.00593-05,S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token,body,1,Path patching + attention analysis,2,3,1,5,0,Claim: 'S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token.' 'Inhibit' is a functional mechanistic verb (R3). Path patching (R2) shows causal effects. Directly matches calibration anchor pattern: functional verb overclaim.
2211.00593,2211.00593-06,the circuit is faithful complete and minimal,abstract,2,Path patching + ablation (faithfulness/completeness/minimality tests),2,3,1,5,0,Claim: 'the circuit is faithful complete and minimal.' 'The circuit' uses definite article implying uniqueness (R3). Faithfulness/completeness/minimality tests use ablation (R2). Per calibration anchor: 'the circuit' = R3 regardless of qualification. R2→R3 overclaim.
2301.04709,2301.04709-01,Causal abstraction provides a theoretical foundation for mechanistic interpretability,abstract,3,Theoretical formalization (causal abstraction framework),1,1,0,3,0,"Claim: 'Causal abstraction provides a theoretical foundation for mechanistic interpretability.' This is a theoretical/meta claim about frameworks, not an empirical claim about model internals. No experimental intervention. Coding as R1/R1 since it's an associational/definitional claim: causal abstraction 'provides' a foundation (correlational framing of theoretical adequacy)."
2301.04709,2301.04709-02,generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation,abstract,3,Theoretical formalization,1,1,0,3,0,"Claim: 'generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation.' Pure theoretical contribution claim. No empirical method. R1/R1 — theoretical extension claim, no model intervention."
2301.04709,2301.04709-03,unifying a variety of mechanistic interpretability methods in the common language of causal abstraction,abstract,3,Theoretical unification / framework paper,1,1,0,3,0,Claim: 'unifying a variety of mechanistic interpretability methods in the common language of causal abstraction.' Theoretical/organizational claim. No empirical method used on model internals. R1/R1.
2301.05217,2301.05217-01,We fully reverse engineer the algorithm learned by these networks,abstract,3,Ablation in Fourier space + weight/activation analysis,2,3,1,5,0,Claim: 'We fully reverse engineer the algorithm learned by these networks.' 'Fully reverse engineer the algorithm' = completeness + uniqueness claim (R3). Ablation (R2) establishes causal necessity of Fourier components. 'Fully' + 'the algorithm' = R3 per calibration rationale.
2301.05217,2301.05217-02,uses discrete Fourier transforms and trigonometric identities to convert addition to rotation,abstract,3,Weight analysis + activation analysis + ablation,2,3,1,5,0,"Claim: 'uses discrete Fourier transforms and trigonometric identities to convert addition to rotation.' 'Uses X to do Y' = mechanistic narrative (R3 per codebook). Ablation supports causal role (R2) but 'the model uses DFT to convert' = functional algorithm claim. Per calibration: 'uses DFT ... to convert' = functional mechanism, R3."
2301.05217,2301.05217-03,grokking arises from the gradual amplification of structured mechanisms encoded in the weights,abstract,3,Weight/activation analysis (observational),1,3,2,4,0,Claim: 'grokking arises from the gradual amplification of structured mechanisms encoded in the weights.' 'Encoded in the weights' = storage language (R3 per decision tree). Evidence is weight analysis (R1 — no intervention on activations). Per calibration: 'encoded in the weights' from weight analysis alone = R1→R3 overclaim (+2 gap).
2304.14997,2304.14997-01,reverse-engineered nontrivial behaviors of transformer models,abstract,3,Activation patching (ACDC automated circuit discovery),2,3,1,4,0,Claim: 'reverse-engineered nontrivial behaviors of transformer models.' 'Reverse-engineered' = R3 (complete mechanistic reconstruction). ACDC uses activation patching (R2). Per calibration: 'reverse-engineering' language = R3 overclaim.
2304.14997,2304.14997-02,ACDC algorithm rediscovered 5/5 of the component types in a circuit,abstract,3,Activation patching (ACDC),2,2,0,4,0,Claim: 'ACDC algorithm rediscovered 5/5 of the component types in a circuit.' This is an empirical performance/coverage claim about the algorithm's output matching prior manual findings. 'Rediscovered' = empirical result of method effectiveness (R2). No strong mechanistic claim about model internals.
2304.14997,2304.14997-03,researchers can understand the functionality of each component,abstract,3,Activation patching (ACDC workflow),2,3,1,3,0,Claim: 'researchers can understand the functionality of each component.' 'Understand the functionality' = functional/mechanistic claim about what components DO (R3). Activation patching (R2) identifies causal effects but 'functionality' implies complete algorithmic understanding. Mild R3 claim.
2304.14997,2304.14997-04,finding the connections between abstract neural network units that form a circuit,abstract,3,Activation patching (ACDC),2,3,1,4,0,Claim: 'finding the connections between abstract neural network units that form a circuit.' 'Form a circuit' with definite framing implies unique mechanistic structure (R3). Activation patching (R2) finds causally relevant connections. 'The circuit' framing = R3 per decision tree.
2309.16042,2309.16042-01,systematically examine the impact of methodological details in activation patching,abstract,3,Activation patching (systematic comparison of variants),2,2,0,5,0,"Claim: 'systematically examine the impact of methodological details in activation patching.' This is a methodological characterization claim — the paper systematically varies hyperparameters and measures effects (R2 method, R2 claim about causal impact of methodological choices). No overclaim."
2309.16042,2309.16042-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,Activation patching (systematic hyperparameter comparison),2,2,0,5,1,Claim: 'varying these hyperparameters could lead to disparate interpretability results.' 'Could lead to' = hedged (hedge_flag=1). Causal claim: varying X causes disparate Y = R2. Method is systematic activation patching experiments (R2). Well-matched.
2311.17030,2311.17030-01,even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway,abstract,3,Subspace activation patching + theoretical analysis,2,2,0,4,1,Claim: 'even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway.' 'May be achieved by' = hedged (hedge_flag=1). The core mechanism claim describes what can happen under interventions (R2). Method is subspace activation patching (R2). Well-matched.
2311.17030,2311.17030-02,patching of subspaces can lead to an illusory sense of interpretability,abstract,3,Subspace activation patching + theoretical analysis,2,2,0,4,0,Claim: 'patching of subspaces can lead to an illusory sense of interpretability.' Causal claim about what patching interventions do (R2 — 'can lead to' implies causal effect of the method). No mechanistic uniqueness claim about model internals. Method is activation patching analysis (R2). Well-matched.
2311.17030,2311.17030-03,we demonstrate this phenomenon in a distilled mathematical example in two real-world domains,body,1,Subspace activation patching + mathematical examples,2,2,0,4,0,Claim: 'we demonstrate this phenomenon in a distilled mathematical example in two real-world domains.' Empirical demonstration claim — 'demonstrate' is empirical result language. Method combines mathematical proof and activation patching experiments (R2). The claim is about demonstrating a phenomenon (R2 — causal effect established in specific domains).
2311.17030,2311.17030-04,there is an inconsistency between fact editing performance and fact localization,abstract,3,Subspace activation patching + ROME editing analysis,2,2,0,4,0,Claim: 'there is an inconsistency between fact editing performance and fact localization.' Empirical observation about the relationship between two intervention-based measures (R2 — comparing causal intervention outcomes). Method is activation patching + editing experiments (R2). Well-matched.
2402.17700,2402.17700-01,MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations,abstract,3,Multi-task Distributed Alignment Search (MDAS) with interchange interventions,2,2,0,4,0,Claim: 'MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations.' Performance claim ('achieves state-of-the-art') + methodological implication ('importance of going beyond'). Intervention-based evaluation (R2). Claim is R2 — about causal sufficiency of distributed features. No unique mechanism claim.
2402.17700,2402.17700-02,If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent,introduction,2,Interchange interventions (activation patching),2,3,1,4,0,"Claim: 'If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent.' 'Encodes the attribute' — applying the decision tree: interventional evidence (YES), claim about result vs mechanism: the paper says 'encodes' in the context of what the intervention reveals about the feature's role = the feature 'encodes' (stores/represents) the concept. Per decision tree 'encodes' with interventional evidence, about the mechanism → R3. The paper is making a mechanistic attribution of encoding, not just 'is decodable from'."
2402.17700,2402.17700-03,Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle,results,1,Interchange interventions (RAVEL benchmark evaluation),2,2,0,4,0,Claim: 'Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle.' Comparative empirical result about method performance (R2 — causal evaluation using interventions). No mechanistic claim about model internals beyond what methods can achieve.
2402.17700,2402.17700-04,The representations of different attributes gradually disentangle as we move towards later layers,results,1,Interchange interventions (layer-by-layer evaluation),2,2,0,4,0,Claim: 'The representations of different attributes gradually disentangle as we move towards later layers.' Empirical finding from interchange intervention evaluations across layers (R2). 'Disentangle' describes the causal structure across layers as measured by interventions. No uniqueness claim.
2402.17700,2402.17700-05,Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects,results,1,Interchange interventions (RAVEL evaluation),2,2,0,3,0,"Claim: 'Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects.' Empirical finding from interventions (R2). 'Seemingly unavoidable ripple effects' describes causal consequences under intervention. No mechanistic uniqueness claim. Slight uncertainty because 'unavoidable' edges toward necessity language, but 'seemingly' softens it."
2403.07809,2403.07809-01,pyvene supports customizable interventions on a range of different PyTorch modules,abstract,3,Library capability demonstration (interventions on PyTorch modules),2,2,0,4,0,Claim: 'pyvene supports customizable interventions on a range of different PyTorch modules.' Technical capability claim about the library. The method is intervention-based (R2 — the library enables interventions). The claim is a functional description of the tool's capabilities (R2 — causal operations are enabled). No mechanistic claim about model internals.
2403.07809,2403.07809-02,pyvene provides a unified and extensible framework for performing interventions on neural models,abstract,3,Unified intervention framework,2,2,0,4,0,Claim: 'pyvene provides a unified and extensible framework for performing interventions on neural models.' Technical description of library functionality. R2 method (interventions); R2 claim (the framework enables interventional analyses). No overclaim about model internals.
2403.07809,2403.07809-03,we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization,abstract,3,Causal abstraction + knowledge localization (via pyvene interventions),2,2,0,3,0,Claim: 'we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization.' Demonstration claim — 'illustrate the power' via case studies. Method is interventions (R2). Claim is about what the library can do for interpretability analyses (R2 — causal analysis demonstrations). No mechanistic uniqueness claim.
2404.03592,2404.03592-01,much prior interpretability work has shown that representations encode rich semantic information,abstract,3,"literature summary / observational (probing, activation logging, correlational analysis cited as prior work)",1,3,2,4,0,"The claim asserts 'representations encode rich semantic information', using 'encode' — a Rung 3 mechanistic verb implying functional storage. The paper cites this as established prior work rather than presenting new interventional evidence; the cited methods include probing and SAE attribution (R1). Per the codebook decision tree: no interventional evidence is provided for this specific claim, and context does not make clear the authors mean merely 'linearly decodable' — they assert encoding as a background fact. Overclaim gap: +2 (R1 method, R3 claim). No hedge present."
2404.03592,2404.03592-02,interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly,introduction,2,distributed alignment search (DAS) / interchange interventions (R2),2,3,1,4,0,"The claim states interventions 'have provided increasing evidence that human-interpretable concepts are encoded linearly'. The method is DAS/interchange interventions (R2). 'Encoded linearly' uses storage/representation language (R3 marker). While 'increasing evidence' softens the claim slightly, it is not a full hedge — the paper treats linear encoding as the established conclusion the interventions support. The claim attributes a representational structure ('encoded linearly') that goes beyond what interventions establish (causal effects under subspace swaps). Overclaim gap: +1 (R2 method, R3 claim). No hedge_flag because 'increasing evidence' is hedging the quantity of evidence, not the claim type."
2404.03592,2404.03592-03,DAS is highly expressive and can effectively localize concepts within model representations,body,1,distributed alignment search (DAS) / interchange interventions (R2),2,2,0,4,0,"The claim says DAS 'can effectively localize concepts within model representations'. This is an interventional causal claim: DAS (an intervention method) is said to be capable of localizing concepts — meaning interventions on a subspace produce targeted causal effects. 'Localize' in this context means identifying subspaces where intervening changes model behavior, which is an R2 causal sufficiency claim. It does not assert the localized subspace is THE unique mechanism (which would be R3). The claim is about DAS's capability as a method, stated as a result of empirical evaluation. No overclaim gap. No hedge."
2404.03592,2404.03592-04,a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks,discussion,2,LoReFT intervention training / distributed interchange intervention (R2),2,2,0,4,0,"The claim is that 'a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks'. This is a causal sufficiency claim: intervening on a linear subspace (R2 method) is sufficient to control behavior across tasks. 'Achieve control' maps to R2 linguistic markers ('is sufficient for', 'can produce'). The claim does not assert this subspace is THE unique controller or claim to reveal the underlying mechanism — it claims interventional sufficiency. No overclaim. No hedge."
2404.03592,2404.03592-05,LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions,discussion,2,LoReFT fine-tuning with low-rank interventions on residual streams (R2),2,2,0,5,0,"The claim states LoReFT 'shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions'. This is a clear causal sufficiency claim (R2): the intervention method produces a measurable behavioral change (instruction following). 'Can induce' is standard R2 language ('can produce'). The claim is empirically well-grounded: LoReFT is trained and evaluated on benchmarks demonstrating this. No mechanism claim, no uniqueness assertion. No overclaim, no hedge."
2404.03646,2404.03646-01,specific components within middle layers show strong causal effects at the last token of the subject,abstract,3,causal tracing / activation patching (R2),2,2,0,5,0,"The claim states 'specific components within middle layers show strong causal effects at the last token of the subject'. This is a textbook R2 claim: causal tracing/activation patching is applied, and causal effects are reported. 'Show strong causal effects' is precisely R2 language — it reports the result of an intervention (patching) in causal terms without asserting a complete mechanism or unique circuit. Method (causal tracing, R2) matches claim rung (R2). No overclaim. No hedge."
2404.03646,2404.03646-02,rank-one model editing methods can successfully insert facts at specific locations,abstract,3,rank-one model editing (ROME-style weight modification applied to Mamba) (R2),2,2,0,4,0,"The claim is that 'rank-one model editing methods can successfully insert facts at specific locations'. This is an interventional causal claim: modifying weights (R2 intervention) produces a measurable effect (fact insertion at specific locations). 'Can successfully insert' = causal sufficiency (R2). The phrase 'at specific locations' refers to the location of the weight modification, not a claim about where facts are stored mechanistically. The method is rank-one editing applied to Mamba modules, which is an R2 intervention. No R3 mechanism claim about uniqueness or storage. No overclaim. No hedge."
2404.03646,2404.03646-03,linearity of Mamba's representations of factual relations,body,1,linear relational embedding analysis / correlation of subject and object representations (R1),1,3,2,4,0,"The claim is about 'linearity of Mamba's representations of factual relations'. 'Representations' is the key term. Per the paper, this is established via measuring linear relationships between subject and object hidden states (following Hernandez et al.'s linear relational embeddings approach — a correlation/regression analysis, R1). The claim uses 'representations' in a storage/encoding sense — asserting that the model structurally represents factual relations in a linear format. Per the codebook decision tree for 'represents': no interventional evidence is provided specifically for this linearity claim (the interventional evidence is for the causal tracing/editing claims). Context does not clearly reduce this to 'linearly decodable'. Therefore: R1 method, R3 claim. Overclaim gap: +2. No hedge."
2404.15255,2404.15255-01,activation patching is a popular mechanistic interpretability technique but has many subtleties,abstract,3,methodological analysis / tutorial — no new empirical intervention (R2 survey/analysis),2,1,0,3,0,"This is a methodological guide paper. The claim states activation patching 'is a popular mechanistic interpretability technique but has many subtleties'. This is a meta-claim about the method itself, not about model internals. It is a descriptive/observational assertion about the technique's properties. As a claim about the method's characteristics (rather than model representations), it sits at R1 in claim rung — associational/descriptive. The method used is analysis of activation patching's behavior across conditions (R2), but the claim itself is a general statement about the technique. This is an unusual case — a methodological paper making claims about a method rather than model internals. Confidence is moderate due to the unusual framing. No hedge present; the claim is stated as fact."
2404.15255,2404.15255-02,varying these hyperparameters could lead to disparate interpretability results,abstract,3,systematic comparison of activation patching hyperparameters (R2),2,2,0,4,1,"The claim is that 'varying these hyperparameters could lead to disparate interpretability results'. This is an interventional claim about the causal effect of hyperparameter choices on patching outcomes — i.e., changing a methodological variable (hyperparameter) changes the result. 'Could lead to' is an explicit hedge ('could' = R1 hedge flag). The method involves systematic comparison of different patching configurations (noising vs. denoising, noise level, ablation type) — R2. The claim rung is R2: it's asserting causal influence of hyperparameter choices on outcomes. Hedge_flag = 1 due to 'could'. No overclaim."
2406.11779,2406.11779-01,The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit,body,1,"mechanistic interpretability analysis — weight decomposition, OV/QK circuit analysis, formal proof (R1/R2 hybrid; primary evidence from weight analysis R1 and ablation R2)",2,3,1,4,0,"The claim states 'The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit'. This is a strong mechanistic claim using functional attribution language: 'attending more to larger tokens via the QK circuit' and 'copying the tokens it attends to via the OV circuit' both use mechanistic verbs ('attending', 'copying via') ascribing specific computational roles to circuits. This is R3: 'the model uses X to do Y' pattern (mechanistic narrative). The method is a combination of weight analysis (R1) and ablation (R2); the proof context adds formal verification, but the claim itself is a mechanistic narrative about how the circuit works. Per calibration: functional verbs like 'copies' + definite circuit references = R3. Overclaim gap: +1 (R2 method, R3 claim). No hedge."
2406.11779,2406.11779-02,EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component,body,1,"SVD / weight matrix decomposition (R1 — observational, no intervention on model)",1,1,0,5,0,"The claim is that 'EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component'. This is a purely observational/structural claim about a weight matrix's spectral properties — obtained by SVD decomposition. No intervention on the model is performed; this is a measurement of the weight structure. The claim uses precise numerical language ('singular value ~7800', '620x larger') and reports a mathematical property of the matrix, not a behavioral or mechanistic claim. Method: weight analysis via SVD (R1). Claim: descriptive/structural property of the weights (R1). No overclaim. No hedge."
2406.11779,2406.11779-03,Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning,body,1,zero ablation (R2 — intervention on model activations),2,2,0,5,0,"The claim states 'Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning'. This is a textbook R2 claim: an ablation intervention is performed and the result (negligible accuracy change) is reported. 'Confirming EQKP is unimportant' is a causal necessity/necessity absence claim — the intervention shows this component does not causally affect performance, which is an R2 causal claim (ablation establishes causal non-necessity). Method (zero ablation, R2) matches claim rung (R2). No overclaim. No hedge."
2406.11779,2406.11779-04,Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds,abstract,3,correlation analysis across proof strategies — measuring relationship between proof length and mechanistic understanding metric (R1),1,3,2,3,1,"The claim states 'Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds'. Two sub-claims: (1) proof length correlates with mechanistic understanding (correlational, R1), and (2) faithful mechanistic understanding 'leads to' tighter bounds (causal claim, R2+). The method is correlational analysis of proof length vs. an 'unexplained dimensionality' metric across 102 proof strategies. 'Seem to' is an explicit hedge (hedge_flag=1). The second part — 'leads to tighter performance bounds' — is a causal/mechanistic narrative about how understanding drives proof quality, which is R3 in framing ('understanding leads to X' = mechanism narrative). However, 'seem to' softens both parts. Assigning method_rung=1 (correlation across proof strategies), claim_rung=3 (mechanistic narrative about role of understanding). Moderate confidence due to unusual meta-level claim about interpretability methods rather than model internals."
2406.11779,2406.11779-05,Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices,body,1,rank-1 approximation analysis — mathematical/algebraic analysis of approximation error propagation (R1),1,1,0,4,0,The claim states 'Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices'. This is a technical/mathematical claim about the behavior of rank-1 approximations — it describes error accumulation when composing rank-1 approximations. The method is mathematical analysis of approximation error (no model intervention). The claim is observational/analytical: it identifies a challenge in the proof methodology based on algebraic properties of matrix approximations. This is R1 in both method and claim. No overclaim. No hedge.
2407.14008,2407.14008-01,partially reverse-engineer the circuit responsible for the Indirect Object Identification task,abstract,3,"edge attribution patching, zero ablation, resample ablation, linear probes (R2 primary)",2,3,1,5,1,"The claim is to 'partially reverse-engineer the circuit responsible for the Indirect Object Identification task'. 'Reverse-engineer the circuit' is a paradigmatic R3 mechanistic claim — implying complete mechanism discovery and using 'THE circuit' (definite article = uniqueness). 'Partially' is an explicit hedge (hedge_flag=1), which softens the completeness but not the mechanistic framing. Per the IOI calibration anchor: this is the canonical R2→R3 overclaim pattern. Methods used are edge attribution patching, zero/resample ablation (R2). The claim language ('circuit responsible for') uses uniqueness and functional attribution — R3. Overclaim gap: +1. The 'partially' hedge is noted."
2407.14008,2407.14008-02,Layer 39 is a key bottleneck,abstract,3,"zero ablation, resample ablation (R2)",2,3,1,4,0,"The claim states 'Layer 39 is a key bottleneck'. 'Key bottleneck' implies a unique, structurally central role — this is a mechanistic/uniqueness claim (R3). While ablation experiments support that layer 39 has a causal effect (R2 evidence), characterizing it as THE key bottleneck implies it is the unique or primary chokepoint in information flow, which is a stronger mechanistic claim than the ablation evidence strictly licenses. Per the codebook: 'is responsible for' and uniqueness language → R3. Overclaim gap: +1. No hedge."
2407.14008,2407.14008-03,Convolutions in layer 39 shift names one position forward,abstract,3,linear probes on intermediate representations / intervention on convolution outputs (R2),2,3,1,4,0,"The claim states 'Convolutions in layer 39 shift names one position forward'. This is a mechanistic functional attribution — 'convolutions... shift names' uses a functional verb ('shift') ascribing a specific computational operation to a specific architectural component. This is R3: 'this component performs X' / 'the model uses X to do Y' pattern. The evidence comes from probing representations before/after the convolution and some intervention on the outputs, but the claim asserts the functional role of the convolution as THE mechanism for name shifting. Per calibration: functional verbs + component attribution = R3. Overclaim gap: +1 (R2 method, R3 claim). No hedge."
2407.14008,2407.14008-04,The name entities are stored linearly in Layer 39's SSM,abstract,3,"linear probes, averaging interventions on SSM representations (R1/R2 hybrid; primary evidence is linear probing R1 with some R2 averaging interventions)",1,3,2,4,0,"The claim states 'The name entities are stored linearly in Layer 39's SSM'. 'Stored linearly' is a classic R3 storage/representation claim — using the verb 'stored' which implies a memory mechanism, combined with 'linearly' describing the representational format. Per the codebook and calibration (ROME lesson): storage language ('stores', 'stored') implies R3 mechanistic claims. The primary evidence is linear probing of SSM states (R1 method) plus some averaging interventions. Per the decision tree for 'stores/encodes': no strong interventional evidence specifically licenses the storage claim — the probing shows decodability (R1), but 'stored' implies mechanism. Method_rung: 1 (linear probing dominant for this claim). Overclaim gap: +2. No hedge."
2409.04478,2409.04478-01,SAEs struggle to reach the neuron baseline,abstract,3,Interchange interventions (DAS/DBM binary masking + feature patching on GPT-2 small residual stream),2,2,0,5,0,"The primary evaluation method is interchange intervention: features are patched from a source prompt into a base prompt forward pass and output accuracy is measured. This is a Rung 2 interventional method. The claim 'SAEs struggle to reach the neuron baseline' is a performance comparison based on the intervention results — it says SAEs produce lower disentangle scores than the neuron baseline, which is a causal performance claim (intervening on SAE features vs. neuron features changes outcome less). The linguistic markers are comparative/performance-based ('struggle to reach') rather than mechanistic. No overclaim: this is a calibration anchor paper (see calibration rationales) and the claim-method alignment is appropriate. The claim matches the codebook example 'SAEs struggle to reach baseline → R2'."
2409.04478,2409.04478-02,sets of SAE features that separately mediate knowledge of which country a city is in,abstract,3,Interchange interventions (DAS/DBM binary masking + feature patching on GPT-2 small residual stream),2,2,0,4,0,"The method is interchange intervention (Rung 2): features are patched from source to base prompt and output is measured. The claim asks whether SAE feature sets 'separately mediate knowledge of which country a city is in.' The word 'mediate' is a canonical Rung 2 linguistic marker — it describes a causal mediating role established by the intervention, not a mechanistic storage claim. The paper explicitly frames this as evaluating whether features mediate (not encode or represent) knowledge. This is consistent with the calibration example 'features that mediate knowledge → R2'. Minor ambiguity: 'knowledge' could imply storage (R3), but in context it refers to the measured intervention outcome, keeping it R2. Confidence slightly reduced because 'knowledge' is a polysemous term."
2410.08417,2410.08417-01,Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling,abstract,3,"Eigendecomposition of bilinear MLP weight tensors (weight-based analysis, no input dataset required)",1,1,0,4,0,"The method is eigendecomposition of bilinear MLP weight matrices — a form of weight analysis that does not intervene on the model. The paper explicitly states these decompositions 'reveal' structure 'directly from the weights, without using inputs.' This is Rung 1 (observational/associational): no intervention is performed, only analysis of the weight structure. The claim 'reveals interpretable low-rank structure' uses 'reveals' which is observational language, equivalent to 'is decodable from' or 'can be extracted.' The claim does not assert causal effects or unique mechanisms, only that low-rank structure is present and interpretable. Slight ambiguity: 'interpretable' could imply a mechanistic claim, but in context it refers to human-readable visual patterns in the eigenvectors, not functional attribution. No overclaim: R1 method, R1 claim."
2410.08417,2410.08417-02,For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors,body,1,"Eigendecomposition of bilinear MLP weight tensors (weight-based analysis, visualization of top eigenvectors)",1,3,2,4,0,"The method is weight-based eigendecomposition — Rung 1 (observational). No intervention is performed on the model; the authors analyze the weight matrices directly. The claim states: 'top eigenvectors represent curve segments specific to each digit class' and 'function as localized edge detectors.' Both 'represent' and 'function as' are Rung 3 linguistic markers. Applying the 'encodes/represents' decision tree: the paper does NOT provide interventional evidence for these specific claims about what eigenvectors represent — the adversarial example experiment (claim 03) provides some causal evidence, but claim 02 itself is based purely on visual inspection of eigenvector patterns. 'Function as localized edge detectors' is a functional attribution (the head DOES X pattern) = R3. This is the classic SAE→'represents' overclaim pattern (R1 method → R3 claim, gap +2), applied here to weight eigenvectors."
2410.08417,2410.08417-03,Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features,body,1,Adversarial input construction using eigenvectors (input-level intervention/perturbation to cause misclassification),2,2,0,3,0,"The method constructs adversarial masks by exploiting eigenvector structure and applies them to inputs, causing misclassification. This is an input-level intervention — closer to Rung 2 (interventional) than Rung 1, as it actively perturbs inputs to measure causal effects on outputs. The claim explicitly asserts 'demonstrating causal importance of extracted features,' which is Rung 2 language ('causally affects,' 'causal importance'). The paper uses the eigenvectors to craft targeted perturbations that change model behavior, establishing that these directions causally matter for classification. This is not a weight-only observational claim. Confidence reduced to 3 because this is an input-level intervention (not a model-internal intervention like patching), which sits at the boundary of R1/R2. However, the explicit causal framing in the claim and the targeted nature of the perturbation (using identified features to predict model behavior) warrants R2 for both method and claim. No overclaim."
2410.08417,2410.08417-04,A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions,body,1,Bilinear tensor decomposition + SAE feature interaction analysis (weight-based analysis of feature interactions via transformed bilinear tensor),1,3,2,4,0,"The method is weight-based analysis: the bilinear tensor is transformed into SAE feature basis to identify top interactions between input and output features. No model interventions are performed for this specific claim — the interactions are read off the weights statistically. This is Rung 1 (observational/weight analysis). The claim asserts: 'a sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions.' Multiple R3 markers are present: (1) 'circuit' with definite article implies uniqueness; (2) 'computes' is a functional verb (R3); (3) 'AND-gate-like interactions' is a mechanistic narrative about the computation. Applying the decision tree for 'computes': the paper provides no interventional evidence (ablation, patching) specifically for this circuit claim — it is inferred purely from weight structure. This is the Attention→'performs' overclaim pattern (R1→R3, gap +2) applied to weight analysis."
2410.08417,2410.08417-05,Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values,body,1,Correlation analysis between SAE output feature activations and low-rank eigenvector approximations of bilinear tensor (observational/statistical association),1,1,0,5,0,"The method is correlation analysis: the paper computes correlation coefficients between SAE output feature activations and low-rank approximations derived from eigenvectors of the bilinear tensor. No model interventions are performed. This is Rung 1 (observational/correlational). The claim states: 'Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values.' The linguistic marker 'well-correlated' is a canonical Rung 1 associational term. The claim makes no causal or mechanistic assertion — it only reports a statistical association between two sets of measurements. No overclaim: R1 method, R1 claim, perfect alignment."
2411.08745,2411.08745-01,the output language is encoded in the latent at an earlier layer than the concept to be translated,abstract,3,Activation patching (extracting residual stream latents from source prompts and inserting at target prompt positions across layers),2,3,1,3,0,"The method is activation patching (Rung 2): latents from source prompts are patched into a target forward pass layer-by-layer to observe when output changes. This establishes causal ordering of information in layers. The claim states: 'the output language is encoded in the latent at an earlier layer than the concept to be translated.' Applying the 'encodes' decision tree: the paper provides interventional evidence (patching reveals at which layer patching first affects language vs. concept output). The claim is about the result of the intervention (what layer the patching takes effect) — this could code as R2. However, 'encoded in the latent' is storage/mechanistic language that goes beyond the causal effect observed (which is that patching at layer X affects language output). The claim asserts that information is stored ('encoded') in specific layers, not just that patching there has an effect. This R2→R3 gap mirrors the ROME calibration case ('stored in layers' vs. 'mediated by layers'). Confidence 3 due to genuine ambiguity between R2 and R3 interpretation."
2411.08745,2411.08745-02,we can change the concept without changing the language and vice versa through activation patching alone,abstract,3,Activation patching (inserting latents from source prompts into target forward pass to change concept or language independently),2,2,0,5,0,"The method is activation patching (Rung 2): the paper explicitly states 'we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone.' The claim directly describes an interventional result — patching changes X without changing Y. This is canonical Rung 2 language: 'intervening on X changes Y' (and more precisely, selective intervention preserves non-targeted outputs). The claim does not assert a unique mechanism or storage claim — it reports what happens under intervention. Clean R2 method, R2 claim, no overclaim. High confidence because the claim text explicitly references the method ('through activation patching alone')."
2411.08745,2411.08745-03,patching with the mean representation of a concept across different languages improves translation,abstract,3,Activation patching (patching mean representation of concept across languages into target translation prompt),2,2,0,5,0,"The method is activation patching (Rung 2): the paper patches the mean representation of a concept (averaged across different language source prompts) into the target forward pass and measures translation performance. The claim states: 'patching with the mean representation of a concept across different languages improves translation.' This is a clean interventional result — the intervention (patching mean representation) produces a measured effect (improved translation accuracy). The linguistic markers are causal but not mechanistic: 'patching with X improves Y' = 'intervening on X changes Y' = R2. No uniqueness claim, no storage/encoding language, no functional attribution. Clean R2 method, R2 claim, no overclaim."
2411.08745,2411.08745-04,results provide evidence for the existence of language-agnostic concept representations,abstract,3,Activation patching (used as causal evidence for existence of language-agnostic concept representations),2,3,1,4,1,"The method is activation patching (Rung 2). The claim states: 'results provide evidence for the existence of language-agnostic concept representations.' hedge_flag=1 because 'provide evidence for' is an explicit hedge — it does not assert the claim as fact but as supported inference. Despite the hedge, the underlying claim is R3: 'language-agnostic concept representations' asserts that the model has a specific representational structure (a shared, language-independent concept space). Per codebook: 'represents' = R3 unless context makes clear it means 'is decodable from.' The patching results show that mean concept representations across languages are usable (a causal effect), but the claim goes further to assert the existence of a specific representational structure ('language-agnostic concept representations'). This is R2→R3 overclaim (+1), softened by the hedge. Per codebook guidance: 'may encode → still R3 if followed by mechanistic narrative'; the underlying claim is clearly R3."
2411.16105,2411.16105-01,circuits within LLMs may be more flexible and general than previously recognized,abstract,3,Activation/path patching on IOI circuit in GPT-2 small across prompt variants (measuring circuit performance and head functionality on DoubleIO and TripleIO prompts),2,2,0,4,1,"The method is activation/path patching (Rung 2), following Wang et al. (2023)'s IOI circuit methodology. The paper measures circuit performance across prompt variants by patching heads and paths. The claim states: 'circuits within LLMs may be more flexible and general than previously recognized.' hedge_flag=1: 'may be' is an explicit hedge. The underlying claim is about circuit generalization — a causal/behavioral property (the circuit produces correct outputs across more conditions than expected), not a mechanistic storage or uniqueness claim. 'More flexible and general' describes behavioral generalization established through intervention results. This is R2 language: it claims a causal effect (the circuit causally mediates correct behavior across more variants), not a mechanistic narrative about how it works internally. The hedge further reduces this from any R3 implication. R2 method, R2 claim, no overclaim."
2411.16105,2411.16105-02,the circuit generalizes surprisingly well reusing all of its components and mechanisms,abstract,3,Activation/path patching on IOI circuit in GPT-2 small (measuring edge overlap and head functionality across prompt variants),2,3,1,4,0,"The method is activation/path patching (Rung 2). The claim states: 'the circuit generalizes surprisingly well reusing all of its components and mechanisms.' Multiple R3 markers are present: (1) 'the circuit' — definite article implying the unique circuit (IOI circuit naming convention, but still R3 per codebook: 'if no qualifications → code as R3'); (2) 'mechanisms' — mechanistic language asserting what the components DO; (3) 'reusing all of its components' — functional attribution implying the components perform the same specific roles. The paper uses path patching to measure edge overlap (92-100% node overlap), which establishes causal sufficiency of the same components, but 'reusing mechanisms' goes beyond showing the same nodes are causally sufficient — it claims the same functional roles are being performed. This is the IOI calibration pattern: patching (R2) → 'the circuit' + 'mechanisms' (R3), gap +1."
2411.16105,2411.16105-03,we discover a mechanism that explains this which we term S2 Hacking,abstract,3,Activation/path patching on IOI circuit in GPT-2 small (identifying S2 Hacking mechanism through circuit analysis on DoubleIO/TripleIO prompt variants),2,3,1,4,0,"The method is activation/path patching (Rung 2). The claim states: 'we discover a mechanism that explains this which we term S2 Hacking.' R3 markers: (1) 'a mechanism that explains' — 'mechanism' is a canonical R3 term; (2) 'explains' implies the mechanism provides a complete or near-complete account of the behavior (mechanistic narrative). The paper discovers S2 Hacking through path patching analysis showing specific attention head interactions — but naming it 'a mechanism that explains' the circuit's generalization is R3 language. Per codebook: 'the mechanism' or 'a mechanism' = R3. The claim goes beyond 'patching this component changes behavior' (R2) to asserting a causal explanatory mechanism. This is the IOI calibration pattern: path patching (R2) → 'mechanism' (R3), gap +1. Note: the claim uses indefinite article 'a mechanism' rather than 'the mechanism,' slightly reducing the uniqueness implication, but 'mechanism that explains' still falls in R3 territory."
2411.16105,2411.16105-04,implement algorithms responsible for performing specific tasks,abstract,3,Activation/path patching on IOI circuit in GPT-2 small (circuit discovery methodology following Wang et al. 2023),2,3,1,4,0,"The method is activation/path patching (Rung 2). The claim 'implement algorithms responsible for performing specific tasks' appears in the abstract describing what circuits do. R3 markers: (1) 'implement algorithms' — 'implement' is a functional verb equivalent to 'computes/executes' = R3; (2) 'responsible for performing' — 'responsible for' is a canonical R3 marker per codebook; (3) combined, 'implement algorithms responsible for performing specific tasks' is a complete mechanistic narrative asserting unique functional roles. This is the IOI calibration pattern: patching establishes causal sufficiency (R2) but the claim asserts that circuits implement specific algorithms and are responsible for behavior (R3). Gap +1. The abstract framing also increases the strength of this as a genuine overclaim rather than hedged inference."
2501.17148,2501.17148-01,prompting outperforms all existing methods followed by finetuning,abstract,3,"AXBENCH benchmark evaluation: comparative performance measurement of prompting, finetuning, and representation-based methods on concept detection and model steering tasks",1,1,0,4,0,"The method is empirical benchmarking: AXBENCH measures performance (AUROC for concept detection, LLM-judge score for steering) across multiple methods without intervening on model internals in a mechanistic sense. Prompting and finetuning modify model inputs/weights as standard use, not as mechanistic interventions probing internal representations. This is Rung 1 (observational/associational): the paper associates method type with performance outcome. The claim 'prompting outperforms all existing methods followed by finetuning' is a performance comparison — 'outperforms' is associational language correlating method identity with benchmark score. No causal or mechanistic claim about model internals. R1 method, R1 claim, no overclaim. Note: steering methods do involve representation interventions, but the claim is about aggregate benchmark ranking, not about a specific causal effect on model internals."
2501.17148,2501.17148-02,SAEs are not competitive,abstract,3,AXBENCH benchmark evaluation: comparative performance measurement of SAEs vs. other methods on concept detection and model steering tasks,1,1,0,5,0,"The method is empirical benchmarking (Rung 1): AXBENCH measures SAE performance on concept detection (AUROC) and steering (LLM judge) and compares to other methods. The claim 'SAEs are not competitive' is a pure performance/comparative claim — 'not competitive' means SAE scores are lower than baseline methods. This is straightforwardly Rung 1: the claim associates SAE method type with low benchmark performance, no causal or mechanistic assertion about model internals. Clean R1 method, R1 claim. This mirrors the calibration pattern from the SAE Evaluation paper (2409.04478) which also makes performance claims about SAEs."
2501.17148,2501.17148-03,representation-based methods such as difference-in-means perform the best,abstract,3,"AXBENCH benchmark evaluation: comparative performance measurement of representation-based methods (difference-in-means, probing, SAEs, etc.) on concept detection and model steering tasks",1,1,0,5,0,"The method is empirical benchmarking (Rung 1): AXBENCH measures concept detection AUROC and steering scores across multiple representation-based methods. The claim 'representation-based methods such as difference-in-means perform the best' is a pure performance comparison — 'perform the best' correlates method type with benchmark score. Difference-in-means itself is a Rung 1 method (computing mean activation differences across conditions without model intervention), and the claim about its performance is itself Rung 1. No causal claim about model internals, no mechanistic attribution. R1 method, R1 claim, no overclaim."
2502.03714,2502.03714-01,USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models,abstract,3,Sparse autoencoder (SAE) feature attribution — jointly trained across multiple models to learn a shared concept space; evaluated via reconstruction loss and qualitative/quantitative concept analysis. No interventions on model internals.,1,3,2,4,0,"Method is observational: USAEs are trained to reconstruct activations and the resulting features are analyzed without intervening on the model to test causal effects. This is Rung 1 (SAE feature attribution). The claim uses 'interpret the internal activations of multiple models', where 'interpret' in context means uncovering what the model internally represents — a Rung 3 mechanistic framing. Per the codebook decision tree for 'encodes/represents': the paper provides no interventional evidence for the interpretation claim; the authors mean more than mere decodability — they claim the learned dictionary captures the underlying concept structure of each model. This is a functional/mechanistic reading (R3). Gap: +2."
2502.03714,2502.03714-02,the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets,abstract,3,"Sparse autoencoder (SAE) feature attribution — shared dictionary learned jointly across multiple vision models; evaluated via reconstruction quality and concept universality metrics (correlation with importance scores, model-specific vs. universal concepts).",1,3,2,4,0,"Method is Rung 1: SAE training and analysis without causal interventions. The claim 'captures common factors of variation concepts' uses 'captures' — a storage/mechanistic verb. Per the codebook, 'captures' in this context implies the dictionary latches onto the underlying computational concepts in the models, not merely that features correlate with inputs. No interventional evidence supports this. The claim is stated as an established fact with no hedge. Gap: +2 (R1 method, R3 claim)."
2502.03714,2502.03714-03,USAEs discover semantically coherent and important universal concepts across vision models,abstract,3,Sparse autoencoder (SAE) feature attribution — USAE training with qualitative concept visualization (coordinated activation maximization) and quantitative universality metrics. No causal interventions on model internals.,1,3,2,4,0,"Method is Rung 1: observational SAE analysis without intervention. The claim 'USAEs discover semantically coherent and important universal concepts' uses 'discover' with definite mechanistic framing — the paper treats these SAE features as actual universal concepts that exist within the models, not merely as correlational features. 'Universal concepts across vision models' implies a unique, shared representational structure (uniqueness language). No interventional evidence establishes that these dictionary features causally mediate model behavior. Gap: +2 (R1 method, R3 claim)."
2503.10894,2503.10894-01,HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states,abstract,3,"HyperDAS: transformer-based hypernetwork performing distributed interchange interventions (activation patching on learned subspaces) on Llama3-8B, benchmarked on RAVEL disentangle score.",2,2,0,5,0,"Method is Rung 2: interchange interventions (activation patching) that establish causal effects on benchmark performance. The claim is a benchmark performance claim — 'achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states.' The RAVEL disentangle score is itself defined by causal intervention success (Cause + Iso scores from interchange interventions). This is an appropriate R2 claim: it asserts causal sufficiency/effectiveness of the method on a causal benchmark, not a mechanistic uniqueness claim. No gap."
2503.10894,2503.10894-02,features that mediate concepts and enable predictable manipulation,abstract,3,HyperDAS: distributed interchange interventions (activation patching on learned linear subspaces) to identify features that causally mediate concepts in Llama3-8B hidden states.,2,2,0,5,0,"Method is Rung 2: interchange interventions establish causal mediation. The claim uses 'mediate concepts and enable predictable manipulation' — 'mediate' is the canonical Rung 2 linguistic marker per the codebook, and 'enable predictable manipulation' describes causal sufficiency from intervention evidence. This is a well-matched R2 claim: the method establishes causal mediators, and the claim uses appropriate causal language. No overclaim. No gap."
2503.10894,2503.10894-03,HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in,abstract,3,HyperDAS: hypernetwork trained end-to-end with interchange interventions to automatically select token positions with highest causal effect on concept-related outputs in Llama3-8B.,2,3,1,4,0,"Method is Rung 2: interchange interventions identify token positions with causal effects on output. The claim 'automatically locates the token-positions of the residual stream that a concept is realized in' uses 'realized in' — a storage/representational Rung 3 marker. The method identifies positions where interventions have causal effects (R2), but 'realized in' implies the concept is stored or implemented at those positions, which is a stronger mechanistic claim. Per the codebook decision tree for 'encodes/represents': interventional evidence exists (R2), but the claim goes beyond the intervention result to assert where the concept mechanistically lives. Gap: +1 (R2 method, R3 claim)."
2503.10894,2503.10894-04,Interchange interventions identify neural representations that are causal mediators of high-level concepts,body,1,"Interchange interventions (activation patching on subspaces): fixing features of an LM processing one input to the values they take on a counterfactual input, then measuring output change to establish causal mediation.",2,2,0,5,0,"Method is Rung 2: interchange interventions directly establish causal mediation. The claim 'interchange interventions identify neural representations that are causal mediators of high-level concepts' uses 'causal mediators' — the canonical Rung 2 language. This is a methodological description stating what interchange interventions can establish, which precisely matches the method's evidential level. The claim is located in the background section explaining the method class. No overclaim. No gap."
2503.10894,2503.10894-05,at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes,results,1,HyperDAS: hypernetwork trained with interchange interventions that automatically selects token positions for intervention; the chosen positions at deeper layers are analyzed post-hoc by categorizing token types.,2,3,1,4,0,"Method is Rung 2: the hypernetwork selects positions via causal intervention training. However, the characterization of what those positions do uses Rung 3 language: 'previously unknown to store attributes' invokes storage/representation language. The intervention evidence shows that patching syntax tokens at deep layers causally affects outputs (R2), but 'store attributes' asserts that these tokens mechanistically contain/implement attribute information — a R3 claim. Per the codebook calibration lesson from ROME: storage language ('stores,' 'contains') goes beyond what causal mediation evidence supports. Gap: +1 (R2 method, R3 claim)."
2504.02976,2504.02976-01,patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed,abstract,3,Activation patching (CLAP): patching clean activations into corrupted runs at specific layers and measuring logit difference recovery in fine-tuned GPT-2 on medical QA task.,2,3,1,4,0,"Method is Rung 2: activation patching establishes causal effects of specific layers on output. The intervention result (56% logit difference recovery from patching first feedforward layer) is a valid R2 finding. However, the interpretive claim 'demonstrating that associative knowledge is distributed across multiple layers' uses 'knowledge is distributed' — a mechanistic claim about how knowledge is organized and stored across the model (R3 storage/representation language). The patching result shows that one layer alone is causally insufficient to fully restore behavior; inferring that 'associative knowledge is distributed' is a stronger mechanistic narrative than the intervention evidence supports. Gap: +1 (R2 method, R3 claim)."
2504.02976,2504.02976-02,patching the final output layer completely restored accuracy indicating that definitional knowledge is localised,abstract,3,Activation patching (CLAP): patching clean activations from the final output layer into corrupted runs and measuring logit difference recovery (100%) in fine-tuned GPT-2 on definitional QA.,2,3,1,4,0,"Method is Rung 2: activation patching establishes that patching one layer restores full output accuracy (100% recovery) — a causal sufficiency finding. The interpretive conclusion 'definitional knowledge is localised' uses 'localized' as a storage/organization claim about where knowledge mechanistically resides (R3). Per the ROME calibration lesson: localization language ('stored in a localized manner') exceeds what causal tracing/patching evidence supports — patching shows causal sufficiency at that layer, not that the knowledge is uniquely stored there. The stronger clean logit difference for definitional questions is additional observational evidence (R1) that reinforces the R3 narrative. Gap: +1."
2504.02976,2504.02976-03,factual knowledge is more localized and associative knowledge depends on distributed representations,abstract,3,Activation patching (CLAP) across layer types in fine-tuned GPT-2: comparing logit difference recovery rates for factual (definitional) vs. associative (multi-hop) QA tasks to infer knowledge organization.,2,3,1,4,0,"Method is Rung 2: activation patching establishes layer-specific causal effects. The summary claim 'factual knowledge is more localized and associative knowledge depends on distributed representations' is a high-level mechanistic conclusion about how two types of knowledge are organized within the model — using 'localized' and 'distributed representations', both R3 storage/organization markers. The patching evidence establishes differential causal effects by layer and task type (R2), but inferring that knowledge is mechanistically organized as localized vs. distributed is a stronger claim than the interventional evidence strictly supports. This pattern matches the ROME calibration anchor exactly. Gap: +1 (R2 method, R3 claim)."
2505.14685,2505.14685-01,LM binds each character-object-state triple together by co-locating their reference information,abstract,3,"Causal mediation analysis / interchange interventions (activation patching on residual stream vectors) in Llama-3-70B-Instruct and Llama-3.1-405B-Instruct, tracing information flow of key input tokens to final output.",2,3,1,4,0,"Method is Rung 2: interchange interventions establish causal pathways. The claim 'LM binds each character-object-state triple together by co-locating their reference information, represented as Ordering IDs (OIs), in low-rank subspaces of the state token's residual stream' uses 'binds' and 'co-locating' — functional/mechanistic verbs asserting what the model computes and how it organizes information. 'Represented as Ordering IDs in low-rank subspaces' further invokes R3 representational language (the model uses OIs as a mechanism). The intervention evidence establishes causal effects of residual stream patches; inferring a binding mechanism with specific computational structure (OIs in subspaces) is a Rung 3 mechanistic narrative. Gap: +1."
2505.14685,2505.14685-02,lookback mechanism which enables the LM to recall important information,abstract,3,Causal mediation analysis / interchange interventions (activation patching) in Llama-3-70B-Instruct to trace information flow and identify the lookback mechanism computationally.,2,3,1,4,0,"Method is Rung 2: interchange interventions establish causal pathways. The claim describes 'a lookback mechanism which enables the LM to recall important information' — 'enables' is a Rung 2 marker in isolation, but in context this is framed as 'THE lookback mechanism' (definite article implying uniqueness) and 'enables the LM to recall' attributes a specific functional role to a named mechanism (functional verb 'enables' + mechanistic narrative). Per the codebook: 'enables' in a mechanistic narrative context with uniqueness framing ('a pervasive algorithmic pattern') maps to R3. The paper explicitly states it is 'reverse-engineering' this mechanism, reinforcing the R3 framing. Gap: +1."
2505.14685,2505.14685-03,the binding lookback retrieves the correct state OI,abstract,3,Causal mediation analysis / interchange interventions (activation patching on residual stream subspaces) in Llama-3-70B-Instruct to identify which attention heads retrieve the answer state ordering ID.,2,3,1,4,0,"Method is Rung 2: interchange interventions establish causal effects of specific subspace patches. The claim 'the binding lookback retrieves the correct state OI' uses 'retrieves' — a functional/mechanistic verb asserting what the mechanism performs (Rung 3). The paper uses 'the binding lookback' with a definite article as a named mechanism performing a specific computation. Per the codebook IOI calibration anchor: using functional verbs ('retrieves', 'moves', 'inhibits') to describe what a named circuit/mechanism does constitutes R3 claiming even when the evidence is from activation patching (R2). Gap: +1."
2505.14685,2505.14685-04,reverse-engineering ToM reasoning in LMs,abstract,3,Causal mediation analysis / interchange interventions (activation patching) combined with attention pattern analysis and subspace analysis to characterize LM belief-tracking mechanisms.,2,3,1,5,0,"Method is Rung 2: interchange interventions. The claim 'reverse-engineering ToM reasoning in LMs' is a canonical Rung 3 overclaim — 'reverse-engineering' implies complete mechanistic recovery of the algorithm, which per the Grokking calibration paper is a definitive R3 claim (completeness claim). The codebook lists 'reverse-engineering' as implying complete mechanism in the Grokking calibration case. Activation patching establishes causal pathways (R2) but cannot establish that the identified mechanism is complete or unique. The paper itself qualifies this as 'taking a step toward' in the abstract, but the phrase 'reverse-engineering ToM reasoning' is unhedged mechanistic language. Gap: +1."
2505.22637,2505.22637-01,all seven prompt types produce a net positive steering effect but exhibit high variance across samples,abstract,3,"Contrastive Activation Addition (CAA) / steering vectors: computing mean difference of residual stream activations for positive vs. negative behavior examples, then adding the vector at inference time and measuring logit-difference shift across 36 datasets and 7 prompt types in Llama2-7B.",2,2,0,5,0,Method is Rung 2: steering vectors are interventions — adding a learned direction to activations and measuring output changes. The claim 'all seven prompt types produce a net positive steering effect but exhibit high variance across samples' is an empirical observation directly about the intervention outcomes (effect size and variance of the causal manipulation). This is a well-matched R2 claim: it reports on the results of interventions without asserting mechanistic uniqueness or storage. 'Net positive steering effect' describes causal sufficiency; 'high variance' qualifies the reliability. No gap.
2505.22637,2505.22637-02,higher cosine similarity between training set activation differences predicts more effective steering,abstract,3,"Contrastive Activation Addition (CAA): computing cosine similarity between individual training-set activation differences and the steering vector, then correlating directional agreement with downstream steering effect size and anti-steerable sample fraction across 36 datasets.",2,1,0,4,0,"Method is Rung 2: the steering vectors are produced by interventions, but the specific analysis supporting this claim is correlational — it computes cosine similarity between activation differences and the steering vector, then observes that higher cosine similarity correlates with better steering. The claim 'higher cosine similarity between training set activation differences predicts more effective steering' uses 'predicts' — the canonical Rung 1 linguistic marker ('predicts,' 'correlates with'). Even though the underlying evaluation uses steering (R2), the relationship between the geometric property (cosine similarity) and steering success is established by correlation/regression, not by intervening on cosine similarity itself. This is a Rung 1 claim supported by a mixed R1/R2 analysis. The method_rung reflects the broader intervention framework, but the specific claim is correlational. No gap (method R2 >= claim R1; if anything the method is stronger than the claim requires)."
2505.22637,2505.22637-03,vector steering is unreliable when the target behavior is not represented by a coherent direction,abstract,3,Contrastive Activation Addition (CAA): analyzing datasets where positive and negative activations are well/poorly separated along the steering vector direction; using difference-of-means line separability as predictor of steering success.,2,3,1,4,0,"Method is Rung 2: steering vectors (interventions on residual stream). The claim 'vector steering is unreliable when the target behavior is not represented by a coherent direction' uses 'represented by a coherent direction' — representational language asserting something about how the behavior is encoded in the model (R3). 'Represented' here is not used in the decodability sense (which would be R1) but in the mechanistic sense: it claims that behaviors failing to have coherent linear representations in activation space are inherently unsteerable, implying a structural claim about the model's internal organization. The evidence is geometric analysis of activation separability (observational, R1) combined with steering outcomes (R2); inferring that the behavior 'is not represented by a coherent direction' as a general claim about model internals is R3. Gap: +1 (R2 method, R3 claim)."
2505.24859,2505.24859-01,steering effectively controls the targeted summary properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,4,0,"The method is CAA (Rimsky et al., 2024), which adds a learned steering vector to residual stream activations at inference time — a clear interventional method (R2). The claim 'steering effectively controls the targeted summary properties' uses 'controls' language. Per the codebook decision tree for 'controls': the evidence is from an intervention (steering), and the paper does not claim this component is the *unique* controller (prompting is also compared). This maps to R2 (causal sufficiency without uniqueness). The claim is stated as an empirical finding without hedging."
2505.24859,2505.24859-02,high steering strengths consistently degrade both intrinsic and extrinsic text quality,abstract,3,Contrastive Activation Addition (CAA) steering vectors,2,2,0,5,0,"Method is CAA steering vectors (R2 — interventional). The claim 'high steering strengths consistently degrade both intrinsic and extrinsic text quality' is a direct empirical observation about the effect of the intervention (varying steering strength λ) on output quality metrics (perplexity, ROUGE, BERTScore). This is a causal effect claim matched to the intervention — the paper measures what happens when you intervene with high λ. Linguistic markers: 'degrade' describes an observable consequence of the intervention. No mechanistic narrative about why degradation occurs. Well-matched R2 claim from R2 method. No hedging."
2505.24859,2505.24859-03,combining steering and prompting yields the strongest control over text properties,abstract,3,Contrastive Activation Addition (CAA) steering vectors combined with prompt engineering,2,2,0,4,0,"Method is CAA steering + prompting, both interventional in nature (R2). The claim 'combining steering and prompting yields the strongest control over text properties' is a comparative effectiveness claim: it asserts that the combined intervention produces the largest causal effect on text properties relative to steering alone or prompting alone. This is a standard R2 claim — observing which intervention produces the strongest effect. 'Yields' and 'strongest control' are causal-effect language but do not assert unique mechanisms or functional identity (no 'the mechanism' or 'performs'). Well-matched R2 claim from R2 method. No hedging."
2506.03292,2506.03292-01,scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods,abstract,3,HYPERSTEER hypernetwork-based activation steering evaluated on AxBench,2,2,0,5,0,"Method is HYPERSTEER, a hypernetwork that generates and applies steering vectors to LM residual streams — a supervised interventional method (R2). The claim 'scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods' is a comparative performance claim grounded in empirical evaluation on AxBench benchmarks. The paper reports that the cross-attention variant scores 0.608 on Concept500-HO versus ReFT-r1's lower scores. This is a straightforward R2 claim: what effect does applying the HYPERSTEER-generated steering vector have on output, relative to other steering interventions? No mechanistic language about why this works. Well-matched R2 claim from R2 method. No hedging."
2506.03292,2506.03292-02,HYPERSTEER performs on par with steering-via-prompting,abstract,3,HYPERSTEER hypernetwork-based activation steering evaluated on AxBench,2,2,0,5,0,Method is HYPERSTEER interventional steering (R2). The claim 'HYPERSTEER performs on par with steering-via-prompting' is a comparative empirical result: the cross-attention variant approaches prompting performance on held-in evaluation (Table 1: cross-attention 1.091 vs. prompting 1.091 on Gemma-2-9b HI). This compares the causal effect of two different interventions (steering vs. prompting) on model output quality. Purely an empirical comparison of intervention effects — R2 matched to R2 method. No hedging.
2506.03292,2506.03292-03,our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline,results,2,HYPERSTEER cross-attention variant evaluated on Concept500-HO (held-out steering prompts),2,2,0,5,0,Method is HYPERSTEER cross-attention variant applying steering vectors (R2 interventional). The claim 'cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline' is a direct empirical comparison of intervention outcomes on a held-out test set (Concept500-HO). The paper shows cross-attention scores 0.608/0.934 on HO vs. ReFT-r1 and other supervised baselines. This is a causal effect claim — how does applying this steering intervention affect output quality relative to other steering interventions? R2 claim matched to R2 method. No hedging.
2506.03292,2506.03292-04,as training data increases HYPERSTEER becomes much more economical than supervised activation steering,results,1,HYPERSTEER compute efficiency analysis (TFLOPS vs. training data scaling),2,2,0,4,0,"Method involves measuring computational cost (TFLOPS) required to achieve equivalent evaluation loss as training data increases — this is an efficiency analysis of the steering intervention (R2). The claim 'as training data increases HYPERSTEER becomes much more economical than supervised activation steering' asserts a causal relationship between training data scale and compute cost. Figure 3 shows HYPERSTEER TFLOPS drop ~2.4x while ReFT-r1 remains approximately constant. This is an empirical causal claim about the effect of scaling training data on compute efficiency. Slightly lower confidence because 'economical' touches on engineering rather than mechanistic claims, but the core claim is still about the effect of an intervention (training scale). R2 claim from R2 method. No hedging."
2506.03292,2506.03292-05,cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance,body,1,t-SNE/PCA geometric analysis of steering vectors (R1) combined with steering performance evaluation (R2),1,1,0,3,0,"This claim comes from Section 5 qualitative analyses. The claim 'cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance' has two parts: (1) 'inter-concept similarity is weakened' — this is derived from geometric analysis of steering vectors using t-SNE/PCA, which is an observational/correlational method (R1); (2) 'not at cost of steering performance' — this references the performance evaluation (R2). The primary novel finding here is the geometric similarity reduction (R1), while the performance result is already reported elsewhere. The claim's primary epistemic content is about the geometric structure of the steering vector space, which is R1 (observational). The method most directly supporting the 'weakened similarity' sub-claim is PCA/t-SNE (R1). This is an R1 claim matched to R1 method — no overclaim. Moderate confidence because the claim is composite and the performance portion is R2."
2506.18167,2506.18167-01,We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors,abstract,3,Difference of Means steering vectors applied to DeepSeek-R1-Distill models,2,2,0,4,0,Method is Difference of Means to extract steering vectors + applying them (adding/subtracting) to residual stream activations — interventional (R2). The claim 'these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors' has two parts: (1) 'mediated by linear directions' — 'mediated' is explicitly R2 language per the codebook; (2) 'can be controlled using steering vectors' — 'controlled' from an intervention is R2. The paper demonstrates causal control: applying the vector changes behavior rates. No claim about unique mechanism or complete explanation. The 'mediated' language is appropriate for an interventional result. Well-matched R2 claim from R2 method. No hedging in the abstract statement.
2506.18167,2506.18167-02,Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence,results,1,Steering vectors (Difference of Means) applied with positive and negative multipliers,2,2,0,5,0,"Method is steering vector intervention — adding/subtracting vectors from activations at inference time (R2). The claim 'Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence' is an explicitly causal claim. The phrase 'confirming the causal influence' directly names the epistemic status: the bidirectional intervention (positive/negative) is used to establish causal direction. This is the canonical R2 pattern: intervene, observe effect, conclude causal relationship. 'Confirming the causal influence' is R2 language, not R3 (no uniqueness or mechanism claim). Well-matched R2 claim from R2 method. No hedging."
2506.18167,2506.18167-03,These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions,results,1,"Steering vectors applied across two DeepSeek-R1-Distill model variants (Qwen-14B, Llama-8B)",2,3,1,4,1,"Method is steering vector intervention (R2). The claim 'These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions' has a clear R3 component: 'encode these reasoning mechanisms as linear directions' uses both 'encode' (storage/mechanistic language, R3 per codebook) and 'mechanisms' (functional attribution, R3). Per the codebook decision tree for 'encodes': the paper does provide interventional evidence, but the claim goes beyond the result of the intervention (what changed) to assert the underlying mechanism (how it works — encoding as linear directions). This is R3. The claim is hedged: 'reinforcing the hypothesis that' is an explicit hedge, placing this as a hypothesis not an established fact. hedge_flag=1. The overclaim gap is +1 (R2 method → R3 claim)."
2506.18167,2506.18167-04,Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors,conclusion,2,Steering vectors (Difference of Means) applied to DeepSeek-R1-Distill models,2,3,1,4,0,"Method is steering vector intervention (R2). The claim 'Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors' has two parts: (1) 'isolated to specific directions' — 'isolated' implies a unique, localized mechanism (not just causal sufficiency but specificity), which is R3 language (similar to 'THE circuit' — uniqueness of the responsible direction); (2) 'enabling precise control' — this follows from the intervention and is R2. The primary overclaim is 'isolated to specific directions,' which asserts that there is a single, identifiable direction responsible for each behavior — a mechanistic/uniqueness claim (R3). The steering method demonstrates causal effect (R2) but not that the direction is the unique or isolated mechanism (R3). No explicit hedge. Overclaim gap: +1 (R2 → R3)."
2506.18167,2506.18167-05,Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process,results,1,Steering vectors applied with attribution patching for layer localization,2,3,1,4,0,"Method is steering vector intervention + attribution patching (R2). The claim 'Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process' uses 'distinct mechanisms' — 'mechanisms' is a canonical R3 term (functional/mechanistic attribution). The claim asserts that the models have identifiable, distinct internal mechanisms for their reasoning process. This goes beyond showing causal effects of interventions (R2) to asserting the existence and distinctiveness of underlying computational mechanisms (R3). The evidence (steering vectors change behavior differently across models) establishes differential causal effects (R2) but the 'distinct mechanisms' framing makes a stronger mechanistic claim. 'Indicate that' provides slight attenuation but does not constitute a clear hedge in the codebook sense. Overclaim gap: +1 (R2 → R3)."
2507.08802,2507.08802-01,any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative,abstract,3,Mathematical proof (theoretical) under assumptions of input-injectivity and output-surjectivity,3,3,0,4,0,"Method is a formal mathematical proof (Theorem 1) showing that under reasonable assumptions (input-injectivity, output-surjectivity), arbitrarily complex alignment maps can always perfectly map any neural network to any algorithm. This is a counterfactual/necessity argument — it proves what would happen under any possible mapping, establishing the uniqueness/necessity of linearity constraints. This constitutes R3 evidence (necessity proofs, uniqueness arguments). The claim 'any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative' is a universal mechanistic/theoretical claim about the nature of causal abstraction itself — R3. Well-matched R3 claim from R3 method (mathematical proof establishes the mechanism by which causal abstraction becomes vacuous). No hedging; stated as proven theorem."
2507.08802,2507.08802-02,it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task,abstract,3,Empirical DAS experiments with non-linear alignment maps on randomly initialised models,2,2,0,4,0,"Method is distributed alignment search (DAS) with interchange interventions — applying non-linear alignment maps and measuring IIA. This is interventional (R2): it intervenes on model representations via interchange interventions and measures whether the model behaves as if it implements the algorithm. The claim 'it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task' is an existential empirical claim about what can be achieved with sufficiently powerful alignment maps. The paper shows >80% IIA (and later 100% IIA) on randomly initialised models. This is an empirical result from interventional experiments — the claim is about what the intervention achieves, not about the mechanism of why. R2 claim from R2 method, well-matched. No hedging."
2507.08802,2507.08802-03,randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task,abstract,3,DAS with non-linear alignment maps and interchange interventions on randomly initialised Pythia language models,2,2,0,5,0,"Method is interchange interventions (DAS) — a standard interventional method (R2). The claim 'randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task' is a direct empirical result from applying the intervention (interchange intervention) and measuring IIA. 100% IIA is the measured outcome of the intervention. This is a precise, quantitative R2 claim: what is the effect of applying these alignment maps (the intervention) on IIA? Well-matched R2 claim from R2 method. No hedging; stated as a concrete numerical result."
2507.08802,2507.08802-04,causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information,abstract,3,Mathematical proof combined with empirical DAS experiments,3,3,0,4,0,"Method combines formal proof (R3) and empirical DAS (R2); the overall claim is grounded in the theorem (R3 method). The claim 'causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information' is a meta-theoretical claim about what causal abstraction can and cannot establish — it makes a statement about the necessary conditions for mechanistic interpretability. 'How models encode information' uses 'encode' in a structural/mechanistic sense. The claim asserts a necessary condition (uniqueness argument: without encoding assumptions, causal abstraction cannot work), which is R3 language. The paper's theorem proves this necessity. Well-matched R3 claim from R3 method. No hedging; stated as the main conclusion of the paper."
2507.20936,2507.20936-01,early MLP layers attend not only to the syntactic structure but also process its semantic content,abstract,3,"Activation patching (de-noising variant, causal mediation analysis) on MLP layers",2,3,1,4,0,"Method is activation patching — replacing activations of components from a corrupted run with those from a clean run and measuring the effect on output logits (R2 interventional). The claim 'early MLP layers attend not only to the syntactic structure but also process its semantic content' uses the functional verb 'process' — per the codebook, 'performs,' 'computes,' 'processes' are R3 linguistic markers (functional attribution). The claim asserts what the MLP layers *do* (process semantic content), not merely that patching them has a causal effect. The patching evidence shows these layers *have causal influence* (R2), but the 'also process its semantic content' framing asserts a functional role — what the layer *is doing*, which is R3. Overclaim gap: +1 (R2 method → R3 claim). No hedging."
2507.20936,2507.20936-02,these layers transform persona tokens into richer representations which are then used by middle MHA layers,abstract,3,"Activation patching on MLP layers and MHA layers (total, direct, and indirect effects)",2,3,1,4,0,"Method is activation patching measuring direct and indirect effects (R2). The claim 'these layers transform persona tokens into richer representations which are then used by middle MHA layers' is a mechanistic narrative: 'transform... into richer representations' (functional computation claim, R3) and 'which are then used by middle MHA layers' (information flow/mechanism, R3). This is the canonical R2→R3 overclaim pattern: patching shows that early MLP layers have causal influence and that MHA layers depend on them (R2), but the framing as 'transform into richer representations which are used by' asserts a specific computational mechanism and information flow — the model 'uses X to do Y' pattern identified in the codebook as R3. Overclaim gap: +1. No hedging."
2507.20936,2507.20936-03,we identify specific attention heads that disproportionately attend to racial and color-based identities,abstract,3,Activation patching on individual attention heads combined with value-weighted attention pattern analysis,2,2,0,3,0,"Method is activation patching on individual attention heads (R2) supplemented by value-weighted attention pattern analysis (R1). The claim 'we identify specific attention heads that disproportionately attend to racial and color-based identities' uses 'attend to' which describes the attention pattern (R1 language — observational) but the heads were *identified* via activation patching (R2 — showing causal influence). The claim is about identifying causally important heads, not about a unique mechanism. 'Disproportionately attend to' describes the observed attention weight pattern (R1), but the identification of these heads as *important* is grounded in patching (R2). The dominant epistemic claim is R2: these heads have a disproportionate causal effect on outputs for racial/color-based personas. No uniqueness or mechanism claim beyond this. Moderate confidence (3) because the attention pattern language is R1 while the identification via patching is R2, and the claim conflates both."
2508.11214,2508.11214-01,the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation,abstract,3,theoretical/conceptual analysis using causal abstraction formalism; no empirical intervention on model internals,1,1,0,3,0,"This is a philosophy-of-computation paper that argues theoretically for causal abstraction as a framework for understanding computational implementation. The method is purely conceptual/theoretical — no empirical data, probes, or interventions on model internals. The claim ('provides a fruitful lens') is a thesis statement about the utility of a theoretical framework, not an empirical claim about a specific model. It is most naturally read as an associational/observational claim (R1) since it asserts a conceptual correspondence rather than a causal effect or mechanism. The paper does use interventional concepts (do-calculus, interchange interventions) as formal tools, but the claim itself is a meta-level theoretical assertion. Confidence is 3 because classifying philosophical position papers is genuinely ambiguous — this is not a standard empirical interpretability claim, and the rung framework was designed for empirical papers. The claim could also be read as outside the rung schema entirely; I code it R1 as the most defensible assignment for a non-interventional thesis claim."
2508.11214,2508.11214-02,we offer an account of computational implementation grounded in causal abstraction,abstract,3,theoretical/conceptual analysis using causal abstraction formalism; formal proofs and examples,1,2,1,3,0,"The claim 'we offer an account of computational implementation grounded in causal abstraction' presents a theoretical framework (causal abstraction) as the basis for a new account of computational implementation. The method is purely theoretical — formal definitions, proofs, and illustrative examples with toy neural networks (e.g., XNOR circuit), not empirical interventions. The claim itself is a contribution claim about providing a framework. The rung classification is ambiguous because this is a philosophical/theoretical paper, not a standard empirical interpretability paper. I code method_rung=1 (no empirical intervention on model internals) and claim_rung=2 because 'grounded in causal abstraction' implies the account is causally grounded — i.e., it makes causal claims about implementation. This is a mild overclaim relative to the theoretical method: the account uses causal formalism but the paper does not empirically validate that the proposed causal abstraction maps correctly onto real neural networks. Confidence is 3 given the unusual paper type."
2508.21258,2508.21258-01,RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs,abstract,3,correlation analysis (Pearson correlation coefficient between RelP/AtP and activation patching ground truth),1,1,0,5,0,"The claim states that RelP 'more accurately approximates activation patching than standard attribution patching, particularly when analyzing residual stream and MLP outputs.' The primary evidence is Pearson correlation coefficients (PCC) between the attribution scores of RelP/AtP and the ground-truth activation patching scores. PCC is a correlation/associational metric — it measures how well the approximation correlates with the ground truth, not whether RelP causally changes model behavior. The method is observational (computing correlations between two sets of attribution scores), placing it firmly at Rung 1. The claim mirrors the method — it says RelP 'more accurately approximates,' which is an associational statement about correlation quality. No claim about causal effects on model internals is made. No hedge. Clear R1-R1 case."
2508.21258,2508.21258-02,For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956,abstract,3,correlation analysis (Pearson correlation coefficient between RelP and activation patching for MLP outputs in GPT-2 Large),1,1,0,5,0,"This is a precise empirical result: 'for MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956.' The method is correlation analysis (PCC) between attribution scores and ground-truth activation patching effects. Reporting a correlation coefficient is a quintessential Rung 1 associational result — it shows how well two sets of scores agree statistically, with no intervention on the model. The claim language ('achieves a Pearson correlation') directly uses associational vocabulary. Clear R1-R1 with high confidence."
2508.21258,2508.21258-03,RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost,abstract,3,faithfulness evaluation metric (ablation-based: circuit vs. mean-ablated model) for sparse feature circuits identified by RelP vs. Integrated Gradients,2,2,0,4,0,"The claim states RelP 'achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost.' Faithfulness is defined in the paper as L(C)-L(empty) / L(M)-L(empty), where L(empty) is the fully mean-ablated model's metric and L(C) is the metric when only the circuit C is active. This requires mean ablation (a Rung 2 intervention) to measure. The claim is about causal faithfulness of the discovered circuits — how well the circuit causally accounts for the model's behavior. This is a Rung 2 claim (causal sufficiency) supported by Rung 2 methods (ablation). The claim is that RelP circuits are 'sufficient for' the behavior to the same degree as IG circuits. No uniqueness claim is made, and no hedge is present. R2-R2, confident assignment."
2508.21258,2508.21258-04,small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance,body,1,"faithfulness evaluation via mean ablation (ablating all nodes except circuit, measuring performance vs. mean-ablated baseline)",2,2,0,4,0,"The claim is: 'small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance.' The evidence comes from faithfulness scores computed by mean-ablating all model components except the circuit and measuring the fraction of original performance retained. This is a Rung 2 method (ablation intervention). The claim uses 'explain' and 'account for' — this is causal sufficiency language (Rung 2): 100 features are sufficient to account for the majority of performance. The paper does not claim these features are THE unique mechanism or that no alternative circuit exists, so this does not rise to Rung 3. The claim is appropriately matched to the ablation method. Confidence 4 — minor ambiguity in whether 'account for' implies mechanistic completeness (R3), but the paper frames it as a sufficiency/faithfulness result."
2508.21258,2508.21258-05,RelP enables more faithful localization of influential components in large models,abstract,3,"relevance patching (RelP) with LRP-derived propagation coefficients, evaluated via Pearson correlation with activation patching ground truth",2,3,1,3,0,"The claim is 'RelP enables more faithful localization of influential components in large models.' The word 'localization' here means identifying which components causally contribute to behavior. The method is RelP, which is an approximation to activation patching (Rung 2 — causal intervention). The evaluation is via PCC correlation with ground-truth activation patching. The claim uses 'localization of influential components' — this is mechanistic language that implies the method correctly identifies the components responsible for behavior, going beyond showing a causal effect to implying identification of the specific components. 'Localization' in mech interp typically connotes identifying THE responsible components, which is a Rung 3 mechanistic identification claim. However, the paper is careful in some places — it measures correlation with activation patching, not true mechanistic uniqueness. Confidence 3 because 'localization' is genuinely polysemous: it could mean 'identifies causally influential components' (R2) or 'identifies the components responsible for behavior' (R3). I lean R3 given the framing in the abstract and the use of 'responsible for specific behaviors' in the intro."
2509.06608,2509.06608-01,the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token,body,1,"steering vectors (added to residual stream at specific layers, trained with RL objective); logit-lens projection; token probability analysis; behavioral ablation (prefixing tokens)",2,3,1,4,0,"The claim is 'the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token.' The methods used to support this are: (1) steering vectors added to the residual stream (Rung 2 intervention), (2) logit-lens projection of the steering vector through the unembedding matrix (Rung 1 — observational), (3) behavioral test by prefixing the top token 'To' to prompts (Rung 2 — causal intervention). The claim uses 'acts like' which is a functional/mechanistic description asserting the vector's mechanism of action (it IS a token-substitution bias). This is Rung 3 language — it describes what the vector DOES mechanistically, not just that it has a causal effect. The paper identifies a specific mechanism ('acts like token-substitution') and characterizes it with specificity ('concentrated on the first generated token'). This pattern matches the calibration examples of R2 methods producing R3 mechanistic narratives. The prefixing experiment supports the claim causally but the mechanistic description ('acts like token-substitution bias') goes beyond showing a causal effect to characterizing the mechanism."
2509.06608,2509.06608-02,the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words,body,1,"steering vectors with ablation-style circuit analysis (Skip-Attn, Skip-Layer, Steer-Q/K/V-Proj experiments) to identify which submodules carry the steering effect",2,3,1,4,0,"The claim is 'the penultimate-layer vector leaves attention patterns largely intact and instead operates through the MLP and unembedding, preferentially up-weighting process words and structure symbols.' The methods are ablation-style interventions: inserting the steering vector at specific submodules (Skip-Attn, Skip-Layer, Steer-Q/K/V-Proj) and measuring accuracy impact (Rung 2). The claim describes a mechanism of action: the vector 'operates through the MLP and unembedding' and 'preferentially up-weighting process words.' This is mechanistic narrative — it describes HOW the vector works (its computational pathway) and WHAT it does (up-weight specific word types). 'Operates through' and 'up-weighting' are functional/mechanistic verbs consistent with Rung 3. The evidence is Rung 2 (ablations show that Skip-Attn preserves most of the gain, pointing to MLP as main contributor), but the claim is stated as a mechanistic description of the circuit, which is R3. Pattern matches R2→R3 overclaim via mechanistic narrative."
2509.06608,2509.06608-03,steering vectors transfer to other models,body,1,"cross-model transfer experiment: steering vectors trained on donor model inserted into recipient model, measuring normalized accuracy gain",2,2,0,4,0,"The claim is 'steering vectors transfer to other models.' The method is a causal intervention: take all-layer steering vectors trained on a donor model, insert them into a recipient model, and measure performance gain (Table 1). This is a Rung 2 intervention — the vectors are added to the residual stream and the effect on performance is measured. The claim 'transfer to other models' means that inserting vectors trained on one model causally produces performance gains in another model. This is appropriately matched to the intervention method: the paper measures the causal effect of the vector intervention across models. The claim does not assert that the vectors identify THE mechanism or that they are the unique cause — it simply states that the causal effect transfers. The paper notes 'non-trivial gain, suggesting that the directions associated with improved math performance are largely preserved,' which is a hedged mechanistic interpretation but the core claim is about causal transfer. No hedge in the primary claim text. R2-R2 match."
2509.18127,2509.18127-01,SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features,abstract,3,"SAE feature attribution (TopKReLU sparse autoencoder trained on MLP/residual stream activations, with correlation-based evaluation metrics)",1,3,2,4,0,"The claim is 'SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features.' The method is SAE feature attribution — training sparse autoencoders to decompose activations into sparse features and observing which features activate on which inputs. This is Rung 1 (observational): no intervention on model internals, just activation logging and decomposition. The claim asserts that SAEs 'clarify model behavior' and that features have 'single-meaning' (monosemantic) properties — this is a mechanistic claim about what the model encodes internally. 'Single-meaning atomic features' implies the features represent specific concepts, which is a Rung 3 representational claim. 'Clarify model behavior' implies the SAE features causally underlie or explain behavior, which goes beyond mere correlation. The default reading of 'explain single-meaning atomic features' is mechanistic (R3) under the codebook decision tree for 'represents/encodes.' No interventional evidence is provided for the claim that these features are truly monosemantic or that they causally drive behavior. Classic R1→R3 overclaim via SAE → 'represents' pattern."
2509.18127,2509.18127-02,Safe-SAIL systematically identifies SAE with best concept-specific interpretability,abstract,3,"concept-specific interpretability metrics (L0,t and ICDF) measuring neuron differentiation across concept/de-concept pairs; correlation-based simulation scoring",1,1,0,4,0,"The claim is 'Safe-SAIL systematically identifies SAE with best concept-specific interpretability.' This is a methodological/performance claim about the framework itself: it asserts that the proposed evaluation metrics (L0,t and ICDF) identify which SAE configuration produces the most concept-specific neurons. The method is observational: measuring activation frequencies of neurons across concept vs. de-concept query pairs and computing correlation scores. This is Rung 1 — no intervention on the model, just logging activations and computing statistics. The claim 'identifies SAE with best concept-specific interpretability' is matched to the method — it says the framework can select the right configuration (a comparative/associational claim about which SAE performs best on the metrics). No causal intervention is claimed, and no mechanistic narrative about model behavior is made. The paper validates this by showing the metrics correlate with ground-truth neuron counts. R1-R1 match, confident."
2509.18127,2509.18127-03,we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors,abstract,3,SAE feature attribution with automated explanation generation and simulation-based scoring (correlation between simulated and actual neuron activations),1,3,2,4,0,"The claim is 'we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors.' The method is SAE feature attribution: train sparse autoencoders, observe which features activate on safety-related content, generate natural language explanations via LLM, score explanations with simulation correlation. This is Rung 1 — purely observational activation analysis with no intervention on the model. The claim asserts that the extracted features 'capture high-risk behaviors' — 'capture' is mechanistic language implying the features represent or underlie these behaviors in the model. This is a Rung 3 claim: it suggests these features are the internal representation of high-risk behaviors, not merely that they correlate with such inputs. The paper's framing throughout uses mechanistic language ('underlying mechanisms that drive risk behaviors,' 'encode specific real-world risk entities'). Under the codebook decision tree: does context make clear 'capture' means 'is decodable from'? No — the paper uses causal/mechanistic language. Default reading is R3. Classic SAE → 'represents/captures' overclaim (+2 gap)."
2510.01070,2510.01070-01,Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor,abstract,3,"white-box interpretability techniques (logit lens, SAE feature attribution) applied to model internals; auditing game measuring success rate",1,2,1,4,0,"The claim is 'our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor.' The logit lens is an observational technique (projecting residual stream through unembedding matrix — Rung 1); SAE feature attribution is also Rung 1. However, the claim is evaluated in an auditing game where the auditor uses the extracted information to guess the secret — this provides a behavioral measure of whether the information is causally useful. The claim 'increase the success rate' is a causal sufficiency claim: the white-box information causally produces better auditor performance. This is Rung 2 — an intervention (providing white-box information) produces a measurable effect (higher success rate). The method_rung is 1 because the white-box extraction itself is observational, but the overall experimental setup treats the information provision as an intervention with measured effect. The claim is thus a mild overclaim: the extraction method is R1 but the causal effect claim (increases success rate) is R2. I code method_rung=1 (the primary extractive method) and claim_rung=2 (causal effect on auditor success rate). No hedge present."
2510.01070,2510.01070-02,secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly,results,1,logit lens (projecting residual stream activations through unembedding matrix at intermediate layers) and SAE feature attribution applied to secret-keeping model internals,1,3,2,4,0,"The claim is 'secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly.' The methods are logit lens (R1 — observational, projects activations through unembedding) and SAE feature attribution (R1 — logs which SAE features activate). These are purely observational — no intervention on the model. The claim asserts that the model's internal states contain secret knowledge that can be extracted — this is a Rung 3 representational claim: the model 'internally represents' the secret knowledge. 'Internal states' containing knowledge that 'can be extracted' implies the model encodes this knowledge as a stable internal representation, which is the mechanistic reading. Under the codebook decision tree for 'encodes/represents': the paper does not provide interventional evidence specifically for this storage claim (only observational evidence that the logit lens and SAE show signals corresponding to the secret). The default reading is R3. However, the paper is careful to frame this as 'extracted from internal states' rather than claiming what mechanism stores it. Confidence 4: clear R3 claim from R1 methods, classic SAE/logit-lens → 'represents' overclaim pattern."
2510.01070,2510.01070-03,Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal,body,1,logit lens (projecting residual stream through unembedding at intermediate layers) and SAE feature attribution applied to User Gender model during refusal responses,1,1,0,4,0,"The claim is 'white-box methods reveal strong signals corresponding to the female gender during this refusal.' The methods are logit lens (showing top tokens like 'girls', 'female', 'ladies' at intermediate layers) and SAE feature attribution (showing features with descriptions like 'references to women', 'gender-related terms'). These are purely observational — no intervention on the model. The claim uses 'reveal strong signals corresponding to' — this is Rung 1 language: signals are present/detectable/associated with the concept. The claim does not assert that the model encodes or represents gender (which would be R3), but rather that there are detectable signals 'corresponding to' the gender. The paper itself frames this as showing that 'white-box methods reveal strong signals' — decodability language. The word 'reveal' is ambiguous but 'signals corresponding to' is more associational than mechanistic. The example is explicitly described as cherry-picked, and the paper notes the methods 'increase success rate' rather than claiming the model encodes gender. R1-R1 with confidence 4, noting mild ambiguity in 'reveal' which could push toward R3."
2510.01070,2510.01070-04,Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it,results,2,downstream task evaluation: measuring model performance on tasks requiring the secret knowledge (behavioral evaluation of internalization),1,3,2,4,0,"The claim is 'Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it.' The method is behavioral: measuring the fine-tuned model's accuracy on downstream tasks that require applying the secret knowledge (e.g., Taboo model consistently hints at the secret word; User Gender model answers gender-dependent questions correctly). This is Rung 1 — observational output evaluation with no intervention on model internals. The claim uses 'internalized' and 'are aware of it' — these are strong mechanistic/cognitive claims asserting that the model has internal representations of the knowledge and has cognitive access to it ('aware'). 'Internalize' implies the knowledge is encoded as part of the model's internal mechanism, not merely that the model outputs consistent with the knowledge. 'Aware of it' adds anthropomorphic mechanistic framing. Both terms are R3 under the codebook: storage/encoding language without interventional evidence for the storage mechanism. The behavioral evidence shows performance consistent with having the knowledge, but 'internalized and aware' overclaims the mechanistic interpretation of that behavioral evidence. R1→R3 overclaim."
2510.01070,2510.01070-05,Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques,body,1,theoretical/logical argument (no empirical method directly supports this specific claim; it is a motivating premise for the white-box approach),1,3,2,3,0,"The claim is 'Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques.' This is a theoretical/motivating argument, not a direct empirical finding. The reasoning is: (1) models must internally represent knowledge to apply it downstream, (2) therefore that representation should be extractable. The 'must internally represent' is a Rung 3 claim — it asserts that the model stores/encodes knowledge as an internal representation (mechanistic claim). There is no method that directly demonstrates this necessity — it is asserted as a logical premise. The method_rung is 1 because the supporting methods are observational (logit lens, SAE show signals), but this specific claim is a theoretical assertion with no direct empirical support. The claim_rung is 3: 'internally represent' is canonical R3 storage/encoding language under the codebook. Confidence 3 because this claim is unusual — it is a theoretical premise rather than an empirical finding, which makes rung classification less straightforward. The hedge_flag is 0 despite 'should be able to extract' because the hedge is about extractability, not about whether the representation exists ('must internally represent' is stated as fact)."
2510.06182,2510.06182-01,LMs implement such retrieval via a positional mechanism,abstract,3,interchange interventions (causal abstraction / DAS-style activation patching),2,3,1,4,0,"The paper uses interchange interventions—a Rung 2 method that establishes causal effects by swapping activations between paired runs—to identify which hidden states are causally relevant for entity retrieval. The claim that LMs 'implement such retrieval via a positional mechanism' uses 'implement', a functional mechanistic verb that implies the model executes a specific algorithm. This places it at Rung 3: the claim is not merely that positional information causally influences output (R2), but that the positional mechanism is the implementation strategy. The definite article and present tense further signal a mechanistic narrative. Overclaim gap: +1 (R2 method → R3 claim)."
2510.06182,2510.06182-02,LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism,abstract,3,interchange interventions (causal abstraction / activation patching across layers and positions),2,3,1,4,0,"Same method as claim 01: interchange interventions (Rung 2). The claim that LMs 'supplement the positional mechanism with a lexical mechanism and a reflexive mechanism' uses mechanistic implementation language—'supplement' implies these are the specific algorithmic strategies the model uses. The framing of three named mechanisms with specific functional descriptions ('retrieving Ann using its bound counterpart pie'; 'a direct pointer') constitutes a mechanistic narrative (R3). The paper builds a causal model combining all three, which is presented as reverse-engineering the algorithm. Overclaim gap: +1 (R2 → R3)."
2510.06182,2510.06182-03,causal model combining all three mechanisms that estimates next token distributions with 95% agreement,body,1,causal abstraction / interchange interventions used to evaluate causal model fit against LM next-token distribution,2,2,0,4,0,"The claim is about a causal model that 'estimates next token distributions with 95% agreement.' This is a quantitative fidelity/faithfulness claim: the authors construct a causal model and measure how well its predictions match the LM's output under interchange interventions. The 95% agreement figure is an empirical result of the causal abstraction evaluation—a Rung 2 method. The claim is about the result of the intervention-based comparison (how well the model fits), not about the unique underlying mechanism. This places it at R2: it is a causal sufficiency claim (the three-mechanism model is sufficient to account for ~95% of behavior), not a uniqueness or mechanistic implementation claim. Confidence is 4; there is minor ambiguity since the causal model framing could be read as R3, but the specific phrasing 'estimates...with 95% agreement' is a performance/fit claim."
2510.06182,2510.06182-04,how LMs bind and retrieve entities in-context,abstract,3,interchange interventions (causal abstraction),2,3,1,5,0,"The claim 'how LMs bind and retrieve entities in-context' is the overarching mechanistic narrative of the paper, asserting that the study establishes THE mechanism by which LMs perform entity binding and retrieval. The abstract closes: 'our study establishes a more complete picture of how LMs bind and retrieve entities in-context.' The phrase 'how LMs [do X]' is canonical Rung 3 language—it is a functional/mechanistic attribution (the model uses X to do Y). The method is interchange interventions (R2), which establishes causal effects but not the unique mechanism. Overclaim gap: +1 (R2 → R3). High confidence because the framing is unambiguously mechanistic."
2511.05923,2511.05923-01,MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information,abstract,3,causal tracing (activation patching with Gaussian noise corruption) — FCCT framework,2,3,1,4,0,"The method is causal tracing / activation patching (Rung 2): the paper runs a clean pass, a corrupted pass (Gaussian noise on the image), and a patched pass restoring specific components, measuring recovery rate as a causal effect estimate. The claim that MHSAs of the last token 'play a critical role in aggregating cross-modal information' uses 'aggregating' as a functional verb describing what the component does (i.e., its computational role), which is Rung 3 language. 'Play a critical role' additionally implies causal necessity. The paper's Finding 1 text states 'plays a particularly crucial role in aggregating information from all preceding tokens'—a mechanistic functional attribution. While activation patching can establish causal relevance (R2), the claim goes further to assert the specific function being performed ('aggregating cross-modal information'), constituting R3. Overclaim gap: +1."
2511.05923,2511.05923-02,FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations,abstract,3,causal tracing (activation patching with Gaussian noise corruption) — FCCT framework,2,3,1,5,0,"The method is causal tracing / activation patching (Rung 2). The claim that FFNs 'exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations' contains canonical Rung 3 language: 'storage' and 'transfer' are mechanistic/functional verbs attributing computational roles to the FFN components. Per the codebook and calibration rationales, storage/memory language ('stores,' 'encodes,' 'contains') is a hallmark overclaim from causal tracing, which can only establish causal mediation (R2), not storage mechanisms. The paper's body text confirms: 'In early layers, visual object tokens are encoded into localized, modality-specific embeddings... In intermediate layers, textual object tokens interact with visual representations... In the deeper layers... the last token's MLPs progressively accumulate a cross-modal and task-relevant representation.' This is a full mechanistic narrative describing what the model computes, not just what causally affects the output. Overclaim gap: +1 (R2 → R3). High confidence."
2511.05923,2511.05923-03,we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow,abstract,3,"inference-time intervention (IRI injects mid-layer activations into later layers, evaluated on hallucination benchmarks)",2,2,0,4,0,"IRI is a training-free inference-time technique that intervenes on internal activations by injecting mid-layer MHSA and MLP outputs into subsequent layers, scaled by recovery rates derived from FCCT. The claim that IRI 'reinforces visual object information flow' is causal intervention language: 'reinforces' describes the effect of the intervention on information propagation, not the unique underlying mechanism. This is appropriately Rung 2: the paper demonstrates that the intervention has a causal effect (improved benchmark performance across five LVLMs and five benchmarks), which supports the 'reinforces' claim at the level of causal sufficiency. The claim does not assert uniqueness or describe the underlying computational mechanism in detail. Confidence 4; there is minor ambiguity since 'information flow' has some mechanistic flavor, but in context it is used to describe what the intervention acts upon rather than asserting a unique mechanism."
2511.09432,2511.09432-01,incorporating group symmetries into the SAEs yields features more useful in downstream tasks,abstract,3,"binary probing tasks over SAE latent activations (XGBoost, logistic regression, kNN probes)",1,1,0,4,0,"The method is probing—training classifiers over SAE latent activations to evaluate downstream usefulness of discovered features (Rung 1: observational/correlational). The claim that 'incorporating group symmetries into the SAEs yields features more useful in downstream tasks' is a comparative performance claim: equivariant SAEs produce latents that achieve higher F1 on binary probing tasks than regular SAEs. This is appropriately Rung 1: the claim is about decodability/usefulness of features for downstream classification, not about causal effects or mechanisms. The language 'more useful in downstream tasks' is purely correlational/predictive. No gap: method and claim are both R1. Confidence 4; slight ambiguity because 'yields features more useful' could be read as causal (the symmetry incorporation causes better features), but the evaluation is entirely probing-based and the authors do not perform any intervention to verify this causally."
2511.09432,2511.09432-02,a single matrix can explain how their activations transform as the images are rotated,abstract,3,linear regression / matrix optimization — fitting a single matrix M to predict transformed activations (R² metric),1,1,0,5,0,"The method is fitting a linear transformation matrix M to predict how activations change under input rotations, evaluated via R² (variance explained). This is observational/correlational (Rung 1): it establishes that a linear relationship exists between original and transformed activations, without any causal intervention on the model. The claim that 'a single matrix can explain how their activations transform as the images are rotated' uses 'explain' in the statistical sense—accounting for variance in the transformed activations. The paper reports R² = 0.987 ± 0.001, a correlational fit statistic. This is appropriately Rung 1: the claim is about linear predictability of activation transformations, not about causal mechanisms. No overclaim gap. High confidence."
2511.09432,2511.09432-03,adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs,abstract,3,"binary probing tasks over SAE latent activations and reconstructions (XGBoost, logistic regression, kNN probes)",1,1,0,5,0,"The method is probing—comparing probing performance of adaptive/equivariant SAEs versus regular SAEs on 180 binary probing tasks (Rung 1). The claim that 'adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs' is a direct empirical performance comparison: equivariant SAEs yield higher F1 on probing tasks. This is Rung 1: it is a correlational/decodability claim about what information can be extracted from SAE features, with no causal intervention on the model. 'Discover features that lead to superior probing performance' is appropriate decodability language. No overclaim gap. High confidence—this is a clear R1 → R1 case with unambiguous observational methodology and matching claim language."
2511.22662,2511.22662-01,The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals,introduction,2,conceptual analysis and case study examination of existing deception evaluation benchmarks (no direct intervention on model internals),1,3,2,3,0,"The paper is an argumentative/analytical paper that critically examines existing deception detection evaluations using conceptual arguments and illustrative case studies. The primary 'method' is observational behavioral analysis—examining model outputs across different prompts without intervening on internal representations. The claim that 'distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals' is a Rung 3 claim: it asserts something about the internal state of the model (beliefs and goals) as the crux of the problem. While framed as a meta-claim about what is required rather than a direct finding about internals, it presupposes that such internal states exist and are the relevant ground truth. Confidence is 3 because this is primarily a conceptual paper, not an empirical one—the 'method' categorization is ambiguous. The claim is stated as established fact with no hedge. Overclaim gap: +2 (R1 behavioral observation → R3 internal state claim)."
2511.22662,2511.22662-02,What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance,body,1,conceptual analysis — discussion of necessary conditions for probe-based deception classifiers (no empirical method applied),1,3,2,3,0,"This claim is a rhetorical question posed in Section 5 of the paper: 'What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance?' The paper uses this as a framing device for conceptual analysis, not as a direct empirical finding. However, the question itself presupposes and investigates Rung 3 territory—internal states, consistent mechanisms, and the structural requirements for deception to be detectable internally. The 'method' supporting the discussion is purely conceptual/analytical reasoning (R1 at best), while the claim territory concerns internal mechanistic structure (R3). Confidence is 3 because the claim is an interrogative framing rather than a declarative empirical statement, making rung assignment inherently ambiguous. The paper does not hedge this question but frames it as an open empirical question."
2511.22662,2511.22662-03,Model beliefs are not stable and are far more context dependent than animal or human beliefs,body,1,behavioral probing experiments — plausibility ratings and roleplay break-out prompts applied to model outputs across different prompt variants,1,1,0,4,0,"The method supporting this claim is observational behavioral analysis: the authors examine how models respond to different prompts and contexts (plausibility ratings, roleplay break-out experiments, identity questions) to infer something about belief stability. This is Rung 1—no intervention on model internals, only observation of input-output behavior. The claim that 'Model beliefs are not stable and are far more context dependent than animal or human beliefs' is itself an observational/correlational claim about the relationship between context and model behavior (used as a proxy for beliefs). The language 'are not stable' and 'context dependent' describes a behavioral correlation, not a mechanistic internal state. Although 'beliefs' invokes internal states, the paper explicitly adopts an intentional-stance/functional view of beliefs grounded entirely in observed behavior. No overclaim gap. Confidence 4; minor ambiguity because 'beliefs' could be read as R3, but the paper explicitly grounds this in behavioral evidence and adopts a functional rather than representationalist view."
2511.22662,2511.22662-04,We find very low agreement between a full-transcript autorater and the MASK labels,results,1,automated rating — full-transcript LLM autorater applied to MASK benchmark transcripts; agreement computed against MASK procedural labels,1,1,0,5,0,The method is observational: the authors run a full-transcript LLM autorater over MASK benchmark rollouts and measure agreement with the MASK procedural labels (which are also observational/behavioral). No intervention on model internals is performed. The claim that 'we find very low agreement between a full-transcript autorater and the MASK labels' is a straightforward observational/measurement claim about the correlation between two labeling systems. The language 'low agreement' is correlational and matches Rung 1 exactly. Figure 7 and Figure 8 in the paper show the disagreement statistics. No overclaim gap. High confidence—this is an unambiguous R1 → R1 case.
2511.22662,2511.22662-05,It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive,body,1,behavioral observation and conceptual analysis of current model deception examples (chain-of-thought verbalization as observable proxy),1,3,2,3,1,"The claim is: 'It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive.' The hedge 'mostly true' and 'today' are explicit qualifications—hedge_flag = 1. The method is behavioral/conceptual: the paper observes that current models verbalise their deceptive reasoning in CoT, and uses this as evidence for a 'consistent mechanism.' This is Rung 1 (behavioral observation, no internal intervention). The claim itself is Rung 3: it asserts the existence of 'a consistent mechanism' for deception—a mechanistic internal structure claim. The paper (Section 5, Example 1) states: 'It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive; as discussed in Section 3.2, they will reliably verbalise their deceptive intent.' The 'mechanism' here is identified via CoT verbalization (behavioral proxy), not through any internal intervention. Overclaim gap: +2 (R1 behavioral → R3 mechanistic). Confidence 3 because the paper is primarily conceptual and the 'mechanism' claim is loosely supported—the authors themselves treat this as an empirical question requiring further investigation."
2512.05534,2512.05534-01,neural networks represent meaningful concepts as directions in their representation spaces,abstract,3,citation of empirical literature on linear representation hypothesis (observational/correlational studies from prior work),1,3,2,4,0,"The claim that 'neural networks represent meaningful concepts as directions in their representation spaces' is stated as an established background fact, citing prior empirical work (Park et al. 2024, Elhage et al. 2022, Marks & Tegmark 2024, Nanda et al. 2023). The supporting evidence from the cited literature is primarily observational: linear probing, PCA/SVD decompositions, and activation analysis—all Rung 1 methods. The claim itself uses 'represent' as a mechanistic/functional verb: per the codebook decision tree for 'represents/encodes', when there is no interventional evidence provided within this paper (only citations to prior observational work), the default reading is R3. The paper formalizes this as the Linear Representation Hypothesis (Assumption 2.3), treating it as a foundational premise. Overclaim gap: +2 (R1 observational evidence → R3 representational claim). Confidence 4; minor ambiguity since the LRH is framed as an assumption/hypothesis rather than a finding, but the claim text is stated without hedge."
2512.05534,2512.05534-02,we develop the first unified theoretical framework considering SDL as one optimization problem,abstract,3,mathematical/theoretical analysis — formal proof of SDL as piecewise biconvex optimization problem with characterization of global solution set,1,1,0,3,0,"The claim is that the paper develops 'the first unified theoretical framework considering SDL as one optimization problem.' This is a methodological/theoretical contribution claim rather than an empirical claim about model internals. The 'method' is mathematical proof and formal analysis—no intervention on neural network internals is performed. The claim does not assert a mechanistic property of any specific neural network; it asserts a property of an optimization framework. This is best classified as Rung 1 (or arguably outside the rung taxonomy, since it is a mathematical rather than empirical claim about models). Confidence is 3 because the rung framework applies most naturally to empirical claims about model internals; a theoretical framework paper is an edge case. The codebook notes review/theoretical papers may be NA for replication, but the rung classification still applies to the nature of the claim. Framed as a contribution statement, not a finding about neural network behavior."
2512.05534,2512.05534-03,we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons,abstract,3,"theoretical analysis — characterization of spurious partial minima in SDL optimization landscape (Theorems 3.7, 3.10) plus synthetic benchmark validation",1,1,0,4,0,"The claim is that the paper provides 'novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons.' The method is theoretical: the paper proves (Theorem 3.7) that spurious partial minima with polysemanticity are prevalent, and (Theorem 3.10) that hierarchical concept structures naturally induce feature absorption patterns. These are mathematical characterizations, not interventions on neural network internals. The claim uses 'explanations for empirically observed phenomena'—this is appropriately Rung 1 because it addresses observational phenomena (features seen in trained SAEs) via a theoretical/mathematical account. The paper does validate the theory on a synthetic benchmark (Linear Representation Bench), but this is also observational—measuring feature recovery rates without intervening on any deployed model. No mechanistic claim about specific model internals is made. Confidence 4; the claim is clearly theoretical, though 'explanations for' could in principle be read as mechanistic (R3), but the context is entirely mathematical theory, not claims about specific neural network behavior."
2512.05794,2512.05794-01,TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation,abstract,3,SAE feature attribution (TopK SAE) + steering vectors,2,2,0,4,0,"The paper uses two method types: (1) TopK SAE feature attribution with linear probing to identify biologically meaningful latent features (R1), and (2) steering vectors to test causal control over generation (R2). The dominant method for evaluating the claim is steering (R2), because the core assertion—that high feature-concept correlation does NOT guarantee causal control—requires an interventional test. The claim makes a causal sufficiency argument at R2: it asserts that observational correlation (R1 evidence) is insufficient to guarantee causal influence, and backs this up with steering experiments. Both parts of the claim are at R2 level: the first part ('reveal meaningful features') is implicitly supported by R1 methods but the second part ('does not guarantee causal control') requires and uses R2 evidence. No hedge language present; the claim is stated as a factual finding. Confidence is 4 because the dual-method nature makes rung assignment slightly ambiguous, but the overall claim's strongest assertion (causal control) is clearly R2."
2512.05794,2512.05794-02,Ordered SAEs impose an hierarchical structure that reliably identifies steerable features,abstract,3,Ordered SAE + steering vectors,2,2,0,4,0,"The paper introduces Ordered SAEs and evaluates their latents by steering on them (R2 method). The claim that Ordered SAEs 'reliably identifies steerable features' is validated through steering vector experiments where specific latents (e.g., latent 12) produce predictable changes in generation (Pearson R=0.939 for IGHJ4 gene usage). 'Steerable' is inherently an interventional concept—a feature is steerable if acting on it causes predictable output changes—so this is an R2 claim backed by R2 methods. The word 'reliably' adds quantitative confidence to the causal claim but does not change the rung. No explicit hedge language is present. Confidence is 4; the claim could be read as partially R3 ('identifies' implies representation, not just effect), but in context it clearly refers to identifying which features respond to steering interventions."
2512.05794,2512.05794-03,SAE latents collectively represent antibody information following sparsification,body,2,SAE feature attribution + linear probing (logistic regression),1,1,0,3,0,"The paper uses SAE feature attribution and linear probing (logistic regression achieving high F1 scores) to show that antibody information is preserved after sparsification. The verb 'represent' is flagged as polysemous in the codebook decision tree. Applying the decision tree: is there interventional evidence for this specific claim? No—the evidence is purely observational (linear probe F1 scores, SAE reconstruction quality). Does 'represent' in context mean 'is linearly decodable from'? Yes—the paper quantifies this via logistic regression probes on SAE latents, not via causal intervention. Therefore this should be coded as R1 (associational: information is decodable from latents). The claim location is body with prominence 2, consistent with a secondary empirical result. Confidence is 3 because the word 'represent' defaults to R3 in the codebook, and the contextual re-coding to R1 requires interpretive judgment about the paper's actual methodology."
2512.05794,2512.05794-04,top latents encoded contextual information of the preceding residues,body,2,SAE activation logging (feature attribution via activation patterns),1,3,2,4,0,"The paper analyzes which SAE latents activate most strongly at specific IMGT positions corresponding to preceding residues—this is purely observational activation logging (R1). The verb 'encoded' is polysemous; applying the codebook decision tree: is there interventional evidence? No. Does 'encoded' in context mean 'is linearly decodable from'? The evidence is about which positions activate specific latents, not about decoding via a linear probe—it is closer to a mechanistic claim about what information is stored in those latents. Therefore the default R3 reading applies: 'encoded contextual information' makes a mechanistic claim about what the latent represents. This is a clear overclaim gap of +2 (R1 method, R3 claim). No hedge language present. Confidence is 4 because the method is unambiguously R1 and the mechanistic 'encoded' language is clearly R3 in the codebook framing."
2512.05794,2512.05794-05,Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939),body,2,Steering vectors (Ordered SAE latent intervention),2,2,0,5,0,"The method is steering vectors applied to Ordered SAE latent 12—an R2 interventional method. The claim 'Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939)' is a direct report of an interventional result: the act of steering (do-operator) causally increased a measured output. This is a textbook R2 claim—it states that an intervention on a model component produced a measurable change in behavior. The Pearson R=0.939 quantifies the dose-response relationship of the intervention. No hedge language. Confidence is 5: the method is unambiguously R2, the claim language ('increased') is unambiguously causal/interventional at R2, and the specific numeric result corroborates the causal claim."
2512.05865,2512.05865-01,Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters,abstract,3,"sparsity-regularized post-training with constrained optimization (L0 regularization on attention edges, GECO algorithm)",2,2,0,4,0,"The method involves an intervention: post-training that actively zeros out attention edges via hard binary gating. This is an interventional procedure (R2) — the authors intervene on the model's attention connectivity and measure effect on pretraining loss. The claim itself is quantitative and empirical ('reduced to ~0.3% of edges while retaining pretraining loss'), using language appropriate to a causal/interventional result without mechanistic overclaiming about what the model 'does' internally. The claim matches the method rung (R2=R2, no gap). Minor ambiguity: the paper reports 0.22% for GPT-2 and 0.44% for OLMo, while the abstract states ~0.3%, which is an average; the claim says 'up to 1B parameters' but OLMo is 7B — this is a minor factual imprecision in the claim text, not affecting rung assignment."
2512.05865,2512.05865-02,Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks,body,1,activation patching for circuit discovery on IOI and Greater-Than tasks,2,2,0,4,0,"The method is activation patching (R2): the authors replace activations of components with corrupted activations and measure effects on logit difference, then select the top-k components that recover 90% of clean-model effect. This is a canonical R2 interventional method. The claim ('requires roughly three times fewer heads to recover 90% of the clean-model effect') is a comparative quantitative statement about the interventional outcome — how many heads are needed to explain model behavior under the patching procedure. The language ('requires... fewer heads to recover') is appropriate for the R2 method: it describes what the intervention reveals about component sufficiency without asserting a unique mechanism or making uniqueness/storage claims. No overclaim gap."
2512.05865,2512.05865-03,Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks,body,1,activation patching at single-instance level (single-sentence importance scores) for circuit discovery,2,2,0,4,0,"The method is single-instance activation patching (R2): importance scores computed per individual input pair, then top-k components selected to reach 90% of cumulative effect. The claim ('50-100x fewer edges to reach 90% of the cumulative single-instance effect') is a quantitative comparative result from this patching procedure. Language is appropriate for R2 — it reports the magnitude of an interventional finding (how many edges are needed) without asserting mechanism or uniqueness. The explicit mention of 'single-instance effect' in the claim text correctly scopes the evidence, slightly increasing confidence. No method-claim gap."
2512.05865,2512.05865-04,Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges,abstract,3,activation patching for circuit discovery combined with sparsity regularization post-training,2,3,1,4,0,"The primary method is activation patching (R2). However, the claim uses R3 mechanistic language: 'local sparsity cascades into global circuit simplification' and 'task-specific circuits involve far fewer components.' The verb 'cascades into' implies a mechanistic causal narrative about how local structural changes propagate to global circuit structure — asserting a mechanism, not merely reporting an intervention result. 'Circuits involve' treats discovered circuits as definitive functional entities (using 'THE circuits' framing). This is the classic R2→R3 overclaim pattern (IOI paper calibration anchor): activation patching establishes causal sufficiency of components but does not prove that 'the circuit' is the unique mechanism or that sparsity mechanistically 'cascades.' Gap = +1."
2512.05865,2512.05865-05,The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges,body,1,attribution graphs (cross-layer transcoders) and attention pattern visualization combined with activation patching,2,3,1,4,0,The methods include attribution graphs via cross-layer transcoders (which involves computing linear causal effects — R2 level) and attention pattern analysis (R1). The claim uses mechanistic narrative language: 'internal information flow of dense models is diffused across many attention edges' and 'sparse post-training consolidates information flow into a small number of edges.' 'Information flow... is diffused' and 'consolidates information flow' are mechanistic claims about what the model internally does — using storage/flow metaphors that go beyond what patching/attribution establishes. The phrase 'internal information flow' asserts a mechanistic interpretation (R3) of what the intervention reveals. This is the typical R2→R3 overclaim via mechanistic narrative framing. Gap = +1.
2512.06681,2512.06681-01,early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals,abstract,3,activation patching across all 12 GPT-2 layers,2,3,1,4,0,"The primary method is activation patching (R2): activations from a clean sentence are substituted at each layer independently, and the resulting change in sentiment classification probability quantifies each layer's causal contribution. The claim uses R3 mechanistic language: 'act as lexical sentiment detectors encoding stable position specific polarity signals.' 'Act as detectors' is a functional attribution verb (R3: 'performs,' 'implements'), and 'encoding stable... polarity signals' uses 'encoding' in a mechanistic sense — the paper does not merely say activations carry decodable information, but that the layers function as detectors. This is the classic R2→R3 overclaim pattern: activation patching establishes that early layers causally mediate sentiment effects (R2), but the claim asserts a functional mechanism ('act as detectors,' 'encoding'). Gap = +1."
2512.06681,2512.06681-02,contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11),abstract,3,activation patching across all 12 GPT-2 layers for contextual integration phenomena,2,3,1,4,0,"Method is activation patching (R2), testing contextual phenomena (negation, sarcasm, domain shifts) by substituting activations at each layer. The claim states these phenomena 'are integrated primarily in late layers (8-11) through a unified, non-modular mechanism.' The verb 'integrated' is a functional/mechanistic term (R3: 'performs,' 'computes'), and 'unified, non-modular mechanism' explicitly asserts a mechanistic characterization of how the model processes context. Activation patching establishes that late layers causally mediate contextual effects (R2), but it does not establish the nature of the mechanism ('unified, non-modular') — that is a mechanistic interpretation beyond what patching can establish. Gap = +1."
2512.06681,2512.06681-03,GPT-2's sentiment computation differs from the predicted hierarchical pattern,abstract,3,activation patching providing causal evidence for sentiment processing layer patterns,2,3,1,3,0,"Method is activation patching (R2). The claim — 'GPT-2's sentiment computation differs from the predicted hierarchical pattern' — uses 'sentiment computation' which is an R3 mechanistic framing (asserting what the model computes, not merely what was causally affected). The claim is a negative finding (falsification of three contextual integration hypotheses), which is somewhat lower-risk for overclaiming than positive mechanistic assertions. However, 'sentiment computation' as a noun phrase still treats the model as having a definite computational mechanism for sentiment (R3), rather than saying 'causal effects of layers on sentiment outputs differ from predictions' (R2). Confidence is 3 because one could argue this is shorthand for the interventional findings rather than a strong mechanistic assertion. Gap = +1."
2512.13568,2512.13568-01,neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions,abstract,3,SAE feature attribution and entropy-based measurement of activation statistics (observational),1,3,2,4,0,"The method is SAE feature attribution applied to neural activations — an observational/correlational method (R1). The paper measures feature distributions and computes entropy-based metrics; no interventions on the model are performed to establish causal claims. The claim uses strong R3 mechanistic language: 'achieve remarkable performance through superposition: encoding multiple features as overlapping directions.' 'Through superposition' asserts superposition as the causal mechanism for performance; 'encoding multiple features as overlapping directions' treats this as an established mechanistic fact about what the network does. This is the SAE→'represents' overclaim pattern from the codebook: R1 method with R3 claim. Gap = +2. Note: this claim largely restates the background hypothesis from Elhage et al. (2022), framing it as established fact."
2512.13568,2512.13568-02,we present an information-theoretic framework measuring a neural representation's effective degrees of freedom,abstract,3,information-theoretic framework: Shannon entropy applied to SAE activation distributions,1,1,0,3,0,"The method is application of Shannon entropy to SAE activation statistics — purely observational/correlational (R1). The claim describes the methodological contribution: 'an information-theoretic framework measuring a neural representation's effective degrees of freedom.' 'Measuring effective degrees of freedom' is a correlational/associational claim — the metric is computed from activation statistics without intervention. The language ('measuring') is appropriate for R1: it does not assert causation or mechanism, only that the metric captures a quantity. This is a methods/framework claim rather than an empirical claim about model internals per se, making rung assignment somewhat ambiguous; however, the claim is best read as R1 since it describes what can be measured (observed) from activations. Confidence 3 due to the meta-level nature of this claim."
2512.13568,2512.13568-03,our metric strongly correlates with ground truth in toy models,abstract,3,correlation analysis between entropy-based superposition metric and ground-truth Frobenius norm baseline in toy models,1,1,0,5,0,"The method is correlation analysis: the paper computes r = 0.94 between the proposed entropy metric and the ground-truth weight-based superposition measure in controlled toy models. This is a canonical R1 method (correlation analysis). The claim uses explicit R1 linguistic marker: 'strongly correlates with ground truth in toy models.' The word 'correlates' is a direct R1 marker. No interventional or mechanistic language. Perfect method-claim alignment (R1=R1, no gap). High confidence."
2512.13568,2512.13568-04,adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability,abstract,3,"entropy-based effective features measurement applied to adversarially trained models (observational, no intervention on model internals)",1,1,0,4,0,"The method is observational measurement of the entropy-based superposition metric on adversarially trained vs. standard models — R1 (SAE attribution + correlation analysis). The claim 'adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability' is an observational finding. The verb 'can increase' and the measurement of 'effective features' are observational/correlational. The claim contradicts a causal hypothesis but the evidence itself is correlational (comparing metric values across training conditions). The word 'causes' in 'superposition causes vulnerability' is in the hypothesis being contradicted, not in the evidence being claimed. The actual claim is that the metric goes up, which is R1. Minor ambiguity: 'adversarial training can increase' could be read as causal, but the evidence is observational comparison. Hedge_flag=0 since 'can' here is permissive ('is capable of') based on observed cases, not epistemic hedging."
2512.18092,2512.18092-01,neuron identification can be viewed as the inverse process of machine learning,abstract,3,theoretical/mathematical analogy: structural comparison between neuron identification optimization and supervised machine learning formalization,1,1,0,3,0,"This is a theoretical/conceptual paper about guarantees for neuron identification methods. The method is formal mathematical analysis — not an empirical intervention on a neural network. The claim 'neuron identification can be viewed as the inverse process of machine learning' is a conceptual/theoretical observation derived from comparing the mathematical formulations (Eq. 3 vs Eq. 4 in the paper). This is at best R1 — a structural association/analogy drawn from observing the mathematical form. There is no intervention and no causal claim about model internals. The rung system was designed for empirical claims about model internals; this meta-level conceptual claim is an edge case. Confidence 3 because the rung system does not cleanly apply to theoretical framework claims. The claim is best coded R1 (observational/associational: structural parallel is noted, not intervened upon)."
2512.18092,2512.18092-02,we derive generalization bounds for widely used similarity metrics to guarantee faithfulness,abstract,3,statistical learning theory: derivation of generalization bounds using PAC-learning style analysis applied to neuron identification,1,1,0,3,0,"The method is formal theoretical derivation — generalization bounds proved mathematically from the machine-learning analogy. No empirical intervention on model internals. The claim 'derive generalization bounds for widely used similarity metrics to guarantee faithfulness' is a theoretical contribution claim. 'Derive' and 'guarantee' describe mathematical proof results, not empirical findings about model internals. This is at best R1: the bounds are derived by analyzing the statistical properties of the similarity metrics under the probing dataset distribution — an observational/formal characterization. The rung system does not map perfectly onto theoretical claims; coding R1 is most appropriate since there is no intervention and no causal/mechanistic assertion about what a model does internally. Confidence 3 due to edge-case nature."
2512.18092,2512.18092-03,we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability,abstract,3,bootstrap ensemble over probing datasets with statistical coverage guarantees (BE method),1,1,0,3,0,"The method is bootstrap resampling of probing datasets — an observational/statistical procedure (R1). The claim 'propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability' is a methodological contribution claim. 'Quantifies stability' and 'guaranteed coverage probability' describe statistical properties of a measurement procedure, not causal interventions on model internals. Bootstrap resampling observes variation in neuron explanations across different probing datasets — purely observational (R1). No intervention on the model is performed. Coding R1 for both method and claim. Confidence 3 because this is a theoretical/methodological claim rather than a standard empirical claim about model internals, making rung assignment an edge case."
2601.02989,2601.02989-01,latent counts are computed and stored in the final item representations of each part,abstract,3,CountScope causal probing (activation patching into blank counting context) to decode implicit counts at token positions,1,3,2,4,0,"The primary method used to support this claim is CountScope, described as a causal probing method that patches a target token's activation into a blank counting context and decodes the implied count. Although the paper uses the word 'causal', CountScope is structurally a probing/decoding method (R1): it reads off information from activations without intervening on downstream computation in the counting task itself. The claim uses 'computed and stored'—canonical R3 storage/mechanism language per the codebook. 'Stored in the final item representations' asserts a specific memory/encoding mechanism. This is the standard probe→encodes overclaim pattern (+2 gap). The zero ablation in Section 5.2 provides some R2 support for causal relevance, but the specific 'stored' framing goes beyond what ablation establishes—ablation shows causal necessity of those tokens, not that the count is literally stored there as a mechanism."
2601.02989,2601.02989-02,counts are transferred to intermediate steps via dedicated attention heads,abstract,3,Attention analysis (attention weight visualization) and attention knockout (selectively blocking attention heads and measuring accuracy drop),2,3,1,4,0,"The methods supporting this claim are attention pattern analysis (R1) and attention knockout (R2)—knocking out heads and measuring the probability drop. Using the highest-rung method directly supporting the claim, method_rung=2. The claim uses 'transferred via dedicated attention heads': 'transferred' is a mechanistic functional verb and 'dedicated' implies unique functional assignment, both R3 markers. Attention knockout establishes causal necessity of certain heads (R2) but does not establish that these are THE dedicated pathway or that information is uniquely transferred through them. The definite functional narrative ('dedicated attention heads') implies a unique mechanism, matching the R2→R3 overclaim pattern (+1 gap). The paper explicitly calls this out as identifying 'mechanisms mediating the System-2 counting process', reinforcing the R3 framing."
2601.02989,2601.02989-03,this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting,abstract,3,Behavioral accuracy experiments on structured vs. unstructured inputs with and without intermediate reasoning steps across multiple open- and closed-source LLMs,1,2,1,4,0,"The evidence for this claim is entirely behavioral: accuracy and MAE measurements across models and context sizes (Tables 1 and 2). No intervention on model internals is performed—the strategy is a test-time prompting method that structures the input. Method_rung=1 (observational/correlational behavioral measurement). The claim uses 'enables LLMs to surpass architectural limitations'—'enables' is causal sufficiency language (R2 marker), claiming the strategy is sufficient to produce the outcome. This is a moderate overclaim: behavioral experiments show the strategy works (correlation between strategy use and accuracy), but do not establish a causal mechanism via internal intervention. The gap is +1 (R1→R2). Confidence is 4 because the causal reading of 'enables' is fairly clear given the experimental design compares with/without the strategy."
2601.03047,2601.03047-01,We successfully reproduce basic feature extraction and steering capabilities,abstract,3,SAE feature extraction (observational activation analysis) and feature steering (activation intervention) on Llama 3.1 8B with Llama Scope SAEs,2,2,0,5,0,"The paper replicates two capabilities: feature extraction (R1, observational) and steering (R2, intervention). For the claim 'successfully reproduce basic feature extraction and steering capabilities', the highest-rung method directly supporting the claim is steering (R2). The claim itself is a performance/capability statement: 'successfully reproduce' means the interventions worked as expected. This is well-matched to R2: the paper demonstrates that steering (an intervention) produces expected output changes, which is a causal sufficiency claim appropriate to R2. No overclaim: the authors do not assert a mechanistic explanation, just that the capability was reproduced. Gap = 0."
2601.03047,2601.03047-02,feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context,abstract,3,"Feature steering experiments (activation intervention) testing sensitivity to layer selection, steering magnitude, and input context",2,2,0,5,0,"The claim is that 'feature steering exhibits substantial fragility with sensitivity to layer selection, steering magnitude, and context'. The evidence comes directly from steering experiments (R2): the authors intervene on model activations via feature clamping and observe that outcomes change substantially depending on layer, magnitude, and context. This is a causal claim about the effects of the intervention (steering) under varying conditions—exactly R2. The claim is appropriately calibrated to the method: it reports empirical properties of interventions without claiming a deeper mechanism. Gap = 0."
2601.03047,2601.03047-03,We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another,abstract,3,SAE feature attribution (observational activation logging and comparison of feature activation patterns across thematically similar inputs),1,1,0,5,0,"The claim has two parts: (1) 'observe non-standard activation behavior' and (2) 'demonstrate the difficulty to distinguish thematically similar features from one another'. Both are observational findings from activation logging and feature attribution (R1). 'Observe' and 'demonstrate difficulty' are explicit R1 linguistic markers—they report what was seen in activation patterns without claiming causal effects or mechanisms. The paper describes activation anomalies (e.g., the coffee feature activating on coffins) and shows that similar features overlap in their activation patterns. This matches R1 exactly. Gap = 0."
2601.03047,2601.03047-04,current methods often fall short of the systematic reliability required for safety-critical applications,abstract,3,Combined SAE feature attribution (R1) and feature steering (R2) experiments used to evaluate systematic reliability of current mechanistic interpretability methods,2,1,0,4,0,"The claim 'current methods often fall short of the systematic reliability required for safety-critical applications' is a generalization/evaluation claim derived from the aggregated findings of the paper. The supporting evidence includes both R1 (activation observation, non-standard behavior) and R2 (steering fragility experiments), so method_rung=2 using the highest-rung method. The claim itself is an associational/descriptive assessment: it observes that reliability is lacking across conditions tested, without asserting a causal mechanism. 'Fall short' and 'systematic reliability' are descriptive/correlational language (R1). This is a case where the claim is more conservative than the method would allow—the interventional steering evidence could support a stronger causal claim, but the authors appropriately stop at a descriptive conclusion. Gap = -1 (claim is below method level, i.e., conservative)."
2601.03595,2601.03595-01,SAEs decompose strategy-entangled hidden states into a disentangled feature space,abstract,3,Sparse Autoencoder (SAE) training and feature attribution—projecting LRM hidden states into a high-dimensional sparse feature space,1,3,2,4,0,"The method is SAE training and the decomposition of activations into sparse features (R1: observational/associational). SAE training produces a learned dictionary but does not intervene on downstream computation to establish causal effects. The claim uses 'decompose ... into a disentangled feature space'—'decompose' and 'disentangled' are strong mechanistic claims asserting that the SAE reveals the true underlying structure of the hidden states (R3: the model's internal representational mechanism). This is the classic SAE→'represents' overclaim pattern: the SAE produces a useful sparse approximation, but claiming the hidden states are literally 'decomposed into' a 'disentangled' space asserts a mechanistic truth about internal organization beyond what the method establishes. Gap = +2 (R1→R3)."
2601.03595,2601.03595-02,SAE-Steering identifies strategy-specific features from the vast pool of SAE features,abstract,3,SAE-Steering two-stage pipeline: logit lens estimation (R1) followed by control effectiveness evaluation via steering interventions (R2),2,1,0,4,0,"The SAE-Steering pipeline first uses logit lens (R1: correlational/observational—no intervention) to identify candidate features, then ranks them by empirical control effectiveness via steering (R2: intervention). Method_rung=2 using the highest-rung component. The claim 'SAE-Steering identifies strategy-specific features' is about feature selection/discovery—'identifies' in this context means finds features that correlate with strategies, which is R1 language (prediction/co-occurrence). The paper is not claiming the features causally determine strategies, just that the pipeline selects them. This is a conservative claim relative to the method level. Gap = -1."
2601.03595,2601.03595-03,SAE-Steering outperforms existing methods by over 15% in control effectiveness,abstract,3,"Control effectiveness evaluation: steering interventions on LRM hidden states with LLM-judge evaluation of whether steered trajectories exhibit target strategies, compared against baselines",2,2,0,5,0,The claim is a direct quantitative performance comparison: 'SAE-Steering outperforms existing methods by over 15% in control effectiveness'. The evidence comes from steering intervention experiments (R2) where control vectors are injected and outcomes measured. The claim is appropriately R2: it reports the causal effect of using one intervention method versus others on the control effectiveness metric. 'Outperforms' is a comparative causal sufficiency claim—appropriate for the interventional evidence. No mechanistic narrative is added. Gap = 0.
2601.03595,2601.03595-04,controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones,abstract,3,"Error correction experiments: deliberate activation steering of reasoning strategy features during LRM generation on AIME problems, measuring accuracy improvement",2,2,0,5,0,The claim 'controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones' is supported by error correction experiments where strategy-specific features are steered via activation injection (R2). 'Redirect' and 'controlling' are causal sufficiency language (R2 markers: intervening on X changes Y). The paper demonstrates a 7% absolute accuracy improvement when steering is applied to correct erroneous reasoning. This is an interventional result cleanly matching R2. No uniqueness claims or mechanistic narrative beyond what the intervention demonstrates. Gap = 0.
2601.05679,2601.05679-01,many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens,abstract,3,Causal token injection: inserting top-activating tokens into non-reasoning text and measuring the resulting SAE feature activation shift (Cohen's d),2,2,0,5,0,The claim is 'many contrastively selected candidates are highly sensitive to token-level interventions with 45%–90% activating after injecting only a few associated tokens into non-reasoning text'. The method is causal token injection (R2): tokens are inserted into inputs and the effect on feature activation is measured. The claim reports the quantitative result of this intervention: a percentage of features activate in response to the injection. This is a clean R2 causal claim—it reports the effect of an intervention. 'Highly sensitive to token-level interventions' and 'activating after injecting' are both interventional causal language. Method and claim are perfectly aligned. Gap = 0.
2601.05679,2601.05679-02,LLM-guided falsification produces targeted non-reasoning inputs that trigger activation,abstract,3,LLM-guided falsification: adversarial counterexample construction producing non-reasoning false positives and meaning-preserving false negatives to test feature activation,2,1,0,4,0,"The method is LLM-guided adversarial counterexample construction and evaluation (R2: constructs targeted interventions—non-reasoning inputs—and measures whether they trigger activation). The claim 'LLM-guided falsification produces targeted non-reasoning inputs that trigger activation' is a description of what the method achieves: it identifies inputs that activate features. 'Trigger activation' and 'produces inputs that [activate]' is observational/correlational language describing what inputs co-occur with activation (R1). The claim reports that non-reasoning inputs can be found that trigger feature activation—this is an observation about activation patterns, not a causal claim about mechanisms. The method is R2 but the claim is phrased as R1. Gap = -1 (conservative claim relative to method)."
2601.05679,2601.05679-03,sparse decompositions can favor low-dimensional correlates that co-occur with reasoning,abstract,3,Theoretical analysis (stylized model of sparsity-regularized decoding) combined with empirical causal token injection and LLM-guided falsification experiments,2,1,0,4,1,"The claim 'sparse decompositions can favor low-dimensional correlates that co-occur with reasoning' is explicitly hedged ('can favor'). The evidence combines theoretical analysis of L1-regularized decoding (mathematical, not a direct empirical method) and empirical falsification experiments (R2). Method_rung=2 using the empirical component. The claim uses 'can favor' and 'co-occur with'—both are hedged correlational/associational markers (R1). 'Favor' suggests a tendency, not a proven causal mechanism; 'co-occur' is explicit R1 language. The hedge_flag=1 captures the 'can' qualifier. This is a conservative, well-calibrated claim: the theoretical analysis motivates the possibility and experiments provide evidence consistent with it, but the authors stop at an associational conclusion. Gap = -1."
2601.11516,2601.11516-01,activation probes may be a promising misuse mitigation technique,abstract,3,"Linear probing on Gemini model activations, evaluating probe performance on cyber-offensive prompt detection across production distribution shifts",1,1,0,5,1,"The method is linear probing (R1: purely observational/correlational—probes are trained to decode labels from frozen activations). The claim 'activation probes may be a promising misuse mitigation technique' is explicitly hedged ('may be') and uses 'promising'—a hedged positive assessment of utility. This is R1 language: it reports that probes correlate well with harmful prompts (predictive/decodable), without asserting causal mechanisms. Per the calibration rationale for this paper (Gemini Probes, Paper 5), the claim is well-calibrated to the probing method. No overclaim. Gap = 0. This matches the calibration anchor exactly: 'probes may be promising' is R1, hedged."
2601.11516,2601.11516-02,probes fail to generalize under important production distribution shifts,abstract,3,"Linear probe evaluation on held-out test sets representing production distribution shifts (long-context, multi-turn, jailbreaks, adaptive red teaming)",1,1,0,5,0,The method is linear probe evaluation (R1). The claim 'probes fail to generalize under important production distribution shifts' reports an empirical observation about probe performance degradation across distribution shifts—a purely observational finding. 'Fail to generalize' is a performance observation (R1: the probe's predictive accuracy decreases). No causal intervention is made; the authors measure probe behavior across different test distributions and observe failure. This matches the calibration anchor for this paper ('probes fail to generalize' = R1). Gap = 0.
