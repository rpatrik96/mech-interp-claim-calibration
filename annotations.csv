paper_id,claim_id,claim_text,claim_location,claim_prominence,method_used,method_rung,claim_rung,gap_score,confidence,notes,replication_status,replication_evidence
2211.00593,2211.00593-01,"presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI)",abstract,3,Activation Patching,2,3,1,5,"'performs' is Rung 3 mechanistic language. Method is path patching (Rung 2).",0.5,"Different ablation strategies yield different circuits"
2211.00593,2211.00593-02,"Our explanation encompasses 26 attention heads grouped into 7 main classes",abstract,3,Activation Patching,2,2,0,5,"Descriptive claim about circuit structure; does not overclaim.",0.5,
2211.00593,2211.00593-03,"this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior",abstract,3,Activation Patching,2,3,1,4,"'reverse-engineering' implies full mechanistic understanding (Rung 3).",0.5,
2211.00593,2211.00593-04,"Name Mover Heads... move the name from the subject to the end position",body,1,Activation Patching,2,3,1,5,"'move' is functional/mechanistic language (Rung 3).",0.5,"Known issue: Different ablation methods yield different circuits"
2211.00593,2211.00593-05,"S-Inhibition Heads... inhibit attention from the Name Mover Heads to the S token",body,1,Activation Patching,2,3,1,5,"'inhibit' is mechanistic functional attribution (Rung 3).",0.5,
2211.00593,2211.00593-06,"the circuit is faithful complete and minimal",abstract,2,Activation Patching,2,3,1,4,"'the circuit' implies uniqueness (Rung 3). NOTE: Paper actually hedges this claim acknowledging limitations.",0.5,
2202.05262,2202.05262-01,"factual associations correspond to localized directly-editable computations",abstract,3,Causal Tracing,2,3,1,5,"'correspond to' + 'computations' is mechanistic (Rung 3).",0.5,"Hase et al. 2023 questions localization interpretation"
2202.05262,2202.05262-02,"a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions",abstract,3,Causal Tracing,2,2,0,5,"'mediate' is appropriate Rung 2 language.",0.5,
2202.05262,2202.05262-03,"mid-layer feed-forward modules... storing factual associations",abstract,3,Causal Tracing,2,3,1,5,"'storing' implies memory mechanism (Rung 3).",0.5,
2202.05262,2202.05262-04,"ROME is effective on a standard zero-shot relation extraction model-editing task",abstract,3,ROME Editing,2,2,0,5,"Empirical effectiveness claim matched to method rung.",1,"ROME editing has side effects; not robust to prompts"
2301.05217,2301.05217-01,"We fully reverse engineer the algorithm learned by these networks",abstract,3,Circuit Analysis + Ablation,2,3,1,4,"'fully reverse engineer' implies complete mechanistic understanding.",0,"Well-replicated; Fourier structure confirmed"
2301.05217,2301.05217-02,"uses discrete Fourier transforms and trigonometric identities to convert addition to rotation",abstract,3,Weight Analysis + Ablation,2,3,1,5,"'uses... to convert' is mechanistic (Rung 3).",0,"Strong replication"
2301.05217,2301.05217-03,"grokking arises from the gradual amplification of structured mechanisms encoded in the weights",abstract,3,Weight Analysis + Ablation,2,3,1,4,"'encoded in the weights' is Rung 3. Paper uses ablation-based progress measures (restricted/excluded loss).",0,
2409.04478,2409.04478-01,"SAEs struggle to reach the neuron baseline",abstract,3,Interchange Intervention,2,2,0,5,"Causal claim appropriate for intervention method.",0,"Directly evaluative; findings replicated"
2409.04478,2409.04478-02,"sets of SAE features that separately mediate knowledge of which country a city is in",abstract,3,Interchange Intervention,2,2,0,5,"'mediate' is appropriate Rung 2 language.",0,
2601.11516,2601.11516-01,"activation probes may be a promising misuse mitigation technique",abstract,3,Linear Probing,1,1,0,5,"Appropriate hedged Rung 1 language.",NA,
2601.11516,2601.11516-02,"probes fail to generalize under important production distribution shifts",abstract,3,Linear Probing,1,1,0,5,"Empirical observation appropriate for method.",NA,
2304.14997,2304.14997-01,"reverse-engineered nontrivial behaviors of transformer models",abstract,3,Activation Patching,2,3,1,4,"'reverse-engineered' is Rung 3 mechanistic language.",NA,
2304.14997,2304.14997-02,"ACDC algorithm rediscovered 5/5 of the component types in a circuit",abstract,3,Activation Patching,2,2,0,5,"Empirical validation claim; appropriate for method.",NA,
2304.14997,2304.14997-03,"researchers can understand the functionality of each component",abstract,3,Activation Patching,2,3,1,4,"'understand functionality' implies mechanistic insight (Rung 3).",NA,
2304.14997,2304.14997-04,"finding the connections between abstract neural network units that form a circuit",abstract,3,Activation Patching,2,2,0,5,"Descriptive claim about circuit structure.",NA,
2407.14008,2407.14008-01,"partially reverse-engineer the circuit responsible for the Indirect Object Identification task",abstract,3,Activation Patching,2,3,1,4,"'reverse-engineer' is R3; 'partially' provides hedge.",NA,
2407.14008,2407.14008-02,"Layer 39 is a key bottleneck",abstract,3,Activation Patching,2,2,0,5,"Causal importance claim appropriate for patching.",NA,
2407.14008,2407.14008-03,"Convolutions in layer 39 shift names one position forward",abstract,3,Activation Patching,2,3,1,5,"'shift' is mechanistic functional language (Rung 3).",NA,
2407.14008,2407.14008-04,"The name entities are stored linearly in Layer 39's SSM",abstract,3,Activation Steering (CAA),2,3,1,4,"'stored' is R3 mechanistic. Paper uses subtract-and-add steering intervention (>95% success).",NA,
2501.17148,2501.17148-01,"prompting outperforms all existing methods followed by finetuning",abstract,3,Steering + Probing,2,2,0,5,"Comparative empirical claim appropriate for benchmark.",0,"Benchmark paper; findings replicated"
2501.17148,2501.17148-02,"SAEs are not competitive",abstract,3,Intervention Benchmark,2,2,0,5,"Empirical claim appropriate for method.",0,
2501.17148,2501.17148-03,"representation-based methods such as difference-in-means perform the best",abstract,3,Probing,1,1,0,5,"Correlational claim for detection task.",0,
2404.03646,2404.03646-01,"specific components within middle layers show strong causal effects at the last token of the subject",abstract,3,Causal Tracing,2,2,0,5,"'causal effects' appropriate for causal tracing.",NA,
2404.03646,2404.03646-02,"rank-one model editing methods can successfully insert facts at specific locations",abstract,3,ROME Editing,2,2,0,5,"Empirical success claim.",NA,
2404.03646,2404.03646-03,"linearity of Mamba's representations of factual relations",body,1,Linear Probing,1,3,2,4,"'representations' implies encoding (R3) from probing (R1).",NA,
2505.14685,2505.14685-01,"LM binds each character-object-state triple together by co-locating their reference information",abstract,3,Causal Mediation,2,3,1,5,"'binds' and 'by co-locating' is mechanistic (Rung 3).",NA,
2505.14685,2505.14685-02,"lookback mechanism which enables the LM to recall important information",abstract,3,Causal Mediation,2,3,1,5,"'mechanism' + 'enables' is mechanistic language.",NA,
2505.14685,2505.14685-03,"the binding lookback retrieves the correct state OI",abstract,3,Causal Abstraction,2,3,1,5,"'retrieves' is functional/mechanistic (Rung 3).",NA,
2505.14685,2505.14685-04,"reverse-engineering ToM reasoning in LMs",abstract,3,Causal Mediation,2,3,1,4,"'reverse-engineering' implies full mechanism.",NA,
2510.06182,2510.06182-01,"LMs implement such retrieval via a positional mechanism",abstract,3,Causal Analysis,2,3,1,5,"'implement' is mechanistic (Rung 3).",NA,
2510.06182,2510.06182-02,"LMs supplement the positional mechanism with a lexical mechanism and a reflexive mechanism",abstract,3,Causal Analysis,2,3,1,5,"Multiple 'mechanisms' implies mechanistic understanding.",NA,
2510.06182,2510.06182-03,"causal model combining all three mechanisms that estimates next token distributions with 95% agreement",body,1,Causal Modeling,2,2,0,5,"Empirical fit claim appropriate for causal model.",NA,
2510.06182,2510.06182-04,"how LMs bind and retrieve entities in-context",abstract,3,Causal Analysis,2,3,1,4,"'how LMs bind' is mechanistic explanation.",NA,
2411.16105,2411.16105-01,"circuits within LLMs may be more flexible and general than previously recognized",abstract,3,Activation Patching,2,2,0,5,"Hedged empirical claim about circuit properties.",NA,
2411.16105,2411.16105-02,"the circuit generalizes surprisingly well reusing all of its components and mechanisms",abstract,3,Activation Patching,2,3,1,4,"'mechanisms' implies mechanistic understanding (Rung 3).",NA,
2411.16105,2411.16105-03,"we discover a mechanism that explains this which we term S2 Hacking",abstract,3,Activation Patching,2,3,1,5,"'mechanism' + 'explains' is mechanistic (Rung 3).",NA,
2411.16105,2411.16105-04,"implement algorithms responsible for performing specific tasks",abstract,3,Activation Patching,2,3,1,5,"'implement algorithms' is strong mechanistic claim (Rung 3).",NA,
2402.17700,2402.17700-01,"MDAS achieves state-of-the-art results on RAVEL demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations",abstract,3,Interchange intervention/DAS,2,2,0,5,"MDAS evaluated via interchange interventions; claim about causal effect of distributed features matches method rung.",NA,
2402.17700,2402.17700-02,"If this leads the LM to output Asia instead of Europe then we have evidence that the feature F encodes the attribute continent",introduction,2,Interchange intervention,2,3,1,4,"Uses 'encodes' language (Rung 3) but method is interchange intervention (Rung 2).",NA,
2402.17700,2402.17700-03,"Methods with counterfactual supervision achieve strong results while methods with unsupervised featurizers struggle",results,1,Interchange intervention evaluation,2,2,0,5,"Comparative claim about method performance; uses causal evaluation metrics appropriately.",NA,
2402.17700,2402.17700-04,"The representations of different attributes gradually disentangle as we move towards later layers",results,1,MDAS with interchange intervention,2,2,0,5,"Claim about gradual disentanglement across layers based on intervention results.",NA,
2402.17700,2402.17700-05,"Some groups of attributes are more difficult to disentangle than others... Changing one of these entangled attributes has seemingly unavoidable ripple effects",results,1,Multiple intervention methods,2,3,1,4,"'Ripple effects' and entanglement language suggests representation structure (R3) from interventions (R2).",NA,
2404.03592,2404.03592-01,"much prior interpretability work has shown that representations encode rich semantic information",abstract,3,Distributed Interchange Intervention,2,3,1,4,"'encode' is Rung 3 language. Method is DAS/DII which is Rung 2 intervention.",NA,
2404.03592,2404.03592-02,"interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly",introduction,2,Distributed Interchange Intervention,2,3,1,4,"'encoded linearly' is Rung 3 mechanistic language. DII is Rung 2 intervention method.",NA,
2404.03592,2404.03592-03,"DAS is highly expressive and can effectively localize concepts within model representations",body,1,Distributed Alignment Search,2,3,1,4,"'localize concepts' implies encoding claim (Rung 3). DAS uses intervention (Rung 2).",NA,
2404.03592,2404.03592-04,"a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks",discussion,2,LoReFT Intervention,2,2,0,5,"Empirical claim about intervention effectiveness; appropriate for Rung 2 method.",NA,
2404.03592,2404.03592-05,"LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions",discussion,2,LoReFT Intervention,2,2,0,5,"Causal efficacy claim appropriate for intervention method (Rung 2).",NA,
2104.08164,2104.08164-01,"The factual knowledge acquired during pre-training and stored in the parameters of Language Models",abstract,3,Hyper-network weight update,2,3,1,4,"'stored in parameters' is R3 language but method shows causal influence (R2) not unique storage.",NA,
2104.08164,2104.08164-02,"our hyper-network can be regarded as a probe revealing which components need to be changed to manipulate factual knowledge",abstract,3,Hyper-network weight prediction,2,2,0,5,"Appropriately hedged; claims the method reveals which components causally affect factual knowledge.",NA,
2104.08164,2104.08164-03,"our analysis shows that the updates tend to be concentrated on a small subset of components",abstract,3,Weight update magnitude analysis,1,1,0,5,"Purely observational claim about where learned updates concentrate - no overclaim.",NA,
2104.08164,2104.08164-04,"our hyper-network can be regarded as a probe revealing the causal mediation mechanisms",body,1,Weight update analysis,2,2,0,4,"Explicitly frames as revealing causal mediation - appropriate Rung 2 language.",NA,
2104.08164,2104.08164-05,"the knowledge manipulation seems to be achieved by primarily modifying parameters affecting the shape of the attention distribution",body,1,Weight update heatmap analysis,1,2,1,3,"'seems to be achieved by' suggests causal mechanism (R2) but evidence is observational (R1).",NA,
2511.22662,2511.22662-01,"The core difficulty we identify is that distinguishing strategic deception from simpler behaviours requires making claims about a model's internal beliefs and goals",introduction,2,Conceptual analysis,NA,3,NA,4,"Meta-level claim about difficulty of making R3 claims. Conceptual argument not empirical method.",NA,
2511.22662,2511.22662-02,"What must be true about the internal state of the language model when it is lying or deceiving for a classifier such as an activation probe to provide good classification performance",body,1,Linear probing (discussed),1,2,1,4,"Claims probes can detect deception implies causal role of internal mechanisms.",NA,
2511.22662,2511.22662-03,"Model beliefs are not stable and are far more context dependent than animal or human beliefs",body,1,Behavioral analysis + prompting,1,3,2,4,"'beliefs are easily modified' implies R3 claim about internal representations being context-dependent.",NA,
2511.22662,2511.22662-04,"We find very low agreement between a full-transcript autorater and the MASK labels",results,1,Behavioral analysis (autorater),1,1,0,5,"Appropriately a Rung 1 observational claim about correlation between labeling methods.",NA,
2511.22662,2511.22662-05,"It is mostly true today that models behaving strategically deceptively have a consistent mechanism when they deceive",body,1,Behavioral analysis (CoT monitoring),1,2,1,3,"Claims about 'consistent mechanism' is R2 but evidence is behavioral observation.",NA,
2503.10894,2503.10894-01,"HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states",abstract,3,DAS/interchange intervention,2,2,0,5,"Main result claim; performance evaluated via interchange interventions.",NA,
2503.10894,2503.10894-02,"features that mediate concepts and enable predictable manipulation",abstract,3,DAS/interchange intervention,2,2,0,4,"'mediate' is Rung 2 causal language appropriate for DAS.",NA,
2503.10894,2503.10894-03,"HyperDAS automatically locates the token-positions of the residual stream that a concept is realized in",abstract,3,DAS/interchange intervention,2,3,1,4,"'realized in' implies the concept IS in those positions (R3) but method shows causal mediation (R2).",NA,
2503.10894,2503.10894-04,"Interchange interventions identify neural representations that are causal mediators of high-level concepts",body,1,DAS/interchange intervention,2,2,0,5,"Background section accurately describes method with 'causal mediators' language.",NA,
2503.10894,2503.10894-05,"at deeper layers the hypernetwork learns to intervene on unintuitive positions... which were previously unknown to store attributes",results,1,DAS/interchange intervention,2,3,1,4,"'store attributes' is R3 representational language; method shows causal effect not storage.",NA,
2506.18167,2506.18167-01,"We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors",abstract,3,Steering Vectors + attribution patching,2,2,0,5,"'mediated by' is appropriate causal language for intervention-based evidence.",NA,
2506.18167,2506.18167-02,"Positive steering increases behaviors such as backtracking and uncertainty estimation while negative steering suppresses them confirming the causal influence",results,1,Steering Vectors,2,2,0,5,"Appropriate causal language matching the intervention method.",NA,
2506.18167,2506.18167-03,"These effects are consistent across both DeepSeek-R1-Distill models reinforcing the hypothesis that Thinking LLMs encode these reasoning mechanisms as linear directions",results,1,Steering Vectors,2,3,1,4,"'encode these reasoning mechanisms' implies model uses these directions TO reason (R3).",NA,
2506.18167,2506.18167-04,"Several reasoning behaviors in thinking models can be isolated to specific directions in the model's activation space enabling precise control through steering vectors",conclusion,2,Steering Vectors,2,2,0,5,"'isolated to' and 'control' are appropriate Rung 2 causal language.",NA,
2506.18167,2506.18167-05,"Our findings indicate that the DeepSeek-R1-Distill models have distinct mechanisms to achieve their reasoning process",results,1,Steering Vectors + attribution patching,2,3,1,4,"'mechanisms to achieve' implies these ARE the model's reasoning implementation.",NA,
2506.03292,2506.03292-01,"scaling HYPERSTEER with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods",abstract,3,Activation steering (hypernetwork-generated),2,2,0,5,"Well-calibrated claim about intervention efficacy.",NA,
2506.03292,2506.03292-02,"HYPERSTEER performs on par with steering-via-prompting",abstract,3,Activation steering (hypernetwork-generated),2,2,0,5,"Performance comparison claim appropriate for steering method.",NA,
2506.03292,2506.03292-03,"our cross-attention HYPERSTEER variant performs better on unseen steering prompts than every supervised activation steering baseline",results,2,Activation steering with cross-attention hypernetwork,2,2,0,5,"Supported by empirical comparison in Table 1.",NA,
2506.03292,2506.03292-04,"as training data increases HYPERSTEER becomes much more economical than supervised activation steering",results,1,Activation steering,2,2,0,4,"Compute efficiency claim supported by Figure 3.",NA,
2506.03292,2506.03292-05,"cross-attention's residual inter-concept similarity is weakened by additional conditioning but not at the cost of steering performance",body,1,Activation steering with cosine similarity analysis,2,2,0,3,"Qualitative analysis about geometric structure of steering vectors.",NA,
2510.01070,2510.01070-01,"Our white-box techniques based on logit lens and sparse autoencoders also consistently increase the success rate of the LLM auditor",abstract,3,Logit lens and SAE feature analysis,1,1,0,5,"Appropriately scoped observational claim about auditor performance.",NA,
2510.01070,2510.01070-02,"secret knowledge can be successfully extracted from the model's internal states even when it is not verbalized explicitly",results,1,Logit lens and SAE-based methods,1,3,2,4,"'extracted' suggests knowledge is stored/represented internally; method shows correlation.",NA,
2510.01070,2510.01070-03,"Model internals reveal secrets that its output conceals... white-box methods reveal strong signals corresponding to the female gender during this refusal",body,1,Logit lens tokens and SAE feature descriptions,1,3,2,4,"'Reveal' and 'conceals' imply representational claims; methods show correlation.",NA,
2510.01070,2510.01070-04,"Fine-tuned model organisms successfully internalize secret knowledge... MOs have successfully internalized their secret knowledge and are aware of it",results,2,Behavioral evaluation (downstream task accuracy),1,3,2,4,"'Internalize' and 'aware' are strong representational claims; evidence is behavioral.",NA,
2510.01070,2510.01070-05,"Since models must internally represent secret knowledge to use it we should be able to extract it through mechanistic interpretability techniques",body,1,Logit lens and SAE analysis,1,3,2,3,"Premise assumes necessity of internal representation; not empirically demonstrated.",NA,
2410.08417,2410.08417-01,"Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across toy tasks image classification and language modeling",abstract,3,Eigendecomposition / weight analysis,1,3,2,4,"Method is weight analysis (R1) but 'reveals interpretable' suggests structure IS interpretable (R3).",NA,
2410.08417,2410.08417-02,"For MNIST top eigenvectors represent curve segments specific to each digit class; for Fashion-MNIST top eigenvectors function as localized edge detectors",body,1,Eigendecomposition / weight visualization,1,3,2,4,"'function as' and 'represent' is R3 language from R1 weight visualization.",NA,
2410.08417,2410.08417-03,"Adversarial masks constructed from eigenvectors cause misclassification demonstrating causal importance of extracted features",body,1,Adversarial mask construction from eigenvectors,2,2,0,5,"Properly supported: constructing adversarial masks and showing they cause misclassification is R2.",NA,
2410.08417,2410.08417-04,"A sentiment negation circuit in layer 4 computes not-good and not-bad features via AND-gate-like interactions",body,1,Bilinear tensor analysis + SAE feature decomposition,1,3,2,4,"'computes' and 'THE circuit' is R3 from R1 evidence.",NA,
2410.08417,2410.08417-05,"Many SAE output features are well-correlated with low-rank eigenvector approximations particularly at large activation values",body,1,Correlation analysis,1,1,0,5,"'well-correlated' is appropriately Rung 1. No overclaim.",NA,
2406.11779,2406.11779-01,"The model outputs the largest logit on the true max token by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit",body,1,SVD analysis of EQKE matrix + weight examination,1,3,2,4,"Core mechanistic claim. Method is R1 (SVD) but claim uses 'performs' and describes THE mechanism.",NA,
2406.11779,2406.11779-02,"EQKE contains a single large rank-one component with singular value ~7800 around 620x larger than the second component",body,1,SVD decomposition of EQKE matrix,1,1,0,5,"Purely observational claim about matrix structure. Method matches claim rung.",NA,
2406.11779,2406.11779-03,"Zero ablating EQKP changes model accuracy from 0.9992 to 0.9993 confirming EQKP is unimportant to model functioning",body,1,Zero ablation,2,2,0,5,"Ablation study with appropriate causal language. Method-claim alignment good.",NA,
2406.11779,2406.11779-04,"Shorter proofs seem to require and provide more mechanistic understanding; more faithful mechanistic understanding leads to tighter performance bounds",abstract,3,Correlation analysis between proof length and unexplained dimensionality,1,2,1,4,"Claims causal relationship ('leads to') but evidence is correlational.",NA,
2406.11779,2406.11779-05,"Compounding structureless errors are a key challenge when making rank-1 approximations of constituent matrices",body,1,Mathematical analysis of error propagation,1,1,0,5,"Observational/mathematical claim about proof tightness. No overclaiming.",NA,
2508.21258,2508.21258-01,"RelP more accurately approximates activation patching than standard attribution patching particularly when analyzing residual stream and MLP outputs",abstract,3,Relevance Patching (LRP-based),1,2,1,5,"Method is gradient-based attribution (R1) used to approximate R2 activation patching.",NA,
2508.21258,2508.21258-02,"For MLP outputs in GPT-2 Large attribution patching achieves a Pearson correlation of 0.006 whereas RelP reaches 0.956",abstract,3,Relevance Patching (LRP-based),1,2,1,5,"Quantitative comparison claim. Method approximates interventional effects.",NA,
2508.21258,2508.21258-03,"RelP achieves comparable faithfulness to Integrated Gradients in identifying sparse feature circuits without the extra computational cost",abstract,3,Relevance Patching + SAE,1,2,1,4,"Claims about circuit faithfulness (R2) from gradient-based method (R1).",NA,
2508.21258,2508.21258-04,"small feature circuits explain most of the model's behavior: in Pythia-70M about 100 features account for the majority of performance",body,1,Relevance Patching + SAE,1,3,2,4,"'explain' and 'account for' implies mechanistic understanding (R3).",NA,
2508.21258,2508.21258-05,"RelP enables more faithful localization of influential components in large models",abstract,3,Relevance Patching (LRP-based),1,2,1,5,"'localization of influential components' is causal importance claim (R2).",NA,
2512.05865,2512.05865-01,"Attention connectivity can be reduced to approximately 0.3% of edges while retaining the original pretraining loss on models up to 1B parameters",abstract,3,Constrained optimization with sparsity regularization,1,2,1,5,"Method is observational (R1); claim is interventional (R2). Gap score corrected to match formula max(0, claim_rung - method_rung).",NA,
2512.05865,2512.05865-02,"Sparse attention requires roughly three times fewer heads to recover 90% of the clean-model effect compared to the standard model on IOI and Greater-Than tasks",body,1,Activation patching,2,2,0,5,"Uses activation patching to measure causal contribution; claim language matches R2.",NA,
2512.05865,2512.05865-03,"Sparse-attention models require 50-100x fewer edges to reach 90% of the cumulative single-instance effect on circuit discovery tasks",body,1,Attribution patching for edge-level importance,2,2,0,5,"Uses attribution patching (R2); claim is appropriately causal and quantified.",NA,
2512.05865,2512.05865-04,"Local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components with up to 100x fewer edges",abstract,3,Attribution patching + circuit visualization,2,3,1,4,"'circuit' language implying THE mechanism exists; moderate overclaim.",NA,
2512.05865,2512.05865-05,"The internal information flow of dense models is diffused across many attention edges whereas sparse post-training consolidates information flow into a small number of edges",body,1,Top-k attention comparison + sparsity measurement,1,3,2,4,"'information flow is diffused/consolidated' is R3 framing for R1 evidence. Gap score corrected to match formula max(0, claim_rung - method_rung).",NA,
2512.05794,2512.05794-01,"TopK SAEs can reveal biologically meaningful latent features but high feature-concept correlation does not guarantee causal control over generation",abstract,3,SAE + Linear Probing + Steering,1,2,1,5,"Paper hedges by noting correlation!=causation. Main insight appropriately cautious.",NA,
2512.05794,2512.05794-02,"Ordered SAEs impose an hierarchical structure that reliably identifies steerable features",abstract,3,Ordered SAE + Steering,2,2,0,5,"'identifies steerable features' is appropriate R2 language for steering intervention.",NA,
2512.05794,2512.05794-03,"SAE latents collectively represent antibody information following sparsification",body,2,Linear Probing,1,3,2,4,"'represent' is R3 mechanistic language. Method is linear probe with 0.99 accuracy.",NA,
2512.05794,2512.05794-04,"top latents encoded contextual information of the preceding residues",body,2,SAE Attribution + Linear Probing,1,3,2,4,"'encoded' is R3 mechanistic language from R1 correlational analysis.",NA,
2512.05794,2512.05794-05,"Positively steering on latent 12 increased IGHJ4 proportion in model generation (Pearson R=0.939)",body,2,Steering Vector Intervention,2,2,0,5,"Appropriate R2 claim with R2 method. Direct causal manipulation with quantified effect.",NA,
2601.03047,2601.03047-01,"We successfully reproduce basic feature extraction and steering capabilities",abstract,3,SAE Attribution + Steering,2,2,0,5,"Empirical claim about replication; appropriate for R2 method.",NA,
2601.03047,2601.03047-02,"feature steering exhibits substantial fragility with sensitivity to layer selection steering magnitude and context",abstract,3,Steering Vectors,2,2,0,5,"Causal claim about steering limitations; appropriate R2 language.",NA,
2601.03047,2601.03047-03,"We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another",abstract,3,SAE Attribution,1,2,1,4,"'difficulty to distinguish' suggests representational analysis (R1) makes R2 claims about separability.",NA,
2601.03047,2601.03047-04,"current methods often fall short of the systematic reliability required for safety-critical applications",abstract,3,SAE + Steering,2,2,0,5,"Evaluative claim about method limitations; appropriately scoped.",NA,
2507.08802,2507.08802-01,"any neural network can be mapped to any algorithm rendering this unrestricted notion of causal abstraction trivial and uninformative",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical claim about causal abstraction limitations; appropriate.",NA,
2507.08802,2507.08802-02,"it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task",abstract,3,Interchange Intervention,2,2,0,5,"Empirical demonstration of causal abstraction weakness; appropriate.",NA,
2507.08802,2507.08802-03,"randomly initialised language models our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task",abstract,3,Interchange Intervention,2,3,1,4,"100% IIA on random models implies IIA can be misleading; R2 method but R3 implications about 'what models compute'.",NA,
2507.08802,2507.08802-04,"causal abstraction is not enough for mechanistic interpretability as it becomes vacuous without assumptions about how models encode information",abstract,3,Causal Abstraction Theory,2,3,1,4,"Claims about what is 'enough' for MI implies understanding mechanism (R3).",NA,
2311.17030,2311.17030-01,"even if a subspace intervention makes the model's output behave as if the value of a feature was changed this effect may be achieved by activating a dormant parallel pathway",abstract,3,Activation Patching,2,3,1,5,"'dormant parallel pathway' is R3 mechanistic claim about what happens internally.",NA,
2311.17030,2311.17030-02,"patching of subspaces can lead to an illusory sense of interpretability",abstract,3,Activation Patching,2,2,0,5,"Evaluative claim about method limitations; appropriate.",NA,
2311.17030,2311.17030-03,"we demonstrate this phenomenon in a distilled mathematical example in two real-world domains",body,1,Activation Patching,2,2,0,5,"Empirical claim about demonstrating the illusion.",NA,
2311.17030,2311.17030-04,"there is an inconsistency between fact editing performance and fact localization",abstract,3,Activation Patching + ROME,2,2,0,5,"Observational claim about method disagreement.",NA,
2404.15255,2404.15255-01,"activation patching is a popular mechanistic interpretability technique but has many subtleties",abstract,3,Activation Patching,2,2,0,5,"Tutorial paper making methodological claims.",NA,
2404.15255,2404.15255-02,"varying these hyperparameters could lead to disparate interpretability results",abstract,3,Activation Patching,2,2,0,5,"Claims about method sensitivity; appropriate.",NA,
2309.16042,2309.16042-01,"systematically examine the impact of methodological details in activation patching",abstract,3,Activation Patching,2,2,0,5,"Methods paper examining patching variants.",NA,
2309.16042,2309.16042-02,"varying these hyperparameters could lead to disparate interpretability results",abstract,3,Activation Patching,2,2,0,5,"Empirical finding about method sensitivity.",NA,
2512.06681,2512.06681-01,"early layers (0-3) act as lexical sentiment detectors encoding stable position specific polarity signals",abstract,3,Activation Patching,2,3,1,5,"'act as detectors' and 'encoding' is R3 language from R2 patching.",NA,
2512.06681,2512.06681-02,"contextual phenomena such as negation sarcasm domain shifts are integrated primarily in late layers (8-11)",abstract,3,Activation Patching,2,3,1,5,"'integrated primarily' implies mechanistic understanding (R3) from patching (R2).",NA,
2512.06681,2512.06681-03,"GPT-2's sentiment computation differs from the predicted hierarchical pattern",abstract,3,Activation Patching,2,3,1,4,"'sentiment computation' is R3 language for what model does internally.",NA,
2511.05923,2511.05923-01,"MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information",abstract,3,Causal Tracing,2,3,1,5,"'play a critical role' and 'aggregating' is R3 mechanistic language.",NA,
2511.05923,2511.05923-02,"FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations",abstract,3,Causal Tracing,2,3,1,5,"'storage and transfer' is strong R3 mechanistic claim.",NA,
2511.05923,2511.05923-03,"we propose Intermediate Representation Injection (IRI) that reinforces visual object information flow",abstract,3,Activation Intervention,2,2,0,5,"Intervention method claim; appropriate R2 language.",NA,
2601.05679,2601.05679-01,"many contrastively selected candidates are highly sensitive to token-level interventions with 45-90% activating after injecting only a few associated tokens",abstract,3,Token Injection + SAE,2,2,0,5,"Empirical claim about SAE feature fragility.",NA,
2601.05679,2601.05679-02,"LLM-guided falsification produces targeted non-reasoning inputs that trigger activation",abstract,3,SAE Attribution,1,2,1,4,"Claims about what causes activation (R2) from observational analysis (R1).",NA,
2601.05679,2601.05679-03,"sparse decompositions can favor low-dimensional correlates that co-occur with reasoning",abstract,3,SAE Analysis,1,2,1,4,"Claims about what SAEs capture implicitly makes causal claims.",NA,
2509.06608,2509.06608-01,"the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token",body,1,Steering Vectors + Logit Lens,2,3,1,5,"'acts like' and 'concentrated on' is R3 mechanistic language.",NA,
2509.06608,2509.06608-02,"the penultimate-layer vector operates through the MLP and unembedding preferentially up-weighting process words",body,1,Steering Vectors + Path Patching,2,3,1,5,"'operates through' is R3 mechanistic language about how steering works.",NA,
2509.06608,2509.06608-03,"steering vectors transfer to other models",body,1,Steering Vectors,2,2,0,5,"Empirical claim about transfer; appropriate.",NA,
2505.22637,2505.22637-01,"all seven prompt types produce a net positive steering effect but exhibit high variance across samples",abstract,3,Steering Vectors,2,2,0,5,"Empirical claim about steering variability.",NA,
2505.22637,2505.22637-02,"higher cosine similarity between training set activation differences predicts more effective steering",abstract,3,Steering Vectors + Correlation,1,2,1,4,"'predicts' from correlational analysis (R1) implies causal relationship (R2).",NA,
2505.22637,2505.22637-03,"vector steering is unreliable when the target behavior is not represented by a coherent direction",abstract,3,Steering Vectors,2,3,1,4,"'represented by a coherent direction' is R3 language about internal structure.",NA,
2301.04709,2301.04709-01,"Causal abstraction provides a theoretical foundation for mechanistic interpretability",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical claim; appropriate framing for the framework paper.",NA,
2301.04709,2301.04709-02,"generalizing the theory of causal abstraction from mechanism replacement to arbitrary mechanism transformation",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical extension claim.",NA,
2301.04709,2301.04709-03,"unifying a variety of mechanistic interpretability methods in the common language of causal abstraction",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical unification claim.",NA,
2403.07809,2403.07809-01,"pyvene supports customizable interventions on a range of different PyTorch modules",abstract,3,Intervention Library,2,2,0,5,"Technical capability claim; appropriate.",NA,
2403.07809,2403.07809-02,"pyvene provides a unified and extensible framework for performing interventions on neural models",abstract,3,Intervention Library,2,2,0,5,"Library functionality claim.",NA,
2403.07809,2403.07809-03,"we illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization",abstract,3,DAS/Interchange Intervention,2,2,0,5,"Demonstration claim; appropriate for library paper.",NA,
2601.03595,2601.03595-01,"SAEs decompose strategy-entangled hidden states into a disentangled feature space",abstract,3,SAE,1,3,2,4,"'disentangled feature space' is R3 representation claim from SAE analysis (R1).",NA,
2601.03595,2601.03595-02,"SAE-Steering identifies strategy-specific features from the vast pool of SAE features",abstract,3,SAE + Steering,2,3,1,4,"'strategy-specific features' implies features encode strategies (R3) from steering (R2).",NA,
2601.03595,2601.03595-03,"SAE-Steering outperforms existing methods by over 15% in control effectiveness",abstract,3,SAE + Steering,2,2,0,5,"Empirical comparison claim; appropriate.",NA,
2601.03595,2601.03595-04,"controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones",abstract,3,Steering Vectors,2,2,0,5,"Causal control claim; appropriate for steering.",NA,
2512.05534,2512.05534-01,"neural networks represent meaningful concepts as directions in their representation spaces",abstract,3,SAE Theory,1,3,2,5,"'represent concepts as directions' is R3 claim from theoretical analysis (R1).",NA,
2512.05534,2512.05534-02,"we develop the first unified theoretical framework considering SDL as one optimization problem",abstract,3,SAE Theory,1,1,0,5,"Theoretical contribution claim.",NA,
2512.05534,2512.05534-03,"we provide novel theoretical explanations for empirically observed phenomena including feature absorption and dead neurons",abstract,3,SAE Theory,1,2,1,4,"'explanations for phenomena' implies causal understanding (R2) from theory (R1).",NA,
2512.13568,2512.13568-01,"neural networks achieve remarkable performance through superposition encoding multiple features as overlapping directions",abstract,3,SAE Analysis,1,3,2,4,"'achieve through superposition' implies mechanism (R3) from SAE measurement (R1).",NA,
2512.13568,2512.13568-02,"we present an information-theoretic framework measuring a neural representation's effective degrees of freedom",abstract,3,SAE Analysis,1,1,0,5,"Measurement framework claim; appropriate.",NA,
2512.13568,2512.13568-03,"our metric strongly correlates with ground truth in toy models",abstract,3,SAE Analysis,1,1,0,5,"Empirical validation claim.",NA,
2512.13568,2512.13568-04,"adversarial training can increase effective features while improving robustness contradicting the hypothesis that superposition causes vulnerability",abstract,3,SAE Analysis,1,2,1,4,"Claims about causal relationship (R2) from correlational analysis (R1).",NA,
2511.09432,2511.09432-01,"incorporating group symmetries into the SAEs yields features more useful in downstream tasks",abstract,3,SAE,1,2,1,4,"'yields features more useful' implies causal improvement from architectural change.",NA,
2511.09432,2511.09432-02,"a single matrix can explain how their activations transform as the images are rotated",abstract,3,SAE + Linear Analysis,1,3,2,4,"'explain how activations transform' is R3 mechanistic claim.",NA,
2511.09432,2511.09432-03,"adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs",abstract,3,SAE + Probing,1,2,1,4,"'discover features' implies representational claim (R3) from probing evaluation (R1).",NA,
2505.24859,2505.24859-01,"steering effectively controls the targeted summary properties",abstract,3,Steering Vectors,2,2,0,5,"Causal control claim; appropriate for steering.",NA,
2505.24859,2505.24859-02,"high steering strengths consistently degrade both intrinsic and extrinsic text quality",abstract,3,Steering Vectors,2,2,0,5,"Empirical finding about steering trade-offs.",NA,
2505.24859,2505.24859-03,"combining steering and prompting yields the strongest control over text properties",abstract,3,Steering Vectors,2,2,0,5,"Empirical comparison claim.",NA,
2508.11214,2508.11214-01,"the language of causality and specifically the theory of causal abstraction provides a fruitful lens on computational implementation",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical framing claim.",NA,
2508.11214,2508.11214-02,"we offer an account of computational implementation grounded in causal abstraction",abstract,3,Causal Abstraction Theory,2,2,0,5,"Theoretical contribution claim.",NA,
2411.08745,2411.08745-01,"the output language is encoded in the latent at an earlier layer than the concept to be translated",abstract,3,Activation Patching,2,3,1,5,"'encoded in the latent' is R3 representation claim from patching (R2).",NA,
2411.08745,2411.08745-02,"we can change the concept without changing the language and vice versa through activation patching alone",abstract,3,Activation Patching,2,2,0,5,"Causal manipulation claim; appropriate for patching.",NA,
2411.08745,2411.08745-03,"patching with the mean representation of a concept across different languages improves translation",abstract,3,Activation Patching,2,2,0,5,"Empirical finding about patching effect.",NA,
2411.08745,2411.08745-04,"results provide evidence for the existence of language-agnostic concept representations",abstract,3,Activation Patching,2,3,1,4,"'existence of representations' is R3 claim from intervention evidence (R2).",NA,
2507.20936,2507.20936-01,"early MLP layers attend not only to the syntactic structure but also process its semantic content",abstract,3,Activation Patching,2,3,1,5,"'process semantic content' is R3 mechanistic claim.",NA,
2507.20936,2507.20936-02,"these layers transform persona tokens into richer representations which are then used by middle MHA layers",abstract,3,Activation Patching,2,3,1,5,"'transform into representations' and 'used by' is R3 mechanistic language.",NA,
2507.20936,2507.20936-03,"we identify specific attention heads that disproportionately attend to racial and color-based identities",abstract,3,Activation Patching,2,2,0,5,"Empirical finding about attention patterns.",NA,
2504.02976,2504.02976-01,"patching the first feedforward layer recovered 56% of correct preference demonstrating that associative knowledge is distributed",abstract,3,Activation Patching,2,3,1,4,"'knowledge is distributed' is R3 claim about storage from patching (R2).",NA,
2504.02976,2504.02976-02,"patching the final output layer completely restored accuracy indicating that definitional knowledge is localised",abstract,3,Activation Patching,2,3,1,5,"'knowledge is localised' is R3 storage claim.",NA,
2504.02976,2504.02976-03,"factual knowledge is more localized and associative knowledge depends on distributed representations",abstract,3,Activation Patching,2,3,1,4,"'depends on distributed representations' is R3 mechanistic claim.",NA,
2502.03714,2502.03714-01,"USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models",abstract,3,SAE,1,3,2,4,"'universal concept space' and 'interpret activations' is R3 from SAE (R1).",NA,
2502.03714,2502.03714-02,"the learned dictionary captures common factors of variation concepts across different tasks architectures and datasets",abstract,3,SAE,1,3,2,4,"'captures concepts' is R3 representation claim.",NA,
2502.03714,2502.03714-03,"USAEs discover semantically coherent and important universal concepts across vision models",abstract,3,SAE,1,3,2,5,"'discover concepts' is strong R3 claim from SAE analysis (R1).",NA,
2509.18127,2509.18127-01,"SAEs facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features",abstract,3,SAE Attribution,1,3,2,4,"'explaining atomic features' implies R3 understanding from SAE (R1).",NA,
2509.18127,2509.18127-02,"Safe-SAIL systematically identifies SAE with best concept-specific interpretability",abstract,3,SAE Attribution,1,2,1,4,"'identifies' implies causal role determination.",NA,
2509.18127,2509.18127-03,"we extract a rich and diverse set of safety-relevant features that effectively capture high-risk behaviors",abstract,3,SAE Attribution,1,3,2,4,"'features capture behaviors' is R3 claim from SAE (R1).",NA,
2601.02989,2601.02989-01,"latent counts are computed and stored in the final item representations of each part",abstract,3,Causal Mediation,2,3,1,5,"'computed and stored' is R3 mechanistic claim.",NA,
2601.02989,2601.02989-02,"counts are transferred to intermediate steps via dedicated attention heads",abstract,3,Causal Mediation,2,3,1,5,"'dedicated attention heads' and 'transferred' is R3 circuit language.",NA,
2601.02989,2601.02989-03,"this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting",abstract,3,Behavioral + Causal Analysis,2,2,0,5,"Empirical claim about method effectiveness.",NA,
2512.18092,2512.18092-01,"neuron identification can be viewed as the inverse process of machine learning",abstract,3,Neuron Attribution Theory,1,1,0,5,"Theoretical framing claim.",NA,
2512.18092,2512.18092-02,"we derive generalization bounds for widely used similarity metrics to guarantee faithfulness",abstract,3,Neuron Attribution Theory,1,1,0,5,"Theoretical contribution claim.",NA,
2512.18092,2512.18092-03,"we propose a bootstrap ensemble procedure that quantifies stability along with guaranteed coverage probability",abstract,3,Neuron Attribution,1,1,0,5,"Methodological claim.",NA,
