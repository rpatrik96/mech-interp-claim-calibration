                 Dissecting Persona-Driven Reasoning in Language Models
                                   via Activation Patching


                                Ansh Poonia†, Maeghal Jain†
                                                †Independent
                             {pooniaansh11, maeghaljain}@gmail.com




                          Abstract                       identifying "correct letter heads" in a 70B Chin-
                                                                           chilla model-attention heads that track answer sym-
                 Large language models (LLMs) exhibit remark-       bols and promote the correct choice based on po-
                   able versatility in adopting diverse personas. In        sitional order. Wiegreffe et al. (2025) examined
                     this study, we examine how assigning a per-2025                                            how successful models implement symbol binding
                 sona influences a model’s reasoning on an ob-
                                                                    in formatted MCQA using vocabulary projection                     jective task. Using activation patching, we take
                                                       and activation patching, and Li and Gao (2025)                  a first step toward understanding how key com-Sep
                 ponents of the model encode persona-specific       analyzed anchored positional bias in the GPT-2
                   information. Our findings reveal that the early        family, showing that models consistently favor the21
                 Multi-Layer Perceptron (MLP) layers attend          first option (’A’). Building on these interpretability
                  not only to the syntactic structure of the input        techniques, we ask whether similar internal circuits
                  but also process its semantic content. These                                                                     also govern how personas steer a model’s reasoning
                    layers transform persona tokens into richer rep-
                                                        and potentially introduce social bias.
                    resentations, which are then used by the middle
                                                               This study takes a step toward bridging the gap                Multi-Head Attention (MHA) layers to shape[cs.LG]
                   the model’s output. Additionally, we identify      between surface-level observations of persona ef-
                    specific attention heads that disproportionately        fects and the underlying mechanisms that produce
                   attend to racial and color-based identities.1          them. We investigate the roles of key model com-
                                                            ponents namely, Multi-Layer Perceptron (MLP)
          1  Introduction                                    layers, Multi-Head Attention (MHA) layers, and
                                                                    individual attention heads in shaping the reasoning
            Recent advances in large language models (LLMs)
                                                                           shifts induced by persona assignments. Using acti-
            have demonstrated their striking ability to adopt a
                                                                 vation patching, we probe the internal circuitry of
           wide range of personas, enabling context-sensitive
                                           LLMs to trace the origins of persona-driven varia-
            and tailored responses (Serapio-García et al., 2023,
                                                                     tion in objective tasks. Our findings challenge the
           Zhang et al., 2024, Joshi et al., 2024, Sun et al.,
                                                                  prevailing assumption that early MLP layers are
             2025). However, studies such as those by Salewski
                                                          concerned solely with syntactic processing. We
               et al., 2023, Deshpande et al., 2023, Gupta et al.,
                                                   show that these layers also encode semantic fea-arXiv:2507.20936v2       2024, Zheng et al., 2024 show that persona as-                                                                       tures related to persona. Furthermore, we identify a
            signment can significantly influence reasoning and,
                                                                small number of attention heads that disproportion-
              in some cases, amplify underlying social biases.
                                                                    ately focus on tokens associated with race-based
           While these works focus on identifying and quanti-
                                                            persona cues.  Although our work is limited to
             fying such effects, they do not examine the causal
                                                            uncovering the origin of persona-driven behavior,
           mechanisms within a pre-trained language model
                                                                                               it contributes to a deeper understanding of LLMs
          (PLM) that give rise to them.
                                                        and lays groundwork for future efforts to mitigate
               Mechanistic interpretability provides framework                                                               deep-seated biases in these systems.
              for uncovering the causal mechanisms by which
            language models carry out specific tasks. Lieberum                                                 2  Experimental Setup
                et al. (2023) introduced this approach in the context
             of multiple-choice question answering (MCQA),   2.1  Dataset Details

                                        We chose the MMLU dataset (Hendrycks et al.,                1Code and some  additional  results  are  available  at
                https://github.com/anshpoonia/Persona-Driven-Reasoning     2021)  for  this  study, which contains 14,024

multiple-choice questions spanning 57 subjects.             good
                                                                                                                       bright
We selected this dataset for two main reasons. First,              sharp
                                                                                                                             intelligent
the objective, multiple-choice format allowed us to              Asian
focus on a fixed set of four answer tokens, giving               AfricanWhite
us a well-defined target. Second, the wide range                      Identity   BrownIndian
of subject areas helped reduce domain-specific or               stupid
                                                                                                      Black
persona-driven biases. The identities or personas2              Yellow
we examined in our study fall into four broad cate-                Britishdumb                                                                                             Race-based
                                                                                                                                  dull                                                                                                     Color-basedPositive                                                                                                                                                                                                                                                                                                                                                      attributesgories: racial identities, color-based identities, and                                                                                                                                                                                                                                                                                      Negative                                                                                                                                                                                                                                                                                                                                                       attributes                                                                                           bad
identities defined by positive or negative attributes;                            0.015      0.010      0.005      0.000      0.005      0.010
                                                                                                                                  Avg. Probability Difference
details of each can be found in the Appendix A.
To maintain consistency and minimize variance,    Figure 1: Average difference in the probability of the
we used student as a gender-neutral subject across    correct token for each identity, relative to the base
all defined identities. For instance: Asian student,    prompt.
white student, good student, etc.

                                                      calculated the probability of the next token, which,2.2  Model and Prompt
                                          by design, corresponds to the selected answer for
Our primary investigation was conducted using the                                                    the given question. This process was repeated for
Llama 3.2 1B Instruct (Meta, 2024b) model to ac-                                                                 all 16 identities, and also the base identity.
commodate our computational constraints. We sup-                                           Our main focus is the change in the probability
plement these main findings with additional results                                                     of the correct token for each persona relative to the
from experiments on the Llama 3.2 3B Instruct                                                  base identity, as shown in Figure 1. This highlights
(Meta, 2024b) and Qwen 2.5 1.5B Instruct (Yang                                                             shifts in reasoning attributable to persona alone. In
et al., 2025). Compact size of these models makes                                          some cases, the differences in probability followed
it well-suited for efficient experimentation, while                                                        patterns that appeared to have a semantic basis. For
still delivering strong performance relative to other                                                       instance, the negatively attributed student persona
open-source models in their class. Our methods                                                performed significantly worse than those described
are scalable and can be extended to larger models                                                with positive attributes, having an average proba-
with sufficient computational resources and minor                                                            bility difference of -0.0027 (T = 11.7, p < 0.001).
adjustments. The Instruct variant of the models                                                   In contrast, the results for racially or color-coded
also permits the use of system prompts3, allowing                                                personas were less consistent, and no definitive
us to specify the identity the model should adopt in                                                 conclusion could be drawn about whether the pat-
its responses. A standard system prompt serves as                                                       terns reflect stereotypical associations. A similar
the base for each question, enabling identity shifts                                                    trend was observed when examining probability
by modifying only two tokens in the entire prompt:                                                     differences across identities for other models, see
just one token distinguishes each identity. We used                                             Appendix C for details. Overall, our results suggest
the prompt structure defined by Meta and Qwen                                                          that the model’s reasoning ability varies even with
team for MMLU dataset (Meta, 2024a), with the                                             minimal changes to the persona being imitated.
addition of the system prompt, see Appendix B.
                                       4  Interpretability Investigation
3  Persona Evaluation
                                                    4.1  Methods
We computed model outputs using zero-shot
                                We focus on activation patching, also referred to
prompts to isolate the variation introduced solely
                                                     as Resample Ablation, or Causal Mediation Analy-
by changes in the persona token, avoiding any influ-
                                                            sis (Vig et al., 2020, Meng et al., 2022), to under-
ence from patterns introduced by few-shot prompt-
                                                  stand the influence of individual components on
ing. This also reduced the length of each prompt, al-
                                                      the model’s selection of the correct answer. Specifi-
lowing for faster computation. For each prompt, we
                                                              cally, we employ "de-noising" variant of activation
   2The terms persona and identity have been used inter-    patching, where the activation of a component from
changeably in this work.                              a corrupted run is replaced with the correspond-
   3System prompts are instructions given to the AI before
                                                  ing activation from a clean run (Heimersheim andany user input. They define the AI’s behavior, role, and re-
sponse style throughout an interaction.                  Nanda, 2024). The clean and corrupted inputs are

 1                                       1

 2                                       2                                                                                                                            0.2


 3                                       3




 4                                       4




 5                                       5

 6                                       6                                                                                                                            0.1




 7                                       7




 8                                       8   Layers                                                                                                                                                                                                                                            Layers
  MLP9                                                                                                                    MHA9                                                                                                                            0.0

   10                                                                               10


   11                                                                               11


   12                                                                               12

                                                                                                                                                                                                                                                                                     0.1
   13                                                                               13


   14                                                                               14


   15                                                                               15

   16                                                                               16                                                                                                                             0.2
                             -bad             0.1 Average0.0      0.1              -bad                                                                        -bad         0.0  Average0.1                                                           -bad                 -Brown -sharp -British -White  -Indian -Asian -Yellow -Black                                                             -Brown-sharp-British-White                                                -Indian -Asian-Yellow -Black
 Asian  helpful Asian  White  goodIndian  good African  helpful  dumb                                   Asian helpful Asian White  goodIndian goodAfrican helpful  dumb

            MLP layer patching                       MHA layer patching

Figure 2: Relative logit difference (∆r) when MLP layer (left) and MHA layer (right) is patched in Llama 1B model.
Accompanying bar chart shows the average ∆r across identity pairs.


constructed using Symmetric Token Replacement,    lated contribution of the component to the output.
a technique in which only one or a few tokens   The indirect effect captures the influence the com-
are altered (Meng et al., 2022, Wang et al., 2023).   ponent has via the behavior of later layers and is
This approach doesn’t throw the model’s internal   computed by subtracting the direct effect from the
state out-of-distribution and preserves the syntactic    total effect of patching the component. The direct
structure of the input (Zhang and Nanda, 2024).    effect of a given component can be measured by
For each experiment, we select a pair of personas    subtracting its output from the residual stream be-
to generate the clean (ID1) and corrupted (ID2)    fore the final-layer norm in the corrupt run, and
prompts. A total of ten persona pairs were chosen    adding the output of that component from the clean
keeping computational constraints in mind; details    run. This preserves the indirect effect of the com-
are provided in the Appendix D. Care was taken to   ponent and only alters its direct effect. Together,
balance pairings identities both within and across    these effects help characterize the role the compo-
categories to ensure reliable results. Pairings with    nent plays in the processing of input information.
a base identity were also included for comparison.
                                                    4.3  MetricsFor each pair, we divide the full set of questions
into four subsets, Appendix E. From these, we se-   To quantify the impact of patching, we use two met-
lect the subset in which the first persona (ID1)    rics. First, we check whether patching causes the
answers correctly while the second persona (ID2)    logit (l) of the correct answer to become the highest
answers incorrectly. In each such pair of prompts,   among all four options. Second, we measure the
the only difference lies in the token representing    relative logit difference (∆r), i.e., change in the
the persona. As a result, any change in the logit    logit of the correct answer relative to the change in
of the correct answer after patching reflects the    the mean logit across all options.
influence of the component (and its downstream ef-
fects) on how the model processes persona-related    ∆r = {lcorrect(P) −lcorrect(C)}
information.
                                               −{µ(lABCD(P)) −µ(lABCD(C))}

4.2  Effects
                                                   Here, l∗(P) is the logit from patched run and
The impact of patching on the model’s output can   l∗(C) from corrupt run, and µ is mean function.
be broken down into direct and indirect effects  We use this relative metric rather than an absolute
(Pearl, 2001). The direct effect measures the iso-   one because some components may support the

                                                                   0.2               1
                                                                                                                 2
                                                                   0.0               3
                                                                                                                 4
                                                                    0.2              5                                                                                               0.1
  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16                            6
                 MLP Layers                                                                                                                           Layers 78                                                                                               0.0
                                                                                                                     MHA 109
Figure 3: Average ∆r when only direct component of a            11                                                                                                0.1
MLP layer is patched.                                                                       1213
                                                                                                                14
                                                                                                                15
                                                                                                                16
                                                                                                                     1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
correct answer not by increasing its logit directly,                                                    Attention Heads
but by decreasing the logits of incorrect options.                                                      Figure 4: Average ∆r for patching individual attention
An absolute measure, like simple logit difference,    heads.
would overlook such cases and only highlight com-
ponents with large magnitude effects on the correct
answer. These metrics are computed across all    to these, 18th MLP layer and 1st MHA in Qwen
selected questions for each persona pair.              also gives higher ∆r, which requires further explo-
                                                      ration in future studies. We hypothesize that the
4.4  Findings                                     early MLP layers develop persona-specific infor-
We begin by patching the MLP and MHA layers    mation, particularly at the token position(s) repre-
across all token positions. The Figure 2 shows ∆r    senting the identity. This information is then picked
across all identity pairs, see Appendix F for results   up and used by the later MHA layers, giving rise
on other metric, and Appendix G for patching re-    to the observed persona-driven behavior. To test
sults of other two models. Initial observations can     this, we patch only the activations at the identity
be made by comparing the semantic similarities of   token position in the MLP layers, rather than all
identity pairs with the results from activation patch-    positions. We observed that patching activations at
ing. The good-sharp identity pair, for instance,   the identity token position in the first MLP layer
produced a noticeably lower ∆r score across all   produced an effect nearly equivalent to patching all
models. This is likely because the terms are seman-   token positions. Beyond the first layer, the effect
tically similar; a good student and a sharp student    rapidly decays, with later layers showing little to
both represent a capable student, and the model   no impact, see Appendix H.
gives nearly identical probabilities to the correct    We also find that only the MLP layers near the
answer for each. A similar, though less distinct,   end of the network have any direct effect on the
pattern can be seen with the bad-dumb identity    output, see Figure 3. This implies that the effect
pair. When identities like Asian and White are    seen in the initial layers is purely indirect. Since the
patched with activations from the neutral base iden-    local syntax around the point of change remains
tity, the effect on the model’s reasoning is much    constant, this suggests that the initial MLP lay-
higher. Since the base identity is a neutral persona,    ers are processing not just structure but also the
the fact that patching activations from it improves    semantics of the input tokens. This counter to ear-
the model’s performance suggests that these com-    lier findings (Belinkov et al., 2017, Peters et al.,
ponents are introducing bias in the presence of   2018, Jawahar et al., 2019) that claimed these lay-
specific identities. This bias hampers the model’s    ers were focused only on syntactic or local features,
reasoning, and patching the output of these compo-   leaving semantic processing to later layers. The
nents essentially prevents this bias from occurring.   subtlety of this semantic processing may explain
The lower ∆r values for pairs like White-Black in   why it was overlooked in previous studies. Our re-
Llama 3B and Asian-Yellow in Qwen 1.5B may    sults align with observations that initial MLP layers
be a result of the specific ways the models were    transform tokens into richer representations (Stolfo
fine-tuned, as this inconsistency was not observed    et al., 2023), supporting the hypothesis that these
in other models.                                      layers induce persona-specific semantics that later
  We observe that patching the early MLP layers    layers utilize.
(layers 1–3) and the middle MHA layers (layers     To investigate MHA layer roles more closely,
9–11) produces a consistently higher total effect   we performed activation patching on individual
across all ten identity pairs. Similar observations    attention heads (Hheadlayer number ), identifying eight
can be made for Llama 3B model, but in additions   heads with a high positive effect on the output

and four with a high negative effect, see Figure 4.    terns and identified a subset that assigned dispropor-
We analyzed the value-weighted attention patterns    tionate weight to racial and color-related attributes
(Lieberum et al., 2023) of these heads on the iden-   associated with a persona.
tity token position across five questions per subject,     These findings also have significant practical im-
categorizing them based on the relative attention    plications for developing fairer and more reliable
given when compared with other identities, see   AI systems. The research demonstrates how to pin-
Appendix I. H259  , H2510, H2611, H313, and H1413 con-   point specific model components, like certain MLP
sistently allocated higher attention to tokens repre-    layers and attention heads, that encode and act upon
senting racial identities. H2611, H313, and H1413 also    persona-driven biases related to race and other at-
showed elevated attention to color-based identities,    tributes. This allows for interventions that are far
though this pattern was less consistent across do-   more targeted than standard fine-tuning. Instead of
mains. H1613 uniquely focused on color-based iden-   making broad adjustments, we can directly modify,
tities, while H2515 prioritized both positive and neg-    steer, or even disable the specific neural circuits
ative attributed identities at early token positions.   responsible for undesirable stereotypical behaviors,
H259  also gives negative identities more attention    leading to more effective and efficient bias miti-
at token positions near the end. We further exam-    gation. Furthermore, when coupled with methods
ined how these attention patterns of these heads    like probing, it serves as a powerful diagnostic tool,
responded to MLP layer patching. When activa-   enabling cheaper, more precise monitoring of how
tions from runs with racial or color-based personas    a model processes sensitive information and offer-
were replaced with those from positive or negative    ing a clear window into its internal reasoning. This
attributed personas, the attention of heads previ-   deeper interpretability is essential for debugging,
ously showing high focus on the identity token    ensuring AI safety, and building systems that are
position decreased significantly. This reduction oc-   not only less biased but also more transparent and
curred regardless of whether all token positions or    controllable in how they adopt these biases.
only the identity token position were patched. For      Overall, our observations offer preliminary in-
most heads, patching layers beyond the first had    sight into the subtle yet significant functions of
minimal impact on attention patterns, for details    certain model components and how they can be
see Appendix J.                                    revealed through constrained but straightforward
  Our findings validate the hypothesis of an in-   experiments. We took initial steps toward under-
teraction between the initial MLP layers and the    standing the origins of persona-driven behavior in
middle MHA layers in driving persona-driven be-  LLMs. In future studies, we will investigate why
havior. The initial MLP layers, especially at the    certain personas answer specific questions correctly
identity token position, form rich, persona-specific   while others do not, and how output vector circuits
semantic representations. These are then taken up    in attention heads use earlier-layer representations
by the middle MHA layers, impacting the model’s    to shape final predictions. These directions will
choice of response.                                 lead to a deeper understanding of what "persona"
                                                         truly means in the context of language models.
5  Discussion
                                            Limitations
In this work, we analyzed the impact of persona-
driven behavior on the reasoning ability of a lan-   Although the MMLU dataset contains a large num-
guage model in objective tasks. Through the lens of    ber of mostly factual, objective questions, we ac-
mechanistic interpretability, we examined the role   knowledge that it is not the only dataset that meets
of different component models: MLP layers, MHA   our selection criteria. Our experiments were con-
layers, and individual attention heads in shaping   ducted on the Llama 3.2 1B, Llama 3.2 3B and
this behavior. We observed that early MLP layers   Qwen 2.5 1.5B Instruct models, while attention
also contributes towards semantic understanding   heads level patching experiments being limited to
of inputs and encode persona-specific information   Llama 1B model, due to computational and time
into richer representation. This information is then    constraints. However, the methods described here
passed to later MHA layers, which use it to influ-   can be readily extended to models of different sizes
ence the model’s response. We further categorized   and architectures, as well as to other datasets with
attention heads based on their relative attention pat-    similar characteristics.

  We selected a set of 16 personas to approximate    Ruizhe Li and Yanjun Gao. 2025. Anchored answers:
the space relevant to our probing efforts. The list       Unravelling positional bias in gpt-2’s multiple-choice
                                                             questions. Preprint, arXiv:2405.03205.of personas is not exhaustive, but it serves as a
practical starting point. Our analysis focused on   Tom Lieberum, Matthew Rahtz, János Kramár, Neel
attention heads identified as important through ac-     Nanda, Geoffrey Irving, Rohin Shah, and Vladimir
                                                         Mikulik. 2023. Does circuit analysis interpretabilitytivation patching. Examining additional heads may
                                                             scale? evidence from multiple choice capabilities in
improve our understanding of how persona-driven                                                                 chinchilla. arXiv preprint arXiv:2307.09458.
behavior develops within the model. At present,
however, attention pattern analysis requires man-   Kevin Meng, David Bau, Alex J Andonian, and Yonatan
                                                          Belinkov. 2022. Locating and editing factual associ-
ual inspection, which remains a slow and labor-
                                                             ations in GPT. In Advances in Neural Information
intensive process.                                       Processing Systems.

                                                   Meta. 2024a. Llama 3.2 Evals.
References
                                                   Meta. 2024b. Llama 3.2 family of models.
Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir
   Durrani, Fahim Dalvi, and James Glass. 2017. Evalu-   Judea Pearl. 2001.  Direct and indirect effects.  In
   ating layers of representation in neural machine trans-      Proceedings of the Seventeenth Conference on Un-
   lation on part-of-speech and semantic tagging tasks.       certainty in Artificial Intelligence, UAI’01, page
   In Proceedings of the Eighth International Joint Con-      411–420. Morgan Kaufmann Publishers Inc.
   ference on Natural Language Processing (Volume
                                               Matthew E. Peters, Mark Neumann, Luke Zettlemoyer,  1: Long Papers), pages 1–10, Taipei, Taiwan. Asian
                                                   and Wen-tau Yih. 2018. Dissecting contextual word   Federation of Natural Language Processing.
                                                      embeddings: Architecture and representation. In Pro-
Ameet Deshpande, Vishvak Murahari, Tanmay Rajpuro-      ceedings of the 2018 Conference on Empirical Meth-
   hit, Ashwin Kalyan, and Karthik Narasimhan. 2023.      ods in Natural Language Processing, pages 1499–
   Toxicity in chatgpt: Analyzing persona-assigned lan-      1509, Brussels, Belgium. Association for Computa-
  guage models.  In Findings of the Association for       tional Linguistics.
  Computational Linguistics: EMNLP 2023, pages
                                                 Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto,  1236–1270, Singapore. Association for Computa-
                                                            Eric Schulz, and Zeynep Akata. 2023. In-context im-   tional Linguistics.
                                                           personation reveals large language models’ strengths
Shashank Gupta, Vaishnavi Shrivastava, Ameet Desh-     and biases. In Thirty-seventh Conference on Neural
  pande, Ashwin Kalyan, Peter Clark, Ashish Sabhar-      Information Processing Systems.
   wal, and Tushar Khot. 2024. Bias runs deep: Implicit
                                                 Gregory Serapio-García, Mustafa Safdari, Clément   reasoning biases in persona-assigned LLMs. In The
   Twelfth International Conference on Learning Repre-      Crepy, Luning Sun, Stephen Fitz, Marwa Abdulhai,
                                                     Aleksandra Faust, and Maja Matari´c. 2023. Person-   sentations.
                                                                     ality traits in large language models. arXiv preprint
Stefan Heimersheim and Neel Nanda. 2024. How to      arXiv:2307.00184.
  use and interpret activation patching. arXiv preprint
  arXiv:2404.15255.                                 Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya
                                                       Sachan. 2023. A mechanistic interpretation of arith-
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,      metic reasoning in language models using causal
  Mantas Mazeika, Dawn Song, and Jacob Steinhardt.      mediation analysis. In Proceedings of the 2023 Con-
  2021. Measuring massive multitask language under-      ference on Empirical Methods in Natural Language
   standing. In International Conference on Learning      Processing, pages 7035–7052, Singapore. Associa-
   Representations.                                            tion for Computational Linguistics.

Ganesh Jawahar, Benoît Sagot, and Djamé Seddah.   Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi Fung,
  2019. What does BERT learn about the structure of     Hou Pong Chan, Kevin Small, ChengXiang Zhai,
  language? In Proceedings of the 57th Annual Meet-     and Heng Ji. 2025. Persona-DB: Efficient large lan-
   ing of the Association for Computational Linguistics,      guage model personalization for response prediction
  pages 3651–3657, Florence, Italy. Association for      with collaborative data refinement. In Proceedings
  Computational Linguistics.                                of the 31st International Conference on Computa-
                                                                tional Linguistics, pages 281–296, Abu Dhabi, UAE.
Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung      Association for Computational Linguistics.
  Kim, and He He. 2024. Personas as a way to model
   truthfulness in language models. In Proceedings of    Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
   the 2024 Conference on Empirical Methods in Natu-     Sharon Qian, Daniel Nevo, Yaron Singer, and Stu-
   ral Language Processing, pages 6346–6359, Miami,       art Shieber. 2020. Investigating gender bias in lan-
   Florida, USA. Association for Computational Lin-     guage models using causal mediation analysis. In
   guistics.                                           Advances in Neural Information Processing Systems,

  volume 33, pages 12388–12401. Curran Associates,
                                                   Category           Identity   Inc.
                                                           Racial-based        Asian
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,                                Indian
  Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-                                                                              African
   pretability in the wild: a circuit for indirect object
                                                                                       British   identification in GPT-2 small. In The Eleventh Inter-
   national Conference on Learning Representations.             Color-based        White
                                                                         Black
Sarah Wiegreffe, Oyvind Tafjord, Yonatan Belinkov,                                                               Brown
  Hannaneh Hajishirzi, and Ashish Sabharwal. 2025.
                                                                        Yellow  Answer, assemble, ace:  Understanding how lms
  answer  multiple  choice  questions.     Preprint,             Positive-attributes   good
   arXiv:2407.15018.                                                                       intelligent
                                                                                     bright
An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
                                                                              sharp  Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
   Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-             Negative-attributes  bad
  hong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,                                   dull
   Jingren Zhou, Junyang Lin, Kai Dang, and 23 oth-                                                                                   stupid
   ers. 2025. Qwen2.5 technical report. arXiv preprint
                                                            dumb   arXiv:2412.15115.

Fred Zhang and Neel Nanda. 2024. Towards best prac-      Table 1: Identities with their respective category.
   tices of activation patching in language models: Met-
   rics and methods. In The Twelfth International Con-
   ference on Learning Representations.                       ID1           ID2
                                                      Asian student    Indian student
Zhehao Zhang, Ryan A Rossi, Branislav Kveton, Yijia
  Shao, Diyi Yang, Hamed Zamani, Franck Dernon-           helpful assistant   Asian student
   court, Joe Barrow, Tong Yu, Sungchul Kim, and 1           Asian student   Yellow student
   others. 2024. Personalization of large language mod-          White student    Black student
   els: A survey. arXiv preprint arXiv:2411.00027.                                                   good student     bad student
Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran,            Indian student   Brown student
  Moontae Lee, and David Jurgens. 2024. When ”a           good student     sharp student
   helpful assistant” is not really helpful: Personas in           African student   British student
  system prompts do not improve performances of                                                             helpful assistant   White student
   large language models. In Findings of the Associ-
                                             dumb student     bad student   ation for Computational Linguistics: EMNLP 2024,
  pages 15126–15154, Miami, Florida, USA. Associa-
   tion for Computational Linguistics.                                  Table 2: Identity Pairs

A  Identities
                                                                                                                        Baseline   helpful
In this study, personas or identities refer to sin-                            brightgood
gle words such as "Asian" or "good." The selected              Positive   sharp
identities fall into four broad categories, with four                           intelligentAsian                                                                                 0.008
identities chosen from each to ensure balanced com-                         African                                                                                 0.004
                                                                                                          Race
parisons. A further criterion in selection was that                          Indian
                                                                                                                                                                         British                                                                                 0.000
each identity should be of a single token, so that                       White
the only variation between prompts would be one               Color  Brown                                                                                  0.004
                                                                                                                                          Black
token. Table 1 lists the identities alongside their                        Yellow                                                                                  0.008
respective categories.                                                                                                  stupid
                                                                                                            dumb
                                                                                                                      Negative
                                                                                                                                                                                dull
B  Prompt Structure                                                                        bad
                                                                                                                                                             helpfulgoodbrightsharpintelligentAsianAfricanIndianBritishWhiteBrownBlackYellowstupiddumb dullbadThe prompt format used throughout the study is
shown in Figure 6. The placeholder {helper} is                                                     Figure 5: Average difference in probability of correct
replaced with "a" or "an" depending on whether    token of identities w.r.t. each-other.
the first letter of {identity_1} is a vowel. The

variable {identity_1} is substituted with the iden-       Identity pair   C1   C2   C3  C4
 tities listed in Table 1, or with "helpful" in the       Asian, Indian   6909  6820  164  149
base prompt. The placeholder {identity_2} is        helpful, Asian   6846  6707  262  227
replaced with "student" in the persona prompts       Asian, Yellow   6888  6771  185  198
and with "assistant" in the base prompt.  The       White, Black   6866  6831  188  157
{question} field is filled with the target question,        good, bad     6834  6691  272  245
and each {option_*} is replaced by the corre-       Indian, Brown   6855  6781  203  203
sponding answer choice.                              good, sharp    7035  6864   71   72
                                                        African, British  6863  6787  197  195
C  Additional Performance Results                helpful, White   6808  6688  300  246
                                               dumb, bad    6824  6726  237  255
The average of relative difference in the proba-
 bility assigned to correct token for each identity    Table 3: Number of questions in each subset for Llama
 w.r.t. base identity, for Llama 3B and Qwen 1.5B is   1B model
shown in Figure 7. For Llama 1B model, average
of difference in the probability assigned to correct
                                                  Questions from subset S3 were chosen for acti-
token for a given identity relative to other identities
                                                    vation patching because they offer a well-defined
 is shown in Figure 5.
                                                         target token to observe during the patched run. In
                                                    the ID2(corrupt) run, the logit of the correct to-D  Identity Pairs
                                              ken is lower than in the ID1(clean) run. Therefore,
Table 2 shows the selected identity pairs. The pair-   components that raise the logit of the correct token
ing of identity terms was partly based on common    during the patched run help restore the model’s be-
stereotypes, such as Asian-Yellow, Indian-Brown,   havior to that of ID1. Questions were divided in
Asian-Indian, and contrasted with pairs like White-    similar sets for other models based on their respec-
Black and African-British. Other identities were    tive results.
chosen from sets of positive and negative attributes,
 to include both semantically similar pairs (good-  F  Additional Patching Results
sharp, dumb-bad) and dissimilar ones (good-bad).
A few were added for comparison with baseline  We also measured, for each identity pair (ID1,
 identities such as helpful-Asian and helpful-White.   ID2), the proportion of questions where ID1 an-
During activation patching, the prompt for ID1   swered correctly and ID2 did not, such that patch-
serves as the clean prompt, while the prompt for    ing the activation of an MLP or MHA layer from
ID2 serves as the corrupt prompt.  Activations    ID1’s run into ID2’s run caused the correct token
from the ID2 run are replaced with those from the    to receive the highest logit. Figure 9 presents the
ID1 run to identify which components restore the    results for both MLP and MHA layers, as well as
model’s output to that of ID1. The only difference    their average across all identity pairs.
between the ID1 and ID2 prompts is the identity
token, except when ID1 corresponds to the base  G  Patching Results of other models
prompt, in which case assistant is replaced with
                                 We replicated the activation patching experiments student.
                                           on the MLP and MHA layers for the Llama 3B
                                            and Qwen 1.5B models. The results are shown inE  Question Subsets
                                                  Figure 8. We did not perform head-level patching
For each identity pair (ID1, ID2), the questions   on these models due to computational and time
 in the dataset are divided into four subsets: S1 –    constraints. The results for the Llama 3B model
questions answered correctly by both identities;    are largely consistent with those of the Llama 1B
S2 – questions answered incorrectly by both; S3 –   model; however, the results for the Qwen 1.5B
questions answered correctly by ID1 but incorrectly   model show some anomalies, such as higher ∆r
by ID2; S4 – questions answered correctly by ID2    values for the 18th MLP layer and the 1st MHA
but incorrectly by ID1. The number of questions    layer, which might be interesting to investigate fur-
 in each category for the selected identity pairs is    ther in future studies. Nevertheless, the main obser-
shown in Table 3.                                    vations made in this work hold for all three models.

            Prompt for Llama Model

              <|start_header_id|>system<|end_header_id|>
              You are {helper}  {identity_1}  {identity_2}.
              <|eot_id|><|start_header_id|>user<|end_header_id|>

              Given the following question and four candidate answers (A, B, C and D), choose the
              best answer.
              Question:  {question}

              A. {option_1}
              B. {option_2}
              C. {option_3}
              D. {option_4}

              Your response should end with "The best answer is [the_answer_letter]" where the
              [the_answer_letter] is one of A, B, C or D.
              <|eot_id|><|start_header_id|>assistant<|end_header_id|>

              The best answer is
            Prompt for Qwen Model

              <|im_start|>system
              You are {helper}  {identity_1}  {identity_2}.<|im_end|>
              <|im_start|>user

              Given the following question and four candidate answers (A, B, C and D), choose the
              best answer.
              Question:  {question}

              A. {option_1}
              B. {option_2}
              C. {option_3}
              D. {option_4}

              Your response should end with "The best answer is [the_answer_letter]" where the
              [the_answer_letter] is one of A, B, C or D.
              <|im_end|><|im_start|>assistant
              The best answer is


                                    Figure 6: MMLU prompt structure.


H  Identity-token-position Patching           tionately more attention to a specific identity or
    Results                                   group of identities.

The relative change in the logit of the correct token,   J  Attention after patching
when only the identity token position’s activation is
                                                 Figure 12 shows the change in value-weighted at-patched for the MLP layer, is shown in Figure 10a.
                                                     tention at the identity position when MLP layerFigure 10b shows the percentage of questions for
                                                      activations are patched from the "good" run intowhich the correct token receives the highest logit.
                                                    the "Asian" run for a given question. The results
I  Attention Visualization                       indicate that racial heads (attention heads that al-
                                                    located higher attention to racial-based identity to-
Attention heads were selected based on high posi-   kens) assigns significantly less attention when the
tive effect (H259  , H2510, H2511, H2611, H2711, H912, H1413,    activations of early MLP layers are patched.
H2515), and high negative effect (H1112, H313, H1613,
H2615). Figure 11 shows the relative value-weighted
attention given at the identity token position by
selected attention heads, for a sample question
from the dataset. Relative value-weighted atten-
tion is computed by subtracting the mean value-
weighted attention across all identities from the
value-weighted attention assigned to a given iden-
tity. This highlights the heads that pay dispropor-

     intelligent                                                                                   sharp
        Asian                                                                         Brown
       White                                                                                   Asian
       African                                                                                  White
        stupid                                                                                                        British
       good                                                                                                        intelligent
        Indian                                                                                       Yellow
  Identity    sharpbright                                                                                                                                                                                                                                 Identity    Indianbright
        Black                                                                            good
      dumb                                                                                                              dull
         British                                                                                             African
      Brown                                                                   dumb
       Yellow                                                                                             Race-based                   stupid                                                                                             Race-based
        bad                                                                                                     Color-basedPositive attributes              bad                                                                                                     Color-basedPositive attributes
            dull                                                                                                     Negative attributes              Black                                                                                                     Negative attributes
                  0.010     0.005    0.000     0.005     0.010     0.015     0.020                     0.015     0.010     0.005    0.000     0.005     0.010     0.015
                                     Avg. Probability Difference                                                                  Avg. Probability Difference

                    Llama 3B                                    Qwen 1.5B

Figure 7: Results for Llama 3B (left) and Qwen 1.5B (right) for average difference in the probability of correct
tokens w.r.t baseline.

 1                                       1


 2                                       2


 3                                          3


 4                                          4                                                                                                                            0.2


 5                                       5


 6                                       6


 7                                       7


 8                                       8


 9                                       9
   10                                                                               10                                                                                                                            0.1
   11                                                                               11
   12                                                                               12
   Layers1314                                                                                                                                                                                                                                       Layers1314
  MLP1516                                                                                                                    MHA1516                                                                                                                            0.0
   17                                                                               17
   18                                                                               18
   19                                                                               19
   20                                                                               20
   21                                                                               21
   22                                                                               22                                                                                                                             0.1
   23                                                                               23
   24                                                                               24
   25                                                                               25
   26                                                                               26
   27                                                                               27
   28                                                                               28                                                                                                                             0.2
                             -bad              0.0 Average0.2              -bad                                                                        -bad              0.0Average  0.2                                                           -bad                 -Brown -sharp -British -White  -Indian -Asian -Yellow -Black                                                             -Brown-sharp-British-White                                                -Indian -Asian-Yellow -Black
 Asian  helpful Asian  White  goodIndian  good African  helpful  dumb                                   Asian helpful Asian White  goodIndian goodAfrican helpful  dumb

         MLP layer patching (Llama-3B)                MHA layer patching (Llama-3B)





 1                                       1


 2                                       2


 3                                          3


 4                                          4                                                                                                                            0.2


 5                                       5


 6                                       6


 7                                       7


 8                                       8


 9                                       9
   10                                                                               10                                                                                                                            0.1
   11                                                                               11
   12                                                                               12
   Layers1314                                                                                                                                                                                                                                       Layers1314
  MLP1516                                                                                                                    MHA1516                                                                                                                            0.0
   17                                                                               17
   18                                                                               18
   19                                                                               19
   20                                                                               20
   21                                                                               21
   22                                                                               22                                                                                                                             0.1
   23                                                                               23
   24                                                                               24
   25                                                                               25
   26                                                                               26
   27                                                                               27
   28                                                                               28                                                                                                                             0.2
                             -bad            0.1  Average0.0      0.1              -bad                                                                        -bad       0.0    0.1Average0.2    0.3                                                           -bad                 -Brown -sharp -British -White  -Indian -Asian -Yellow -Black                                                             -Brown-sharp-British-White                                                -Indian -Asian-Yellow -Black
 Asian  helpful Asian  White  goodIndian  good African  helpful  dumb                                   Asian helpful Asian White  goodIndian goodAfrican helpful  dumb

        MLP layer patching (Qwen-1.5B)                MHA layer patching (Qwen-1.5B)

Figure 8: Relative logit difference (∆r) when MLP layers (left) and MHA layers (right) are patched in Llama 3B
(top row) and Qwen 1.5B (bottom row). Accompanying bar charts show the average ∆r across identity pairs.

                                                                                                                                                                                                                                       100
 1                                       1


 2                                       2


 3                                       3

 4                                       4                                                                                                        80


 5                                       5


 6                                       6


 7                                       7                                                                                                        60



 8                                       8   Layers                                                                                                                                                                                                                                            Layers


                                          9 MLP9                                                                                                                    MHA

   10                                                                               10                                                                                                        40

   11                                                                               11

   12                                                                               12

   13                                                                               13
                                                                                                                                                                                                                                       20

   14                                                                               14

   15                                                                               15


   16                                                                               16
                                                                                                                                                                                                                                       0
                             -bad      0      20Average40 %   60              -bad                                                                        -bad     0      25Average50%    75                                                           -bad                 -Brown -sharp -British -White  -Indian -Asian -Yellow -Black                                                             -Brown-sharp-British-White                                                -Indian -Asian-Yellow -Black
Asian  helpful Asian  White  goodIndian  good African  helpful  dumb                                   Asian helpful Asian White  goodIndian goodAfrican helpful  dumb

            MLP layer patching                       MHA layer patching

Figure 9: Percentage of questions whose logit of correct token became maximum after patching MLP layer (left)
and MHA layer (right).





               (a) Relative logit difference (∆r).                 (b) Percentage of questions whose logit of correct token became
                                                maximum.

              Figure 10: Comparison of MLP layer patching effects at the identity token position.

                Layer: 9; Head: 25                               Layer: 10; Head: 25                               Layer: 11; Head: 25
     0                                               0                                               0
     6                                               6                                               6
    12                                              12                                              12
    18                                              18                                              18
    24                                              24                                              24
    30                                              30                                              30
    36                                              36                                              36
    42                                              42                                              42
    48                                              48                                              48
    54                                              54                                              54 Positions 60                                                                                                                                                Positions 60                                                                                                                                                Positions 60    66                                              66                                              66
 Token 7278                                                                                Token 7278                                                                                Token 7278    84                                              84                                              84
    90                                              90                                              90
    96                                              96                                              96
   102                                             102                                             102
   108                                             108                                             108
   114                                             114                                             114
   120                                             120                                             120
                  helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid
                Layer: 11; Head: 26                               Layer: 11; Head: 27                               Layer: 12; Head: 9
     0                                               0                                               0
     6                                               6                                               6
    12                                              12                                              12
    18                                              18                                              18
    24                                              24                                              24
    30                                              30                                              30
    36                                              36                                              36
    42                                              42                                              42
    48                                              48                                              48
    54                                              54                                              54 Positions 60                                                                                                                                                Positions 60                                                                                                                                                Positions 60    66                                              66                                              66
 Token 7278                                                                                Token 7278                                                                                Token 7278    84                                              84                                              84
    90                                              90                                              90
    96                                              96                                              96
   102                                             102                                             102
   108                                             108                                             108
   114                                             114                                             114
   120                                             120                                             120
                  helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid
                Layer: 12; Head: 11                               Layer: 13; Head: 3                               Layer: 13; Head: 14
     0                                               0                                               0
     6                                               6                                               6
    12                                              12                                              12
    18                                              18                                              18
    24                                              24                                              24
    30                                              30                                              30
    36                                              36                                              36
    42                                              42                                              42
    48                                              48                                              48
    54                                              54                                              54 Positions 60                                                                                                                                                Positions 60                                                                                                                                                Positions 60    66                                              66                                              66
 Token 7278                                                                                Token 7278                                                                                Token 7278    84                                              84                                              84
    90                                              90                                              90
    96                                              96                                              96
   102                                             102                                             102
   108                                             108                                             108
   114                                             114                                             114
   120                                             120                                             120
                  helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid
                Layer: 13; Head: 16                               Layer: 15; Head: 25                               Layer: 15; Head: 26
     0                                               0                                               0
     6                                               6                                               6
    12                                              12                                              12
    18                                              18                                              18
    24                                              24                                              24
    30                                              30                                              30
    36                                              36                                              36
    42                                              42                                              42
    48                                              48                                              48
    54                                              54                                              54 Positions 60                                                                                                                                                Positions 60                                                                                                                                                Positions 60    66                                              66                                              66
 Token 7278                                                                                Token 7278                                                                                Token 7278    84                                              84                                              84
    90                                              90                                              90
    96                                              96                                              96
   102                                             102                                             102
   108                                             108                                             108
   114                                             114                                             114
   120                                             120                                             120
                  helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid                                   helpfulAsianIndianAfricanBritishWhiteBlackBrownYellowgoodintelligentbrightsharpdullbaddumbstupid

Figure 11: Relative value-weighted attention given by selected attention heads at identity token position. The
prompt used is the first question from "Abstract Algebra" subject.

                Layer: 9; Head: 25                               Layer: 10; Head: 25                               Layer: 11; Head: 25
     0                                               0                                               0
     7                                               7                                               7
    14                                              14                                              14
    21                                              21                                              21
    28                                              28                                              28
    35                                              35                                              35
    42                                              42                                              42
    49                                              49                                              49
    56                                              56                                              56 Positions 63                                                                                                                                                Positions 63                                                                                                                                                Positions 63
    70                                              70                                              70
 Token 7784                                                                                Token 7784                                                                                Token 7784
    91                                              91                                              91
    98                                              98                                              98
   105                                             105                                             105
   112                                             112                                             112
   119                                             119                                             119
        runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch
               'good''Asian'                                                                                                   'good''Asian'                                                                                                   'good''Asian'                    MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16

                Layer: 11; Head: 26                               Layer: 11; Head: 27                               Layer: 12; Head: 9
     0                                               0                                               0
     7                                               7                                               7
    14                                              14                                              14
    21                                              21                                              21
    28                                              28                                              28
    35                                              35                                              35
    42                                              42                                              42
    49                                              49                                              49
    56                                              56                                              56 Positions 63                                                                                                                                                Positions 63                                                                                                                                                Positions 63
    70                                              70                                              70
 Token 7784                                                                                Token 7784                                                                                Token 7784
    91                                              91                                              91
    98                                              98                                              98
   105                                             105                                             105
   112                                             112                                             112
   119                                             119                                             119
        runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch
               'good''Asian'                                                                                                   'good''Asian'                                                                                                   'good''Asian'                    MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16

                Layer: 12; Head: 11                               Layer: 13; Head: 3                               Layer: 13; Head: 14
     0                                               0                                               0
     7                                               7                                               7
    14                                              14                                              14
    21                                              21                                              21
    28                                              28                                              28
    35                                              35                                              35
    42                                              42                                              42
    49                                              49                                              49
    56                                              56                                              56 Positions 63                                                                                                                                                Positions 63                                                                                                                                                Positions 63
    70                                              70                                              70
 Token 7784                                                                                Token 7784                                                                                Token 7784
    91                                              91                                              91
    98                                              98                                              98
   105                                             105                                             105
   112                                             112                                             112
   119                                             119                                             119
        runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch
               'good''Asian'                                                                                                   'good''Asian'                                                                                                   'good''Asian'                    MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16

                Layer: 13; Head: 16                               Layer: 15; Head: 25                               Layer: 15; Head: 26
     0                                               0                                               0
     7                                               7                                               7
    14                                              14                                              14
    21                                              21                                              21
    28                                              28                                              28
    35                                              35                                              35
    42                                              42                                              42
    49                                              49                                              49
    56                                              56                                              56 Positions 63                                                                                                                                                Positions 63                                                                                                                                                Positions 63
    70                                              70                                              70
 Token 7784                                                                                Token 7784                                                                                Token 7784
    91                                              91                                              91
    98                                              98                                              98
   105                                             105                                             105
   112                                             112                                             112
   119                                             119                                             119
        runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch               runrunpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatchpatch
               'good''Asian'                                                                                                   'good''Asian'                                                                                                   'good''Asian'                    MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16                                MLP-1MLP-2MLP-3MLP-4MLP-5MLP-6MLP-7MLP-8MLP-9MLP-10MLP-11MLP-12MLP-13MLP-14MLP-15MLP-16

Figure 12: Relative value-weighted attention given by selected heads at identity token position after patching
activation at identity token position for MLP layer. The prompt used is the first question from "Abstract Algebra"
subject.