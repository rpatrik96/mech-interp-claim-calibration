              Compact Proofs of Model Performance via
                        Mechanistic Interpretability




                       Jason Gross∗   Rajashree Agrawal   Thomas Kwa†   Euan Ong†   Chun Hei Yip†

                              Alex Gibson‡           Soufiane Noubir‡          Lawrence Chan
2024
Dec                 We propose using mechanistic interpretabilityAbstract– techniques for reverse engineering
                          model weights into human-interpretable algorithms – to derive and compactly
24                       prove formal guarantees on model performance. We prototype this approach by
                                formally proving accuracy lower bounds for a small transformer trained on Max-of-
                       K, validating proof transferability across 151 random seeds and four values of K.
                    We create 102 different computer-assisted proof strategies and assess their length
                            and tightness of bound on each of our models. Using quantitative metrics, we find
                                   that shorter proofs seem to require and provide more mechanistic understanding.
                             Moreover, we find that more faithful mechanistic understanding leads to tighter[cs.LG]                       performance bounds. We confirm these connections by qualitatively examining a
                               subset of our proofs. Finally, we identify compounding structureless errors as a
                             key challenge for using mechanistic interpretability to generate compact proofs on
                           model performance.


                1  Introduction

                 One approach to ensuring the safety and reliability of powerful AI systems is via formally verified
                      proofs of model performance [48, 11].  If we hope to deploy formal verification on increasingly
                        large models [24, 27] with powerful emergent capabilities [56] across more diverse and broader
                     domains [5, 46], we will need compact proofs of generalization bounds on specific models that certify
                      global robustness. However, existing approaches tend to use proof strategies that suffer from bad
                       asymptotic complexity, while verifying either generalization properties of training procedures or local
                       robustness properties of specific models.

                  One key challenge to verification is that neural network architectures are highly expressive [51, 58],
                    and models with similar training procedure and performance may still have learned significantlyarXiv:2406.11779v14
                          different weights [38, 9]. This expressivity makes it difficult to adequately compress explanations of
                       global model behavior in ways that correspond closely enough to the model’s actual mechanisms to
                     be useful for efficient verification without being too lossy, especially when using only knowledge of
                        the architecture or training procedure. We propose verifying model performance using understanding
                       derived from mechanistic interpretability (Section 2) – that is, reverse engineering the specific
                     implementation of the algorithm from the learned weights of particular models. Knowledge of the
                         specific implementation allows us to construct less lossy simplifications of the model, and more
                           efficiently reason about model performance over possible inputs.

                       In this work, we provide a case study of translating mechanistic interpretations into compact proofs.
               We train an attention-only transformer on a Max-of-K task with 151 random seeds (Section 3), and

                          ∗Corresponding author. Please direct correspondence to jgross@mit.edu.
                          †These authors contributed equally to this work.
                          ‡These authors contributed equally to this work.


                            Preprint. Under review.

               True Model                     Brute Force Proof                Cubic Proof                 Subcubic Proofs

     Logits      (ℓ0, ℓ1, . . . , ℓ63)                          (ℓ0, ℓ1, . . . , ℓ63)                      (ℓ0, ℓ1, . . . , ℓ63)                     (ℓ0, ℓ1, . . . , ℓ63)
                                                                                            QK Circuit decomposes
                                                                                                                                                                         into large “size” and small
                                                                                                                                                                   . . .                                                           “noise” components
              Unembed                                                                            Unembed
                                                                          ...


                                                                                                                                                                   Size component                                                                                                                                                         w/
                                                                                                                                                                      103                                                                                                                                                                           singular                                                                                                                                                                         value 7.4 ×                                                                                                                                                       Path               Circuit                Circuit          OV  QK
                                                                                                                                                                                                                                   Direct    QK    OV      ?...
               Embed                                                                                                                       . . .                      Embed                       singularOther componentsvalue < 1.5 have× 101


     Input      t0    t1   t2    t3                         t0    t1   t2    t3                    t0    t1   t2    t3                    t0    t1   t2    t3


         FLOPs Required:                           1.41 × 1014                       3.51 × 107                       4.68 × 106
           Accuracy Lower Bound:                      99.92 %                         95.31 %                        28.41 %
           Unexplained Dimension:                     1.07 × 109                        1.28 × 104                       4.42 × 103
           Asymptotic Complexity:                  O(dvocab nctx)                  O(dvocab 3 · nctx)            O(dvocab · dmodel 2 · nctx)


Figure 1: We construct proofs using different degrees of mechanistic interpretation. (Left) The models we
consider in this paper are one-layer attention-only transformers, and so contain three “paths”: the OV circuit, the
QK circuit, and the direct path. (Right) For the brute-force proof (Section 4.3.1), we treat the model as a black box
and thus need to check all possible combinations of inputs. For the cubic proof (Section 4.3.1), we decompose
the model into its three corresponding paths, but still check the correctness of each path via brute force. Finally,
in some subcubic proofs (Section 4.3), we use all parts of the mechanistic interpretation presented in Section 3.
(Bottom) For each of the three categories of proof, we report the number of FLOPs used in computing the
certificate (lower=better, Appendix A.6), lower bound on model accuracy (higher=better), effective dimension
of the unexplained parts of the model (lower=better, Appendix A.5), and asymptotic complexity of the proof
strategy as we scale the inputs and model (lower=better). Significantly more compact proofs have vacuous
accuracy bounds by default. Using more mechanistic understanding allows us to recover some, but not all, of the
accuracy bounds on these more compact proofs, as our understanding is not fully faithful to the model internals.


then reverse engineer the models using standard mechanistic interpretability techniques. We use
our understanding to define a set of 102 different computer-assisted proof strategies with varying
tightness of bound and with different asymptotic complexity and number of required floating-point
operations (Section 4).4 We validate our technique against an additional 604 models for varying
values of K (Appendix A.2.1).

We define a quantitative metric to assess the mechanistic understanding used in a proof strategy
by the dimensionality of the function space that the proof strategy must consider, which we deem
the unexplained dimensionality of the proof strategy (Sections 5.1, and A.5). Using this metric, we
find a negative relationship between proof length and degree of understanding. We qualitatively
examine proof strategies to confirm and explain this relationship, finding that more compact proofs
both require and provide more mechanistic understanding. We also find suggestive evidence that
the trade-off between proof length and tightness of bound is modulated by the faithfulness of the
mechanistic understanding used to derive the proof (Section 5.2).5

However, we also identify compounding structureless error terms as a key challenge for generating
compact proofs on model behavior (Sections 5.3, and G.2.5). The implementation of algorithms
inside of neural networks may contain components that defy mechanistic understanding and appear
to us as “noise”. When we don’t know how noise composes across model components, establishing a
bound requires pessimizing over the ways the composition could occur. Worst-case noise can quickly
grow over components even when the empirical noise is small, leading to vacuous performance
bounds (Appendix G.2.5).


2  Mechanistic interpretability for proofs

Generalization bounds on global performance In the style of prior mechanistic interpretability
evaluation work [6], we target theorem templates that establish bounds on the expected global
performance of the model. Let M : X →Y be a model (here assumed to be a neural network), D be
a probability distribution over input-label pairs (l, t) ∈L × X, notated as D|X when marginalized

   4Our 102 proof strategies are can be broken up as 1 + 1 + 10 × 5 × 2: two standalone strategies, and a class
of strategies parameterized on three axes of cardinality 10, 5, and 2 (Appendix H).
   5Code for reproducing our results can be found at https://github.com/JasonGross/guarantees-
based-mechanistic-interpretability/. A cache of generated data can be found at https://github.
com/JasonGross/guarantees-based-mechanistic-interpretability/tree/with-data.


                                       2

over labels, and f : L × Y →R be a scoring function for evaluating the performance of the model.
Then, we seek to establish lower bounds b on the expected ¯s as the form:
                                                  ¯s := E(l,t)∼D [f(l, M(t))] ≥b.                                (1)
As f can be any metric, this is a fully general template for theorems that can capture any aspect of
model performance for which we have a formal specification. However, in this work we restrict f to
be the accuracy and D|X to be uniform, so our theorems lower bound the accuracy of the model. Our
proof methodology generalizes straightforwardly to other input distributions (Appendix A.8), and
only a little work is required to generalize from accuracy to log-loss (Appendix A.11).

Proof template The proofs of model performance in this work have two components: a computational
component C  : model weights →R and a non-computational component Q arguing that for any
model M′, C(M′) ≤E(l,t)∼Df(l, M′(t)), thus implying that C generates a valid lower bound
for the performance of M. The whole proof is Q paired with a trace of running C that certifies its
output on M.6 Here, b = C(M). As even the size of the model parameters is much larger than any
reasonable Q, we approximate the length of a proof pair C, Q by the length of a trace of C(M).

Proof compactness vs. tightness of bound Different proof strategies make different tradeoffs
between compactness and tightness of bound. For example, consider two extreme proof strategies:
We can “prove” a vacuous bound using a null proof. On the other hand, in the brute-force proof, we
simply run the model on the entirety of D to achieve b = ¯s, albeit with a very long proof.

We quantify the length of C(M) using two metrics: the asymptotic time complexity of C as we
scale the size of the model and the input t, as well as the empirical average number of floating point
operations required to evaluate C(M′) over a given set of models {Mi}. We measure tightness of
bound of C(M) using the ratio of the bound to the true accuracy: b/¯s.

Proof as pessimal ablation A standard way of assessing the faithfulness of mechanistic interpretabil-
ity is by ablating the parts of the model that your interpretation does not explain [54, 6, 23]. In this
framework, proofs can be thought of as performing a pessimal ablation over the unexplained parts
of the model – we set the remaining components of the model (the “noise” or error terms) to values
over X that minimize the performance of the model. However, the number of ablations required for a
complete argument might be quite high. Thus, we construct relaxations (Appendix A.4) over input
sequences, such that performing pessimal ablations on a smaller number of relaxed input sequences
is sufficient to lower bound the performance on D.

3  Experimental setting

We study our approach to generating compact proofs in a simple toy setting: Max-of-K.

Model Architecture We study one-layer, one-head, attention-only transformers with no biases
but with learned positional embeddings, with vocabulary size dvocab, model and head dimension
d = dmodel = dhead, and context length nctx := k. The model parameters consist of the nctx×dmodel
positional embedding P; the dvocab × dmodel token embed E; the dmodel × dmodel query, key, value,
and output matrices of the attention head Q, K, V , and O; as well as the dmodel × dvocab unembed
matrix U. We assume (as is standard in language modeling) that dmodel < dvocab.
For an nctx × dvocab one-hot encoding x =  [x0, x1, . . . , xnctx−1] of an input sequence t =
[t0, t1, . . . , tnctx−1], we compute the logits of the model as follows:
           h(0) = xE + P                             Initial residual stream (nctx × dmodel)
                 √
         α = h(0)QKT h(0)T /  d            Attention matrix (nctx × nctx)
           h(1) = σ∗(α) · h(0)V O + h(0)         Final residual stream (nctx × dmodel)
      M(t) = ℓ= h(1)nctx−1U                  Final seq. position logits (dvocab)

where σ∗is the masked softmax function used in causal attention. Because we only look at outputs
of the model above the final sequence position i = nctx −1, we also denote this position as the

   6Other components of the proof to account for the difference between floating point numbers and reals are
described in Appendix A.7. Note that all proofs explicitly given in this paper are of Q only; we do not include
any traces of running C.


                                       3

“query position” and the value of the token in this position as tquery, one-hot encoded as xquery. The
model’s prediction is the token corresponding to the max-valued logit ℓmax.

Task Specifically, we study the setting with nctx = k = 4 because it is the largest sequence length for
which we can feasibly evaluate the brute-force proof. We set hidden dimension dmodel = 32 and a
vocabulary of size dvocab = 64 comprising integers between 0 and 63 inclusive. For an input sequence
t = [t0, t1, t2, t3], we denote the true maximum of the sequence by tmax. Outputting the correct
behavior is equivalent to outputting logits ℓsuch that ∆ℓt∗:= ℓt∗−ℓmax < 0 for all t∗̸= tmax. We
trained 151 models on this task. Models achieved an average accuracy of 0.9992 ± 0.0015 over the
entire data distribution.

Path decomposition Following prior work [13], we expand the logits of the model and split the paths
through the model into three components – the QK circuit, the OV circuit, and the direct path:


                          √
M(t) = σ∗  (xqueryE + Pquery) QKT (xE + P)T /  d   · (xE + P) V OU + (xqueryE + Pquery) U
             |            QK {zcircuit             }       |   OV {zcircuit   }  |          direct{zpath      }
                                                                                                    (2)

Intuitively, the QK circuit determines which tokens the model attends to from a particular query token
and sequence position, while the OV circuit processes the tokens and sequence positions the model
attends to. The direct path is simply the skip connection around the attention head.

We further divide the QK and OV circuits into token (position-independent) and position-dependent
components. Let Pavg = Pi Pi/nctx be the average position embeds across positions (of size
dmodel), and let ¯P denote either 1nctx ⊗Pavg or 1dvocab ⊗Pavg depending on context, the result of
broadcasting Pavg back into the shape of P or E (that is, nctx ×dmodel or dvocab ×dmodel). Similarly,
let Pq = 1dvocab ⊗Pquery be the result of broadcasting Pquery. Then for one-hot encoded x, we can
rewrite the QK and OV circuits, as well as the direct path, as follows:

         QK circuit = xquery EqQKT ¯ET xT + EqQKT ˆPT
                                   | EQKE{z   }      |  EQKP{z    }
          OV circuit = x ¯EV OU + ˆPV OU     Direct Path = xquery EqU
                             |EVOU{z }  |PVOU{z  }                      |{z}EU
where ˆP = P −¯P and ¯E = E + ¯P and Eq = E + Pq (since h(0) = x¯E + ˆP).

3.1  Mechanistic interpretation of learned models

Using standard empirical mechanistic interpretability tech-                       Logits  (ℓ0, ℓ1, . . . , ℓ63)
niques, we interpret one of our learned models (our “mainline”                           Unembed
model) by independently examining the QK and OV circuits        toQKlargerCircuittokensattendsmore
and the direct path.7 We find that the model outputs the largest
logit on the true max token tmax by attending more to larger    OV Circuit performs
                                                                                                                                                                               Direct                                                                                                                                                                               pathtokens via the QK circuit and copying the tokens it attends to       low-rank copying                                                                                                                                                                          nothing”                                                                                                                   Embed        “does
via the OV circuit. We then quantitatively confirm that these
interpretations hold for all 151 models by reporting the mean                        Input     t0   t1   t2   t3
plus minus standard deviation for various summary statistics.
                                                            Figure 2: The models in our setting
Plots for this section are available in Appendix B.2.                                                            implement Max-of-K by attending ex-
QK  circuit By  qualitatively  examining  the  position-  ponentially more to larger tokens and
independent QK component EQKE, we find the amount of  copying the attended-to tokens (Sec-
pre-softmax attention paid to a key token is approximately in-  tion 3.1).
dependent of the value of the query token tquery, and increases monotonically based on the size of the
key token. We confirm this hypothesis by performing a singular-value decomposition (SVD) of the
EQKE matrices (Appendix G.2.3), and find that it contains a single large rank-one component with
singular value around 7800 ± 380, around 620 ± 130 times larger than the second largest component
with singular value 13 ± 3. The left (query-side) singular vector is approximately constant in all

    7All of our trained models behave similarly; see Appendix B.3.


                                        4

dimensions, with value 0.1243 ± 0.0003 ≈1⁄8 = 1/√dvocab. The right (key-side) singular vector√   of this
component is monotonically increasing as we increase the size of the key token, with (1/  d-scaled)
pre-softmax attention increasing by an average of 1.236 ± 0.056 when the key token increases by 1.8
           √
In comparison, each 1/  d-scaled entry of the position-dependent QK component EQKP has negligi-
ble size (average 0.31 ± 0.18), suggesting that EQKP is unimportant to the functioning of the model.
We confirm this by zero ablating EQKP, which changes the models’ accuracies from 0.9992±0.0015
to 0.9993 ± 0.0011. Combined with our interpretation of EQKE, this implies that the attention
pattern of the model depends only on the token values and not the ordering of the sequence.

OV circuit Then, by qualitatively examining the position-independent OV component EVOU, we
see that it has large positive entries along the diagonal. In fact, the entry along the diagonal is the
largest in the row for all rows corresponding to t > 6.6 ± 1.2. Since each entry in the sequence is
uniformly sampled and dvocab = 64, this means that EVOU is a good approximation for the identity
matrix for all but ≈(7/64)4 ≈1.2 × 10−2 % of the sequences.

As with the position-dependent QK component, the position-dependent OV component PVOU also
has negligible size and is unimportant to model performance. Taken together with the above results
on EVOU, this suggests that the attention head copies the tokens it attends to.

Direct path As with the two position-dependent components, the entries in EU have small absolute
magnitude 2.54 ± 0.20,9 and contribute negligibly to model performance.

4  Proofs of model performance

In this section we describe intuitions for three categories of proof that are developed around different
mechanistic interpretations and methods for using the interpretations. The strategies result in proofs
of different complexities with varying bound tightness (Table 1). We provide detailed theorem
statements, proofs, algorithms, and explanations of proof search in Appendices C, D, E, F, and G.

Our theorem statements for Q will all be of the form
                ∀M′, Cspecific strategy(M′) ≤Et∼D|Xf(tmax, M′(t)).
We leave implicit the traces of running Cspecific strategy on our specific models to give the overall
theorem. We report the computational complexity or estimated FLOPs of running Cspecific strategy as
approximations for our proof lengths.

4.1  The brute-force baseline

We start by considering the brute-force proof (Appendix D), which treats the model as a black box and
evaluates it on all possible sequences.10 However, this proof strategy has bad asymptotic complexity
and is untenable for larger models and larger input distributions. So in subsequent sections, we use
knowledge of the model drawn from the interpretation in Section 3.1 to derive more compact proofs.

4.2 A cubic proof

Next, we use the fact that the model is composed of the direct path and the QK and OV circuits
(Section 3) to decrease the number of sequences that we need to consider, and the fact that only
the position-independent components EQKE and EVOU contribute meaningfully to performance
(Section 3.1) to pessimize over sequence ordering.

First, let a pure sequence ξ be a sequence with at most three distinct tokens: the max token tmax, the
final token tquery ≤tmax, and optionally a third token t′ < tmax, and let Ξpure be the set of all pure
sequences in X.11 For a given input sequence t, define the adjacent pure sequences Adj(t) as the set
of sequences that share the same max and query token, and only take on values in t:
             n                                      o          Adj(t) =  ξ ∈Ξpure max ξi = tmax, ξquery = tquery, ∀i < nctx, ξi ∈t
                                                     i

   8This implies that the ratio of attention paid to token t and t −1 is approximately exp(1.236) ≈3.442.
   9For comparison, the average off-diagonal element of EVOU is 21.68 ± 0.83 below the corresponding
diagonal element.
  10Appendix A.10 discusses how to compute the “brute-force” accuracy of a model on an infinite distribution.
   11In Section 4.3, we will consider a smaller set of “pure sequences”.


                                       5

Table 1: We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as
unexplained dimensionality (Section 5). We round the FLOP and unexplained dimension counts to the closest
power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. As we include
more aspects of the mechanistic interpretation (reflected by a lower number of unexplained dimensions), we get
more compact proofs (in terms of both asymptotic complexity and FLOPs), albeit with worse bounds. For space
reasons, we use k := nctx, d := dmodel, and v := dvocab.

  Description         Complexity Cost              Bound                Est.     Unexplained
  of Proof                                                   FLOPs  Dimensions

  Brute force         O(vk+1kd)                      0.9992 ± 0.0015      247       230
  Cubic             O(v3k2)                        0.9531 ± 0.0087      225       214
  Sub-cubic          O(v2 · k2 + v2 · d)               0.702 ± 0.033        221       213
    w/o mean+diff                                     0.349 ± 0.080        221       213
  Low-rank QK       O(v2k2 + vd2 + v2d )           0.675 ± 0.035        222       212
   SVD only                   |{z}QK   EU&OV|{z}            0.284 ± 0.072        222       212
  Low-rank EU       O(v2k2 + vd + v2d )           0.633 ± 0.062        221       213
   SVD only                   |{z}EU   QK&OV|{z}           (3.38±0.06)×10−6   221       213
  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.610 ± 0.060        221       213
   SVD only                   |{z}QK    |{z}EU    |{z}OV      (3.38±0.06)×10−6   222       213
  Quadratic QK       O(v2k2 + vd + v2d )           0.316 ± 0.037        221       212
                                 |{z}QK   EU&OV|{z}
  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.283 ± 0.036        221       213
                              QK&EU|{z}   |{z}OV



Using the convexity of softmax and the fact that the model contains three paths, we can show that
one-layer attention-only transformers satisfies a variant of the following convexity property: for a
given t, if M(ξ) is correct for all ξ ∈Adj(t), then M(t) is correct. That is, for these transformers,
we can bound the accuracy on all sequences by evaluating M on only the O(dvocab 3(nctx −1)!)
pure sequences. This allows us to bound the accuracy of our actual M on all dvocab nctx sequences,
while evaluating it on O(dvocab 3(nctx −1)!) sequences.

We can reduce the number of sequences that we need to evaluate by pessimizing over the order of a
sequence. For a given tuple of (tmax, tquery, t′), there are (nctx −1)! pure sequences, corresponding
to the permutations of the tuple. Pessimizing over the order of sequences reduces the number of
sequences to consider for each (tmax, tquery, t′) tuple to the number of t′ in the pure sequence, and
the total number of sequences to O(dvocab 3nctx). By precomputing the five component matrices
EU, EQKE, EQKP, EVOU, PVOU and cleverly caching intermediate outputs, we can reduce the
additional work of each sequence to the O(nctx) required to compute the softmax over nctx elements,
resulting in asymptotic complexity O(dvocab 3nctx2) (Theorem 12, additional details in Appendix E).

4.3  Sub-cubic proofs

We now consider proofs that are more compact than O(dvocab 3). These require avoiding iteration
over any set of size O(dvocab 3) (e.g. the set of pure sequences) and performing operations that take
O(dvocab) time on each of O(dvocab 2) combinations. Unfortunately, some methods of avoiding
these operations can lead to vacuous bounds (i.e. accuracy lower bounds near 0%). In order to
recover non-vacuous bounds, we introduce two tricks: the “mean+diff trick” to better approximate
the sum of two components with unequal variance, and the “max row diff trick” to improve upon
the low-rank approximations for EU and EQKE. We consider applying variants of these tricks at
different locations in the naïve subcubic proof, leading to 100 distinct subcubic proof strategies. See
Appendix G.2 for a formal description of these strategies.

4.3.1  Removing cubic-time computations

Reducing the number of cases by pessimizing over sufficiently small tokens Previously, we
considered Θ(dvocab 3nctx) pure sequences ξ, with ξ parameterized by (tmax, tquery, t′, c). Recall


                                       6

from our mechanistic interpretation in Section 3.1 that the pre-softmax attention paid from tquery to
a key token t′ is broadly invariant in tquery and increases roughly linearly with the size of t′. This
allows us to pessimize over the OV circuit over all “sufficiently small” tokens.
More formally, suppose we are given some gap g ∈N. For each pure sequence ξ with max token tmax,
query token tquery, such that tquery ≤tmax −g, and c copies of the third token type t′ ≤tmax −g,
we pessimally ablate the OV circuit over the set Ξpure(tmax, tquery, c; g) of pure sequences ξ′ with
the same max and query tokens and c copies of the third token type t′. If the model gets all sequences
in Ξpure(tmax, tquery, c; g) correct, then we can conclude that it gets ξ correct, otherwise, we treat
the model as having gotten ξ wrong. This means that it suffices to only consider the O(dvocab 2nctx)
pessimal pure sequences of each of the O(dvocab 2nctx) sets of the form Ξpure(tmax, tquery, c; g).
Decoupling and pessimizing computations that require O(dvocab 3) computations Many parts
of our cubic certificate require iterating through O(dvocab 2) cases parameterized by (tmax, tquery)
or (tmax, t′). For example, as part of the pessimization procedure over pure sequences, for each
of the dvocab possible values of tmax, we need to consider the relative effects on the dvocab-sized
logits of attending to each of the O(dvocab) other tokens t′ < tmax, and for each tmax and tquery, we
need to check that the contribution of the direct path on logits xqueryEU is not sufficiently large as
to overwhelm the contribution from xmaxEVOU. We independently pessimize over each of these
components over one of the dvocab-sized axes: for example, instead of computing xmaxEVOU +
xqueryEU for each tmax, tquery pair, we first pessimally ablate the direct path along the query
token (which takes O(dvocab 2) time as it does not depend on the tmax, and then consider the sum
xmaxEVOU + maxx′ x′EU. Since this sum no longer depends on tquery, we only need to perform it
O(dvocab) times, for a total cost of O(dvocab 2).

Low rank approximations to EQKE and EU Recall from Section 3.1 that EQKE is approximately
rank 1, where the sole direction of variation is the size of the key token. By computing only the low
rank approximation to EQKE, we can more cheaply compute the most significant component of the
behavior in the QK circuit. To bound the remaining error, we can use the fact that after pulling off the
first principal component from each of the four matrices we multiply, very little structure remains.

We can find the rank 1/2 approximations by performing SVD on EQKE. We can efficiently compute
the SVD in O(dvocabdmodel 2) time by using the fact that EQKE can be written as the product of
a dvocab × dmodel matrix and a dmodel × dvocab matrix. This allows us to avoid performing the
O(dvocab 2dmodel)-cost matrix multiplications to explicitly compute EQKE.

Similarly, we can more efficiently check that the direct path EU contributes negligibly to the model
outputs, by using SVD to decompose EU into a sum of rank 1 products (which we can evaluate
exactly) and a high-rank error term that we can cheaply bound.

4.3.2  Additional subcubic proof strategies

Tighter bounds for sums of variables with unequal variance via the “mean+diff trick” Suppose
we want to lower bound the minimum of the sum of two functions over three variables h(x, y, z) =
f(x, y) + g(y, z), while only iterating over two variables at a time. The naïve way is to minimize
f(x, y) and g(x, y) independently:

                    min h(x, y, z) ≥min f(x, y) + min g(y, z)
                                x,y,z               x,y              y,z

Here, the error comes from setting the ys in f and g to different values. But in cases where g(y, z)
varies significantly with y and only slightly with z, rewriting g as a sum of a component that is
independent of z (only varying along y), and a component that depends on z, yields a better lower
bound:
         min h(x, y, z) ≥min (f(x, y) + E′zg(y, z′)) + min(g(y, z) −E′zg(y, z′))
              x,y,z               x,y                                y,z

This estimate will have error at most ε, while the naïve estimator can have arbitrarily large error. We
refer to this rewrite as the “mean+diff trick”.12 From the mechanistic interpretation in Section 3.1,

   12In fact, this is the motivation behind the standard rewrites of QK and OV into position-independent and
position-dependent components (Section 3).


                                       7

    Bound  1.0                                                                                                brute force (acc: 0.9992 ± 0.0015)
      0.8                                                                                           cubic (rel acc: 0.9539 ± 0.0080)
      0.6                                                                                         subcubic (rel acc: 0.700 ± 0.036)
                                                                                                                                   2 (rel acc: 0.675 ± 0.035)      Accuracy  0.4                                                                                                attention-dvocabdmodel
                                                                                                             direct-quadratic (rel acc: 0.633 ± 0.062)
      0.2
                                                                                                         attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.610 ± 0.060)
      0.0                                                                                                   attention-quadratic (rel acc: 0.316 ± 0.037)        Normalized                               229                         239                                    attention-quadratic, direct-quadratic (rel acc: 0.283 ± 0.037)
                          FLOPs to Verify Proof (approximate)                                 brute-force linear baseline


Figure 3: For each of the proofs in Section 4, we plot the number of FLOPs used to compute the certificate,
as well as the normalized accuracy lower-bound (b/¯s). The brute-force proof (Section 4.1) computes the exact
performance uses orders of magnitude more compute than other approaches. The cubic proof (Section 4.3)
uses a small amount of mechanistic understanding and less compute, while still retaining good accuracy lower
bounds. Finally, subcubic proofs (Section 4.3) require the entirety of the mechanistic interpretation of the model
to attain non-vacuous bounds; this understanding allows us to further reduces compute costs, but we still achieve
worse bounds. See Appendix H.2.1 for a detailed description of the various proof strategies.


we know that some of the components barely vary among one or more axes. So we can apply the
mean+diff trick to get tighter lower bounds.

Avoiding matrix multiplications using the “max row-diff trick” Using properties of linear algebra,
we derive a cheap approximation to the max row-diff for the product of matrices AB in terms of the
product of the max row-diff of B and the absolute value of A, which we deem the “max row-diff”
trick. We apply this trick to get a better cheap bound on the error terms of low-rank approximations,
without having to multiply out the full matrices. See Appendix G.2.2 for more details.

See Appendix F for more variants and combinations of these strategies.

5  Results

We run each of 151 transformers on the various proof strategies of different asymptotic complexity,
and analyze these proofs to empirically examine the relationship between proof length, bound
tightness, and degree of understanding. For each proof on each transformer, we approximate the
length of the proof by estimating the number of FLOPs used, and plot this against the ratio of certified
bound the true accuracy b/¯s (Equation 2) in Figure 3. There exists a clear trade-off between bound
tightness and compactness of the proof – more compact proofs yield looser bounds, and tighter
bounds are associated with more expensive proofs.

5.1  Compact proofs both require and provide mechanistic understanding

Quantifying mechanistic understanding using unexplained dimensionality We first quantify the
amount of mechanistic understanding used in a proof by measuring its unexplained dimensionality
– the number of free parameters required to fully describe model behavior, assuming the structural
assumptions of the proof are correct. More detailed mechanistic interpretations will leave fewer free
parameters that need to be filled in via empirical observation. (Details in Appendix A.5.) In Figure 5,
we plot the two axes and find a suggestive correlation – that is, proofs based on less mechanistic
understanding are longer.

More mechanistic understanding allows for more compact proofs In addition to the constructions
in Section 4, the parts of proofs we were unable to compact seem to correspond to components that
we do not mechanistically understand. For example, we could not cheaply bound the behavior of
EVOU without multiplying out the matrices, and this seems in part because we do have a mechanistic
understanding of how EVOU implements low-rank copying.

Compact proofs seem to provide understanding By examining compact proofs, we can extract
understanding about the model. For example, the fact that replacing each row of EU with its average
across rows has little effect on the bound implies that EU does not vary much based on tquery.

5.2  Proof length vs. bound tightness trade-off is modulated by faithfulness of interpretation

Compact proofs are less faithful to model internals To derive more compact proofs, we use our
mechanistic understanding to simplify the model computation in ways that diverge from the original
model internals. For example, in some subcubic proofs (Section 4.3), we approximate EQKE with a


                                        8

     1.0
  Bound  0.8                                                                                mean+max-diffmax-diff
                                                                                           mean+max-diff-subproduct
     0.6                                                                                        max-diff-subproduct
                                                                                                  max-diff-exact   Accuracy
     0.4                                                                                   svd
                                                                                                 mean-recursive+max-diff-subproduct-recursive
     0.2                                                                                       mean+max-diff-subproduct-recursive
                                                                                                  max-diff-subproduct-recursive   Normalized
     0.0
                400               600               800              1000
                        EPQKE Singular Ratio: σ1/σ2

Figure 4: We plot the normalized accuracy bound versus the ratio of first and second singular values of EQKE,
for various types of subcubic proofs that depend on a rank-1 approximation EQKE. For each class of proof, the
closer EQKE is to rank-1, the tighter the accuracy bound. This suggests that more faithful interpretations lead to
tighter bounds even holding proof length fixed. Note that the “svd” proof strategy has the clearest upward trend
(b/¯s = 0.000 20(σ1/σ2) + 0.44, R2 = 0.41). See Appendix H.2.2 for a detailed description of the various
proof strategies.


     229                                                                                             brute force
                                                                                                cubic      (Estimated)  226                                                                                      subcubic
     223                                                                                             attention-dvocabdmodel 2
                                                                                                          direct-quadratic
     220     Dimension                                                                                                  attention-dvocabdmodel 2, direct-quadratic
     217                                                                                                attention-quadratic
     214                                                                                                  attention-quadratic, direct-quadratic
      Unexplained               223        227        231        235        239        243        247
                          FLOPs to Verify Proof (approximate)

Figure 5: We plot, for each proof, the approximate number of flops required to evaluate the proof, versus the
unexplained dimensionality (Section 5.1). More mechanistic understanding leaves fewer dimensions unexplained.
We observe that more compact proofs seem to leave fewer unexplained dimensions, which is indicative of the
relationship of mechanistic understanding and compact proofs. See Appendix H.2.1 for a detailed description of
the various proof strategies.


rank-1 approximation corresponding to the “size direction”. However, while other components are
small, they’re nonzero; this approximation harms model internals.

Less faithful interpretations lead to worse bounds on performance To confirm that faithfulness of
understanding affects the tightness of bound independent of proof length, we plot the normalized
accuracy bound of subcubic proofs that perform a rank-1 approximation to EQKE, versus the ratio
of the first two singular components. A larger ratio between the components implies that the rank-1
approximation is more faithful. In Figure 4, we see a positive correlation between the two axes: when
the interpretation is more faithful, the bounds are tighter, even at a fixed proof length.

5.3  Compounding structureless noise is a big challenge for compacting global-behavior proofs

Pessimal error terms compound in the absence of known structure The rank-1 approximation of
EQKE has small error. However, when making rank-1 approximations of each of the constituent
matrices E, Q, K, pessimizing over the worst way to composing the individual small error terms
leads to a bound on the error term of EQKE that is orders of magnitude larger than the actual error
term. Because we don’t understand how the matrices compose in a way that doesn’t cause errors to
compound (without just multiplying out the matrices), this approximation leads to a trivial bound on
performance (Appendix G.2.5). We speculate that in many cases, there is no short human-interpretable
description for why random noise or approximation errors do not compound across layers of neural
networks (e.g., see the error correction results on randomly initialized neural networks from Hänni
et al. [21]), and thus that compounding structureless errors may be an issue in practice.

6  Related Work

Generalization Bounds Prior work in the PAC-Bayes framework [58, 36, 12] proves generalization
bounds over learning procedures, which are similar to the global performance bounds we consider in


                                       9

this work. These proofs tend to provide statistical guarantees [25, 26] about the outputs of a known
stochastic training procedure, while we seek to bound the performance of particular trained models.

Formally verifying neural networks Most prior work formally verifies neural networks either via
model checking [28, 7] or by relaxing the problem setting and taking an automated theorem proving
approach [17, 50, 18, 35, 43] to verify local robustness properties. These proof strategies tend to be
derived by examining only the network architecture. We take an approach more akin to interactive
theorem proving [22] and verify global performance properties by reverse-engineering the neural
network weights.

Mechanistic Interpretability Finally, mechanistic interpretability is the subfield of the broader field
of understanding model internals [45], which is too large to faithfully summarize. Our work takes
most direct inspiration from efforts to deeply understand how either toy models [38, 9, 53, 2] or small
pretrained text transformers [54, 20] implement algorithmic tasks, generally by performing ablations
and SVD. In contrast, we formally prove that a transformer implements an algorithm.

Nichani et al. [39] proves that, in a significantly simplified 2-layer, 1-head attention-only transformer
model and for the task of in-context bigram statistics, gradient descent will create induction heads [40].
Our results concern transformers with fixed weights. In concurrent work, Michaud et al. [34] use
techniques inspired by mechanistic interpretability to perform automated program synthesis on
2-dimensional RNNs, while our work works with significantly larger transformer models.

7  Conclusion and Future Work

Summary In this work, we used a Max-of-K setting to prototype the use of mechanistic interpretabil-
ity to derive compact proofs of model behavior. Using varying amounts of understanding, we derived
more efficient proof computations lower bounding model accuracy. We found preliminary evidence
that mechanistic understanding can compactify proofs. Moreover, we observed that the tightness
of the lower bound offered by various proof strategies can be used to grade the faithfulness our
mechanistic interpretation. Finally, we identified compounding structureless errors as a key obstacle
to deriving compact proofs of model behavior.

Limitations and future work We study one-layer attention-only transformers on a toy algorithmic
task. Future work should explore the viability of deriving proofs via interpretability using larger
models featuring MLPs or layernorm on more complex domains. In addition, we were unable to
significantly compact the part of the proof involving the OV circuit, which future work can explore.
The proofs we explored in this work also did not lead to qualitatively novel insights; future work
may be able to derive such insights with improved techniques. Finally, future work can address the
problem of compounding structureless errors, perhaps by relaxing from worst-case pessimal ablations
to typical-case heuristic guarantees [8].

Acknowledgments and Disclosure of Funding

We are immensely grateful to Paul Christiano for providing the initial support for this project and for
his invaluable research advice, encouragement, and feedback throughout its duration.

Additionally, we are thankful for clarifying discussions and feedback from Jacob Hilton, Matthew
Coudron, Adrià Garriga-Alonso, Aryan Bhatt, Leo Gao, Jenny Nitishinskaya, Somsubhro Bagchi,
Gabriel Wu, Erik Jenner, Ryan Greenblatt, Ronak Mehta, Louis Jaburi, and many others. Louis Jaburi
in particular contributed the text of the final proof of Theorem 11 in Appendix E.

We are indebted to various organizations for their support:

        • Alignment Research Center for funding this project and making it possible at all

        • Mentorship for Alignment Research Students (MARS) program of the Cambridge AI Safety
      Hub (CAISH) for setting up the collaboration between a subset of authors, and providing
        funding for compute and in-person research sprints

        • Constellation and FAR Labs for hosting a subset of the authors and providing an excel-
          lent research environment, including as part of the Visiting Fellows Program and Astra
        Fellowship


                                       10

Author Contributions

Jason Gross led the project, including managing the team and conceptualizing the proofs approach.
He ran the Max-of-4 experiments, devised the proof strategies, and wrote up the formal proofs. He
worked on various case studies and developed general methodology for computing complexity and
length bounds for proofs. He also developed the particular convex relaxations presented in the paper.

Rajashree Agrawal was invaluable in steering the direction of the project, including contributing to
the preliminary experiment on Max-of-2 and developing the pessimal ablation approach. She worked
on framing the results, and contributed text to the paper.

Thomas Kwa and Euan Ong extended the preliminary experiments to larger values of k and
contributed substantially to the cubic proof. Chun Hei Yip, Alex Gibson, and Soufiane Noubir
worked on case studies other than the Max-of-K task and informed discussion on proof complexity.

Lawrence Chan spearheaded the writing of the paper, including turning informal claims into formal
theorem statements, creating figures, and writing the core text. He also developed the unexplained
dimensionality metric for clarifying the takeaway of the paper.





                                       11

References

 [1] Behzad Akbarpour, Amr Abdel-Hamid, Sofiène Tahar, and John Harrison. Verifying a syn-
     thesized implementation of IEEE-754 floating-point exponential function using HOL. The
    Computer Journal, 53:465–488, May 2010. doi: 10.1093/comjnl/bxp023.

 [2] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
     algorithm is in-context learning? investigations with linear models, 2022.

 [3] Andrew Appel and Ariel Kellison. VCFloat2: Floating-point error analysis in Coq. In Proceed-
     ings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs,
    CPP 2024, pages 14–29, New York, NY, USA, 2024. Association for Computing Machinery.
    ISBN 9798400704888. doi: 10.1145/3636501.3636953.

 [4] Sylvie Boldo and Guillaume Melquiond. Flocq: A unified library for proving floating-point
     algorithms in Coq. In 2011 IEEE 20th Symposium on Computer Arithmetic, pages 243–252,
     July 2011. doi: 10.1109/ARITH.2011.40.

 [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
    Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
     Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
      Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
     Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
     Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
   URL https://arxiv.org/abs/2005.14165.

 [6] Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny
     Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas.  Causal scrub-
     bing, a method for rigorously testing interpretability hypotheses.  AI Alignment Forum,
     2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-
    scrubbing-a-method-for-rigorously-testing.

 [7] Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artificial
     neural networks. In Automated Technology for Verification and Analysis: 15th International
    Symposium, ATVA 2017, Pune, India, October 3–6, 2017, Proceedings 15, pages 251–268.
     Springer, 2017.

 [8] Paul Christiano, Eric Neyman, and Mark Xu. Formalizing the presumption of independence.
     arXiv preprint arXiv:2211.06738, 2022. doi: 10.48550/arxiv.2211.06738.

 [9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse
     engineering how networks learn group operations, 2023.

[10] Edmund M. Clarke, William Klieber, Miloš Nováˇcek, and Paolo Zuliani. Model Checking and
     the State Explosion Problem, pages 1–30. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
    ISBN 978-3-642-35746-6. doi: 10.1007/978-3-642-35746-6_1. URL https://doi.org/10.
    1007/978-3-642-35746-6_1.

[11] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia,
     Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards guar-
     anteed safe AI: A framework for ensuring robust and reliable AI systems. arXiv preprint
     arXiv:2405.06624, 2024.

[12] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds
      for deep (stochastic) neural networks with many more parameters than training data. Proceedings
     of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2016, August
     11–15, 2017, Sydney, NSW, Australia, 2017.

[13] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
    Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep
     Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,
    Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and
     Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,
     2021. URL https://transformer-circuits.pub/2021/framework/index.html.


                                       12

[14] Martín H. Escardó. Synthetic topology of data types and classical spaces. Electronic Notes in
     Theoretical Computer Science, 87:21–156, November 2004.

[15] Martín H. Escardó. Infinite sets that admit fast exhaustive search. In Proceedings of the 22nd
    Annual IEEE Symposium on Logic in Computer Science (LICS 2007), Wrocław, Poland, July
     2007.

[16] Martín H. Escardó. Seemingly impossible functional programs, 2007. URL https://math.
    andrej.com/2007/09/28/seemingly-impossible-functional-programs/. Accessed:
     2024-05-15.

[17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2:
     Safety and robustness certification of neural networks with abstract interpretation. In 2018
    IEEE Symposium on Security and Privacy (SP), pages 3–18, Los Alamitos, CA, USA, May
     2018. IEEE Computer Society. doi: 10.1109/SP.2018.00058.

[18] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
     Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
    bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715,
     2018.

[19] Jason S. Gross. Performance Engineering of Proof-Based Software Systems at Scale. PhD
      thesis, Massachusetts Institute of Technology, February 2021. URL https://dspace.mit.
    edu/handle/1721.1/130763.

[20] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than.
     Interpreting mathematical abilities in a pre-trained language model, 2:11, 2023.

[21] Kaarel Hänni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of
     computation in superposition. In ICML 2024 Workshop on Mechanistic Interpretability, 2024.
   URL https://openreview.net/forum?id=OcVJP8kClR.

[22] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In
    Handbook of the History of Logic, volume 9, pages 135–214. Elsevier, 2014.

[23] Stefan Heimersheim and Neel Nanda. How to use and interpret activation patching. arXiv
     preprint arXiv:2404.15255, 2024.

[24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
     Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
     Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[25] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo,
    Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks:
      Verification, testing, adversarial attack and defence, and interpretability.  arXiv preprint
     arXiv:1812.08342, 2018.

[26] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek
     Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei
    Xu, Dengyu Wu, Andre Freitas, and Mustafa A. Mustafa. A survey of safety and trustworthiness
     of large language models through the lens of verification and validation, 2023.

[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
     Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
     models. arXiv preprint arXiv:2001.08361, 2020.

[28] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An
      efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th
     International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings,
     Part I 30, pages 97–117. Springer, 2017.

[29] A. E. Kellison, A. W. Appel, M. Tekriwal, and D. Bindel. LAProof: A library of formal proofs
     of accuracy and correctness for linear algebra programs. In 2023 IEEE 30th Symposium on
    Computer Arithmetic (ARITH), pages 36–43, Los Alamitos, CA, USA, September 2023. IEEE
    Computer Society. doi: 10.1109/ARITH58626.2023.00021.


                                       13

[30] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin,
    Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell,
     Harvey Tuch, and Simon Winwood. seL4: Formal verification of an OS kernel. In Proceedings
      of the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP ’09, pages 207–
     220, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605587523.
      doi: 10.1145/1629575.1629596. URL https://doi.org/10.1145/1629575.1629596.

[31] Xavier Leroy. A formally verified compiler back-end. Journal of Automated Reasoning, 43:
     363–446, 2009.

[32] Chuan Li. OpenAI’s GPT-3 language model: A technical overview, June 2020. URL https:
    //lambdalabs.com/blog/demystifying-gpt-3. Lambda Labs Blog, accessed October
     30, 2024.

[33] Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt
     and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56–61,
     2010. doi: 10.25080/Majora-92bf1922-00a.

[34] Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge,
     Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukeli´c, and Max Tegmark. Opening the AI
     black box: program synthesis via mechanistic interpretability, 2024.

[35] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for
     provably robust neural networks. In International Conference on Machine Learning, pages
     3578–3586. PMLR, 2018.

[36] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain
     generalization in deep learning. Advances in Neural Information Processing Systems, 32, 2019.

[37] Neel  Nanda  and  Joseph  Bloom.     TransformerLens.     https://github.com/
    TransformerLensOrg/TransformerLens, 2022.

[38] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress
     measures for grokking via mechanistic interpretability. arXiv preprint, 2023. doi: 10.48550/
     arXiv.2301.05217.

[39] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with
     gradient descent. arXiv preprint, 2024. doi: 10.48550/arXiv.2402.14735.

[40] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
     Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
    Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
     Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
    Sam McCandlish, and Chris Olah.  In-context learning and induction heads. Transformer
     Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/in-context-
    learning-and-induction-heads/index.html.

[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
     Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative
      style, high-performancedeep learning library.  Advances in neural information processing
     systems, 32, 2019.

[42] Plotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly.

[43] Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semidefinite relaxations for certifying
     robustness to adversarial examples. Advances in neural information processing systems, 31,
     2018.

[44] Tahina Ramananandro, Paul Mountcastle, Benoıˆt Meister, and Richard Lethin. A unified Coq
    framework for verifying C programs with floating-point computations. In Proceedings of the
     5th ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2016, pages 15–26,
    New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341271. doi:
     10.1145/2854065.2854066.


                                       14

[45] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI:
   A survey on interpreting the inner structures of deep neural networks. In First IEEE Conference
    on Secure and Trustworthy Machine Learning, 2022. doi: 10.48550/arxiv.2207.13243.

[46] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
     Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
     Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
     Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL
    https://arxiv.org/abs/2205.06175.

[47] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation.
     In International Conference on Learning Representations, 2022. URL https://openreview.
    net/forum?id=oapKSVM2bcj.

[48] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence
    making AI more trustworthy with a formal methods-based approach to AI system verification
     and validation.

[49] Alex K. Simpson. Lazy functional algorithms for exact real functionals. In Luboš Brim, Jozef
     Gruska, and Jiˇrí Zlatuška, editors, Mathematical Foundations of Computer Science 1998, pages
     456–464, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. ISBN 978-3-540-68532-6.

[50] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain
     for certifying neural networks. Proc. ACM Program. Lang., 3(POPL), January 2019.  doi:
     10.1145/3290354.

[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
     low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
     2013.

[52] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
     Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.
    van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
     R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, ˙Ilhan Polat, Yu Feng, Eric W.
     Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
     Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul
    van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific
     computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.

[53] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
     Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
     gradient descent.  In International Conference on Machine Learning, pages 35151–35174.
    PMLR, 2023.

[54] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
      Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. arXiv
      preprint, 2022. doi: 10.48550/arXiv.2211.00593.

[55] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6
     (60):3021, 2021. doi: 10.21105/joss.03021.

[56] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
    Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
     language models. arXiv preprint arXiv:2206.07682, 2022.

[57] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
     outer adversarial polytope. In International conference on machine learning, pages 5286–5295.
    PMLR, 2018.

[58] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
    deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):
    107–115, 2021.



                                       15

A  Subtleties of our approach

In this section, we address some subtleties and frequently asked questions about our approach.

A.1 Why study this simple task?

Formal reasoning is computationally expensive; very few large software projects have ever been
verified [31, 30], none of them comparable to large transformer models [10, 19]. Separately, there is
a high fixed cost to taking on any verification project, regardless of computational efficiency of the
verification itself. Thus, we picked the simplest setting to study the question of interest: Is it even
possible to formally reason more efficiently than by brute force about model behavior?

A.2  Scalability

In this section, we address concerns about the scalability of our approach.

A.2.1  Larger input spaces

We demonstrate that our proof strategies can be reused on larger input spaces while scaling better
than the brute force approach does.

We applied our proof strategies to models trained for Max-of-5, Max-of-10, and Max-of-20. While
running the brute force proof on Max-of-20 would require approximately 2148 FLOPs, which is about
270× the cost of training GPT-3 [32], our cubic proof achieves bounds of (94.1 ± 1.1) % (Max-of-5),
(91.4 ± 2.1) % (Max-of-10), and (88.4 ± 4.0) % (Max-of-20) in under two minutes. See Tables 2, 3,
4, and 5 for more detailed numbers, and Figures 6, and 7 for visualizations. These results demonstrate
that proof strategies can be reused on larger input spaces while scaling better than the brute force
approach does.

A.2.2  Different tasks

In this paper, we worked on highly optimizing our relaxation to make our bounds as tight as possible
when incorporating as little understanding as possible. This is not necessary for deriving proofs. Our
general formalization of mechanistic interpretability is replicable: (1) theorem statements are exact
expressions for the difference between the actual behavior of the model and the purported behavior,
and (2) proofs are computations that bound the expression. Furthermore, our convexity theorems and
proofs are applicable much more generally generally to element retrieval tasks.

A.2.3  More complicated architectures

We worked on a simple model studied in A Mathematical Framework for Transformer Circuits [13].
In follow-up work, we will extend this approach to proving bounds on 1L transformers with ReLU
MLP trained on modular addition.

A.2.4  Larger models

It is an open question whether or not the mechanistic interpretability approach to proofs can scale to
larger models. However, a large part of this question lies in the feasibility of deriving a high degree of
faithful mechanistic understanding from large models — that is, whether mechanistic interpretability
itself will scale. This is widely recognized in the field, and scaling interpretability approaches while
getting both a high degree of mechanistic understanding and assurances that said understanding is
faithful to the model is an active area of research. Broadly, we see the compact proofs approach
as a metric on the quality of mechanistic understanding — we are not purporting to have a general
solution to the problem of scaling interpretability, but instead claim that the challenges in proofs are
in fact challenges in understanding networks.





                                       16

Table 2: Version of Table 1 from Section 4.1 with nctx = 5, dvocab = 64. We report the proof complexity,
accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).
Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate
the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and
unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound
averaged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.

  Description         Complexity Cost              Bound                Est.     Unexplained
  of Proof                                                   FLOPs  Dimensions

  Brute force         O(vk+1kd)                      0.9990 ± 0.0018      254       236
  Cubic             O(v3k2)                        0.941 ± 0.011        226       214
  Sub-cubic          O(v2 · k2 + v2 · d)               0.705 ± 0.031        222       213
    w/o mean+diff                                     0.405 ± 0.073        222       213
  Low-rank QK       O(v2k2 + vd2 + v2d )           0.682 ± 0.033        222       212
   SVD only                   |{z}QK   EU&OV|{z}            0.335 ± 0.066        222       212
  Low-rank EU       O(v2k2 + vd + v2d )           0.649 ± 0.055        221       213
   SVD only                   |{z}EU   QK&OV|{z}             (4.8 ± 0.1) × 10−8    221       213
  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.628 ± 0.053        222       213
   SVD only                   |{z}QK    |{z}EU    |{z}OV       (4.8 ± 0.1) × 10−8    222       213
  Quadratic QK       O(v2k2 + vd + v2d )           0.354 ± 0.034        221       212
                                 |{z}QK   EU&OV|{z}
  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.335 ± 0.033        221       213
                              QK&EU|{z}   |{z}OV





Table 3: Version of Table 1 from Section 4.1 with nctx = 10, dvocab = 64. We report the proof complexity,
accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).
Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate
the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and
unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound
averaged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.

  Description         Complexity Cost              Bound                Est.     Unexplained
  of Proof                                                   FLOPs  Dimensions

  Brute force         O(vk+1kd)                      0.9988 ± 0.0013      286       266
  Cubic             O(v3k2)                        0.914 ± 0.021        228       214
  Sub-cubic          O(v2 · k2 + v2 · d)               0.674 ± 0.028        223       213
    w/o mean+diff                                     0.539 ± 0.061        223       213
  Low-rank QK       O(v2k2 + vd2 + v2d )           0.657 ± 0.028        223       212
   SVD only                   |{z}QK   EU&OV|{z}            0.469 ± 0.059        223       212
  Low-rank EU       O(v2k2 + vd + v2d )           0.639 ± 0.032        223       213
   SVD only                   |{z}EU   QK&OV|{z}            (0 ± 100) × 10−12    222       213
  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.625 ± 0.031        223       213
   SVD only                   |{z}QK    |{z}EU    |{z}OV       (2.9 ± 0.1) × 10−17   223       213
  Quadratic QK       O(v2k2 + vd + v2d )           0.392 ± 0.030        222       212
                                 |{z}QK   EU&OV|{z}
  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.390 ± 0.028        222       213
                              QK&EU|{z}   |{z}OV





                                       17

Table 4: Version of Table 1 from Section 4.1 with nctx = 10 and dvocab = 128. We report the proof complexity,
accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).
Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate
the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and
unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound
averaged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.

  Description         Complexity Cost              Bound                Est.     Unexplained
  of Proof                                                   FLOPs  Dimensions

  Brute force         O(vk+1kd)                      0.9972 ± 0.0031      296       277
  Cubic             O(v3k2)                        0.882 ± 0.012        231       216
  Sub-cubic          O(v2 · k2 + v2 · d)               0.622 ± 0.031        224       215
    w/o mean+diff                                     0.390 ± 0.070        224       215
  Low-rank QK       O(v2k2 + vd2 + v2d )           0.594 ± 0.035        224       214
   SVD only                   |{z}QK   EU&OV|{z}            0.320 ± 0.053        225       214
  Low-rank EU       O(v2k2 + vd + v2d )           0.607 ± 0.031        224       215
   SVD only                   |{z}EU   QK&OV|{z}             (5.4 ± 0.2) × 10−20   224       215
  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.595 ± 0.030        224       214
   SVD only                   |{z}QK    |{z}EU    |{z}OV       (5.4 ± 0.2) × 10−20   225       214
  Quadratic QK       O(v2k2 + vd + v2d )           0.350 ± 0.029        224       214
                                 |{z}QK   EU&OV|{z}
  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.384 ± 0.025        224       214
                              QK&EU|{z}   |{z}OV





Table 5: Version of Table 1 from Section 4.1 with nctx = 20, dvocab = 64. We report the proof complexity,
accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5).
Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate
the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and
unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound
averaged across all 151 models. For space reasons, we use k := nctx, d := dmodel, and v := dvocab.

  Description         Complexity Cost              Bound                Est.     Unexplained
  of Proof                                                   FLOPs  Dimensions

  Brute force         O(vk+1kd)                      0.995 ± 0.015         2148      2126
  Cubic             O(v3k2)                        0.884 ± 0.040        229       214
  Sub-cubic          O(v2 · k2 + v2 · d)               0.561 ± 0.043        224       213
    w/o mean+diff                                     0.486 ± 0.060        224       213
  Low-rank QK       O(v2k2 + vd2 + v2d )           0.547 ± 0.043        224       212
   SVD only                   |{z}QK   EU&OV|{z}            0.431 ± 0.060        224       212
  Low-rank EU       O(v2k2 + vd + v2d )           0.538 ± 0.043        224       213
   SVD only                   |{z}EU   QK&OV|{z}             (1.0 ± 6.0) × 10−4    224       213
  Low-rank QK&EU   O(v2k2 + vd2 + vd + v2d )     0.526 ± 0.041        224       213
   SVD only                   |{z}QK    |{z}EU    |{z}OV       (1.0 ± 5.0) × 10−4    224       213
  Quadratic QK       O(v2k2 + vd + v2d )           0.322 ± 0.035        224       212
                                 |{z}QK   EU&OV|{z}
  Quadratic QK&EU   O(v2k2 + vd + v2d )           0.321 ± 0.035        224       213
                              QK&EU|{z}   |{z}OV





                                       18

   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9990 ± 0.0018)
     0.8                                                                                        cubic (rel acc: 0.942 ± 0.010)
     0.6                                                                                      subcubic (rel acc: 0.702 ± 0.033)
                                                                                                         direct-quadratic (rel acc:                                                                                                                   0.649 ± 0.055)    Accuracy  0.4
                                                                                                                               2
                                                                                                     attention-dvocabdmodel  (rel acc: 0.682 ± 0.033)
     0.2
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.629 ± 0.053)
     0.0                                                                                                attention-quadratic (rel acc: 0.354 ± 0.034)     Normalized                               231                        243                        255     attention-quadratic, direct-quadratic (rel acc: 0.336 ± 0.033)
                         FLOPs to Verify Proof (approximate)

                                           (a) nctx = 5, dvocab = 64
   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9988 ± 0.0013)
     0.8                                                                                        cubic (rel acc: 0.915 ± 0.021)
     0.6                                                                                      subcubic (rel acc: 0.673 ± 0.030)
                                                                                                         direct-quadratic (rel acc:                                                                                                                   0.640 ± 0.032)    Accuracy  0.4
                                                                                                                               2
                                                                                                     attention-dvocabdmodel  (rel acc: 0.658 ± 0.028)
     0.2
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.626 ± 0.031)
     0.0                                                                                                attention-quadratic (rel acc: 0.393 ± 0.030)     Normalized                               242                        265                        288
                         FLOPs to Verify Proof (approximate)

                                        (b) nctx = 10, dvocab = 64
   Bound  1.0                                                                                             brute force (estimated) (acc: 0.9972 ± 0.0031)
     0.8                                                                                        cubic (rel acc: 0.885 ± 0.012)
     0.6                                                                                                 direct-quadratic (rel acc: 0.609 ± 0.030)
                                                                                              subcubic (rel acc: 0.613 ± 0.037)    Accuracy  0.4
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.596 ± 0.030)
     0.2                                                                                                                       2
                                                                                                     attention-dvocabdmodel  (rel acc: 0.595 ± 0.035)
     0.0                                                                                                  attention-quadratic, direct-quadratic (rel acc: 0.385 ± 0.025)     Normalized                                247                         274
                         FLOPs to Verify Proof (approximate)

                                          (c) nctx = 10 and dvocab = 128
   Bound  1.0                                                                                             brute force (estimated) (acc: 0.995 ± 0.015)
     0.8                                                                                        cubic (rel acc: 0.888 ± 0.036)
     0.6                                                                                      subcubic (rel acc: 0.563 ± 0.041)
                                                                                                                               2 (rel acc: 0.549 ± 0.041)    Accuracy  0.4                                                                                             attention-dvocabdmodel
                                                                                                         direct-quadratic (rel acc: 0.540 ± 0.040)
     0.2
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.528 ± 0.039)
     0.0                                                                                                  attention-quadratic, direct-quadratic (rel acc: 0.322 ± 0.034)     Normalized                                263                         2109                                 attention-quadratic (rel acc: 0.323 ± 0.035)
                         FLOPs to Verify Proof (approximate)

                                        (d) nctx = 20, dvocab = 64

Figure 6: Version of Figure 3 from page 8 with varying nctx and dvocab. The brute-force proof (Section 4.1)
computes the exact performance uses orders of magnitude more compute than other approaches; unlike in
Figure 3, here we use importance sampling to estimate the bound.





                                       19

     1.0
 Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct
                                                                                           mean+max-diff-subproduct
     0.6                                                                                        max-diff
                                                                                       mean+max-diff  Accuracy
     0.4                                                                                   svd
                                                                                              mean+max-diff-subproduct-recursive
     0.2                                                                                           max-diff-subproduct-recursive
                                                                                                mean-recursive+max-diff-subproduct-recursive   Normalized
     0.0
                 400           600           800          1000          1200
                        EPQKE Singular Ratio: σ1/σ2

(a) nctx = 5, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 15(σ1/σ2) + 0.48,
R2 = 0.37.
     1.0
 Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct
                                                                                           mean+max-diff-subproduct
     0.6                                                                                        max-diff
                                                                                       mean+max-diff  Accuracy
     0.4                                                                                   svd
                                                                                                  max-diff-subproduct-recursive
     0.2                                                                                       mean+max-diff-subproduct-recursive
                                                                                                mean-recursive+max-diff-subproduct-recursive   Normalized
     0.0
         400       600       800      1000      1200      1400      1600      1800
                        EPQKE Singular Ratio: σ1/σ2

(b) nctx = 10, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 074(σ1/σ2)+0.51,
R2 = 0.23.
     1.0
 Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct
                                                                                           mean+max-diff-subproduct
     0.6                                                                                        max-diff
                                                                                       mean+max-diff  Accuracy
     0.4                                                                                   svd
                                                                                                  max-diff-subproduct-recursive
     0.2                                                                                         mean-recursive+max-diff-subproduct-recursive
                                                                                              mean+max-diff-subproduct-recursive   Normalized
     0.0
             800      1000      1200      1400      1600      1800      2000
                        EPQKE Singular Ratio: σ1/σ2

(c) nctx = 10 and dvocab = 128. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 085(σ1/σ2) +
0.40, R2 = 0.42.
     1.0
 Bound  0.8                                                                                           max-diff-exactmax-diff-subproduct
                                                                                           mean+max-diff-subproduct
     0.6                                                                                mean+max-diff
                                                                                               max-diff  Accuracy
     0.4                                                                                   svd
                                                                                                mean-recursive+max-diff-subproduct-recursive
     0.2                                                                                       mean+max-diff-subproduct-recursive
                                                                                                  max-diff-subproduct-recursive   Normalized
     0.0
              400         600         800        1000        1200        1400
                        EPQKE Singular Ratio: σ1/σ2

(d) nctx = 20, dvocab = 64. The “svd” proof strategy best-fit line has equation b/¯s = 0.000 098(σ1/σ2)+0.41,
R2 = 0.22.

Figure 7: Version of Figure 4 from page 9 with varying nctx and dvocab. Note that the “svd” proof strategy has
a clear upward trend, especially on early points.





                                       20

A.3 Why is more mechanistic understanding correlated with worse bounds?

Figure 3 exhibits Simpson’s Paradox: although more faithful mechanistic understanding is correlated
with better bounds within each class of proof (and moreover the most extensive mechanistic under-
standing results in the greatest improvement in bound tightness over baseline), when we aggregate
across all proof strategies, we find that more mechanistic understanding is correlated with worse
bounds.

This relationship is summarized in Figure 8.

From the compression perspective, more mechanistic understanding is about having more compres-
sion. Unless the model is losselessly compressible, we should expect that more compression will
inherently be more lossy, no matter how good our compression scheme is. Correspondingly, using
more understanding to get more compression will often result in a weaker bound, no matter how
good our understanding is.

Conversely, we can think of the quality of proofs (the combination of tightness of bound, and length of
proof) as a metric for how good our mechanistic understanding is. From this perspective, the fact that
mechanistic-interpretability-derived bounds are bad suggests gaps in our mechanistic understanding.
As the field matures and we develop tools that enable more faithful and complete understanding of
model behavior, we expect that the quality of bounds we derive from mechanistic understanding will
improve.


A.4  Convex relaxation

In this work, we construct convex relaxations to perform the pessimal ablations for our proofs.

In what sense are we using “convexity”? The intuition is that we are attempting to optimize a function
f over its domain X by incrementally making local changes to the input sequence, such as replacing
one token by another, or by changing the order of tokens. The reason that convex optimization
problems are easy to solve is that all local extrema are global extrema. This is not the case for our
optimization problem, so we find a relaxation of f and its domain such that all local extrema are in
fact global extrema.

Furthermore, most convex optimizers perform optimization at runtime by repeatedly stepping towards
extrema. In this work, we “optimize by hand”, performing the optimization in the proof of our general
theorems. The computation of the bound then only needs to instantiate the precomputed possible
extrema with the actual values of the model’s parameters to determine the the extrema actually are.

We now give a formal description of what we mean by “convex relaxation”.
For a set of inputs Xi, we define a set of “relaxed inputs” Xrelaxedi      with an injection Ti  : Xi ,→
P(Xrelaxedi      ) mapping input to the model to the set of corresponding relaxed inputs. On the relaxed
input, we define a function hi : Xrelaxedi   →R such that for all t ∈Xi and all labels l for which (l, t)
is supported by (has non-zero probability in) D, we can find trelaxed ∈Ti(t) with f(l, M(t)) ≥
hi(trelaxed). We proceed by finding a small subset of “boundary” examples Bi ⊂Xrelaxedi         , proving
that if hi(trelaxed) ≥bi for all trelaxed ∈Bi then hi(trelaxed) ≥bi for all trelaxed ∈Xrelaxedi         .
Then, the computational component C of the proof validates that that hi(trelaxed) ≥bi for some bi
for all trelaxed ∈Xrelaxedi         . This allows us to conclude that f(l, M(t)) ≥bi for all t ∈Xi.


A.5  Computing unexplained dimensionality

We claim in Figure 5 that we can use unexplained dimensionality as a metric for understanding. Here
we describe how we compute the unexplained dimensionality of a proof strategy.

As in Figure 1, for any given proof, we can separate our treatment of transformer components into
“black-box” (e.g., matrix multiplication) and “white-box” components (e.g., specifying that the QK
circuit is approximately rank one; pessimizing over non-max tokens). Considering the performance
score as a large white-box component which may reference black-boxes internally, we define the
unexplained dimensionality of a single black-box computation as the log-cardinality of it function
space (so, e.g, 2 · 64 for a function 64 →R2, whose cardinality is (R2)64, where 64 denotes the finite


                                       21

                                                                                   Brute Force Proof

                                  True Performance



               Performance                                                                          Impractical baseline of
              Lower Bound       Trivial Proof                           guaranteeing
                                                                    performance via
                                                                             inference

                                         FLOPs to Verify Proof


                             (a) The baseline of using inference to generate proofs.



                                   True Performance


                                                     Understanding
                                                               partially recovers
                Performance                   bound tightness
              Lower Bound

                                                             Decreased length of proof
                                                                          leads to looser bounds



                                         FLOPs to Verify Proof

             (b) Shorter proofs by default have a worse performance bound b. Faithful under-
             standing allows us to recover significant — but not complete — bound tightness
            with minimal proof-length overhead.

             Figure 8: The theoretical relationship between proof length and bound tightness.


set on 64 elements). The unexplained dimensionality of the entire proof is the sum of the unexplained
dimensions of all black-box components.

Intuitively speaking, unexplained dimensionality tries to capture the degrees of freedom that we
have to check via brute enumeration over black-box computations. Proofs with less unexplained
dimensionality contain more mechanistic understanding, and vice versa.

A.6  Computing approximate FLOPs

In Figure 3 and Table 1 on page 6 and on page 8, we display approximate floating point operations.
We instrument our code to execute on phantom tensors that track their shape and accumulate an
approximate count of floating point operations. We compute matrix additions and multiplications in
the obvious way. We take the instruction count of SVD to be the cost of verifying that the output of
SVD is a valid decomposition: that we have a pair of orthonormal bases which when multiplied out
give the original basis.

A.7  IEEE 754 vs. R

In Section 2 we defined C and Q and glossed over whether we were reasoning over reals or floats.
Here we clarify this point that we’ve so far been sweeping under the rug.
Let F denote the set of the relevant flavor of IEEE 754 Floating Point numbers (generally 32-bit for
our concrete models, but everything would hold just as well for 64-bit). Let F∗denote F restricted to
finite numbers (that is, without NaNs and without ±∞).
We parameterize C, M, and D over the real field13 they operate on, so that,  e.g., CF   :
model weights →F.  Then we have Q establishing that for any model M′, CR(M′R) ≤
E(l,t)∼DRfR(l, M′R(t)), and we have a trace demonstrating that CF(MF) = b.

   13Technically the floating point numbers are not a field. We gloss over this point here, since they define all of
the field operations, even if those operations do not satisfy the field axioms.


                                       22

Let i : F∗→R be any injection that maps each floating point number to some real number that it is
“closest to”. Supposing that b ∈F∗and thus b ∈R, we need two additional components of the proof.
We need to find ε, ε′ ∈R+ prove that
|CR(MR) −i (CF(MF))| < ε    and       E(l,t)∼DRfR(l, MR(t)) −i  E(l,t)∼DFfF(l, MF(t)) < ε′

Then we can chain these proofs to prove that
                                   i  E(l,t)∼DFfF(l, MF(t)) ≥b −ε −ε′

Such ε-ball robustness proofs should be well within the scope of existing approaches to formal
methods on neural nets, see, e.g., [44, 3, 4, 29, 1, 57]. We leave actually dealing with the gap between
floating point numbers and real numbers to future work.

A.8  Non-uniform distributions

In Equation 1 in Section 2 we defined the expected model performance as the expectation of the
distribution D:
                                                  ¯s := E(l,t)∼D [f(l, M(t))] ≥b.

We then immediately specialized to the case where the marginalization D|X of D over labels is
uniform. As we’ll see in Theorem 1 in Appendix D and Algorithm 3 in Appendix E, the bound
computation is modularized between a function that bounds the performance f(l, M(t)) over a
restricted collection of inputs, and a much simpler function that combines the bounds on individual
cases into a bound on the expectation over the entire distribution. The per-input bound computation
is CORRECTNESS in Algorithm 1 and RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION in
Algorithm 3; the expectation computation is BRUTE-FORCE in Algorithm 1 and CUBIC in Algorithm 3.

Since the expectation computation is modularized, it is straightforward to extend our approach to
non-uniform distributions simply by adjusting the weighting of each region of inputs. However, if the
distribution is too far off from the uniform training distribution, the bound we get may not be very
good, as we may not be allocating adequate computation to the high-probability regions of the input
space.

A.9  Adversarial robustness via flexibility in D

There is flexibility inherent in Equation 1. Normally, by out-of-distribution (OOD) or adversarial
inputs, we suppose that there is a distribution Din that’s used for training and (in-distribution)
validation, and another distribution D′ that is the deployment distribution or generated by an adversary.
If we had knowledge of D′, we could compute the expected performance from inputs sampled from
D′. Even if we don’t have exact knowledge of D′, we can still define a very broad distribution D that
covers possible D′s.
In this work, D is the distribution of all 644 possible valid input sequences. In addition, as our proofs
partition D into subdistributions, and bound the performance on each subdistribution, we can bound
the model’s performance on any possible distribution over valid input sequences.

A.10   Infinite distributions

In the brute force proof in Section 4.1, we run the model on the entirety of D. This operation
is straightforward when X is finite. Perhaps surprisingly, we can do this even if X is infinite
as long as the PDF L × X →R of D is computable and the natural computational topology of
X is compact [16, 15, 14], because integration of computable functions on computable reals is
computable [49].

A.11  Using alternate loss functions

Building on the point from Appendix A.8, it is also relatively straightforward to extend our approach
from bounding expected accuracy to bounding log-loss. We will see in Figure 12 that the accuracy
and log-loss share a subterm ∆ℓi. Since we compute this subterm in all of our algorithms, we can


                                       23

easily extend our approach to log-loss by combining ∆ℓi directly rather than merely checking that
the value is negative as we currently do in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION
in Algorithm 3. Although this is sufficient for the brute-force and cubic proofs, for the subcubic
proof using Algorithm 6 in Appendix F, we would additionally have to compute a log-loss bound for
the sequences where the largest non-max token is “too close” to the max token, which we currently
neglect by considering the model to get them wrong in the worst case.

A.12  Proving upper bounds

In this work, we focus on proving lower bounds on model performance. Most of our theorems,
for example in Appendices E, and F, prove two-sided bounds. Most of the other theorems can be
straightforwardly adapted to proving upper bounds by swapping uses of min and max. Therefore,
we expect that proving upper bounds on model performance should be straightforward.

A.13  What proof system?

Length of proof depends on what proof system we use. We permit any proof system where proof-
checking time is linear in the length of the proof. This excludes dependently typed proof systems
such as Martin-Löf type theory, but such proof systems can easily be accommodated by considering
a proof-checking-trace rather than the proof object itself. Alternatively, a more conventional proof
system like ZF, ZFC, or the proof system underlying Isabelle/HOL should suffice.

B  Experimental details

B.1  Training details

To train each model, we generate 384,000 random sequences of 4 integers picked uniformly at random,
corresponding to less than 2.5% of the input distribution. We use AdamW with batch_size = 128,
lr = 0.001, betas = (0.9, 0.999), weight_decay left at the default 0.01. We train for 1 epoch (3000
steps). Over our 151 seeds, models trained with this procedure achieve (99.92 ± 0.15) % train
accuracy and a loss of (4 ± 8) × 10−3.14 When qualitatively examining a single model (for example
in Section 3.1 or Appendix H.1), we use the model with config seed 123, model seed15 613947648.

As our models as sufficiently small, we did not have to use any GPUs to accelerate training our
inference. Each training run takes less than a single CPU-hour to complete. In total, the experiments
in this paper took less than 1000 CPU-hours.

We use the following software packages in our work: Paszke et al. [41], Plotly Technologies Inc.
[42], Nanda and Bloom [37], Rogozhnikov [47], Virtanen et al. [52], McKinney [33], Waskom [55]

B.2  Additional details supporting our mechanistic interpretation of the model

We provide heatmaps of the matrices corresponding to the five components described/defined in
Section 3, for the mainline model.





  14Numbers reported as mean across training runs ± std dev across training runs of mean accuracy and loss.
  15The model seed is deterministically pseudorandomly derived from the seed 123.


                                       24

                                                                         0

                0

                                                                                200                                                                                                                 1
                                                                                                                                                 20
                                20                                                                                                                                                                                                0
                                                                                100                                                                                                                                                            token                                                                         token
                                                                         query 40                                             0                                                                                                                                                                 query 40                                    −1
                                                                      −100                                                                                       −2

                                60                                        −200                                                                60                                    −3
                                   0     10    20    30    40    50    60                                                                                  0         1         2         3
                                                      key token                                                                                                                key position

              (a) EQKE = EqQKT ¯ET                             (b) EQKP = EqQKT ˆPT

Figure 9: The QK circuit can be decomposed into the position-independent and position-dependent components
EQKE and EQKP. It computes the pre-softmax attention score for the model. The positional contribution to
the attention score, as shown in Figure (b), is minimal. In Figure (a), the gradient from left to right along the
key axis indicates that the single attention head pays more attention to larger tokens. The uniformity along the
query axis suggests that this behavior is largely independent of the query token. Further, the light and dark bands
imply that some queries are better than others at focusing more on larger tokens.





  0                                                                                                                                                               0.8                                                                                    0                                             2
                                                   30                 0
                                                                                                                                                               0.6
                                                   20                                                                                                                                                                                                                       1                                                                                                                                                               0.4
                                                                                                                                                                        20    20
                                                   10                 1
                                                                                                                                                               0.2
                                                                                                                                                                                                                       0  token                                                0                                                                                                                                  position                                                         0.0                                                                            token
  input 40                                      −10                                                                           input 2                                            −0.2                                                                        input 40                                    −1

                                           −20                                                                                                                                    −0.4
                                                                                                                                                                         −2                                           −30                 3    60                                                                                                                                    −0.6                                60

      0     10    20    30    40    50    60                                              0     10    20    30    40    50    60                                             0     10    20    30    40    50    60
                       output logit token                                                                                  output logit token                                                                                 output logit token
   (a) EVOU = ¯EV OU                   (b) PVOU = ˆPV OU                    (c) Direct Path = EqU

Figure 10: The OV circuit is a sum of EVOU and PVOU. In Figure (a) we see that EVOU “copies” — with
the exception of input tokens ≤5 (6.6 ± 1.2 across all models) — by virtue of the fact that above 5, the diagonal
is larger than all the other elements in the same row. We see that the range on Figure (b) is much smaller than
Figure (a), indicating that positional contribution to the copying is minimal. In Figure (c) we see that direct path
values matter a bit more than PVOU, being only ≈20× smaller than the typical EVOU difference. They don’t
matter that much, though, being so small. Additionally, the vertical banding indicates that the primary effect of
this is a largely-query-independent bias towards larger numbers, reflecting the fact that the input distribution is
biased towards larger numbers being the maximum. The weak diagonal pattern indicates a slight bias towards
upweighting the query token itself as a (possible) maximum token.


B.3  Distribution of model mechanisms

We provide some analysis of the distribution of the mechanisms of the models trained on the same
configuration. At a glance, there is not that much variation across models.

The statistics of interest are: (1) σ1/σ2, the ratio of the first two singular values of EQKE, a measure
of the extent to which the attention score computation is low-rank; (2) ¯s, the average score (accuracy)
of the model across the entire input distribution; (3) bcubic/¯s, the percent-score-recovered accuracy
bound achieved by the cubic proof from Section 4.2; (4) bsubcubic/¯s, the percent-score-recovered
accuracy bound achieved by the (per-model best)16 subcubic proof from Section 4.3.

For each statistic of interest, Table 6 presents an eleven-number summary of the statistic. Plots, seeds,
and statistic values are shown for models whose values are closest to each of the corresponding
summary statistics.17 Additionally, each group contains a boxplot of the summary:

        • the minimum, maximum; the first and third quartiles; the median and mean; percentiles
        2.15 %, 97.85 %, 8.87 %, and 91.13 %; these are displayed as:
        • top and bottom of the vertical whisker lines; top and bottom of the box; horizontal line
         inside the box, and the square; horizontal whisker lines and whisker crosshatches.




   16“Per-model best” here means that for each model seed, we select the variant of the subcubic proof with the
highest bound.
   17If a single model is the closest to two statistics, for example when the mean and median are very similar, the
model is shown only once.


                                       25

Table 6: Plots of various models. The statistics of interest are: (1) σ1/σ2, the ratio of the first two singular values
of EQKE, a measure of the extent to which the attention score computation is low-rank; (2) ¯s, the average score
(accuracy) of the model across the entire input distribution; (3) bcubic/¯s, the percent-score-recovered accuracy
bound achieved by the cubic proof from Section 4.2; and (4) bsubcubic/¯s, the percent-score-recovered accuracy
bound achieved by the (per-model best) subcubic proof from Section 4.3. The y axes are: for EQKE and
EQKP, the query token; for EVOU, the input token; for EqU, the input query token; for PVOU, the input
position. The x axes are: for EQKE, the key token; for EQKP, the key position; for EVOU, PVOU, and
EqU, the output logit token. All token axes range from 0 at the top (or left) to dvocab −1 = 63 at the bottom
(or right). All position axes range from 0 at the top (or left) to nctx −1 = 3 at the bottom (or right).

               statistic of interest    EQKE     EQKP    EVOU  PVOU
  seed                                                      EqU
           distribution    value  EqQKT ¯ET   EqQKT ˆPT    ¯EV OU   ˆPV OU




                                                          −400       −200         0          200         400       −40        −20          0           20           40

  123        (selected seed)


 24262      σ1/σ2    1061.1

 13654           1,000         626.5


                           800
  4810             600         350.0

  6204             400         333.4


 12457              ¯s      0.9999

                            1

 19451          0.998         0.9992



                           0.996
 15662               0.9932
                           0.994

 16197          0.992         0.9915


 32103       bcubic/¯s    0.971


                              0.97
  6082             0.96          0.954


                              0.95
  6155             0.94          0.926


                              0.93
  2306             0.92          0.922


 20415     bsubcubic/¯s   0.80


                                0.8
 29725                 0.70
                              0.75

 20976              0.7          0.63


                              0.65
  6155               0.6          0.61





                                       26

C  Mathematical definitions

We provide a detailed breakdown of the mathematical notation used in the appendix.

Let                     M         of type X →Y be the model18; sometimes we
 n       be the finite set on n elements; we write                             write
 N<n    := {0, 1, . . . , n −1} when we care about the ele-   ℓ            for the logits of the model M(t)
           ments of n                      D         be a probability distribution over input-label pairs
 σ(v)    be the softmax function ev/ Pi evi                                     (t, l) ∈X × L
 σ∗(v)   be   the   casually-masked  softmax   function,   D|X       be D marginalized to a distribution over t ∈X
            σ∗(v)i := evi/ Pj≤i evj                      f          of type L × Y →R be a scoring function for
  dvocab   be the size of the vocabulary                                        evaluating the performance of the model
 d       be the dimension of the attention head, in our   Pavg      be the average position embeds across positions
              case, equal to the hidden dimension of the model                   (of size dmodel), nctx1 Pi Pi
            dmodel (assumption: d < dvocab)                    ¯P         be either 1nctx ⊗Pavg or 1dvocab ⊗Pavg depending
  nctx    be the context length, the number of tokens in the               on context – that is the result of broadcasting
             input sequence, equal to K in Max-of-K                       Pavg back into the shape of P or E (that is,
 P       be the nctx × dmodel positional embedding                         nctx × dmodel or dvocab × dmodel)
 E      be the dvocab × dmodel token embed             Pq        be 1dvocab ⊗Pquery, the broadcasting of Pquery
 Q, K,   be the dmodel ×dmodel query, key, value, and output     ˆP, ¯E, Eq   be P −¯P, E + ¯P, and E + Pq respectively
  V , O     matrices of the attention head
 U       be the dmodel × dvocab unembed matrix            For any vector-valued function v of length dvocab, parameter-
 t       be the input token sequence [t0, t1, . . . , tnctx−1]          ized over the input sequence t, let
 x       be the nctx × dvocab one-hot-encoded input token  ∆vi := vi−vtmax be the difference between the ith element of v
            sequence [x0, x1, . . . , xnctx−1]                     and the element of v corresponding to the true maximum
  xquery  := x−1 := xnctx−1 be the query token                   of the input sequence tmax.
  tquery   := t−1 := tnctx−1 be the one-hot encoded query  For our particular model, we will have
            token                                                dvocab := 64    d = dmodel := 32     nctx := 4
 tmax    be the true maximum token in the input sequence,   X := (N<dvocab)nctx ∼= 644   L := N<dvocab ∼= 64
          maxi ti                              D|X := U(0, 1, . . . , dvocab −1)nctx, the uniform distribution
                                          Figure 11: Preliminary model definitions

                                  E(l,t)∼D [f(l, M(t))] ≥  b      (theorem statement)                                  (3)
                                                      |{z}bound                                |   ¯s (average{zmodel score) }  lower

We define the two typical performance functions corresponding to accuracy and log-loss. Note the shared subterm ∆ℓi.
                       f accuracy(tmax, ℓ) := 1[argmax ℓi = tmax] = 1[0 > max ℓi −ℓtmax ]                          (4)
                                                                              i                            i̸=tmax
                                                                              |  ∆ℓi{z  }
                        f log-loss(tmax, ℓ) := (σ(ℓ))tmax = log(Pi exp(ℓi −ℓtmax ))                                    (5)
                                                                    |  ∆ℓi{z  }

We present the model definition in four different regroupings to define via underbrace labels various useful quantities:
                                  √
     M(t) = ℓ(t) = σ∗  (xqueryE + P query) QKT (xE + P)T /  d   · (xE + P) V OU + (xqueryE + P query) U     (6)
                          |            QK {zcircuit             }       |   OV {zcircuit   }  |          direct{zpath      }
                                  √
           = σ∗  xquery EqQKT ¯ET xT + EqQKT ˆPT  /  d   · x ¯EV OU + ˆPV OU  +xquery EqU        (7)
                                 | EQKE{z   }      |  EQKP{z    }            |EVOU{z }  |PVOU{z  }         |{z}EU
                                                                     |     EPVOU(t){z         }                      |                       α∗(t){z                     }
           = α∗(t) · x¯EV OU + α∗(t) · ˆPV OU + xqueryEqU                                                    (8)
                      |   ℓEVOU(t){z      }  |  ℓPVOU(t){z     }  |ℓEU(xquery){z   }
           =  Pnctx−1i=0   (α∗(t))ixi¯EV OU + (α∗(t))i ˆPiV OU + xqueryEqU                                  (9)
                                |    ℓEVOU,i(t){z      }  |   ℓPVOU,i(t){z      }    |ℓEU(xquery){z   }
                                |                    ℓi(t){z               }


                                         Figure 12: Definitions of the model behavior





                                                  27

D  Brute-force proof

Theorem 1. For BRUTE-FORCE(dvocab, nctx, M) as defined in Algorithm 1,

     Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥BRUTE-FORCE(dvocab, nctx, M)
                                                i                         i


Proof. In fact the two sides of the inequality are equal by definition. Hence the inequality follows by
reflexivity of ≥.



Algorithm 1 Counting Correct Sequences By Brute Force
  1: function CORRECTNESS(M, input-sequence)
  2:    return MODEL-BEHAVIOR(M, input-sequence) == MAX(input-sequence)
  3: end function
  4: function BRUTE-FORCE(dvocab, nctx, M)
  5:    return                    1   SUM(CORRECTNESS(M, tokens) for tokens ∈(RANGE(dvocab))nctx)                  dvocabnctx
  6: end function


E  Details of cubic proof

In this section, we prove formally the result used in Section 4.2, A cubic proof.
At its heart, the convexity of softmax19 is an extension to a simple idea: a weighted average of scalar
values is extremized by putting 100% of the weight on an extremal value.

Using this simple version of the theorem, however, gives a useless bound of 0% accuracy: if we pay
no attention to the maximum of the sequence, of course we’re going to get the wrong answer. Since
in fact the space of possible weightings we may see in practice is much smaller (finite, in fact, with
at most dvocabnctx values), we may look for a more general version of this idea that gives us tighter
bounds that still cover the space of possible weightings.

The weights are not linearly independently choosable (softmax is non-linear), so extremal values do
not necessarily result from putting maximal attention on the worst token. It may be, when trying to
find the worst case, that some positions are so dis-preferred that it makes more sense to choose a
token that is “less bad” for those positions, if it draws enough attention away from the correct token.
See Lemma 3 for details.

We thus spend this section characterizing a relaxation of the constraints on weights:

      1. that contains all actually possible weightings,

      2. that is extremized at weights that still correspond to some notion of “put the most weight on
         the extremal tokens”, and

      3. for which computing the extremal weightings is computationally efficient.

Before diving in, let’s recall the proof that a weighted average of scalar values is extremized by
putting 100% of the weight on extremal values:
Theorem 2 (Warmup: Extremizing weighted averages). Fix a set of values vi ∈R. The weighted
average is bounded by the extremal values: for any wi such that Pi wi = 1 and 0 ≤wi ≤1,

                         min vi ≤ X wivi ≤max vi
                                                    i                              i
                                                                  i

   18Note that while in the main body, M(t) referred to the pre-softmax output logits, in the appendix we abuse
notation and occasionally use it to refer to maximum token indicated by the logits where appropriate.
   19See Appendix A.4 for the reason that we call this “convexity”. Note that our use of “convexity” is purely
descriptive in this section; all theorems are written out explicitly.


                                       28

Proof. The proof is simple. We have

       X wivi −min vi = X wi(vi −min vj) ≥0
                                                        i                          j
                                       i                              i

and
                 max vi − X wivi = X wi(max vj −vi) ≥0
                                       i                                   j
                                                     i                i
so the result follows.

E.1  Proof strategy

The model computes the true maximum tmax when its outputs logits ℓare such that ∆ℓt∗:=
 ℓt∗−ℓtmax < 0 for all t∗̸= tmax.20 As a result, it suffices to lower-bound the proportion of
sequences where (an upper bound on) ∆ℓt∗is negative for all t∗̸= tmax. In particular, we will
upper-bound the contribution from incorrect tokens t in positions i to the difference ∆ℓi between
incorrect (t∗) and correct (tmax) output tokens ∆ℓit∗= ℓit∗−ℓitmax.
We do this by arguing that the logit difference ∆ℓt∗satisfies a certain notion of convexity over the
space of a relaxation of sequences (Theorem 6), and constructing a set of Θ(dvocab3nctx) “extremal”
relaxed sequences where the position and token embedding components of attention are pessimized
independently.

We start by first rewriting the contribution of each token through the attention head to the logit
difference into the contributions involving PVOU and EVOU:


                     ∆ℓtt∗(t) = ∆ℓPVOU,i t∗(t) + ∆ℓEVOU,i t∗(t)


We then upper bound ∆ℓPVOU,it∗(t) by noting that because the softmax attention is a weighted
average of PVOU,


               ∆ℓPVOU,i t∗(t) = ℓPVOU,i(t)t∗−ℓPVOU,i(t)maxj tj
                  = α∗i (t)PVOUi,t∗−α∗i (t)PVOUi,maxj tj
                  = α∗i (t) PVOUi,t∗−PVOUi,maxj tj
                    ≤α∗i (t) max PVOUi,t∗−PVOUi,maxj tj
                                                                 i
Since Pi α∗i (t) = 1, we have

                  nctx−1
      X ∆ℓPVOU,i t∗(t) ≤max PVOUi,t∗−PVOUi,maxj tj
                                                                 i
                   i=0

We then construct a set Ξpure of “pure sequences” consisting of only three types of tokens in one of
two orders, and show that for each input sequence t and readoff logit t∗, we bound the logit difference
from the token embeddings ∆ℓEVOU,it∗(t) using a small subset X of Ξpure:

                       nctx−1                       nctx−1
       X ∆ℓEVOU,i t∗(t) ≤max X ∆ℓEVOU,i t∗(ξ)
                                       ξ∈X
                       i=0                         i=0

We construct a set Xrelaxed of relaxed sequences, where each relaxed sequence trelaxed consists of a
sequence and a position (t, i), where ∆ℓt∗(t, i) is evaluated by separately considering the positional

  20We use the logit difference ∆ℓt∗because: (a) it is shared in the computation of f 0-1 and f log-loss; (b) it is a
linear function of the various paths through the model, which can therefore be analyzed separately; (c) it leaves
open both the options of pessimizing over output logit before or after combining contributions of various paths
through the model.


                                       29

contribution through attention (that is, the attention weighted PVOU) and the token contribution
(that is, the attention-weighted EVOU) and direct contribution (the logit difference through the skip
connection EU). Note that i indicates the position that we pay 100% of the attention to for the PVOU
contribution.

We argue that ∆ℓt∗(t, i) satisfies a certain notion of convexity over mixtures of sequences, such
that we can evaluate it only on a set of Θ(dvocab3nctx) “extremal” sequences in a way that takes
O(dvocab3nctx) total time to bound ∆ℓt∗(t, i) for every possible input sequence. We then use the
extremal sequences that the model gets correct to lower bound the proportion of all sequences that
the model will get correct. Specifically, we argue that Algorithm 3 provides a valid lower bound on
the proportion of sequences the model gets correct.

E.2  Proof outline

We now proceed to the main results of this section.
Math fact: For each token t∗, the logit difference ∆ℓt∗for any sequence t can be decomposed into
the direct contribution from the embeds ℓEU, the attention-weighted position contribution (PVOU),
and the attention-weighted token contribution (EVOU). Therefore, it suffices to upper bound each of
the three components independently, since summing these upper bounds gives a valid upper bound
on the logit difference.
We can compute the direct contribution ℓEU exactly by first computing EU = EqU and then, for each
max, subtracting the logit of the max token from each row of the matrix. No theorems needed. For
each max token, we can bound the position contribution by its maximum over positions (Theorem 6).

In order to upper bound the token contribution, we argue that any mixed sequence will be upper
bounded by the maximum of the corresponding pure sequences (Theorem 7). We then argue that for
pure sequences, it suffices to consider orderings where same tokens appear contiguously (Theorem 4).

E.3  Formal proof

For this subsection, all theorems are parameterized over the following quantities.
Definition 1 (Common theorem parameters). Fix a token value function (à la a row difference in
EVOU) v  : N<dvocab →R and a token attention function (à la EQKE for a fixed query token)
a : N<dvocab →R. Fix a position value function (à la a row difference in PVOU) w : N<nctx →R
and a position attention function (à la EQKP for a fixed query token) b : N<nctx →R.

In practice, we’ll take, for fixed query token tquery, fixed output token of interest t∗, and fixed
maximum token tmax,
                                       √
               vt = EVOUt,t∗−EVOUt,tmax              at = EQKEtquery,t/  d
                                       √
            wi = PVOUi,t∗−PVOUi,tmax                 bi = EQKPtquery,i/  d

Definition 2 (of a sequence via sorted tokens and a position permutation). We can define a sequence
of tokens via sorted tokens and a position permutation by specifying a non-decreasing sequence of
tokens t0 ≤· · · ≤tnctx−1 ∈N<dvocab paired with a permutation σ : N<nctx →N<nctx.
Definition 3 (sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈
N<dvocab and a permutation σ : N<nctx →N<nctx define the sequence score st0,...,tnctx−1,σ as:

                  ,
                           st0,...,tnctx−1,σ := X vtieati+bσ(i) X eati+bσ(i)
                                     0≤i<nctx            0≤i<nctx

We will drop the token subscript, writing only sσ, when the token values are unambiguous by context.

The sequence score here will be computing ∆ℓEVOUt∗for some fixed t∗and tmax. The way we’ve
set up our definitions, high scores predict t∗(and are thus bad), negative scores predict tmax (and are
thus good), and more negative the scores, the stronger the prediction of tmax.
Definition 4 (swap permutation). Given a permutation σ : N<nctx →N<nctx of the nctx positions
and two indices 0 ≤i, j < nctx, define the swap permutation σi↔j to be the permutation that is σ


                                       30

except swapping i and j:
                     σ(i)    if k = j
                                                 σi↔j(k) =   σ(j)    if k = i
                     σ(k)  otherwise

Define ∆σ,i↔j to be the difference in sequence scores when you swap i and j:

                               ∆σ,i↔j := sσi↔j −sσ

Lemma 3 (Characterization of swapping tokens). Fix a non-decreasing sequence of tokens t0 ≤· · · ≤
tnctx−1 ∈N. Fix σ : N →N be a permutation of the nctx positions. Fix indices 0 ≤i, j < nctx.
Then there are two cases for sign (∆σ,i↔j):

      1.  If ati = atj then sign (∆σ,i↔j) = −sign  bσ(i) −bσ(j)  sign  vti −vtj  .

                                                                                                       vtieati −vtj e atj
      2. Otherwise, sign (∆σ,i↔j) = sign  ati −atj  sign  bσ(i) −bσ(j)  sign  sσ −    eati −e atj         .

Intuitively, Lemma 3 says that, if the token contribution to attention is equal between tokens ti and
tj, then the impact of swapping their positions σ(i) and σ(j) is entirely determined by how much
attention is paid to the positions of i and j and the relative difference in their value. (Notably, by
swapping these tokens, we don’t affect the attention paid on other tokens, and so the effect of the
change does not depend on the values of the other tokens.) Alternatively, if the attentions are not
equal, then swapping the positions changes the allocation of attention to other tokens in the sequence,
and so it may the case that this change in allocation in attention dominates the attention-weighted
values of these two tokens.

Proof. First note that the theorem is trivial for i = j.

For the rest of the proof, we take i ̸= j.

The proof proceeds just by algebraic manipulation with no deep insight. We first list the facts we use,
the proceed to computing sign (∆σ,i↔j). We abbreviate σi↔j as σ′ for brevity.

                          sign  ebσ(i) −ebσ(j) = sign  bσ(i) −bσ(j)

sign (∆σ,i↔j) = sign (sσ′ −sσ)

         P                         0≤p<nctx vtpeatp+bσ′(p)   !
        = sign                  −sσ
          P                           0≤p<nctx eatp+bσ′(p)

Now multiply through by the denominator, which is positive

                              
        = sign X   vtpeatp+bσ′(p) −sσ X eatp+bσ′(p)                              
                       0≤p<nctx                0≤p<nctx

          
        = sign X   vtpeatp+bσ(p) −vtieati  ebσ(i) −ebσ′(i)  −vtjeatj  ebσ(j) −ebσ′(j)          
                       0≤p<nctx

                                                
                −sσ X   eatp+bσ(p) + sσeati  ebσ(i) −ebσ′(i) + sσeatj  ebσ(j) −ebσ′(j)                                                
                             0≤p<nctx

          
        = sign X   vtpeatp+bσ(p) −vtieati  ebσ(i) −ebσ(j)  −vtjeatj  ebσ(j) −ebσ(i)          
                       0≤p<nctx

                                                
             − X   vtpeatp+bσ(p) + sσeati  ebσ(i) −ebσ(j) + sσeatj  ebσ(j) −ebσ(i)                                                
                          0≤p<nctx


                                       31

        = sign   vtjeatj −vtieati    ebσ(i) −ebσ(j) + sσ (eati −eatj )  ebσ(i) −ebσ(j)
        = sign  ebσ(i) −ebσ(j)  sign   vtjeatj −vtieati + sσ (eati −eatj )
        = sign  bσ(i) −bσ(j)  sign sσ (eati −eatj ) −  vtieati −vtjeatj

Divide through by non-zero values when possible

        = sign  bσ(i) −bσ(j)
         sign  vti −vtj                                              if ati = atj
                                   ·                                                              vtieati −vtj e atj
                                                                  otherwise                    sign (eati −eatj ) sign  sσ −                                                                   eati −e atj         
        −sign  bσ(i) −bσ(j)  sign  vti −vtj                                         if ati = atj
                =                                                                                  vtieati −vtj e atj
                  sign  ati −atj  sign  bσ(i) −bσ(j)  sign  sσ −    eati −e atj       otherwise        


Definition 5 (σ fixes F). Fix a set of fixed indices F ⊆N<nctx and an assignment of token values
to each of the fixed positions tF  : F →N<dvocab. (F is the set of positions for which we are not
pessimizing over the value of the token in that position.) Fix a non-decreasing sequence of tokens
t0 ≤· · · ≤tnctx−1 ∈N.
Given a permutation σ : N<nctx →Nnctx, say that σ fixes F (relative to t0, . . . , tnctx−1) if ti =
tF (σ(i)) whenever σ(i) ∈F.

Note that in this section, for the cubic proofs, we will in fact generally take F = {nctx −1}, so that
we are fixing the final query token, though in Theorems 7, 8, and 9 F will also contain all positions
with the maximum token tmax. In Appendix F, we will take F = ∅or F to be the set of positions of
the maximum token. However, none of these theorems are specific to F being subsingleton, and we
prove them in generality.
Definition 6 (position-sorting permutation). Fix a set of fixed indices F ⊆N<nctx and an assignment
of token values to each of the fixed positions tF  : F →N<dvocab.
Define the position-sorting permutation fixing indices in F σs  : N<nctx →N<nctx to be the
permutation that sorts the indices not in F according to b: for 0 ≤i, j < nctx with i, j ̸∈F, bi ≤bj
whenever σs(i) < σs(j); and σs(i) = i for i ∈F.
Definition 7 (contiguous on equal tokens). Fix a set of fixed indices F ⊆N<nctx and an assignment
of token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of
tokens t0 ≤· · · ≤tnctx−1 ∈N.
Say that the sequence represented by a permutation σ : N<nctx →N<nctx is contiguous on equal
tokens if, for all 0 ≤i, j, k < nctx with ti = tj ̸= tk and i, j, k ̸∈σ−1(F), it is never the case that
σs(σ(i)) < σs(σ(k)) < σs(σ(j)).
Theorem 4 (Pessimization over sequence ordering is possible and results in contiguous sequences).
Fix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of the fixed positions
tF  : F →N<dvocab. Fix a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.
Let σmin, σmax : N →N be permutations of the nctx positions, fixing positions in F, satisfying the
following property: For all σ : N →N a permutation fixing F, we have

                                       sσmin ≤sσ ≤sσmax                                  (10)

(Such permutations are guaranteed to exist because the permutation group on nctx elements is finite.)

Then σmax and σmin may be taken to be contiguous on equal tokens. That is, there exist σmax and
σmin satisfying the property of Equation 10 which additionally satisfy the definition of Definition 7.

The basic idea is that we will assume that one of σmax and σmin cannot be contiguous on equal
tokens and derive a contradiction. We will pick the extremal permutation that is closest to being
contiguous, take a contiguity violation, and then show that either we can correct the contiguity
violation without changing the score—thus violating the presumption that the permutation is closest


                                       32

to being contiguous—or we will find one swap of indices that decreases the score and another swap
of indices that increases the score, thus violating the presumption of extremality.

In slightly more detail, but still informally, we will consider the sign of the difference between
scores of our purported extremal permutation and a permutation that has swapped some indices. The
theorem follows from showing that there exists a triple of indices i, j, k such that the sign of the score
difference from swapping i and j is different from the sign of the score difference from swapping j
and k.

First, a definition and some helpful facts about it.
Definition 8 (contiguous on equally-attended positions). Fix a set of fixed indices F ⊆N<nctx and
an assignment of token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing
sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.
Say that a permutation σ is contiguous on equally-attended positions if, for all 0 ≤i < nctx with
i ̸∈σ−1(F), the sorting order according σs on the contiguous block of positions with contribution
to the attention score equal to that of σ(i),  σ(j)  bσ(j) = bσ(i) and σ(j) ̸∈F   , is the same as the
sorting order according to the fraction of tokens equal to tj with b-values greater than bσ(i), with ties
broken by the value of tj. Equationally, this second sorting order is defined by the score

                                                                   tj
     k  tk = tj and bσ(k) > bσ(i) and σ(k) ̸∈F  +           |{k | tk = tj and σ(k) ̸∈F}| .
                                                     dvocab

Most importantly, any permutation that is contiguous on equally-attended positions has the property
that for any indices 0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) and ti = tj ̸= tk and σs(σ(i)) <
σs(σ(k)) < σs(σ(j)), we will have the strict inequality bσ(i) < bσ(k) < bσ(j). Additionally, we
may always sort equally-attended positions to make any permutation contiguous on equally-attended
positions.

We will define an additional notion of contiguity-violations which we avoid up-front by arbitrarily
swapping involved indices without changing the score sσ.
Definition 9 (needlessly non-contiguous). Fix a set of fixed indices F ⊆N<nctx and an assignment
of token values to each of the fixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of
tokens t0 ≤· · · ≤tnctx−1 ∈N.
Say that a permutation σ is needlessly non-contiguous at i, j, k (for i, j, k ̸∈σ−1(F)) if ∆σ,i↔k = 0
or ∆σ,j↔k = 0, for 0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) with ti = tj ̸= tk and σs(σ(i)) <
σs(σ(k)) < σs(σ(j)).

Say that a permutation σ is needlessly non-contiguous if it is needlessly non-contiguous at any
i, j, k ̸∈σ−1(F).
Lemma 5. Fix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of the
fixed positions tF  : F →N<dvocab. Fix a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N.
Any needlessly non-contiguous sequence σ which fixes F can be made into a sequence σ′ which
still fixes F and is both simultaneously contiguous on equally-attended positions and not needlessly
non-contiguous, and for which sσ = sσ′.


Proof. First, sort regions of equally-attended positions to make σ contiguous on equally-attended
positions. If the resulting permutation is not needlessly non-contiguous, then we are done.

Otherwise, we have ∆σ,i↔k = 0 or ∆σ,j↔k = 0 for some i, j, k, for 0 ≤i, j, k < nctx with
i, j, n ̸∈σ−1(F) and ti = tj ̸= tk and σs(σ(i)) < σs(σ(k)) < σs(σ(j)). Since the sequence is
contiguous on equally-attended positions, we have the strict inequality bσ(i) < bσ(k) < bσ(j).
By Lemma 3, we have two cases. Noting that ti = tj, we can write them as


      1. vtk = vti and ati = atk

                                   vtieati −vtk eatk
      2. ati ̸= atk and sσ =     eati −eatk


                                       33

In the first case, we may fully freely interchange tokens equal to ti with tokens equal to tk without
changing the score; in this case we may use the token value as a sorting tie-breaker and swap tokens
until there are no more needlessly non-contiguous triples falling into case (1).

In the second case, since swapping tokens does not change sσ, the property will continue to hold for
these tokens after the swap. We may then swap tokens, again using token value as a tie-breaker, until
there are no more needlessly non-contiguous triples falling into case (2).

We can now finally make our argument for Theorem 4 more precise.

Proof of Theorem 4. Choose σmax and σmin to be contiguous on equally-attended positions and
not needlessly non-contiguous, and suppose that we have σ ∈{σmax, σmin} such that for some
0 ≤i, j, k < nctx with i, j, k ̸∈σ−1(F) and ti = tj ̸= tk, we have bσ(i) < bσ(k) < bσ(j). We will
derive a contradiction with the presumption that σ is extremal by showing that we can swap i and k
to change the score in one direction and that we can swap j and k to change the score in the other
direction.
Take σ′0 to be σ but swapping i and k, and take σ′1 to be σ but swapping j and k.
Now we will consider the cases for the sign of the score difference ∆0 := sσ′0−sσ and ∆1 := sσ′1−sσ.
By the presumption of not being needlessly non-contiguous, ∆z ̸= 0 for z ∈{0, 1}. If we can show
that the sign of ∆0 is distinct from the sign of ∆1, then we will have a contradiction with extremality
because we will have either sσ′0 < sσ < sσ′1 or sσ′1 < sσ < sσ′0. That is, we would be able to swap
i ↔k and j ↔k to get a lower and higher score, making σ not extremal.

Noting that ti = tj,

                  (sign (vtk −vti)                                          if ati = atk
  sign (∆0) = sign  bσ(i) −bσ(k)                                      vtieati −vtk eatk
                                   sign (ati −atk) sign  sσ −    eati −eatk       otherwise
                  (sign (vtk −vti)                                          if ati = atk
  sign (∆1) = sign  bσ(j) −bσ(k)                                      vtieati −vtk eatk
                                   sign (ati −atk) sign  sσ −    eati −eatk       otherwise

Noting that the product is non-zero by presumption, that right multiplicand is equal for ∆0 and ∆1,
and sign  bσ(i) −bσ(k) = −1 and sign  bσ(j) −bσ(k) = 1, we have our desired contradiction.

Note that the proof of Theorem 4 does not go through if we include the position value function w in
the score, because we may trade off the position value function against the token value function. We
now show that we can independently pessimize over positional attention.
Definition 10 (full sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈
N<dvocab and a permutation σ : N<nctx →N<nctx define the full sequence score s′t0,...,tnctx−1,σ as:
                    ,
                    s′                         := X(vti + wσ(i))eati+bσ(i) X eati+bσ(i)                       t0,...,tnctx−1,σ
                                 0≤i<nctx                      0≤i<nctx
We will drop the token subscript, writing only s′σ, when the token values are unambiguous by context.
The sequence score here will be computing ∆ℓEPVOUt∗     := ∆ℓEVOUt∗+ ∆ℓPVOUt∗for some fixed
t∗and tmax. As with Definition 3, with the way we’ve set up our definitions, high scores predict t∗
(and are thus bad), negative scores predict tmax (and are thus good), and more negative the scores,
the stronger the prediction of tmax.
Definition 11 (relaxed sequence score). Given a non-decreasing sequence of tokens t0 ≤· · · ≤
tnctx−1 ∈N<dvocab and a permutation σ : N<nctx →N<nctx define the relaxed sequence scores
rt0,...,tnctx−1,σ,min and rt0,...,tnctx−1,σ,max as:
                               rt0,...,tnctx−1,σ,min := st0,...,tnctx−1,σ +0≤i<nctxwimin
                              rt0,...,tnctx−1,σ,max := st0,...,tnctx−1,σ +0≤i<nctxwimax
We will drop the token subscript, writing only rσ,min or rσ,max, when the token values are unambigu-
ous by context.


                                       34

Theorem 6 (Independent pessimization over positional contributions is possible). Fix non-decreasing
sequences of tokens t0 ≤· · · ≤tnctx−1 ∈N and t′0 ≤· · · ≤t′nctx−1 ∈N and permutations
σ, σ′ : N<nctx →N<nctx. Let rσ,min and rσ,max denote rt0,...,tnctx−1,σ,min and rt0,...,tnctx−1,σ,max;
let sσ denote st0,...,tnctx−1,σ; and let sσ′ and s′σ′ denote st′0,...,t′nctx−1,σ′ and s′t′0,...,t′nctx−1,σ′.
Then we have
                  0≤i<nctxwimin  = rσ,min −sσ ≤s′                                                   σ′ −sσ′ ≤rσ,max −sσ =0≤i<nctxwi.max

That is, the difference between the relaxed sequence score and the sequence score of any given
sequence always bounds the difference between the full sequence score and the sequence score for
any (related or unrelated) sequence.

Proof. This proof follows straightforwardly from the softmax weighting being an affine weighting.

We have
                      rσ,min −sσ =0≤i<nctxwσ(i)min   =0≤i<nctxwimin
                    rσ,max −sσ =0≤i<nctxwσ(i)max   =0≤i<nctxwimax
                  ,
                            s′σ′ −sσ′ = X wσ′(i)eat′i+bσ′(i) X eat′i+bσ′(i)
                                  0≤i<nctx                0≤i<nctx
                                                                       at′i+bσ′(i)                                              e
                  = X wσ′(i)
                                                                              at′j +bσ′(j)                                 0≤i<nctx  P0≤j<nctx e
Since ex is non-negative for all real x, we have

                                                                                                          at′i+bσ′(i)                                       at′i+bσ′(i)            P       P                                                                     0≤i<nctx e                   0≤i<nctx e
                                                                                                                                     .                            ≤s′σ′ −sσ′ ≤ 0≤j<nctxwσ′(j)max    0≤j<nctxwσ′(j)min                                       at′j +bσ′(j)                                                                                                          at′j +bσ′(j)       P                          P                   0≤j<nctx e                                                                   0≤j<nctx e

Thus we get as desired
                                                   σ′ −sσ′ ≤ 0≤i<nctxwi.max                                 0≤i<nctxwimin   ≤s′



Note that we could prove a more fine-grained theorem, that pessimizes over attention paid to positions
only for sequences compatible with the chosen fixed tokens F and tF , but since the positional
contribution is so small we do not bother.
Theorem 7 (For a fixed ordering, softmax is convex over token counts and only pure sequences need
be considered). Fix a set of fixed indices F ⊆N<nctx and an assignment of token values to each of
the fixed positions tF  : F →N<dvocab. Fix a set S ⊆N<dvocab of valid other tokens in the sequence.
(In our uses of this theorem, S will be the largest subset of N<tmax for which we can guarantee that
the model behaves correctly on all sequences compatible with F and tmax and with tokens otherwise
drawn from S.)

Define a comparison on non-negative integers less than dvocab:

          c := X vtF (i)eatF (i)+bi        d := X eatF (i)+bi         f := X ebi
              i∈F                             i∈F                       0≤i<nctx
                                                                                                   i̸∈F

   cmp(x, y) := sign d(eaxvx −eayvy) −c(eax −eay) + feax+ay  vxeax+ay −vyeax+ay
Let tcmp min and tcmp max be the minimum and maximum elements of S according to cmp.21
For a given choice of a non-decreasing sequence of tokens t0 ≤· · · ≤tnctx−1 ∈N compatible with
F and S and a given choice of permutation σ : N →N of the nctx positions fixing F (ti = tF (σ(i))

  21We will prove that cmp is transitive in the process of proving this theorem.


                                       35

for σ(i) ∈F; and ti ∈S for σ(i) ̸∈F):  let sσ,min (and sσ,max) denote st0,...,tnctx−1,σ when
ti = tcmp min for all σ(i) ̸∈F (or tcmp max, respectively).

Then for all such choices of sequence-permutation pairs,

                               sσ,min ≤st0,...,tnctx−1,σ ≤sσ,max.

This theorem follows by chaining two lemmas: that scores are extremized by considering pure
sequences, and that the extremal pure sequences match the comparison function defined in the
theorem statement.
Lemma 8 (Sequences scores are extremized on purer sequences). Fix all the same quantities as in
Theorem 7.

For any indices 0 ≤i < j < nctx, token values x, y ∈S, the score for a sequence with ti = x ̸=
y = tj is bounded on both sides by sequences with ti = tj = x and ti = tj = y.

Proof. Let sα,β be the sequence score with ti = α and tj = β, and define the score differences
∆x := sx,x −sx,y and ∆y := sy,y −sx,y. It suffices to show that sign(∆x∆y) ≤0. To show this,
we must only compute the sign of ∆α for α ∈{x, y} and show that whenever both ∆x and ∆y are
non-zero, they have opposite signs.

We proceed by computation after defining some convenience variables for brevity:

          C := X vtkeatk +bσ(k)         D := X eatk +bσ(k)
                    0≤k<nctx                              0≤k<nctx
                              k̸=i,j                                               k̸=i,j


            x   if α = y                     i    if α = x                     i    if ˜α = x
            ˜α :=                       iα :=                                  i˜α :=
              y    if α = x                 j   if α = y                  j   if ˜α = y

                   vαeaα+bσ(i) + vαeaα+bσ(j) + C −vxeax+bσ(i) + vyeay+bσ(j) + Csign (∆α) = sign
                       eaα+bσ(i) + eaα+bσ(j) + D         eax+bσ(i) + eay+bσ(j) + D

                                                                                                     ˜ + C                   vαeaα+bσ(iα) + vαeaα+bσ(iα)                                                        ˜ + C −vαeaα+bσ(iα) + v˜αeaα+bσ(i˜     α)      = sign
                       eaα+bσ(iα) + eaα+bσ(iα)˜ + D         eaα+bσ(iα) + eaα+bσ(i˜     α)˜ + D

Multiply through by positive denominators and simplify

                                                                       aα+bσ(i˜     α)˜      = sign C  e                       aα+bσ(i˜     α)˜ −ebσ(iα)+aα˜    + D  vαebσ(iα)+aα                                                                            ˜     −v˜αe

                                                                                      ˜ + ebσ(iα)  eaα+bσ(i˜    α)+aα˜           + vα  e                                  bσ(iα)˜ + ebσ(iα)  eaα+bσ(i˜    α)+aα˜     −v˜α  ebσ(iα)

Pulling out ebσ(iα)˜

      = sign  eaα+aα˜     ebσ(iα)˜ + ebσ(iα)  (vα −v˜α) + C (ea                                                           α˜ −eaα) + D (eaαvα −eaαv˜α)˜


Note that swapping α and ˜α negates the sign. Hence, we have sign(∆x) = −sign(∆y) and hence
sx,x ≤sx,y ≤sy,y or sy,y ≤sx,y ≤sx,x as desired.

Lemma 9 (Pure sequences are sorted according to cmp in Theorem 7). Fix all the same quantities as
in Theorem 7.

Fix tokens x, y ∈S. Let n := nctx −|F| be the number of non-fixed tokens. Fix sequences with
n copies of x and y respectively: fix tx,0 ≤· · · ≤tx,nctx−1 ∈N and ty,0 ≤· · · ≤ty,nctx−1 ∈N
compatible with F and S and given choices of permutations σx, σy : N →N of the nctx positions
fixing F: tx,i = tF (σx(i)) for σx(i) ∈F; ty,i = tF (σy(i)) for σy(i) ∈F; tx,i = x for σx(i) ̸∈F;
and ty,i = y for σy(i) ̸∈F.

Then
                    sign((sσx,tx,0,...,tx,nctx−1) −(sσy,ty,0,...,ty,nctx−1)) = cmp(x, y)


                                       36

                                 √
                    EQKE(t−1, ti) := t−1EqQKT ¯ET ti T /  d
                               √
                    EQKP(t−1, i) := t−1EqQKT ˆPTi /  d
                       EVOU(ti) := ti¯EV OU
                       PVOU(i) := ˆPiV OU
                            ℓEU(t−1) := t−1EqU
               ∆ℓEU t∗(t−1, max ti) := ℓEU(t−1)t∗−ℓEU(t−1)maxi ti
                                                     i

Figure 13: Recapitulation of some relevant definitions from Figure 12, parameterized by the arguments they
actually depend on.


Proof. The proof goes by straightforward computation.

           sign((sσx,tx,0,...,tx,nctx−1) −(sσy,ty,0,...,ty,nctx−1))
                vxeaxf + c −vyeayf + c     = sign
                  eaxf + d     eayf + d

Multiply through by non-negative denominators
     = sign ((vxeaxf + c) (eayf + d) −(vyeayf + c) (eaxf + d))
     = sign −cfeax + cfeay + dfvxeax −dfvyeay + f 2vxeax+ay −f 2vyeax+ay

Use f > 0
     = sign −ceax + ceay + dvxeax −dvyeay + fvxeax+ay −fvyeax+ay
     = sign c (eay −eax) + d (vxeax −vyeay) + f  vxeax+ay −vyeax+ay
     = cmp(x, y)


Corollary 10. Define the relation ≤cmp by x ≤cmp y if and only if cmp(x, y) ∈{−1, 0}. The
relation ≤cmp is always transitive.

Proof. Note that by Lemma 9, cmp is comparing two sequence scores. Since ≤is transitive over the
reals, the relation ≤cmp is also transitive.

Finally, we combine the previous lemmas to complete our proof of Theorem 7:

Proof of Theorem 7. Extremal sequences with scores sσ,min and sσ,max are guaranteed to exist
because there are only finitely many elements of S and therefore only finitely many sequences.
By Lemma 8, the extremal sequences must be pure (have ti = tj whenever σ(i), σ(j) ̸∈F). By
Lemma 9, the extremal sequences must have tokens that are extremal according to cmp.

We now have all the tools necessary to prove the following theorem. We refer to Algorithm 3 and
Algorithm 4 or the proof of Theorem 11 for a definition of the CUBIC algorithm.
Theorem 11.

         Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥CUBIC(dvocab, nctx, M)
                                                      i                         i

Before we give the proof of this theorem, we introduce some helpful notation.
Definition 12. Fix an element (rm, rq, c) ∈{0, ..., dvocab}2 ×{0, ...3} such that rm ≥rq. We define
X(rm,rq,c) to be the set of tokens t such that

      1. The max token tmax is equal to rm,


                                       37

Algorithm 2 Counting Correct Sequences in Cubic Time: Preliminaries
  1: function CORRECTNESS(M, input-sequence)
  2:    return MODEL-BEHAVIOR(M, input-sequence) == MAX(input-sequence)
  3: end function
  4: function MODEL-BEHAVIOR(M, input-sequence)
Require: input-sequence is a tensor of shape (nctx, ) with values in N<dvocab
  5:    tmax ←MAX(input-sequence)                               ▷tmax ←max-token
  6:    t ←input-sequence
  7:    skip-scoret∗←∆ℓEUt∗(tnctx−1, tmax)
  8:     attn-weights-unscaledi ←EQKE(tnctx−1, ti) + EQKP(tnctx−1,√              i)
  9:     attn-weights ←SOFTMAX(attn-weights-unscaled/  d)
10:     vt ←EVOU(t)
11:    wi ←PVOU(i)
12:    ∆vt,t∗←vt,t∗−vt,tmax
13:    ∆wi,t∗←wi,t∗−wi,tmax
14:    return maxt∗̸=tmax(skip-scoret∗+ Pnctx−1i=0   (∆vi,t∗+ ∆wi,t∗) · attn-weightsi)
15: end function
16: function CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW(M, input-sequence)
17:    t ←input-sequence
18:    return ALL(CORRECTNESS(M, perm + [t−1]) for all perm ∈PERMUTATIONS(t0:−1))
19: end function


      2. The query token tquery is equal to rq,

      3. The cardinality of tokens that are not at the query position and not equal to tmax is equal to
           c.

For clarity, we list all the possible cases. We always take tquery ≤tmax and let S3 act on sequences
by permuting the first three factors (i.e. keeping the query position fixed).

      1.  If c = 0, then X(tmax,tquery,0) = {[tmax, tmax, tmax, tquery]},
      2.  If c = 1, then X(tmax,tquery,1) = S3.{[t1, tmax, tmax, tquery] | t1 < tmax},
      3.  If c = 2, then X(tmax,tquery,2) = S3.{[t1, t2, tmax, tquery] | ti < tmax},
      4.  If c = 3, then X(tmax,tmax,3) = S3.{[t1, t2, t3, tmax] | ti < tmax}.
Definition 13. Let t ∈X be a sequence. We say t is pure, if it has at most three distinct tokens: the
max token tmax, the query token tquery, and optionally a third token t∗< tmax.
We denote by Xpure the subset of pure tokens. For any subset Y ⊂X, we set Y pure := Y ∩Xpure.

We now come to the proof of Theorem 11. We will show how to use the previous theorems to get
explicit bounds and explain how CUBIC(dvocab, nctx, M) computes these bounds.

Proof of Theorem 11. First of all, we note that the algorithm CUBIC = CUBIC(dvocab, nctx, M)
yields a lower bound for the accuracy on the set X(tmax,tquery,c). We can therefore compute the bound
on X = `(tmax,tquery,c) X(tmax,tquery,c) by computing it for each such choice (tmax, tquery, c) and
summing over them

  Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti ≥ X     CUBIC(X(tmax,tquery,c)).
                                              i                         i
                                                                       (tmax,tquery,c)
So from now on we will fix one such subset X(tmax,tquery,c).
We begin by defining a map
                         f : X(tmax,tquery,c) →{0, ..., dvocab}c
which sends a sequence to the subsequence of elements which are not at the query position and not
equal to tmax. Then Theorem 7 can be restated as follows22:

   22In fact, the theorem yields a stronger result, but we will only need the following formulation.


                                       38

Algorithm 3 Counting Correct Sequences in Cubic Time, Part I. Lines are annotated with comments
indicating the parameters for a cache to avoid duplicate computations.
  1: function MODEL-BEHAVIOR-RELAXED(M, query-tok, max-tok, non-max-tok, n-copies-
   nonmax)
  2:     tquery ←query-tok, tmax ←max-tok, t′ ←non-max-tok, c ←n-copies-nonmax
Require: 0 ≤tquery ≤tmax < dvocab, 0 ≤t′ ≤tmax < dvocab, 0 ≤c < nctx
Require:  if n-copies-nonmax = 0 then non-max-tok = max-tok
Require:  if query-tok ̸= max-tok then n-copies-nonmax < nctx −1
Ensure: return ≥MODEL-BEHAVIOR(M, t) for all t with specified tquery, c copies of t′ in non-
    query positions, and the remainder of the tokens equal to tmax
  3:    skip-scoret∗←∆ℓEUt∗(tquery, tmax)                    ▷Cache by tmax, tquery, t∗
  4:    wi ←PVOU(i)     for 0 ≤i < nctx                              ▷Cache by i
  5:    ∆wmax,t∗←max0≤i<nctx(wi,t∗−wi,tmax)                    ▷Cache by tmax, t∗
  6:     vt ←EVOU(t), ∆vt,t∗←vt,t∗−vt,tmax     for t ∈{tquery, tmax, t′} ▷Cache by tmax, t,
     t∗             √
  7:     at ←EQKE(tquery, t)/  d     for t ∈{tquery, tmax, t′}          ▷Cache by tquery, t                      √
  8:     bnctx−1 ←EQKP(tquery, nctx −1)/  d                        ▷Cache by tquery                       √
  9:     b0,:−1 ←SORT(EQKP(tquery, : −1))/  d                     ▷Cache by tquery, i
10:     b1,:−1 ←REVERSE(b0,:−1)
11:     attn-weights-unscaled:,nctx−1 ←atquery + bnctx−1                 ▷Cache by tquery
12:     attn-weights-unscaled0,i ←atmax + b0,i   for 0 ≤i < nctx −c −1 ▷Cache by tmax, c, i,
     tquery
13:     attn-weights-unscaled1,i ←atmax + b1,i   for 0 ≤i < nctx −c −1 ▷Cache by tmax, c, i,
     tquery
14:     attn-weights-unscaled0,i ←at′ + b0,i   for nctx −c −1 ≤i < nctx −1 ▷Cache by t′, c, i,
     tquery
15:     attn-weights-unscaled1,i ←at′ + b1,i   for nctx −c −1 ≤i < nctx −1 ▷Cache by t′, c, i,
     tquery
16:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0)      ▷Cache by tmax, t′, c, i, tquery
17:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1)      ▷Cache by tmax, t′, c, i, tquery
18:       if c = 0 then ▷In this case, attn-weights0,i = attn-weights1,i, so we drop the first subscript
19:        return   maxt∗̸=tmax(skip-scoret∗ +  ∆wmax,t∗ +  ∆vt−1,t∗attn-weights−1 +
   ∆vtmax,t∗Pnctx−2i=0    attn-weightsi)
20:     else
21:       ∆vi,t∗←∆vtmax,t∗   for 0 ≤i < nctx −c −1
22:       ∆vi,t∗←∆vt′,t∗   for nctx −c −1 ≤i < nctx −1
23:        ∆vnctx−1,t∗←∆vtquery,nctx−1      (Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights0,i
24:        return maxt∗̸=tmax skip-scoret∗+max                                          Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights1,i
25:    end if
26: end function
27: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, tquery, tmax, t′, c)
28:       ▷runs the model on a relaxed variant of input sequences compatible with the arguments
Ensure: return is False if CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW(M, t) is False
    for any t with specified tquery, c copies of t′ in non-query positions, and the remainder of the
    tokens equal to tmax
29:    return MODEL-BEHAVIOR-RELAXED(M, tquery, tmax, t′, c) < 0
30: end function





                                       39

Algorithm 4 Counting Correct Sequences in Cubic Time, Part II
  1: function CUBIC(dvocab, nctx, M)
  2:     count ←0                                    ▷# of correct sequences
  3:     for tmax ∈RANGE(dvocab) do                               ▷tmax ←max-token
  4:        for 0 ≤tquery ≤tmax do                                   ▷tquery ←query-token
  5:          cmax ←nctx −1 if tquery = tmax else nctx −2    ▷maximum copies of nonmax
  6:            for 0 ≤c ≤cmax do          ▷number of valid choices for the non-max token
  7:            RCPOP(⃗χ) ←RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, ⃗χ)
  8:                     if c = 0 then
  9:                    t-count ←1 if RCPOP(tquery, tmax, tmax, 0) else 0
10:                 else
11:                    t-count ←Ptmax−1t′=0   1 if RCPOP(tquery, tmax, t′, c) else 0
12:             end if
13:               count ←count +  nctx−1c      · (t-count)c         ▷taking 00 = 0 conventionally
14:          end for
15:       end for
16:    end for
17:    return count ·    1                          dvocabnctx
18: end function

Let S ⊂{0, ..., dvocab}. Then full accuracy f −1(Sc)pure := Xpure(tmax,tquery,c) ∩f −1(Sc), implies full
accuracy on f −1(Sc).
Now instead of computing the output of the model for every element f −1(Sc)pure, we use Theorem 4
(combined with Theorem 6) to run a relaxed version of this. In particular, we may assume that the pure
sequence is contiguous on equal tokens. Here contiguous on equal tokens means that for the positional
part of the attention (i.e. the EQKP part), we have either btmax < {bi, bj} or btmax > {bi, bj}, where
i, j ∈{0, ..., nctx −1} are indices of tokens not equal to tmax.
For the algorithm CUBIC(dvocab, nctx, M) we fix a t∗∈{0, ..., tmax−1} (unless c = 0, in which case
there is no such choice). We then run the relaxed accuracy computation RCPOP(tquery, tmax, t′, c)
as described in Theorem 6. If RCPOP(tquery, tmax, t′, c) < 0, we add t′ to S. If we do, we add t∗to
S. Therefore by construction of S we know that we get full accuracy on f −1(Sc)pure and therefore
we get full accuracy on f −1(Sc).
Now we count the cardinality of f −1(Sc) and add it to the count of correct sequences.



Theorem 12. The running time of Algorithm 3, after using caching to avoid duplicate computations,
is O(dvocab3nctx2).

Proof. The nested loops in CUBIC execute the innermost body O(dvocab2nctx) times, and the
summation on Line 13 costs O(nctx) per iteration.  What remains is to show that the call
to RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION(M, tquery, tmax, t′, c) costs O(nctx)
when c ̸= 0 and at most O(dvocabnctx) when c = 0 and t′ = tmax.
The matrix multiplications in EQKE, EQKP, EVOU, PVOU, and ℓEU can be cached upfront,
costing O(max(dvocab, dmodel, nctx)2dmodel) ≤O(dvocab3) since we assume dvocab > dmodel and
dvocab > nctx.
The sorting on Line 9 can also be cached upfront (per tquery), costing O(dvocabnctx log nctx).

Note that each variable assignment in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION can
be cached into a table parameterized over at most three variables which range over dvocab and over at
most two variables that range over nctx.

What remains is the return statements.

When  c =   0, we have on Line  19:   return maxt∗̸=tmax(skip-scoret∗+ ∆wmax,t∗+
∆vt−1,t∗attn-weights−1 + ∆vtmax,t∗Pnctx−2i=0    attn-weightsi). This is O(dvocabnctx) as desired.


                                       40

When c ̸= 0, we have on Line 24:
                          (Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights0,i
return max skip-scoret∗+max        t∗̸=tmax                  Pnctx−1i=0   maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) · attn-weights1,i
We can cache maxt∗̸=tmax skip-scoret∗per tmax and tquery, costing O(dvocab3nctx). We can cache
maxt∗̸=tmax(∆wmax,t∗+ ∆vi,t∗) per tmax and t′ costing O(dvocab3), since each ∆vi,t∗will be
∆vt,t∗for some t ∈{tquery, tmax, t′}. Finally, we can compute the summation in cost O(nctx) per
loop iteration, as required.

F  Quadratic counting for a sub-cubic proof

In this section we fill in the details lacking from Section 4.3.

In Appendix E we proved an intricate version of convexity of softmax where, modulo pessimizing in
unrealistic ways over the attention paid to positions for the computation done on positional encodings,
all extremal relaxed sequences correspond to actual sequences.
When we only get a budget of O(dvocab2nctx) extremal relaxed cases to consider, though, we must
pessimize more, which gives us a simpler version of the convexity theorem and proof. Notably, when
we restrict our sequences to have only two tokens (the max token tmax and the non-max token t′),
most of the theorems from Appendix E.3 get significantly simpler.

Additionally, we must pessimize separately over the token value (v) and token attention (b) computa-
tions in order to allow efficient computation (Theorem 15).

F.1  Proof of baseline sub-cubic result

For this subsection, all theorems are parameterized over the following quantities.
Definition 14 (Common theorem parameters). Fix a total number of tokens nctx. Fix a token value
function (à la a row-difference in EVOU) v : N<dvocab →R and a token attention function (à la
EQKE for a fixed query token) a : N<dvocab →R. Fix a position value function (à la a row-difference
in PVOU) w : N<nctx →R and a position attention function (à la EQKP for a fixed query token)
b : N<nctx →R.

In practice, as in Appendix E.3, we’ll take, for fixed query token tquery,
                                       √
               vt = EVOUt,t∗−EVOUt,tmax              at = EQKEtquery,t/  d
                                       √
            wi = PVOUi,t∗−PVOUi,tmax                 bi = EQKPtquery,i/  d

Note that unlike in Appendix E.3, we pessimize independently over the query token and the non-max
token, so the “fixed” query token may not in fact appear in any key-side position in the relaxed
sequence we consider.
Definition 15 (of a sequence via mapping from positions). We can define a sequence of tokens via
mapping from positions by specifying a subset of valid tokens S ⊆N<dvocab paired with a function
T  : N<nctx →S specifying which token is in each position.
Definition 16 (sequence score). Given a subset of valid tokens S ⊆N<dvocab and a function
T  : N<nctx →S specifying which token is in each position, define the sequence score

                 ,
                        sT := X vT (i)eaT (i)+bi X eaT (i)+bi
                               0≤i<nctx              0≤i<nctx
Definition 17 (swapped mapping). Given a subset of valid tokens S ⊆N<dvocab and a function
T  : N<nctx →S specifying which token is in each position and two indices 0 ≤i, j < nctx, define
the swapped mapping Ti↔j be the function that is T except swapping i and j:

                     T(i)    if k = j
                                                 Ti↔j(k) =  T(j)    if k = i
                     T(k)  otherwise


                                       41

Lemma 13 (Characterization of swapping tokens in a two-token sequence). Fix two tokens t0 <
t1 ∈N and a function T  : N<nctx →{t0, t1} specifying which token is in each position.
Define ∆T,i↔j to be the difference in sequence scores when you swap i and j:

                              ∆T,i↔j := sTi↔j −sT

Then
                     sign (∆T,i↔j) = −sign (bi −bj) sign vT (i) −vT (j)

Proof. Lemma 3 gives us the result directly when aT (i) = aT (j). Otherwise, we get
   sign (∆T,i↔j) = sign aT (i) −aT (j)  sign (bi −bj) sign  sT −vT (i)eaT (i)(i) −vT (j)eaT(j)    (j)                                                                    eaT  −eaT

Hence all that remains is to show that
         sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j) = −sign vT (i) −vT (j)

Define ¯v := 12(vT (i) + vT (j)) and define ∆v := 12(vT (i) −vT (j)) so that vT (i) =  ¯v + ∆v and
vT (j) = ¯v −∆v. Assume WLOG that T(i) = 0 and T(j) = 1 so that vT (p) = ¯v + (−1)T (p)∆v for
all p.

Then we have

 sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j)
= sign (sT (eaT (i) −eaT (j)) −¯v (eaT (i) −eaT (j)) −∆veaT (i) −∆veaT (j))
     X   vT (p)eaT (p)+bp                             
= sign  0≤p<nctx               (eaT                                                           (i) −eaT (j)) −¯v (eaT (i) −eaT (j)) −∆v (eaT (i) + eaT (j))        X eaT (p)+bp                                   
            0≤p<nctx
     X       ¯v + (−1)T (p)∆v  eaT (p)+bp                             
     0≤p<nctx                                                     (i)                             (i)                                (i)     = sign                                       (eaT  −eaT (j)) −((((((((¯v (eaT  −eaT (j)) −∆v (eaT  + eaT (j))
        X eaT (p)+bp                                                                                      
                   0≤p<nctx
         eaT (i) X ebp −eaT                                                         (j) X ebp                

                      0≤p<nctx                                      0≤p<nctx                                                                       (p)=T (i)      T                                        (p)=T (j)           T                                                                                                                          (i)                         (i)         (j)= sign(∆v) sign                              (eaT  −eaT (j)) −eaT  −eaT 
          X eaT (p)+bp                                                          
             0≤p<nctx                       

              eaT (i) X ebp −eaT                                                                      (j) X ebp                

                               0≤p<nctx                                               0≤p<nctx                                                                                          (p)=T (i)      T                                                 (p)=T (j)                T                                                                                                                                              (i)                         (i)         (j)= sign(vT (i) −vT (j)) sign                              (eaT  −eaT (j)) −eaT  −eaT 
               X eaT (p)+bp                                                               
                  0≤p<nctx                       

Define
                     Pi := X ebp                    Pj := X ebp
                          0≤p<nctx                           0≤p<nctx
                       T (p)=T (i)                        T (p)=T (j)


so that we get
 sign sT (eaT (i) −eaT (j)) −vT (i)eaT (i) + vT (j)eaT (j)


                                       42

                            eaT (i)Pi −eaT (j)Pj        (i)                         (i)         (j)= sign(vT (i) −vT (j)) sign                      (eaT  −eaT (j)) −eaT  −eaT
                            eaT (i)Pi + eaT (j)Pj

Multiply through by the positive denominator and expand out so that we get
= sign(vT (i) −vT (j)) sign −2eaT (i)+aT (j)Pi −2eaT (i)+aT (j)Pj
= −sign(vT (i) −vT (j)) sign eaT (i)+aT (j)Pi + eaT (i)+aT (j)Pj
= −sign(vT (i) −vT (j))




Theorem 14 (Pessimization over sequence ordering for two-token sequences is simple). Let σs :
N →N denote a permutation of the nctx positions that sorts them according to b: for 0 ≤i, j < nctx,
bi ≤bj whenever σs(i) < σs(j). Fix two tokens t0 < t1 ∈N.
Let nt0 be the number of p ∈[0, nctx) with T(p) = t0 and let n1 be the number of p ∈[0, nctx) with
T(p) = tt1. Note that nt0 + nt1 = nctx.
Define tmin := argmint∈{t0,t1} vt and define tmax := argmaxt∈{t0,t1} vt.
Define Tmin, Tmax : N<nctx →{t0, t1} to be the assignment of tokens to positions that pays the least
(respectively, most) attention to tmax:

                                    tmax    if 0 ≤σs(i) < ntmax
                         Tmin(i) :=
                                       tmin    if ntmax ≤σs(i) < nctx
                                       tmin    if 0 ≤σs(i) < ntmin
                       Tmax(i) :=
                                    tmax    if ntmin ≤σs(i) < nctx

Then we have that
                                      sTmin ≤sT ≤sTmax

Proof. The extremality of sTmin and sTmax follows straightforwardly from Theorem 4.
All that remains is sTmin ≤sTmax.
This follows from noting by Lemma 13 that swapping two tokens in Tmin increases the sequence
score, while the reverse is true of sTmax, thus showing that it must be sTmin that is the minimum and
sTmax that is the maximum and not vice versa.
Definition 18 (full sequence score). Given a subset of valid tokens S ⊆N<dvocab and a function
T  : N<nctx →S specifying which token is in each position define the full sequence score s′T :

                  ,
                            s′T := X(vT (i) + wi)eaT (i)+bi X eaT (i)+bi
                            0≤i<nctx                     0≤i<nctx

Theorem 15 (Independent pessimization over positional contributions and token attention and token
value is possible). Fix two tokens t0 < t1 ∈N. Let Tmin, Tmax : N<nctx →{t0, t1} and tmax, tmin
be as in Theorem 14. Fix a set S of valid tokens with t0, t1 ∈S.
Define relaxed versions T max,′   Tmin′     : N<nctx →S of Tmax and Tmin:

                            (Tmax(i)                if Tmax(i) = tmax
                T max(i)′     :=  argmin j∈S  aj   otherwise
                                                  j̸=tmax
                             (Tmin(i)                  if Tmax(i) = tmax
                T min(i)′     :=  argmax j∈S  aj   otherwise
                                                   j̸=tmax
That is, T max′   replaces tmin with whatever token in S draws the least attention away from tmax, while
T min′   replaces tmin with whichever token in S draws the most attention away from tmax.


                                       43

Define relaxed extremal sequence scores rTmax, rTmin:

                       ,        
              rTmin := min  +  X vTmin(i)eaT min(i)+bi′   X eaT min(i)+bi′                       0≤i<nctxwi                      
                                   0≤i<nctx                   0≤i<nctx
                        ,        
             rTmax :=0≤i<nctxwimax  +  X vTmin(i)eaT ′max(i)+bi X eaT ′max(i)+bi 
                                   0≤i<nctx                   0≤i<nctx

Then rTmin ≤s′Tmin and s′Tmax ≤rTmax.

Proof. (sketch) Essentially the same as the proof of Theorem 6.

Note that in practice, we take S to be the set of all tokens less than tmax −g for some minimum
gap g. This allows us to share computation across the various maximum tokens to reduce overall
computational complexity.

Algorithm 5 Counting Correct Sequences in Subcubic Time, Preliminaries
  1: function INPUT-SEQUENCE-COMPATIBLE-WITH(input-sequence, dvocab, nctx, tmax, tquery, c,
    g)
  2:    t ←input-sequence
  3:    return False if t ̸∈(N<dvocab)nctx            ▷the sequence is not made of valid tokens
  4:    return False if t−1 ̸= tquery                               ▷wrong query token
  5:    return False if maxi ti ̸= tmax                              ▷wrong max token
  6:    return False if |{i ∈N<nctx | ti ̸= tmax}| ̸= c          ▷wrong count of non-max toks
  7:    return ALL(ti = tmax or tmax −ti ≥g for 0 ≤i < nctx)   ▷check gap on non-max toks
  8: end function
  9: function CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW(M, dvocab, nctx, tmax, tquery, c, g)
10:    return ALL(CORRECTNESS(M, t) for all t s.t. INPUT-SEQUENCE-COMPATIBLE-WITH(t,
    dvocab, nctx, tmax, tquery, c, g))
11: end function
12: function SUBCUBIC(dvocab, nctx, M, G)
13:     count ←0                                    ▷# of correct sequences
14:     Gtmax,tquery,c ←MIN(tmax, MAX(1, Gtmax,tquery,c))             ▷Clip G to valid range
15:   G∗tmax,c ←mint≤tmax Gtmax,t,c                          ▷Cache running minima
16:     for tmax ∈RANGE(dvocab) do                               ▷tmax ←max-token
17:        for 0 ≤tquery ≤tmax do                                   ▷tquery ←query-token
18:           cmin ←0 if tquery = tmax else 1               ▷minimum copies of nonmax
                     0             if tmax = 0
19:          cmax ←                              ▷maximum copies of nonmax
                         nctx −1  otherwise
20:            for cmin ≤c ≤cmax do         ▷valid choices for the number of non-max tokens
21:              g ←Gtmax,tquery,c
22:         g∗←G∗tmax,c
23:              q-gap ←tmax −tquery
24:            RCPOG(⃗χ) ←RELAXED-CORRECTNESS-PESSIMIZING-OVER-GAP(M, ⃗χ)
25:                     if (q-gap = 0 or q-gap ≥g) and RCPOG(dvocab, nctx, tmax, tquery, c, g, g∗)
    then
26:                        c′ ←c if tquery = tmax else c −1      ▷# of non-max non-query tokens
                                         nctx−127:                  count ←count +      c′   (tmax −g)c′     ▷taking 00 = 1 conventionally
28:             end if
29:          end for
30:       end for
31:    end for
32:    return count ·    1                          dvocabnctx
33: end function



                                       44

Algorithm 6 Counting Correct Sequences in Subcubic Time
  1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗)
Ensure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False
Require: 0 ≤g∗≤g ≤tmax
Require:  if c = 0 then tquery = tmax
  2:     skip-score ←maxt∗ℓEU(tquery)t∗−mint∗ℓEU(tquery)t∗            ▷Cache by tquery
  3:     vt ←EVOU(t)
  4:    wi ←PVOU(i)
  5:   ∆wmax,t∗←maxi wi,t∗−wi,tmax                           ▷Cache by tmax, t∗
  6:    ∆wmax,max ←maxt∗∆wmax,t∗                               ▷Cache by tmax
  7:    ∆vt ←maxt∗vt,t∗−mint∗vt,t∗                                 ▷Cache by t
  8:    ∆vmax ←max0≤t≤tmax−g∗∆vt                           ▷Cache by tmax −g∗
  9:    ∆vtmaxt∗   ←vtmax,t∗−vtmax,tmax                                ▷Cache by tmax
10:    ∆vtmaxmax ←maxt∗̸=tmax ∆vtmaxt∗                                  ▷Cache by tmax
11:       if c = 0 then
12:      ℓt∗←ℓEU(tmax)t∗+ vtmax,t∗+ ∆wmax,t∗
13:        return maxt∗̸=tmax (ℓt∗−ℓtmax)
14:    end if                √
15:     b:,nctx−1 ←EQKP(tquery, nctx −1)/  d                       ▷Cache by tquery                       √
16:     b0,:−1 ←SORT(EQKP(tquery, : −1))/  d                     ▷Cache by tquery, i
17:     b1,:−1 ←REVERSE(b0,:−1)                √
18:     at ←EQKE(tquery, t)/  d                                ▷Cache by tquery, t
19:     amin,t ←min0≤t′′≤t at′′         ▷Cache by tquery, t, compute in amortized O(dvocab2)
20:     amax,t ←max0≤t′′≤t at′′        ▷Cache by tquery, t, compute in amortized O(dvocab2)
21:    ∆amax ←atmax −amin,tmax−g                         ▷Cache by tquery, tmax, c
22:    ∆amin ←atmax −amax,tmax−g                         ▷Cache by tquery, tmax, c
23:     idx-set ←{0, . . . , nctx −c −1} if tmax ̸= tquery else {0, . . . , nctx −c −2, nctx −1}
24:     attn-weights-unscaled0,i ←b0,i + (∆amin if i ∈idx-set else 0)
25:     attn-weights-unscaled1,i ←b1,i + (∆amax if i ∈idx-set else 0) ▷Cache by tquery, tmax, i,
    c
26:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0)        ▷Cache by tquery, tmax, i, c
27:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1)        ▷Cache by tquery, tmax, i, c
28:     attn-max0 ←Pi∈idx-set attn-weights0,i
29:     attn-max1 ←Pi∈idx-set attn-weights1,i
30:     attn-max ←attn-max0 if ∆vtmaxmax < ∆vmax else attn-max1
31:                  ▷Recall that ∆vtmaxmax is negative when the model outputs the correct answer
32:    return skip-score + ∆wmax,max + attn-max · ∆vtmaxmax + (1 −attn-max)∆vmax
33: end function
34: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-GAP(M, dvocab, nctx, tmax, tquery, c,
     g, g∗)
35:       ▷runs the model on a relaxed variant of input sequences compatible with the arguments
Ensure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False
Ensure: return is False if CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW(M, t) is False for any
    t with specified tmax, tquery, and c tokens not equal to tmax
36:    return MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗) < 0
37: end function





                                       45

Theorem 16. For all G,

     Et∼U(0,1,...,dvocab−1)nctx  argmax(M(t))i = max ti  ≥SUBCUBIC(dvocab, nctx, M, G)
                                                 i                         i

Proof. (sketch) Apply preceding lemmas and theorems to Algorithm 6

Theorem 17. The running time of Algorithm 6, after using caching to avoid duplicate computations,
is O(dvocab2dmodel + dvocab2nctx2).

Proof. (sketch) Sum the complexities indicated along the right side of Algorithm 3.  The
dvocab2dmodel term comes from the precomputing EVOU, EU, and EQKP. The dvocab2nctx2
term comes from the softmax over nctx tokens for O(dvocab2nctx) pessimized pure sequences. Con-
firming that none of the complexities on the right side exceeds O(dvocab2dmodel + dvocab2nctx2)
completes the proof.



G  Subcubic proof strategies

In this section, we present a number of proof strategies that we use to reduce the computational cost
of the proof, ultimately driving down the cost of EU and EQKE verification to O(dvocabdmodel),
while unfortunately leaving the cost of EVOU verification at O(dvocab2dmodel).

The three main tricks we cover are the mean+diff trick (Appendix G.1), the max row-diff trick
(Appendix G.2.2), and the rank one / rank two SVD decomposition of EQKE (Appendix G.2.3).
While the mean+diff trick is useful for getting slightly better bounds, the SVD decomposition of
EQKE is the place where we get to insert the most understanding (without which we’d have no
hope of non-vacuous bounds below O(dvocab2dmodel)), and the max row-diff trick is the workhorse
that allows us to drive down the error term computations from cubic to quadratic without getting
completely vacuous bounds.


G.1  The mean+diff trick

Suppose we have quantities fx,y and gy,z and we want to pessimize (WLOG, suppose minimize) the
quantity fx,y + gy,z over x, y, and z in time less than O(nxnynz), say we allow O(nxny + nynz +
nxnz). Also suppose the variation of f over the y axis is much larger than the variation of f over the
x-axis.

We can of course say
                       min fx,y + min gy,z ≤fx,y + gy,z
                                    x,y          y,z

But we can do better!

Note that
                                 fx,y = Exfx,y + (fx,y −Exfx,y)

Suppose that fx,y varies much less over x than it does over y, and much less than gy,z varies over
either of y and z. This will make the following bound a good approximation, though the bound is
sound even without this assumption. We can write

                     fx,y + gy,z ≥minx,y,z[fx,y + gy,z]
                 = x,y,z[Exfx,ymin    + gy,z + fx,y −Exfx,y]
                   ≥minx,y,z[Exfx,y + gy,z] + x,y,z[fx,ymin    −Exfx,y]
                 = min[Exfx,y + gy,z] + min[fx,y −Exfx,y]
                                         y,z                      x,y

By averaging the variation over certain axes, we have


                                       46

Theorem 18 (Mean+Diff).
             min fx,y + gy,z ≥min[Exfx,y + gy,z] + min[fx,y −Exfx,y]
                     x,y,z                  y,z                      x,y
            max fx,y + gy,z ≤max [Exfx,y + gy,z] + max [fx,y −Exfx,y]
                     x,y,z                  y,z                       x,y

and the RHSs can be computed in time O(nxny + nynz + nxnz) for nx, ny, and nz the number of
possible values of x, y, and z, respectively.

Example for how this helps with small variation:

Take any function k(y) and then take

                                     fx,y := k(y) + ε1(x, y)
                                       gy,z := −k(y) + ε2(y, z)

Then we have
                            x,y,z[fx,ymin   + gy,z] = x,y,z[ε1(x,min      y) + ε2(y, z)]
                min fx,y + min gy,z = min k(y) + min −k(y) + min ε1(x, y) + min ε2(y, z)
                         x,y          y,z          y           y              x,y               y,z
                      = min k(y) −max k(y) + min ε1(x, y) + min ε2(y, z)
                                               y           y            x,y               y,z
min[fx,y −Exfx,y] + min[gy,z + Exfx,y] = min ε1(x, y) + min[ε2(y, z) + Exε1(x, y)]
 x,y                       y,z                      x,y               y,z

If ε1 and ε2 are small compared to miny k(y) −maxy k(y), then using Exfx,y gives a much better
bound.

Note, though, that this could be a worse bound if the assumption of small variation does not hold.
Note also that this trick is not restricted to adding and subtracting Exfx,y. If f is a matrix indexed
by x and y, we might also try taking SVD and using the first principal component instead. A basic
application of the triangle inequality gives the following, more general, result:
Theorem 19 (Summarize+Diff). For any hy which can be computed in time O(nh),
                 min fx,y + gy,z ≥min[hy + gy,z] + min[fx,y −hy]
                          x,y,z                  y,z                 x,y
               max fx,y + gy,z ≤max [hy + gy,z] + max [fx,y −hy]
                          x,y,z                  y,z                  x,y

and the RHSs can be computed in time O(nxny + nynz + nh) for nx, ny, and nz the number of
possible values of x, y, and z, respectively.

We see that if the variation of f in the x-axis is indeed much smaller than the variation in the y-axis,
then letting
                                          fx,y = hy + εx,y
we get

                 min fx,y + gy,z −min[hy + gy,z] −min[fx,y −hy]
                           x,y,z                  y,z                 x,y

            ≤  x,y,z[fx,ymin   + gy,z] −miny,z [hy + gy,z] + minx,y [εx,y]

            ≤2 max |εx,y|
                               x,y

so indeed this bound isn’t too much worse and we are able to compute it in quadratic rather than
cubic time.

G.2  Details of SVD of QK proof

As discussed in Section 4.3.1, to further reduce the computation cost of proof, we need to avoid
computing the residual stream, EVOU, and EPQKE matrices fully. Using mechanistic insight or
otherwise, we observe that these matrices (apart from EVOU) can be well-approximated by rank one
matrices. This will remove the dominant computation cost of O(dvocab2 · dmodel).


                                       47

G.2.1  Comments on relationship between mechanistic insight and proof size

Up to this point, we haven’t really said much in our proofs about what the model is doing. All the
mechanistic insight has been of the form “the model varies more along this axis than this other axis”
or “the input data is distributed such that handling these inputs is more important than handling these
other inputs” or, at best, “the model computes the answer by attending to the maximum token of the
sequence; everything else is noise”.

Here, finally, our proof-size constraints are tight enough that we will see something that we could
plausibly call “how the model pays attention to the maximum token more than anything else”, i.e., (if
we squint a bit) “the model pays more attention to larger tokens in general.

G.2.2  The max row-diff trick

As stated above, we are breaking matrices into their rank one approximation and some error term. To
bound the error, i.e. to bound expressions of the form Qi(Ai + Ei) −Qi Ai, where Ei denote the
matrix errors, we can use the following trick:
Lemma 20 (Max Row-Diff (vector-matrix version)). For a row vector a and a matrix B,
               max ((aB)i −(aB)j) ≤ X |ak| max (Bk,i −Bk,j)
                                  i,j                                             i,j
                                               k

Moreover, for a collection of n row vectors Ar, if the shape of B is m × p, the right hand side can be
computed for all r in time O(nm + mp).

Proof.
                 max (aB)i −(aB)j
                                       i,j
              = max X ak (Bk,i −Bk,j)
                                           i,j
                                 k
              ≤ X max ak (Bk,i −Bk,j)
                                                i,j
                             k
                                  maxi,j (Bk,i −Bk,j)   if ak ≥0              = X ak
                                    mini,j (Bk,i −Bk,j)    if ak < 0
                             k
                                  maxi,j (Bk,i −Bk,j)      if ak ≥0              = X ak
                             −maxi,j (Bk,i −Bk,j)   if ak < 0
                             k
              = X |ak| max (Bk,i −Bk,j)
                                                      i,j
                             k

The asymptotic complexity of computing the result follows from caching the computation of
maxi,j (Bk,i −Bk,j) for each k independently of r, as the computation does not depend on Ar.

Theorem 21 (Max Row-Diff). For matrices A and B,
           max ((AB)r,i −(AB)r,j) ≤max X |Ar,k| max (Bk,i −Bk,j)
                         r,i,j                              r                    i,j
                                                   k

Proof. By taking the max of Lemma 20 over rows r of A.

Lemma 20 can also be applied recursively for a product of more than two matrices.
Lemma 22 (Max Row-Diff (vector-matrix recursive version)). For a row vector a and a sequence of
n matrices Bp of shapes rp × cp,
       !       ! 
max   a Y Bp  −  a Y Bp  X |ak0| · · · X (Bn−1)kn−1,kn max ((Bn)kn,i −(Bn)kn,j)  i,j             ≤                                                   i,j
            p         i        p        j       k0          kn

Moreover, for a collection of q row vectors Aα, the right hand side can be computed for all α in time
O(qr0 + Pp rpcp).


                                       48

Proof. We proceed by induction on n.

For n = 1, the statement is identical to Lemma 20.

Suppose the theorem holds for all positive n = s; we show the theorem holds for n = s + 1. We
reassociate the matrix multiplication as

      s+1  !       s+1  ! 
max   a Y Bp  −  a Y Bp  i,j               
          p=1        i      p=1       j
         s+1  !     s+1  ! 
= max    Y Bp  − Y Bp       i,j (aB1)             
                  p=2        i    p=2       j

Using the induction hypothesis gives

≤ X X ak0(B1)k0,k1 X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j
                                                                                            i,j
    k1   k0                 k2                ks+1

The triangle inequality gives

≤ X X |ak0(B1)k0,k1| X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j
                                                                                            i,j
    k1  k0                  k2                ks+1

and algebra gives

= X |ak0| X |(B1)k0,k1| X |(B2)k1,k2| · · · X (Bs)ks,ks+1 max (Bs+1)ks+1,i −(Bs+1)ks+1,j
                                                                                              i,j
    k0        k1              k2                ks+1

The asymptotic complexity of computing the right hand side also follows straightforwardly by
induction.

Theorem 23 (Max Row-Diff (recursive)). For a sequence of n + 1 matrices A0, ..., An,

      !       ! 
max
 r,i,j    Y Ap   − Y Ap  ≤maxr X |(A0)r,k0| · · · X (An−1)kn−1,kn maxi,j  ((An)kn,i −(An)kn,j)
          p         r,i      p         r,j            k0               kn

Proof. By taking the max of Lemma 22 over rows r of A0.

Note that Theorem 21 is compatible with the mean+diff trick of Appendix G.1.
Theorem 24 (Combined Mean+Diff and Max Row-Diff). For matrices A and B, and any column-
wise summary vector Hk of A (for example we may take Hk := ErAr,k)

                              !
max ((AB)r,i −(AB)r,j) ≤  max X Hk (Bk,i −Bk,j) +max X |Ar,k −Hk| max (Bk,i −Bk,j)
 r,i,j                                         i,j                                r                             i,j
                                    k                            k

Proof.

       max ((AB)r,i −(AB)r,j)
                r,i,j
      = max X Ar,k (Bk,i −Bk,j)
                    r,i,j
                   k
      = max X (Hk + (Ar,k −Hk)) (Bk,i −Bk,j)
                    r,i,j
                   k
                                         !
      = max X Hk (Bk,i −Bk,j) + max X (Ar,k −Hk) (Bk,i −Bk,j)
                     i,j                                 r
                     k                           k


                                       49

                      !
      ≤  max X Hk (Bk,i −Bk,j) + max X max (Ar,k −Hk) (Bk,i −Bk,j)
                        i,j                                 r           i,j
                     k                             k
                      !
      ≤  max X Hk (Bk,i −Bk,j) + max X |Ar,k −Hk| max (Bk,i −Bk,j)
                        i,j                                 r                             i,j
                     k                             k


Theorem 25 (Combined Mean+Diff and Vector-Matrix Recursive Max Row-Diff). For a row vector
a, a vector of summaries h corresponding to a (for example, if a is a row of a matrix, h might be the
average of the rows), a sequence of n matrices Bp of shapes rp × cp, and a corresponding sequence
of column-wise summary vectors hp of Bp (for example we may take (hp)k := Er(Bp)r,k),
        !       ! 
  max   a Y Bp  −  a Y Bp     i,j               
              p         i        p        j
                                 !
  ≤max X hk0 · · · X(Bn−1)kn−1,kn ((Bn)kn,i −(Bn)kn,j)
         i,j
              k0         kn
                                     !
   + X |ak0 −hk0| · · ·   max X(hn−1)kn ((Bn)kn,i −(Bn)kn,j)
                                               i,j
          k0                        kn
                                                !
                + X (Bn−1)kn−1,kn −(hn−1)kn max ((Bn)kn,i −(Bn)kn,j)
                                                                                                 i,j
                               kn

Moreover, for a collection of q row vectors Aα, the right hand side can be computed for all α in time
O(qr0 + Pp rpcp).

Proof sketch. Apply the triangle inequality recursively, fusing the proofs of Lemmas 22, and 24.

Theorem 26 (Combined Mean+Diff and Recursive Max Row-Diff). For a sequence of n+1 matrices
A0, . .., An, and corresponding column-wise summary vectors h0, ..., hn−1 of A0, ..., An−1,
       !       ! 
max Y Ap   − Y Ap r,i,j              
           p         r,i      p         r,j
                                  !
≤max X(h0)k0 · · · X(An−1)kn−1,kn ((An)kn,i −(An)kn,j)
     r,i,j
            k0           kn
                                        !
  + X |(A0)k0 −(h0)k0| · · ·   max X(hn−1)kn ((An)kn,i −(An)kn,j)
                                                     i,j
        k0                              kn
                                                   !
                  + X (An−1)kn−1,kn −(hn−1)kn max ((An)kn,i −(An)kn,j)
                                                                                                       i,j
                                    kn

Moreover, if the matrices Ap have shapes rp × cp, the right hand side can be computed in time
O(Pp rpcp).

Proof. By taking the max of Theorem 25 over rows r of A0.

G.2.3  Exploring rank one approximation via SVD

Let us first look at
                     EQKE := EqQKT ¯ET .

From Figure 9a, we see that there is not much variation along long query token direction. We can
confirm this by performing a singular value decomposition (SVD) on EQKE, as seen in Figure 14.


                                       50

                      Query-Side SVD                                         Singular Values                                      Key-Side SVD
     0                                                                                              0
                                                                                                                                                   20
                                                  7000
    10                                                                                             10                                              15
                                                  6000
                                                                                                                                                   10
    20                                                                                             20
  Token 30                                            50004000                                                                                                          Token 30                                              50                                                                                                                     Value
  Query 40                                            3000                                                               Key 40                                    −5
                                                  2000                                                                               −10
    50                                                                                             50
                                                                                                                            −15
                                                  1000
    60                                                                                             60                                       −20
                                                     0
       0     10    20    30    40    50    60           0    10    20    30    40    50    60           0     10    20    30    40    50    60
                         Singular Index                                          Singular Index                                          Singular Index

Figure 14: SVD of EQKE for seed 123, with principal component vectors scaled by the square root of the
corresponding singular value. This scaling allows us to see visually that there is not much going on beyond the
first singular component. Numerically: the first singular value is just over 7440, while the second singular value
is just under 15.


The first singular value is just over 7440 (7800 ± 380 across all seeds), while the second singular
value is just under 15 (13.1 ± 2.8 across all seeds). The ratio across all seeds is 620 ± 130. There’s
really not much going on here beyond the first singular component.23

Call the first singular component of EQKE the “query direction” dq and the “size direction” dk on
the query-side and key-side, respectively.

There are two ways that we can decompose EQKE into a low-rank component that we can compute
exactly, and a full-rank error term that we approximate bounds for.

G.2.4  The simple SVD decomposition of QK

In time O(dvocabdmodel2) we can perform SVD on each of the four component matrices Eq, Q, K,
¯E and perform low-rank SVD on the matrix product EqQKT ¯ET .

We can then bound the difference between two elements in the same row of EQKE by computing
exactly the difference between the two elements in the same row of the rank one approximation of
EQKE, and adding to that a bound on the difference between the two elements in the same row of
the error term.
That is, we can decompose E into a part parallel to dq and a part orthogonal to dq, say Eq = Eq +E⊥q ,
and similarly ¯E = Ek + E⊥k . Note that Eq and Ek are both rank one, and hence can be multiplied
with other matrices of shape dmodel × a in time O(dmodela) rather than time O(dvocabdmodela).
Hence we can define EQKE_err1 (subscript one for “rank one”) and decompose EQKE as

                EQKE = EqQKT (Ek)T + EQKE_err1.

Define for any vector v

                                    ∆i,jv := vi −vj

so that we get

               ∆i,j(EqQKT (Ek)T )tquery + min ∆i,j(EQKE_err1)tquery
                                                                  i̸=j
                              ≤∆i,jEQKEtquery ≤
               ∆i,j(EqQKT (Ek)T )tquery + max ∆i,j(EQKE_err1)tquery
                                                                  i̸=j

Then we may use any method we please to pessimize ∆i,j(EQKE_err1)tquery quickly. For example,
since for any matrix M we have σ1(M) = supx ∥Mx∥/ ∥x∥, considering vectors with one 1, one

  23We might be tempted to keep analyzing the SVD, and notice that the query direction is mostly uniform,
while the key direction is monotonic (nearly linear, even). But the proof complexity doesn’t demand this level of
analysis, yet, and so we can’t expect that any automated compact proof discovery system will give it to us.


                                       51

                                   0                                                   1.00

                                                                                                                                     0.75

                                                                                                                                     0.50
                                                                       20
                                                                                                                                                                         token                                                       0.250.00
                                                                                                                                                                         query 40                                            −0.25

                                                                                                                   −0.50

                                                                                                                   −0.75
                                                                       60
                                                                                                                   −1.00
                                                                         0     10    20    30    40    50    60
                                                                                              key token

                         Figure 15: The error term EQKE_err for seed 123.


−1, and zero elsewhere, the maximum difference between elements in a row upper bounded by√
  2σ1(M):
                          √
                       ∆i,jEQKE_err1tquery ≤  2σ1(EQKE_err1)                     (11)

G.2.5  The complicated SVD decomposition of QK

While the “most mechanistic” interpretation would proceed with the analysis in terms of Eq and
Ek, perhaps decomposing them further, we can get more bang for our buck by extracting out all the
low-rank structure available E, Q, and K, so as to make our error bounds as tight as possible.
To this end, we perform SVD on E⊥q , E⊥k , Q, and K and peel off the first singular components so as
to get the decomposition
                           Eq = Eq + Eq,2 + E⊥q,2
                                         ¯E = Ek + Ek,2 + E⊥k,2
                   Q = Q0 + Q⊥
                  K = K0 + K⊥

Then EQKE, a product of these four matrices, can be expressed as a sum of 2232 −1 = 35
rank one products and one high-rank error term. We can compute the sum of the rank one
products in time O(dvocab2) and express EQKE as, say, EQKE2 + E⊥q,2Q⊥(E⊥k,2K⊥)T .  Call
the second term EQKE_err (Figure 15). We must now bound for each q and m the quantity
maxi≤m−G EQKE_err[q, i] −EQKE_err[q, m].

How big is this?

Even if we relax to maxi,j EQKE_err[q, i] −EQKE_err[q, j], the maximum such value across all
rows is under 1.85 (1.99 ± 0.68 across all seeds). And the rows don’t have any particular structure to
them; the maximum absolute element of the entire matrix is just barely over 1 (1.12 ± 0.40 across all
seeds), so doubling that doesn’t give too bad an estimate.

But we somehow need to compute this value without multiplying out the four matrices.

One option is to try to use singular value decomposition again. Since σ1(M) = supx ∥Mx∥/ ∥x∥,
considering vectors with one 1, one −1, and zero elsewhere, the maximum difference between                 √
elements in a row upper bounded by  2σ1(M). The largest singular value of EQKE_err (Figure 16)
is just under 7.6 (8.4 ± 2.0 across all seeds), giving a row-diff bound of about 10.7 (11.8 ± 2.8 across
all seeds), which is large but not unusably so.

If we perform SVD before multiplying out the matrices (Figure 17), however, their first singular                                              √
values are about 4, 1.4, 1.4, and 4, giving a product of about 30, which when multiplied by  2
is about 43. (Across all seeds, these numbers are 3.79 ± 0.12, 1.525 ± 0.067,√  1.513 ± 0.073, and
3.78 ± 0.12, giving a product of about 33.1 ± 2.9, which when multiplied by  2 is about 46.8 ± 4.2.)
This works because σ1(AB) ≤σ1(A)σ1(B), but note that we can do factored SVD without needing
to use this technique. This bound is still usable, but pretty big.
Can we use Frobenius? Note that using anything close to this method to drop below dvocabdmodel2
might seem infeasible (it’ll eventually turn out not to be). For example, the best bound we know
on the largest singular value that can be verified even in the worst-case in strictly less time than


                                       52

                                 U                                      Singular Values for EQKE_err                           V
                        0                                                                                        0

                       10                                            7                                           10
                                                                     6
                       20                                                                                       20
                                                                     5
                       30                                            4                                           30

                       40                                            3                                           40

                       50                                            2                                           50

                                                                     1
                       60                                                                                       60
                                                                     0
                          0     10    20    30    40    50    60        0    10    20    30    40    50    60        0     10    20    30    40    50    60

                            Figure 16: SVD of EQKE_err for seed 123.



                         U                                         Singular Values for E⊥q,2                             V                                      U                                         Singular Values for Q⊥                              V
            0                                                 4.0                                            0                                                    0                                                 1.4                                            0
           10                                                 3.5                                            5                                                    5                                                 1.2                                            5
           20                                                 3.0                                                                                                     10                                                   10                                                 1.0                                           10
                                                                 2.5
           30                                                 2.0                                           15                                                   15                                                 0.8                                           15
           40                                                 1.5                                           20                                                   20                                                 0.6                                           20
           50                                                 1.0                                           25                                                   25                                                 0.4                                           25
                                                                                                                                                                                                                                        0.2                                                                 0.5
           60

               0     5     10    15    20    25    30                                                                 0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30           30  0     5     10    15    20    25    30                                                                                                                                                                                                                                        0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30

                         U                                         Singular Values for K⊥                              V                                      U                                        Singular Values for E⊥k,2                             V
            0                                                 1.4                                            0                                                    0                                                 4.0                                            0

            5                                                 1.2                                            5                                                    5                                                 3.5                                           10
           10                                                 1.0                                           10                                                   10                                                 3.0                                           20
           15                                                 0.8                                           15                                                   15                                                 2.52.0                                           30
           20                                                 0.6                                           20                                                   20                                                 1.5                                           40
           25                                                 0.4                                           25                                                   25                                                 1.0                                           50
                                                                 0.2
           30

               0     5     10    15    20    25    30                                                                 0.0   0     5    10    15    20    25    30    30  0     5     10    15    20    25    30           30  0     5     10    15    20    25    30   0.50.0   0     5    10    15    20    25    30    60  0     5     10    15    20    25    30

   Figure 17: SVD of the four component matrices of EQKE_err for seed 123. Matrices look like noise.


it takes to compute the full SVD is the Frobenius norm, which is defined as tr(MM T ), can be
computed in dmodeldvocab time, and is equal to the square root of the sum of the squares of the
singular values. While the Frobenius norm of EQKE_err is only about 12 (giving a bound of about
17 on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over 10, 4, 4,
and 10, giving a product of 1932 and a bound of 2732(!). (Across all seeds, the Frobenius norm of
EQKE_err is about 13.1 ± 1.9 (giving a bound of about 18.6 ± 2.7 on the row-diff), the Frobenius
norms of the four multiplicand matrices are a bit over 9.92 ± 0.19, 4.43 ± 0.01, 4.361 ± 0.095, and
9.85 ± 0.19, giving a product of 1888 ± 99 and a bound of 2670 ± 140.) This is unusably large.

However, we can get a much better bound on the max row-diff of EQKE_err without having to
multiply out all four matrices. We can use an approach vaguely similar to the mean+diff trick, as
follows.

If we want to compute the max row-diff of a product of matrices AB, we can compute by Theorem 21

           max ((AB)r,i −(AB)r,j) ≤max X |Ar,k| max (Bk,i −Bk,j)             (12)
                         r,i,j                              r                    i,j
                                                   k

or by combining this approximation with Theorem 18 via Theorem 24 we may compute

     max ((AB)r,i −(AB)r,j)
           r,i,j
                      !
    ≤  max X ErAr,k (Bk,i −Bk,j) + max X |Ar,k −ErAr,k| max (Bk,i −Bk,j)
                   i,j                                     r                                  i,j
                 k                                 k

taking whichever bound is better.

The first gives us a bound of 7.94 on the maximum row-diff, which is better than we can get by doing
SVD on the product of the matrices! We can get an even better bound by peeling off the first two
singular values of all four matrices before multiplying them; this gives us a bound of 5.67. Combining
it with the avg+diff trick wouldn’t give us much (8.05 and 5.66 respectively), as we’ve effectively
already done this by peeling off the leading singular contributions; the mean of EQKE_err over
dimension zero has norm 0.025 (0.030 ± 0.012 across all seeds).

Although this error bound is no longer the leading asymptotic bottleneck, we can peek ahead to what
we get if we want to be linear in parameter count. In this case, we can apply the recursive version of
Equation 12 via Theorem 23, giving a bound of 97.06 on the maximum row-diff.


                                       53

The mechanistic understanding we get here is roughly “for any given basis vector of the residual
stream, the difference between the overlap of any two input tokens with this direction is small once
we factor out the first two singular components”, and this is sufficient to drive a low error term overall
if we factor out the leading singular components in other places. We don’t mechanistically understand
how to combine the EqQKT (without multiplying them out) in a way that allows getting a good
bound, though, which corresponds to our inability to drop below dvocabdmodel2 here.

If we use this trick on QK only, and use the mean+diff trick on final attention handling (without
which we lose about 19 %), we can achieve a bound of 0.7840 (0.661 ± 0.035 across all seeds).

If we use this trick on the skip connection (EU) only, we can achieve a bound of 0.6768 (0.632±0.061
across all seed).

Using this trick on both EU and QK drops us down only to 0.6354 (0.601 ± 0.060 across all seeds).

If we use this trick on EU and use the recursive version of this trick on QK, we get a bound of 0.2927
(0.281 ± 0.036 across all seeds).

Unfortunately, it’s not clear how this trick would apply to EVOU. A fancier convex hull checking
algorithm seems required, and an analysis thereof is in progress.

G.3  The algorithm

We now put all of these tricks together into the subcubic algorithm Algorithm 7, which is the full
version of Algorithm 6. The format we give here is parameterized over the summarization strategy
(from Theorem 19 in Appendix G.1), the decomposition of EQKE, and the handling of EQKE_err
and EU.





                                       54

Algorithm 7 Counting Correct Sequences in Subcubic Time
  1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP(M, tmax, tquery, c, g, g∗)
Ensure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False =⇒result is False
Require: 0 ≤g∗≤g ≤tmax
Require:  if c = 0 then tquery = tmax
  2:    skip-scoret∗←SUMMARIZEEU,tquery(ℓEU(tquery)t∗)                  ▷Cache by t∗
  3:     skip-score ←maxt∗ℓEU(tquery)t∗−mint∗ℓEU(tquery)t∗            ▷Cache by tquery
  4:     vt ←EVOU(t)
  5:    wi ←PVOU(i)
  6:   ∆wmax,t∗←maxi wi,t∗−wi,tmax                           ▷Cache by tmax, t∗
  7:    ∆wmax,max ←maxt∗∆wmax,t∗                               ▷Cache by tmax
  8:    ∆vt ←maxt∗vt,t∗−mint∗vt,t∗                                 ▷Cache by t
  9:    ∆vmax ←max0≤t≤tmax−g∗∆vt                           ▷Cache by tmax −g∗
10:    ∆vtmaxt∗   ←vtmax,t∗−vtmax,tmax                                ▷Cache by tmax
11:    ∆vtmaxmax ←maxt∗̸=tmax ∆vtmaxt∗                                  ▷Cache by tmax
12:       if c = 0 then
13:      ℓt∗←ℓEU(tmax)t∗+ vtmax,t∗+ ∆wmax,t∗
14:        return maxt∗̸=tmax (ℓt∗−ℓtmax)
15:    end if
16:     b:,nctx−1 ←EQKP(tquery, nctx −1)                           ▷Cache by tquery
17:     b0,:−1 ←SORT(EQKP(tquery, : −1))                         ▷Cache by tquery, i
18:     b1,:−1 ←REVERSE(b0,:−1)
19:    EQKE(1), EQKE_err ←DECOMPOSE(EQKE)
Require: EQKE(1)(tquery, t) −EQKE(1)(tquery, tmax) −EQKE_errtquery ≤EQKE(tquery, t) −
   EQKE(tquery, tmax) ≤EQKE(1)(tquery, t) −EQKE(1)(tquery, tmax) + EQKE_errtquery
20:     at ←EQKE(1)(tquery, t)                                  ▷Cache by tquery, t
21:     amin,t ←min0≤t′′≤t at′′         ▷Cache by tquery, t, compute in amortized O(dvocab2)
22:     amax,t ←max0≤t′′≤t at′′        ▷Cache by tquery, t, compute in amortized O(dvocab2)
23:    ∆amax ←atmax −amin,tmax−g + EQKE_errtquery          ▷Cache by tquery, tmax, c
24:    ∆amin ←atmax −amax,tmax−g −EQKE_errtquery          ▷Cache by tquery, tmax, c
25:     idx-set ←{0, . . . , nctx −c −1} if tmax ̸= tquery else {0, . . . , nctx −c −2, nctx −1}
26:     attn-weights-unscaled0,i ←b0,i + (∆amin if i ∈idx-set else 0)
27:     attn-weights-unscaled1,i ←b1,i + (∆amax if i ∈idx-set else 0) ▷Cache by tquery, tmax, i,
    c                           √
28:     attn-weights0 ←SOFTMAX(attn-weights-unscaled0/ √ d)    ▷Cache by tquery, tmax, i, c
29:     attn-weights1 ←SOFTMAX(attn-weights-unscaled1/  d)    ▷Cache by tquery, tmax, i, c
30:     attn-max0 ←Pi∈idx-set attn-weights0,i
31:     attn-max1 ←Pi∈idx-set attn-weights1,i
32:     attn-max ←attn-max0 if ∆vtmaxmax ≥∆vmax else attn-max1
33:     attn-max ←SUMMARIZEattn,tquery(attn-max)                   ▷Cache by tmax, c
34:     attn-max′ ←attn-max −attn-max
35:   summaryt∗←∆wmax,t∗+ skip-scoret∗+ attn-max∆vtmaxt∗  + (1 −attn-max)∆vmax    ▷
   Cache by tmax, t∗
36:    return skip-score + attn-max′ · ∆vtmaxmax + (−attn-max′)∆vmax + maxt∗̸=tmax summaryt∗
37: end function





                                       55

H  Comparison of proof strategies

In this section, we compare the various proof strategies that we have developed in Appendix G. We
do some traditional mechanistic interpretability analysis to justify that the choices that we made could
be expected to lead to reasonably good bounds in Appendix H.1. We then compare the complexities
and performance of various proof strategies in Appendix H.2 to line up with the legends of Figures 3,
and 4. We close with a figure relating the various categories of proof strategies.


H.1  Justification of pessimization choices

In Sections 4.3, F, and G we make a number of choices about which axes of variation are more or
less important to track at various points in the bound computation.

Here we do some more traditional mechanistic interpretability analysis to justify that the choices that
we made could be expected to lead to reasonably good bounds.


H.1.1  Justifying the gap

We take advantage of the fact that attention is mostly monotonically increasing in input integers and
that for most sequences, the attentional contribution of the particular query token matters much more
than the particular non-max token in the sequence.

We justify this as follows.

We can look at the typical diff, when attending to the max token, between the largest non-max logit
and the max logit. As shown in Figure 18a, the largest difference between an off-diagonal entry of
EVOU and the diagonal of that row is typically at most −7.24 The typical worst contribution to the
wrong logit from a non-max token (this is typical over non-max tokens, worst over choice of output
token-logit index) is around 43, as shown in Figure 18b.

The difference in attention between tokens is approximately linear in the gap between the tokens, as
seen in Figure 19. The slope of the line, that is, the difference in pre-softmax attention scores divided
by the gap between the key token and the max token, is approximately 1.2.

Exponentiating, the post-softmax attention paid to the max is typically about 3× larger than to the
token one below the max; here the logit difference between the max and non-max token is significant,
typically being around 13 (43/3) for the worst output logit. But by the time the gap is 3, this difference
has dropped to about 1.1, and by the time the gap is 4 it is around 0.3.

   24“Typically” here means about 96 % of the time.



               ·106
          7
          max
          6                                                                    20.0
                given                                                                       17.5          5
             with                                                                       15.0
          4
                                                                                                                                                                            count 12.510.0          3                             sequences                                                                                     7.5
   #          2
   ×                                                                           5.0
                count 1                                                                        2.5
          0                                                                        0.0
             −10   −5      0      5      10                          30  35  40  45  50  55  60  65  70
                                     logit - diag                                                                           logit diff

(a) The attention computation weighted by the number  (b) Histogram of the maximum difference between
of sequences with the particular max. The computation  two logits contributed by a single row of EVOU.
is maxj s.t. j̸=i EVOUi,j −EVOUi,i. µ ± σ: −9.9 ±  The computation is, for each i, maxh EVOUi,j −
2.1; range: (−0 ± 120) × 10−1                   minj EVOUi,j. µ ± σ: 43.4 ± 9.0; range: 52 ± 20

  Figure 18: Plots of the difference in logit for the attention computation, EVOU := ¯EV OU for seed 123.


                                       56

                                                                                           ·109
                                                                        max 1.0
           75
                                                                                    0.8           50                                                                                                given
           25                                                                             with 0.6                     difference
            0
                                                                                    0.4        −25                                                                                                                                                                              sequences
                        #                   attention −50
                                  ×  0.2
        −75                                                                                                count                                                                                    0.0
             −60  −40  −20   0    20    40    60                       −2      0       2       4       6
                               token gap                                                    attention difference / gap
                √                       √
        (a) (EQKEi −EQKEj)/  d vs. i −j         (b) (EQKEi −EQKEj)/(  d(i −j)), weighted by
                                                 sequence count. µ ± σ = 1.22 ± 0.13

Figure 19: Plots of attention difference vs. token gap, for EQKE := EqQKT ¯ET for seed 123. The difference
in attention between tokens is approximately linear in the gap between the tokens.


                                                                   ·106

                                                     6

                                                     5

                                                     4                                                                                                                                                                                                                              sequences
                         # 3                         ×
                                                                                                                            count 2

                                                     1

                                                     0
                                                         1   2   3   4   5   6   7   8   9
                                                                            gap

Figure 20: Histogram of the minimum gap between the max token and the largest non-max token, for the seed
123.



So for sequences where the largest non-max and the max are close together, the particular structure
of the non-max EVOU matters a lot; but when the max is separated from the largest non-max by a
modest gap, the structure of the non-max EVOU does not matter so much.

The upshot is that to handle most sequences, we need only ask an oracle for the minimum gap g > 0
between the max token tmax and largest non-max tokens t′ ̸= tmax, such that the model outputs the
correct answer for all sequences where the non-max, non-query tokens have value at most tmax −g.

While computing this gap may be expensive (and indeed the naïve computation of the oracle takes
longer than the brute-force proof—though it should be very easy to optimize), we don’t have to
pay the cost of computing the gap in the size of the proof, only the cost of storing the gap table
(O(dvocab2nctx)) and of verifying the gap. Empirically, gaps are typically 1–5, as seen in Figure 20.

If we rely on the gaps, this results in leaving behind about 6.9 % of sequences.

Picking up more sequences In this paragraph / bulleted list, we sketch out how we might go about
picking up more sequences to get a tighter bound. This is not coded up, and is left as future work.
We propose computing the following quantities:

        • First, we could build in time (O(dvocab2)) a table indexed on pairs (t, tmax) of the maximum
        token and a non-maximum token: the table would store pessimal logit contributions from
          t to maximum output tokens ≤the tmax parameter. The table could be further split to
        pessimize separately for tokens within and outside of the gap window.

        • Compute a table of pre-softmax attention differences between tokens t and t + 1 in time
         (O(dvocab2)).

        • Next sort the queries by overlap with the query direction.


                                       57

                                  E⊥q,2                                  Q⊥

                         200                                                 120
                                                   N(−0.00, 0.22)                                     N(−0.01, 0.14)
                         175
                                                                             100
                         150
                                                                              80
                         125
                                                        count 100                                                                                                                  count  60

                          75
                                                                              40
                          50
                                                                              20
                          25

                           0                                                   0
                               −0.6 −0.4 −0.2  0.0  0.2  0.4  0.6  0.8            −0.4    −0.2      0.0       0.2       0.4
                                               matrix element value                                        matrix element value

                               K⊥                                  E⊥k,2

                         120
                                                   N(−0.00, 0.14)          200                            N(0.00, 0.22)

                         100                                                 175

                                                                             150
                          80
                                                                             125
                                                        count  60                                                                                                                   count 100

                          40                                                  75

                                                                              50
                          20
                                                                              25

                           0                                                   0
                              −0.4    −0.2      0.0       0.2       0.4               −0.6 −0.4 −0.2  0.0   0.2   0.4   0.6   0.8
                                               matrix element value                                        matrix element value

Figure 21: The distribution of entries of the four residual matrices (after removing two principal components
from Eq and ¯E and one principal component from Q and K). Distributions look pretty close to normal. Plots
are for the seed 123.


        • Compute for each number of queries handled (where we assume we handle all queries with
         greater overlap than the current one) and for each maximum input token tmax, how many
        of the query tokens tquery fall strictly below the max tmax (and whether or not the model
        succeeds when tmax = tquery). This will tell us how many query tokens we can count for a
        given maximum token.
        • Compute a table indexed on pairs of # of queries handled and input tokens t which stores
         the smallest difference in more attention paid to t + 1 than to t (O(dvocab2)).
        • Compute a table indexed on pairs tmax, t storing an upper bound on amount more attention
        paid to non-maximum tokens than to tmax by Oracle-permitted query tokens (the Oracle is
        indexed only on tmax) (O(dvocab2)).
        • For each # queries permitted: compute for each tmax, t, c, if the non-maximum token t
         contributes little enough to incorrect logits that even with the worst skip connection the
       model still gets the correct answer.

H.1.2  Stopping after 1–2 principal components of QK

Did we miss out on any structure in the error term of EQKE? The distribution of entries of the four
matrices looks pretty close to normal as seen in Figure 21.





                                       58

  Bound  1.0                                                                                             brute force (acc: 0.9992 ± 0.0015)
     0.8                                                                                        cubic (rel acc: 0.9539 ± 0.0080)
     0.6                                                                                      subcubic (rel acc: 0.821 ± 0.013)
                                                                                                                               2 (rel acc: 0.795 ± 0.014)   Accuracy  0.4                                                                                             attention-dvocabdmodel
                                                                                                         direct-quadratic (rel acc: 0.653 ± 0.060)
     0.2
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.628 ± 0.060)
     0.0                                                                                                attention-quadratic (rel acc: 0.390 ± 0.032)   Normalized                    225              231              237              243                     attention-quadratic, direct-quadratic (rel acc: 0.286 ± 0.036)
                         FLOPs to Verify Proof (approximate)
  Bound  1.0                                                                                             brute force (acc: 0.9992 ± 0.0015)
     0.8                                                                                        cubic (rel acc: 0.9539 ± 0.0080)
     0.6                                                                                      subcubic (rel acc: 0.821 ± 0.013)
                                                                                                                               2 (rel acc: 0.795 ± 0.014)   Accuracy  0.4                                                                                             attention-dvocabdmodel
                                                                                                         direct-quadratic (rel acc: 0.653 ± 0.060)
     0.2
                                                                                                     attention-dvocabdmodel 2, direct-quadratic (rel acc: 0.628 ± 0.060)
     0.0                                                                                                attention-quadratic (rel acc: 0.390 ± 0.032)   Normalized                    225              231              237              243                     attention-quadratic, direct-quadratic (rel acc: 0.286 ± 0.036)
                         FLOPs to Verify Proof (approximate)

Figure 22: Recreations of Figure 3 for ease of viewing of the legend. Top is a strict recreation; bottom includes
points not on the Pareto frontier.


If we replace the entries of E⊥q,2, E⊥k,2, Q⊥, and K⊥with randomly sampled values, we get (sample
size 100) that the maximum row-diff of the product of the matrices is approximately 1.31 ± 0.13
(sampling without replacement from the empirical distribution) or 1.31 ± 0.14 (sampling from the
normal distribution). So in fact our max row-diff is unusually high (by about 4σ).25

H.2  How various combinations of tricks perform

Recall Figures 3 and 4 on page 8 and on page 9 from Section 5, recapitulated here without captions
for convenience as Figures 22, and 23.

We describe what each subcubic proof strategy in the legend means. Note that all subcubic proof
strategies (that is, all proof strategies except for “brute force” and “cubic”) use the quadratic counting
algorithm of Appendix F.

H.2.1  Proof strategies grouped by complexity

In Figures 3, and 22, proof strategies are grouped by computational complexity.

The 102 proof strategies break down into 1 + 1 + 2 × 5 × 10 × 2 strategies.

The brute force and cubic proofs (1 + 1) were fully covered in Appendices D, and E.

There are 5 options for handling EU:

direct-quadratic refers to handling EU in time O(dvocabdmodel) with either the max row-diff trick
(Appendix G.2.2)26 or the max row-diff trick fused with mean+diff or some other summary statistic
(Theorem 24)27.
When direct is not mentioned, this indicates that we handle EU in time O(dvocab2dmodel) by first
multiplying out EqU and then either taking the maximum row-diff in each row28 or by taking the
maximum row-diff across all rows29. The latter is included purely for comparison’s sake, and never
gives a tighter bound than the former.

There are 10 options for handling the high-rank attention error term EQKE_err:

   25This shows up in the bias towards having larger values (both positive and negative) in the lower-right
corner of the plot, indicating that errors are larger for larger query and key values. We hypothesize that this is
due to the distribution of data: larger values are more likely to have more space between the maximum and
next-most-maximum token, so a bit of noise matters less for larger maxes than for smaller ones.
   26This strategy is labeled “max_diff” in the Python source code.
   27These strategies are labeled “mean_query+max_diff” and “svd_query+max_diff” in the Python source
code
  28“max_diff_exact”
  29“global_max_diff_exact”


                                       59

     1.0
  Bound  0.8                                                                                           max-diff-exactmean+max-diff-subproduct
                                                                                               max-diff-subproduct
     0.6                                                                                        max-diff
                                                                                       mean+max-diff   Accuracy
     0.4                                                                                   svd
                                                                                              mean+max-diff-subproduct-recursive
     0.2                                                                                           max-diff-subproduct-recursive
                                                                                                 mean-recursive+max-diff-subproduct-recursive   Normalized
     0.0
                400               600               800              1000
                        EPQKE Singular Ratio: σ1/σ2

                  Figure 23: Recreation of Figure 4 for ease of viewing of the legend.


attention-quadratic refers to handling the high-rank attention error term EQKE_err from Ap-
pendix G.2.5 in time O(dvocabdmodel) either with the recursive max row-diff trick (Theorem 23)30 or
with the recursive max row-diff trick fused with the mean+diff trick either just on the query side31 or
throughout32 (Theorem 26).
attention-dvocabdmodel2 indicates that we use one of the various O(dvocabdmodel2) strategies for
handling   √ EQKE_err1 from Appendix G.2.4 or EQKE_err from Appendix G.2.5. These include
using  2σ1—computed via low-rank SVD—as the bound (Equation 11)33, considering all ways of
multiplying out a subset of the matrices and taking the maximum row-diff of the resulting pair of
matrices34 (Theorem 21), or fusing the max row-diff trick with the mean+diff trick35 (Theorem 24).

When attention is not mentioned, this indicates that we handle the attention error term in time
O(dvocab2dmodel), either by taking the per-row maximum row-diff36 or by using the full rank EQKE
matrix and taking the per-row maximum row diff37.

Finally, note that in combining the rank one attention computation with EVOU, PVOU, and EU, we
may either use the mean+diff trick38 (Appendix G.1) or not39; this makes up the final factor of 2.

H.2.2  Proof strategies grouped by attention handling

This section slightly reorganizes the information just covered in Appendix H.2.1, for convenience
of legend correspondence. Here we group by the strategy used to handle the attention error term.
Strategies that involve using the full rank EQKE matrix are elided. The dashed descriptors here
correspond to underscore-joined descriptors in footnotes of Appendix H.2.1.
max-diff-exact (O(dvocab2dmodel)) corresponds to taking the full rank EQKE_err1 term and taking
the maximum row-diff in each row.

mean+max-diff-subproduct (O(dvocabdmodel)) corresponds to fusing the max row-diff trick with
the mean+diff trick (Theorem 24) and considering all ways of associating the multiplication of
EQKE_err.

max-diff-subproduct (O(dvocabdmodel)) corresponds to using the max row-diff trick (Theorem 21)
and considering all ways of associating the multiplication of EQKE_err.
max-diff (O(dvocabdmodel2)) corresponds to using the max row-diff trick (Theorem 21) on the
factored SVD of EQKE_err1.
mean+max-diff (O(dvocabdmodel2)) corresponds to fusing the max row-diff trick with the mean+diff
trick (Theorem 24) and applying it on the factored SVD of EQKE_err1.

  30“max_diff_subproduct_recursive”
  31“mean+max_diff_subproduct_recursive”
  32“mean_recursive+max_diff_subproduct_recursive”
  33“svd”
  34“max_diff” for EQKE_err1, “max_diff_subproduct” for EQKE_err
  35“mean+max_diff” for EQKE_err1, “mean+max_diff_subproduct” for EQKE_err
  36“max_diff_exact”
  37“exact_EQKE+max_diff_exact”
  38“mean_query+diff”
  39“drop_average_query_per_output_logit_reasoning”


                                       60

                      √
svd (O(dvocabdmodel2)) corresponds to using  2σ1—computed via low-rank SVD—as the bound
(Equation 11).

mean+max-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the high-rank
attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick fused
with the mean+diff trick on the query-side only (Theorem 26, taking all but the first summary vector
to be zero).

max-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the high-rank attention
error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick (Theorem 23).

mean-recursive+max-diff-subproduct-recursive (O(dvocabdmodel)) corresponds to handling the
high-rank attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick
recursively fused with the mean+diff trick (Theorem 26).

H.2.3  What understanding do we get from each proof strategy?

Throughout most of this paper, we talk about doing mechanistic interpretability and using understand-
ing to allow more compact proofs to have tighter bounds. We can also look at the reverse problem:
we can take a collection of proof strategies, check by brute force which strategies give the tightest
bounds for each model, and ask what this implies about how that model works. We do this here.

In general, which proof methods perform best is an indication of where structure exists in
the model.   For example,  in quadratic EU proofs, when max_diff performs worse than
mean_query+max_diff and svd_query+max_diff, this indicates that E has a relatively strong
behavioral component shared across query tokens that U is not that good at filtering out. Sim-
ilarly, when, e.g., mean_recursive+max_diff_subproduct_recursive performs better than
max_diff_subproduct_recursive, this indicates that even after removing the first one or two
principle components from Eq, Q, K, and ¯E, there is still enough common structure that it is worth
factoring out the mean behavior.





                                       61