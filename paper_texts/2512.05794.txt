          Mechanistic Interpretability of Antibody Language Models Using SAEs



                    Rebonto Haque1  Oliver Turnbull1  Anisha Parsan2,3  Nithin Parsan2
                                   John J. Yang2   Charlotte M. Deane1
                               1Department of Statistics, University of Oxford, UK   2Reticular, San Francisco, USA
            2Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (MIT), Cambridge, MA, USA
                                                    Correspondence: deane@stats.ox.ac.uk

                         Abstract                           (heavy) chain, and V and J for the VL (light) chain–give rise
                                                                              to the final sequence diversity within the VH/VL chains. So-
              Sparse autoencoders (SAEs) are a mechanistic                                                                   matic hypermutations, characterised by random nucleotide2025          interpretability technique that have been used to                                                                              substitutions occurring at rates considerably higher than the
               provide insight into learned concepts within large                                                           genomic background, in the joint V(D)J segment further
                protein language models. Here, we employ TopK                                                                          increases sequence diversity (Andreano & Rappuoli, 2021).Dec        and Ordered SAEs to investigate an autoregres-
                 sive antibody language model, p-IgGen, and steer       The ability to bind any target antigen with high specificity
5
                    its generation. We show that TopK SAEs can re-        and affinity makes antibodies ideal candidates for drug dis-
               veal biologically meaningful latent features, but         covery. As a result, antibody drugs hold a major and grow-
              high feature–concept correlation does not guar-         ing share of the total pharmaceutical market (Crescioli et al.,
               antee causal control over generation. In contrast,        2025). Antibody drug development pipelines need to iden-
              Ordered SAEs impose an hierarchical structure           tify candidates which bind specifically and with high affinity
                  that reliably identifies steerable features, but at the          to the target antigen, while also being ‘developable’ (Jarasch[cs.LG]
              expense of more complex and less interpretable          et al., 2015). ‘Developability’ refers to properties required
                activation patterns. These findings advance the          for a successful drug such as immunogenicity, solubility,
               mecahnistic interpretability of domain-specific           specificity, stability, manufacturability, and storability (Ray-
                protein language models and suggest that, while        bould & Deane, 2022).
           TopK SAEs suffice for mapping latent features                                                                Antibody language models have been used to optimise mul-
                to concepts, Ordered SAEs are preferable when                                                                                  tiple steps of antibody-drug development pipelines from
                precise generative steering is required.                                                                             library generation (Turnbull et al., 2024) to humanisation
                                                                      during lead optimisation (Chinery et al., 2024). p-IgGen is a
                                                                   GPT-like decoder-only model trained on antibody-sequence
          1. Introduction                                              data, consisting of 17M parameters (Turnbull et al., 2024).
                                                         The authors released a paired model, as well as a finetuned
          Antibodies are a key part of the body’s adaptive immune
                                                                         version capable of generating diverse antibody libraries with
           response. They are characterised by their ability to bind to a
                                                                    developable properties.
            specific antigen and subsequently neutralise it or initiate an
        immune response. Their extensive sequence—and therefore   The lack of interpretability of machine learning models con-arXiv:2512.05794v1
           structural—diversity enables binding to virtually any target     tributes to a lack of trust in model predictions, difficulty
           antigen (Chiu et al., 2019).                                  determining whether biologically relevant features are being
                                                                  used to make predictions and difficulty detecting overfitting.
        The antigen-binding domain of antibodies is made up
                                                                              Collectively, these pose a barrier when employing language
           of variable heavy (VH) and variable light (VL) chains.
                                                             models for drug discovery (Chen et al., 2023). SAEs offer
          Antibody-antigen binding specificity and affinity are largely
                                                                  a promising approach to identify human-interpretable con-
          determined by structural units known as complementarity-
                                                                       cepts learned by models and steer their generation (Chen
          determining regions (CDRs), with each VH and VL chain
                                                                                 et al., 2025; Templeton et al., 2024).  Prior works have
          having 3 CDRs, making a total of 6. Within our genome,
                                                                used SAEs to understand the inner mechanisms of PLMs
           there are numerous V, D, and J gene segments which to-
                                                       (Adams et al., 2025; Parsan et al., 2025b; Simon & Zou,
           gether code for the VH and VL chains.
                                                                    2024), and steer model output. However, to date, SAEs
        The combinatorial assembly of the discrete gene seg-    have not been used to interrogate autoregressive protein or
         ments–V (variable), D (diversity), and J (joining) for the VH


                                                         1

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

antibody-specific language models.                                                                   ˆx = Wdec z + bdec                  (2)

This work advances the interpretability of antibody language                                                    where W are the weight matrices and b are the bias vectors,
models, using SAEs to identify biologically relevant fea-                                                       enc and dec denote the encoder and decoder respectively,
tures of interest learned by p-IgGen, and predictably steer
its generation. We identify antibody-specific features, such   x is the original hidden representation, z the latent repre-
as the complementarity-determining region (CDR) identity    sentation, and ˆx the reconstructed hidden representation.
and germline gene identity, and use them to steer p-IgGen   ReLU activation is applied to the latent representation fol-
generation for specific germline gene identities. Overall,    lowing encoding and g is a sparsification function.
this work shows the applicability of SAEs for incorporat-
ing rational design principles to antibody library generation.    2.2.1. TOPK SAES
We show that TopK SAEs can accurately identify inter-
                                            TopK SAEs (Gao et al., 2024) limit the number of active
pretable latents underpinning model generation, whereas
                                                                     latents to k, where k ≪din ≪dsae. din is the input hidden
Ordered SAEs can identify steerable features capable of
                                                          dimensions, and dsae is the latent or dictionary dimensions.
tuning model generation.
                                                        Equation 3 shows the loss computation.
2. Related Work                                         L(x) =  ∥x −ˆx∥22  +       c             (3)
                                                                                                            Reconstruction|  {z  }loss      Sparsity|{z}constraint
2.1. Mechanistic Interpretability
                                                The L(x) reconstruction loss compares the decoded repre-
Mechanistic interpretability refers to the approach of ex-    sentation ˆx with the original hidden representation x. When
plaining complex machine learning systems through the    a sparsification function is not directly applied during encod-
behaviour of their functional units (K¨astner & Crook, 2024)     ing, a separate sparsity constraint is added in loss computa-
by decomposing or reverse-engineering systems into their     tions, which is usually a variation of an L1 regularisation
more elementary computations (Rai et al., 2025). The even-    loss (Zhang et al., 2018).
tual goal is to discover causal relationships between model
inputs and corresponding outputs.                            2.2.2. ORDERED SAES (O-SAES)

Within the context of transformer-based language models,                                                     Ordered SAEs follow a nested SAE architecture, enabling
there are three main ideas relevant for mechanistic inter-                                                                  hierarchical ordering of SAE latents. Importantly, compared
pretability research: features, circuits and universality (Rai                                                                to the traditional TopK SAE architecture which arbitrarily
et al., 2025). Features refer to human-interpretable proper-                                                            orders hierarchical latents within the dictionary space, O-
ties that are encoded by model activations (Templeton et al.,                                            SAEs enforce a strict, consistent, hierarchical ordering of
2024).  Circuits inform how these features are extracted                                                                        latents. This is because TopK SAEs enforce sparsity within
from model inputs and processed to influence model out-                                                             the entire dictionary space in one go, whereas O-SAEs
puts (Olah et al., 2020b). Finally, universality determines                                                          follow a nested approach and effectively train a number of
whether features and circuits identified for a specific model                                                                   individual, nested SAEs which occupy an increasing portion
and task exist in other models and tasks (Olah et al., 2020a).                                                             of the dictionary space.
This paper specifically focuses on the identification of fea-
tures from language models and using these features to steer   O-SAEs introduce two core components:  (i) per-index
model generation.                                          nested grouping, and (ii) strictly decreasing truncation
                                                          weights in order to ensure consistent ordering.
2.2. Sparse Autoencoders                                              (i) For each truncation level m ∈{1, . . . , dsae}, the first m
                                                     rows of the encoder and decoder are isolated:Sparse Autoencoders (SAEs) have specifically been em-
ployed in mechanistic interpretability for feature discovery.            (m)                   (m)                              W enc = [Wenc]1:m, :, Wdec = [Wdec]1:m, :    (4)
They are able to tackle the issue of feature superposition
resulting in polysemantic neurons, where any given neuron    In Eq. (4) the encoder–decoder pair W enc(m) , W dec(m)  re-uses
encodes multiple, often unrelated features. SAEs overcome    the first m rows of the full weight matrices. Because every
this problem by projecting dense neuron activations into    smaller autoencoder is a strict subset of the larger one, any
a sparser latent space using a sparse encoder, Equation     latent i ≤m is shared across all groups that follow. This
1, whilst ensuring the latent representation can be recon-   “per-index nested grouping” forces early latents to model
structed back into the original neuron representation by a    global structure that remains useful for every deeper stage.
decoder following sparsification, Equation 2.                 Per-index grouping ensures non-random sampling of dic-
                                                               tionary sizes, unlike in Matryoshka SAEs (Bussmann et al.,
            z = g(ReLU Wenc x + benc )            (1)    2025), increasing the overall consistency of results.

                                                2

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

(ii) Each partial reconstruction is weighted by a monoton-    used expansion factor r = 8, yielding a dictionary size
ically decreasing probability pM(m), so that early (low-    dsae = din × r = 768 × 8 = 6,144. Sparsity was again set
index) features incur a higher penalty when failing to cap-    to k = 32, ensuring the top 32 latents are used during recon-
ture coarse structure. The per-truncation loss is                 struction. All models were trained with Adam optimisers
                                                  2             at a fixed learning rate η = 1 × 10−4. We chose a smaller
      Lm(x) = pM(m)  x −W dec(m)⊤ W enc(m) x        (5)   maximum dictionary size for the O-SAEs to speed up train-
                                                  2
                                                                 ing, effectively reducing the total number of nested SAEs
and summing over all m promotes the model to learn the                                                         being trained. Due to per-index grouping, O-SAEs need to
most “abstract” elements first, with progressively finer de-                                                                      train several nested SAEs based on the total dictionary size,
tails later. Combining the decreasing probability weights                                                     whereas the regular TopK architecutre only trains a single
with nested latents further enforces ordering of identified                                                       model.
latents and maintains a stricter hierarchy.

                                                                    3.2. Targeted Feature Identification using SAEs
3. Methods
                                                             3.2.1. TRAINING DATA
3.1. Sparse Autoencoder Training
                                                           Paired antibody sequence data were obtained from OAS,
We  adapted  TopK  Sparse  Autoencoders  (SAEs)    Coronavirus Antibody Database (CoV-AbDab) (Raybould
from   the   EleutherAI/sparsify  GitHub   repository     et  al., 2021), and the Patent and Literature Antibody
(https://github.com/EleutherAI/sparsify),  and  Ordered    Database (PLAbDab) (Abanades et al., 2024). A total
SAEs from the Reticular GitHub repository (Parsan et al.,    of 149,069 sequences were obtained from the respective
2025b). Training parameters were taken directly from the     datasets, based on their binding specificities to SARS-CoV2
original repositories.                         RBD (binder and non-binder).

We trained both the TopK and Ordered SAEs on hidden   The data was clustered based on CDR sequence similarity
layer activations of p-IgGen, generated from the original    using CD-HIT (Li et al., 2001), with a 0.8 similarity thresh-
pIg-Gen training set (Turnbull et al., 2024). The training set    old on the total CDR sequence. The clusters were then
contained 1,800,545 VH/VL paired sequences from the Ob-   randomly split into the training-validation-test set, whilst
served Antibody Space database (OAS) (Olsen et al., 2022;    ensuring members of the same cluster were in only one of
Kovaltsuk et al., 2018). During training, we concatenated    the three possible splits. The splits were further stratified
the paired VH and VL sequences together, with appropriate    based on binding specificity to SARS-CoV2 RBD.
start and end tokens added, and passed them into p-IgGen to
                                                         This specific dataset was originally prepared for a separate
generate hidden activations. This generated 4 sets of hidden                                                                  project, and the SARS CoV2 RBD binding properties of
activations, one from each hidden layer. For Ordered SAE                                                              the antibodies are not relevant for this study. Qualitatively,
training, we randomly subsampled 100,000 sequences to                                                        a dataset of equivalent size randomly sampled from OAS
decrease training time.                                                         should produce the same results.

3.1.1. TOPK SAE                                  The following concepts were studied to identify associated
                                                                     latents: CDR identity, which refers to whether a given
p-IgGen input dimensions din = 768 were projected onto                                                            residue lies within a specific CDR region, and V/J gene
a higher-dimensional latent/dictionary size dsae, where                                                                     identity, which refers to the germline V or J gene segment
dsae = din × r = 768 × 32 = 24,576. r = 32 is the expan-                                                was used to code for the final antibody sequence. For the
sion factor. ReLU activation was applied to the projection,                                                           CDR-identity, the training matrix was the latent activations
z = ReLU Wenc x + benc  , followed by a Top-k sparsifi-                                                                for each residue. The CDR identity dataset had 7 classes
cation with k = 32, retaining only the top 32 activations                                                            (6 CDR identities and non-CDR regions). For sequence-
by magnitude. The resulting dictionary size was 24,576.                                                                    level concepts, V/J gene identity, the training matrix was the
Decoder weights were initialised as the unit-normalised                                              mean pool of the residue-level latent activations in a given
transpose of the encoder weights to stabilise training. Train-                                                          sequence.
ing used a batch size of 8 and Adam optimisers throughout,
                                2×10−4with a custom learning rate η = √                                                                          .                3.2.2. LINEAR PROBE                                       dsae/16,384

                                      We trained a logistic regressor to act as a linear probe on
3.1.2. ORDERED SAE
                                                             the training-validation data. A logistic regressor (LR) was
Ordered Sparse Autoencoders (O-SAEs) were adopted to     trained, employing 3-fold cross-validation grid search to
retain higher-level, abstract features within our latent space    optimise hyperparameter C. In logistic regression, C is
and hierarchically arrange the latents.  In our setup, we    the inverse of the regularisation strength: larger C applies

                                                3

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

less regularisation and can overfit, while smaller C applies    within the latents, we compared the accuracies of using
more regularisation and can improve generalisation. Cross-    latents and hidden neurons on residue and sequence-level
validation was done during training by randomly shuffling    property prediction tasks.
and splitting the training data into 3 cross-validation sets.
                                      We began the analysis by focusing on TopK SAEs trained
Correlation weights of all latents were stored and the latents
                                                  on the final layer activations (layer 3), as seen in previous
with the top 500 positive correlation weights were used for
                                                   works (Parsan et al., 2025a), since they contain the most
further validation.
                                                        complete representation of the sequence. Logistic regressor
                                                                  training on CDR identities using latent activations obtained
3.2.3. LATENT SELECTION
                                                        a validation accuracy of 0.99, while using hidden neuron
The top correlated latents were further validated on the     activations resulted in a validation accuracy of 0.98. This
validation set. Based on the strategy by Simon and Zhou    showed that residue-level CDR identity information is pre-
(Simon & Zou, 2024), the latent activations across the vali-    served in the activated latents.
dation set were normalised using MinMax scaling; for each
                                                            In order to check sequence-level features, we investigated
normalised latent, binary latent-on/latent-off labels using
                                                          germline gene predictions. We focused on heavy J genes for
activation thresholds of 0.1, 0.2, 0.5, 0.8, 0.9 were applied.
                                                                 simplicity, given they have less allelic variation compared
For each latent-concept pair, a latent was defined as an
                                                                to heavy V genes. LR training resulted in a validation F1
interpretable feature if its F1 score for any of the tested
                                                    macro score of 0.93. We used an F1 score due to significant
thresholds was greater than 0.5. At this boundary the har-
                                                                class imbalance in heavy J genes within the training and
monic mean guarantees both precision and recall are at least
                                                              validation sets. Table 1 reports the precision, recall and
0.5, ruling out latents that are either mostly false positives
                                                           F1-scores of the different gene identities studied and shows
or that miss the majority of true activations.
                                              how the model accurately predicts each IGHJ class using the
                                       SAE latents. Overall, the high F1 macro score shows how
3.2.4. ANTIBODY SEQUENCE ALIGNMENT
                                                              the activated latents also preserve sequence-level features.
Antibody sequences were aligned using ANARCI (Dunbar                                                 Our results indicate that SAE latents collectively represent
& Deane, 2016) and the IMGT numbering (Lefranc et al.,                                                           antibody information following sparsification when applied
2003).                                                                to an antibody language model, similar to results seen in
                                                            previous works for general protein language models (Simon
3.3. Steering                                 & Zou, 2024; Adams et al., 2025; Parsan et al., 2025a). This
Steering was implemented based on the strategy by Tem-     justifies using SAE latents for further analysis to investigate
pleton et al. (Templeton et al., 2024). Each latent can be     their overall interpretability.
represented by its corresponding decoder vector d(i) =
Wdec[i, :], where d(i) is the decoder vector for latent i and         Table 1. Precision, recall, and F1-score per IGHJ class.
Wdec is the decoder weight matrix. Steering is performed
                                                               Class            Precision   Recall   F1-score
by scaling the decoder vector and adding it to the original
hidden state (Equation 6).                                IGHJ1            0.91      0.71      0.80
                                                   IGHJ2            0.94      0.93      0.94
          hl∗←hl + α · d(i)                (6)       IGHJ3            0.96      0.96      0.96
                                                   IGHJ4            0.98      0.99      0.98
Here, α is the steering factor and hl is the hidden state                                                   IGHJ5            0.94      0.95      0.95
before the intervention and hl∗is the hidden state following                                                   IGHJ6            0.98      0.97      0.98
the intervention.
                                                    Macro average     0.95      0.92      0.93

4. Results

4.1. TopK SAE-identified Features are Interpretable,       4.1.2. TOPK LATENT ACTIVATIONS ARE VISUALLY
    Antibody-specific Concepts, but Not Steerable             INTERPRETABLE

4.1.1. TOPK SAE LATENTS PRESERVE BIOLOGICAL       To investigate whether SAE latents provide an interpretable
     INFORMATION FOLLOWING SPARSIFICATION          alternative to understanding model generation, we compared
                                                             the activated patterns of latents and neurons correlated to
TopK sparsification represents each token with far fewer la-
                                                              properties of interest. As a baseline, we compared activa-
tents compared to the hidden neurons, creating a possibility
                                                                 tions of the top correlated latents and neurons for CDRH3.
for information loss during sparsification. To qualitatively
check whether antibody-specific information is retained    Visual investigation revealed latent activations are sparse

                                                4

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

and specific to CDRH3 residues, compared to neurons    concepts rather than an undefined set of all possible ones.
which activate across the sequence without any immediately    Here, we examine gene identity, as it directly influences
recognisable pattern (Figure 1). This may be explained    antibody binding affinity and specificity(Deng et al., 2025).
by the polysemanticity of neurons, where multiple features     Specifically, we pick IGHJ4 genes for our analysis, due to
specific to several unrelated residues are represented by     their widespread clinical significance underpinned by the
the same neuron. When investigating activation patterns,    fact that they are the most widely utilised J genes in our
this complicates using neurons as a tool for interpretability   immune repertoire. That is, the majority of heavy chain
and highlights the potential greater explainability of SAE-    antibodies within any given individual originate from the
derived latents.                                     IGHJ4 germline gene. For our analysis, we decided to focus

In order to check the utility of SAE activations for the mech-
                                                IGHJ     Layer    Features (F1 > 0.5)  Max F1-scoreanistic interpretability of sequence-level concepts, as op-
posed to residue-level concepts, we investigated heavy J
gene activations as sequence-level concepts.  Similar to              Layer 0         197              0.930
residue-level observations, latents corresponding to heavy              Layer 1         266              0.949
                                                 IGHJ4
J gene identity activate on residues representing the con-              Layer 2         189              0.930
cept, i.e. gene identity. In this instance, the top correlated              Layer 3         85              0.752
latents were activated on the J domain of examined antibody
sequences. Sequence-level representations are mean pools    Table 3. IGHJ4 feature statistics. Latents which can be used as
of the original residue-level representations, leading to an    a binary predictor of gene identity with an F-score greater than
overall loss of positional information. Therefore, the top     0.5, based on threshold-activation patterns are termed as ’features’.
                                                             For each layer in p-iGgen, features corresponding to IGHJ4 werecorrelated latents also encode intrinsic positional informa-
                                                                              identified. The maximum F-score of all the identified features from
tion (Figure 1c). This provides an opportunity to identify
                                                               a given layer are reported as Max F-score.
the residues responsible for a global, sequence-level feature,
with potential implications in understanding the sequence                                                   on the final layer (Table 3).
and structural basis of antibody properties.
                                                                        First, we looked at the absolute positional activations of
To quantify the predictive properties of our identified fea-                                                                     this latent across all the sequences in our validation set
tures, we carried out an activation-threshold analysis (Table                                                   which had an IGHJ4 heavy J gene, and compared it to
2, Methods 3.2.3). Interestingly, there was a strong prefer-                                                                activations on specific IMGT positions. Whilst activations
ence for IGHJ4 features in the final layer, with no features                                                  on absolute positions were distributed near the end of the
being identified for IGHJ1 and 5. However, we identified                                                    heavy chain corresponding to the J region, the activations
features correlated to the two identities in earlier layers. This                                                   on IMGT positions were more consistent and concentrated.
highlighted a potential flaw in TopK SAEs; SAEs cannot                                                        This implies that the model does not base its activation
consistently generate monosemantic features for all the con-                                                                 pattern on the absolute sequence length alone, but rather the
cepts represented in the hidden state. Given latents could                                                           underlying sequence alignment (Figure 2).
collectively be used to predict IGHJ1 and IGHJ5 2, the
underlying information was represented within the latent   We then chose to investigate the specific residue identities on
space, just not in the form of a single monosemantic latent.    which the latents were activated. Based on the heavy J gene
                                                        sequence alignments, the top two latents activated at IMGT
                                                              positions 120 and 119, which are a Q and G, respectively.
Table 2. Layer 3 feature counts and maximum F-scores for IGHJ    These are conserved across all human IGHJ genes. The third
genes                                                             top latent activated on Y at position 117, which is unique for
    Gene   Number of features  Max F-score                                               IGHJ4 (Scaviner et al., 1999). These results indicated that
     IGHJ1                  0          0.366                                                            top latents encoded contextual information of the preceding
     IGHJ2                  5          0.521                                                                 residues.
     IGHJ3                 12          0.866
     IGHJ4                 85          0.752         Previous studies have highlighted how highly correlated
     IGHJ5                  0          0.486          features may be used to steer model outputs (Templeton
     IGHJ6                 17          0.866            et al., 2024; Simon & Zou, 2024). We attempted to steer on
                                                      each identified feature to investigate how it affects model
                                                              generation. We positively steered on each latent, which we
                                                         hypothesised should increase the proportion of IGHJ4 in
4.2. Case Study of Heavy J Gene Identity for IGHJ4
                                                          generated sequences. However, steering on these latents
The functional importance of SAEs for studying LLMs   was unpredictable and did not consistently increase IGHJ4
lies in their ability to interrogate specific, domain-relevant    proportions (Figure 3).

                                                5

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

(a)





(b)





(c)





Figure 1. Latent activations (a) and neuron activations (b) for CDRH3 identity, and latent activations for IGHJ3 (c). The x-axis shows the
amino-acid sequence of the VH region of a test antibody; the y-axis shows normalised activation. CDRs are coloured CDRH1 (red),
CDRH2 (blue), and CDRH3 (green). Latent activations localise to the expected regions—CDRH3 in (a) and the heavy J region in
(c)—whereas neuron activations (b) are scattered across the sequence with no discernible pattern.


To check if this phenomenon was somehow exclusive for    enforcing sparsity in a dictionary consisting of hierarchical
IGHJ4 and layer 3, we attempted to steer across all the     features. In this instance, if the identified latents correspond
layers for a number of different features for various gene     to only single residues within the J-domain, it essentially
identities, but were unable to predictably steer model gener-   becomes a residue-level feature as opposed to a sequence-
ation [data not shown]. The lack of steerability may indicate     level feature. If the feature activates on a residue specific
how these features individually do not contribute to the gene     to the gene identity, it may be a good predictor for the gene
identity, making them informative features when used for     identity, but not a steerable feature. This points to the pos-
downstream predictions, but not for biasing model output.      sibility that several features together confer J gene identity,
                                                    and that these features are likely correlated to each other.
This may be due to feature splitting (Chanin et al., 2024)
                                                       Hence, activating one but not the others does not necessarily
which has been reported for TopK SAEs. Feature splitting
                                                                     result in a predictable shift in model performance.
refers to the phenomenon where higher-order features are
broken down into specific contextual examples. In the case   The case study on IGHJ4 indicated that identified features re-
of text-based language models, ’math’ may be split into     tain biologically relevant context information. Most highly
’algebra’ and ’geometry’. These phenomena arise when    correlated features (based on LR correlation weight and f-


                                                6

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

   (a)                                                           (b)





Figure 2. Comparison of absolute positional (a) and IMGT (b) activations of top three IGHJ4 latents. The sequence/IMGT positions are
shown on the x-axis. For the sequence positions, the amino acid sequences were end-padded to a constant length of 350. Percentage of
total activations on any given position across validation IGHJ4 sequences is shown on the y-axis. The most frequent IMGT position for
activation is highlighted for each latent. Latent activations show a distribution near the end of the heavy chain when aligned based on
absolute sequence position. In contrast, latents demonstrate discrete activations when aligned based on IMGT numbering.


score) tend to be residue-specific. Targeted approaches such   One of the reasons behind the lack of steerability of TopK
as this cannot easily find abstract, higher-order features, as-    latents may stem from feature splitting (Chanin et al., 2024),
suming they are represented within the latent space to begin    as highlighted earlier. A solution may be to use nested
with. Concept-specific targeted feature identification might     architectures which limit feature splitting, allowing greater
identify highly correlated features that are biologically infor-    steerability (Chanin et al., 2024).
mative. For instance, two of the three top features (463 and
4720) activate on conserved residues preceded by sequence     4.3. Ordered SAEs Identify More Steerable Features
motifs specific to the gene identity. The third, Latent 6276      Compared to TopK SAEs
activated on an IGHJ4-specific residue, which may explain
                                                   Based on the previous case study, it is evident that the F1why this feature can be used to accurately identify IGHJ4.
                                                           score alone is not a good metric for predicting the impor-
Highly predictive features may be correlated with other    tance of a feature in informing model generation. Feature
biologically informative features. To understand whether    steering and ablation are likely to be better indicators, es-
highly predictive features influence model behaviour, we    pecially positively steering generation. Following unsuc-
tried to steer along these features to increase the proportion     cessful steering with TopK SAEs, we tested Ordered SAEs,
of IGHJ4 in generated sequences. This did not produce    which construct a hierarchical latent space capable of re-
predictable results, making it difficult to interpret the contri-    taining both high-level and fine-grained features (Bussmann
bution of each individual latent to model generation. Over-     et al., 2025).
all, TopK SAEs can identify features in targeted concept
                                      We conducted a linear probe and subsequent activation-analysis which are intuitively interpretable, however, not
                                                            threshold analysis to identify features correlated to IGHJ4necessarily steerable.
                                                                in layer 3. Due to the implicit hierarchy in features, we


                                                7

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

(a)                                                               features. We identified latent 12, which was positively cor-
                                                                related with IGHJ4, and latent 49, which was negatively
                                                                  correlated, and steered on these features (Figure 4).

                                                                 Positively steering on latent 12 increased IGHJ4 proportion
                                                                  in model generation (Pearson’s R = 0.939, p = 6.458×10−7
                                                    and Spearman’s correlation = 0.921, p = 2.982 × 10−6).
                                                             Conversely, positively steering on latent 49 decreased IGHJ4
                                                           proportion in model generation (Pearson’s R of -0.705, p-
                                                          value = 4.89 × 10−3 and Spearman’s correlation = -0.657,
                                                   p = 0.0106).

                                                To investigate the features, we plotted their activations on
                                                                   specific IMGT positions across a random set of IGHJ4 con-
                                                               taining sequences (Figure 5.) The latents activate more
                                                          broadly across the sequence, unlike TopK latents. One rea-
(b)
                                                        son for this may be the abstraction which enables steerability.
                                                The latents inform downstream generation, and therefore
                                                                 signal early on during sequence generation about the identity
                                                              of the J-region. Given the latents communicate longer-range
                                                            concepts, it is possible that latent activation is not linked
                                                                to any specific residue, rather a range of residues within a
                                                            defined length of the sequence.

                                                        5. Conclusions and Future Outlook

                                       We show that SAEs can successfully be used to evaluate au-
                                                                 toregressive antibody language models and identify learned
                                                            domain-specific features. Current SAE implementations on
                                           pLMs for identifying features are limited by the assumption
(c)                                                               that feature correlation is equal to biological concept cau-
                                                                  sation. Analysis of activated latents reveal high predictive
                                                       performance does not always correspond to steerability.

                                       We found that some latents correlated to gene identity sim-
                                                            ply mark conserved positions without a clear causal link to
                                                     gene identity. Some highly correlated features may high-
                                                                    light non-specific aspects of a concept, but be correlated
                                                                to more concrete features. When we attempted to steer p-
                                                  IgGen by amplifying these individual latents, the resulting
                                                           antibody libraries did not show a reliable increase in IGHJ4
                                                             usage, further underscoring that high predictive power alone
                                                             doesn’t guarantee steerability.

                                                       Ordered SAEs solve this by identifying hierarchical features
                                                     which correspond to more abstract and higher-level concepts
                                                                rather than simple residue-level identity.  However, this
Figure 3. Results of IGHJ4 feature steering for latent 463 (a), 4720
                                                 comes at the cost of intuitive interpretability of activation
(b), 6276 (c). Y-axis shows the proportion of generated sequences.
                                                                   patterns.Plots are coloured by heavy J gene identity. X-axis shows the
steering factor used (alpha). Results are for a library of 1000 p-   One of the major limitations of employing SAEs to antibody
IgGen-generated sequences. For each latent tested (a-c), steering                                                         language models is the lack of labelled datasets, unlike gen-
did not result in a predictable change in library composition.                                                                   eral proteins (Suzek et al., 2007). This prevents automated
                                                               feature identification and annotation, which has allowed
                                                                 for their easy application for general pLMs (Simon & Zou,
ranked features with an F-score > 0.5 based on their dictio-    2024). Lack of functionally annotated data remains a prob-
nary index, with smaller indices representing higher-level

                                                8

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

  (a)                                                        (b)





Figure 4. Results of IGHJ4 steering using Ordered latent 12 (a) and 49 (b). Y-axis shows the proportion of generated sequences. Plots
are coloured by heavy J gene identity. X-axis shows the steering factor used (alpha). Results are for a library of 1000 p-IgGen-
generated sequences. Latent 12—positively correlated with IGHJ4—increases IGHJ4 proportion under positive steering, whereas latent
49—negatively correlated—decreases IGHJ4 under the same steering.


lem within the domain of antibody language models.

Several open problems remain to enable the use of SAEs
for PLM interpertability. First, exhaustive steering analyses
should be conducted to quantitatively measure the steerabil-
ity of Top-K and Ordered SAE features. Secondly, SAE
training should be scaled using more expansive, annotated
datasets like FLAb (Chungyoun et al., 2024) to enable mod-
els to identify domain-specific abstractions and facilitate
targeted manipulation of generative outputs. This would
enable the generation of antibody libraries with properties
such as developability and specificity, improving modern li-
brary generation methods and optimising the antibody drug
development pipeline.





                                                9

                         Mechanistic Interpretability of Antibody Language Models Using SAEs





    (a)





    (b)





Figure 5. IMGT activations of latent 12 (a) and 49 (b). Activation patterns of both latents show scattered distribution across the range of
IMGT positions.





                                                10

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

References                                  nucleic-acids/abstract/S2162-2531(23)
                                               00039-2. Publisher: Elsevier.
Abanades, B., Olsen, T. H., Raybould, M. I. J., Aguilar-
  Sanjuan, B., Wong, W. K., Georges, G., Bujotzek, A.,    Chinery, L., Jeliazkov,  J. R., and Deane, C. M.  Hu-
  and Deane, C. M.  The Patent and Literature Anti-     match  -  fast,  gene-specific  joint  humanisation  of
  body Database (PLAbDab): an evolving reference set      antibody heavy and  light  chains, September 2024.
  of functionally diverse, literature-annotated antibody se-    URL  https://www.biorxiv.org/content/
  quences and structures.  Nucleic Acids Research, 52     10.1101/2024.09.16.613210v1.        Pages:
  (D1):D545–D551, January 2024.  ISSN 0305-1048.      2024.09.16.613210 Section: New Results.
   doi: 10.1093/nar/gkad1056. URL https://dx.doi.
  org/10.1093/nar/gkad1056. Publisher: Oxford    Chiu, M. L., Goulet, D. R., Teplyakov, A., and Gilliland,
  Academic.                                          G. L.   Antibody Structure and Function:  The Ba-
                                                                         sis for Engineering Therapeutics. Antibodies, 8(4):55,
Adams, E., Bai, L., Lee, M., Yu, Y., and AlQuraishi,     December 2019.  ISSN 2073-4468.   doi:  10.3390/
  M. From Mechanistic Interpretability to Mechanistic      antib8040055.  URL https://www.mdpi.com/
  Biology: Training, Evaluating, and Interpreting Sparse     2073-4468/8/4/55. Number: 4 Publisher: Multi-
  Autoencoders on Protein Language Models, Febru-       disciplinary Digital Publishing Institute.
  ary 2025.  URL https://www.biorxiv.org/
  content/10.1101/2025.02.06.636901v1.      Chungyoun,  M.,  Ruffolo,  J., and Gray,  J.   FLAb:
  Pages: 2025.02.06.636901 Section: New Results.           Benchmarking  deep  learning  methods  for   anti-
                                                    body  fitness  prediction,  January  2024.   URL
Andreano,  E. and  Rappuoli,  R.   Immunodominant                                            https://www.biorxiv.org/content/
  antibody  germlines  in COVID-19.    The  Journal                                             10.1101/2024.01.13.575504v1.        Pages:
   of Experimental Medicine, 218(5):e20210281, March                                                          2024.01.13.575504 Section: New Results.
  2021. ISSN 0022-1007.  doi: 10.1084/jem.20210281.
 URL https://www.ncbi.nlm.nih.gov/pmc/    Crescioli, S.,   , H´el`ene, K.,   , Lin, W.,   , Jyothsna, V.,   ,
  articles/PMC7933983/.                                 Vaishali, K., , and Reichert, J. M. Antibodies to watch
                                                                  in 2025. mAbs, 17(1):2443538, December 2025. ISSN
Bussmann, B., Nabeshima, N., Karvonen, A., and Nanda, N.                                                        1942-0862.    doi:  10.1080/19420862.2024.2443538.
  Learning Multi-Level Features with Matryoshka Sparse                                       URL  https://doi.org/10.1080/19420862.
  Autoencoders, March 2025. URL http://arxiv.                                              2024.2443538. Publisher: Taylor & Francis  eprint:
  org/abs/2503.17547. arXiv:2503.17547 [cs].                                                             https://doi.org/10.1080/19420862.2024.2443538.
Chanin, D., Wilken-Smith, J., Dulka, T., Bhatnagar, H., and
                                                   Deng, W., Niu, X., He, P., Yan, Q., Liang, H., Wang,
  Bloom, J. A is for Absorption: Studying Feature Split-
                                                                      Y., Ning,  L., Lin,  Z., Zhang,  Y., Zhao, X., Feng,
   ting and Absorption in Sparse Autoencoders, Septem-
                                                                      L., Qu,  L., and Chen, L.  An allelic atlas of im-
  ber 2024. URL http://arxiv.org/abs/2409.
                                                       munoglobulin heavy chain  variable regions  reveals
  14507. arXiv:2409.14507 [cs].
                                                           antibody  binding  epitope  preference  resilient  to
Chen, J.-Y., Wang, J.-F., Hu, Y., Li, X.-H., Qian, Y.-R.,     SARS-CoV-2 mutation  escape.    Frontiers  in Im-
  and Song, C.-L. Evaluating the advancements in protein      munology, 15:1471396, January 2025.  ISSN 1664-
  language models for encoding strategies in protein      3224.    doi:  10.3389/fimmu.2024.1471396.  URL
  function prediction: a comprehensive review. Frontiers     https://www.frontiersin.org/journals/
   in Bioengineering and Biotechnology, 13, January 2025.    immunology/articles/10.3389/fimmu.
  ISSN 2296-4185.  doi:  10.3389/fbioe.2025.1506508.     2024.1471396/full. Publisher: Frontiers.
 URL  https://www.frontiersin.orghttps:
                                                      Dunbar, J. and Deane, C. M. ANARCI: antigen receptor
  //www.frontiersin.org/journals/
                                                      numbering and receptor classification. Bioinformatics,
  bioengineering-and-biotechnology/
                                                          32(2):298–300, January 2016. ISSN 1367-4803.  doi:
  articles/10.3389/fbioe.2025.1506508/
                                                             10.1093/bioinformatics/btv552. URL https://doi.
  full. Publisher: Frontiers.
                                             org/10.1093/bioinformatics/btv552.
Chen, W., Liu, X., Zhang, S., and Chen, S.  Artificial
                                                    Gao,  L., Tour, T. D.  l., Tillman, H., Goh, G.,  Troll,   intelligence for drug discovery: Resources, methods,
                                                                  R., Radford,  A., Sutskever,  I., Leike,  J., and Wu,  and applications.  Molecular Therapy Nucleic Acids,
                                                                                 J.     Scaling  and  evaluating  sparse  autoencoders,  31:691–702, March 2023.  ISSN 2162-2531.   doi:
                                                        June 2024. URL http://arxiv.org/abs/2406.  10.1016/j.omtn.2023.02.019.  URL https://www.
                                                 04093. arXiv:2406.04093 [cs] version: 1.  cell.com/molecular-therapy-family/


                                                11

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

Jarasch,  A.,  Koll,  H., Regula,  J.  T., Bader, M., Pa-    com/doi/abs/10.1002/pro.4205.        eprint:
  padimitriou, A., and Kettenberger, H.  Developability       https://onlinelibrary.wiley.com/doi/pdf/10.1002/pro.4205.
  Assessment  During  the  Selection  of Novel  Ther-
                                                            Parsan, N., Yang, D. J., and Yang, J. J.  Towards Inter-  apeutic  Antibodies.     Journal  of  Pharmaceutical
                                                                   pretable Protein Structure Prediction with Sparse Autoen-  Sciences,  104(6):1885–1898,  June  2015.    ISSN
                                                               coders, March 2025a. URL http://arxiv.org/  0022-3549,  1520-6017.     doi:   10.1002/jps.24430.
                                              abs/2503.08764. arXiv:2503.08764 [q-bio]. URL    https://jpharmsci.org/article/
  S0022-3549(15)30084-8/abstract. Publisher:                                                            Parsan, N., Yang, D. J., and Yang, J. J.  Towards Inter-
   Elsevier.                                                                pretable Protein Structure Prediction with Sparse Au-
                                                              toencoders.  In ICLR 2025 Workshop on GenerativeKovaltsuk, A., Leem, J., Kelm, S., Snowden, J., Deane,
                                                     and Experimental Perspectives for Biomolecular Design  C. M., and Krawczyk, K. Observed Antibody Space: A
                                                  (GEM), 2025b. URL https://arxiv.org/abs/  Resource for Data Mining Next-Generation Sequencing
                                              2503.08764.  eprint: 2503.08764.  of Antibody Repertoires. The Journal of Immunology,
  201(8):2502–2509, October 2018.  ISSN 0022-1767.                                                             Rai,  D.,  Zhou,  Y.,  Feng,   S.,  Saparov,  A.,  and
   doi:   10.4049/jimmunol.1800708.   URL https:                                                         Yao,  Z.  A  Practical Review  of Mechanistic  In-
  //doi.org/10.4049/jimmunol.1800708.                                                                      terpretability for Transformer-Based Language Mod-
   eprint:    https://academic.oup.com/jimmunol/article-                                                                            els, March 2025. URL http://arxiv.org/abs/
  pdf/201/8/2502/61431152/ji1800708.pdf.                                              2407.02646. arXiv:2407.02646 [cs].
K¨astner, L. and Crook, B. Explaining AI through mech-
                                                      Raybould, M.  I.  J. and Deane, C. M.   The Thera-
   anistic interpretability. European Journal for Philoso-
                                                               peutic Antibody ProfilerTherapeutic antibody profiler
  phy of Science, 14(4):52, October 2024. ISSN 1879-
                                                    (TAP) for Computational Developability Assessment.
  4920. doi: 10.1007/s13194-024-00614-4. URL https:
                                                              In Houen, G. (ed.), Therapeutic Antibodies: Methods
  //doi.org/10.1007/s13194-024-00614-4.
                                                    and Protocols, pp. 115–125. Springer US, New York,
Lefranc, M.-P., Pommi´e, C., Ruiz, M., Giudicelli, V.,     NY, 2022. ISBN 978-1-0716-1450-1.  doi: 10.1007/
  Foulquier, E., Truong, L., Thouvenin-Contet, V., and      978-1-0716-1450-1 5. URL https://doi.org/10.
  Lefranc, G. IMGT unique numbering for immunoglob-    1007/978-1-0716-1450-1_5.
   ulin and T cell receptor variable domains and Ig super-
                                                      Raybould, M.  I.  J.,  Kovaltsuk,  A., Marks,  C., and
  family V-like domains. Developmental and Comparative
                                                       Deane, C. M.   CoV-AbDab:  the coronavirus an-
  Immunology, 27(1):55–77, January 2003. ISSN 0145-
                                                             tibody  database.     Bioinformatics,  37(5):734–735,
  305X. doi: 10.1016/s0145-305x(02)00039-3.
                                          May 2021.   ISSN 1367-4803,  1367-4811.    doi:
Li, W., Jaroszewski, L., and Godzik, A.  Clustering of      10.1093/bioinformatics/btaa739.    URL  https:
  highly homologous sequences to reduce the size of large     //academic.oup.com/bioinformatics/
  protein databases.  Bioinformatics (Oxford, England),     article/37/5/734/5893556.
  17(3):282–283, March 2001.  ISSN 1367-4803.  doi:
                                                            Scaviner, D.,  Barbi´e,  V., Ruiz, M., and Lefranc, M.-
  10.1093/bioinformatics/17.3.282.
                                                                          P.  Protein Displays of the Human Immunoglobulin
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,      Heavy, Kappa and Lambda Variable and Joining Re-
  M., and Carter, S. Zoom In: An Introduction to Cir-      gions. Experimental and Clinical Immunogenetics, 16
   cuits.  Distill, 5(3):10.23915/distill.00024.001, March      (4):234–240, November 1999. ISSN 0254-9670.  doi:
  2020a. ISSN 2476-0757. doi: 10.23915/distill.00024.001.     10.1159/000019115. URL https://doi.org/10.
  URL https://distill.pub/2020/circuits/     1159/000019115.
  zoom-in.
                                                      Simon, E. and Zou, J. InterPLM: Discovering Interpretable
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,      Features in Protein Language Models via Sparse Autoen-
  M., and Carter, S. Zoom In: An Introduction to Circuits.      coders, November 2024. URL http://arxiv.org/
   Distill, 2020b. doi: 10.23915/distill.00024.001.           abs/2412.12101. arXiv:2412.12101 [q-bio].

Olsen, T. H., Boyles, F., and Deane, C. M.  Observed    Suzek, B. E., Huang, H., McGarvey, P., Mazumder, R.,
  Antibody  Space:  A  diverse  database  of  cleaned,     and Wu, C. H.   UniRef:  comprehensive and non-
  annotated, and translated unpaired and paired  anti-      redundant UniProt reference clusters.  Bioinformatics,
  body sequences.    Protein Science,  31(1):141–146,      23(10):1282–1288, May 2007. ISSN 1367-4803. doi:
  2022.  ISSN 1469-896X.   doi:  10.1002/pro.4205.      10.1093/bioinformatics/btm098. URL https://doi.
 URL       https://onlinelibrary.wiley.    org/10.1093/bioinformatics/btm098.


                                                12

                         Mechanistic Interpretability of Antibody Language Models Using SAEs

Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken,      Deane, C. M.  p-IgGen:  a paired antibody genera-
   T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones,       tive language model.  Bioinformatics, 40(11):btae659,
  A., Cunningham, H., Turner, N. L., McDougall, C.,     November 2024.  ISSN 1367-4811.   doi:  10.1093/
  MacDiarmid, M., Freeman, C. D., Sumers,  T. R.,      bioinformatics/btae659. URL https://doi.org/
  Rees,  E., Batson,  J., Jermyn, A., Carter,  S., Olah,     10.1093/bioinformatics/btae659.
   C.,  and Henighan,  T.    Scaling Monosemanticity:
                                                     Zhang,  L.,  Lu,  Y., Wang,  B.,  Li,  F.,  and Zhang,  Extracting Interpretable Features from Claude 3 Sonnet.
                                                            Z.        Sparse   Auto-encoder   with  Smoothed  Transformer Circuits Thread, 2024.  URL https:
                                                             $$l 1$$Regularization.     Neural  Processing  Let-  //transformer-circuits.pub/2024/
                                                                          ters, 47(3):829–839, June 2018.  ISSN 1573-773X.  scaling-monosemanticity/index.html.
                                                                  doi:  10.1007/s11063-017-9668-5.  URL https:
Turnbull, O. M., Oglic, D., Croasdale-Wood, R., and     //doi.org/10.1007/s11063-017-9668-5.





                                                13