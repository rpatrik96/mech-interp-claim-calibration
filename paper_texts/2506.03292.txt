          HYPERSTEER: Activation Steering at Scale with Hypernetworks

                   Jiuding Sun*           Sidharth Baskaran*         Zhengxuan Wu
                  Stanford University            Pr(Ai)2R Group            Stanford University
             sunjd24@stanford.edu  Georgia Institute of Technology zhengxuan@stanford.edu
                                     sidnbaskaran@gmail.com

                 Michael Sklar            Christopher Potts             Atticus Geiger
                  Confirm Labs             Stanford University            Pr(Ai)2R Group
           michaelbsklar@gmail.com   cgpotts@stanford.edu      atticusg@gmail.com

                          Abstract                                             Original Generation                     Steering Vector

                                                                                                      Area and perimeter are
                                                                                                            fundamental                                                                                                                      concepts                                                                                                              in                   Steering language models (LMs) by modifying
                                                                                                         geometry:                                                                                                         the                                                                                                                       perimeter                                                                                                               is                   MLP
                    internal activations is a popular approach for               the total distance around a
                   controlling text generation. Unsupervised dic-                          shape…                          Add & Norm
                   tionary learning methods, e.g., sparse autoen-                  Steered Generation2025                                                                                                                                 FFN
                   coders, can be scaled to produce many steering                …Perimeter measures the total
                                                                                                          boundary length in units                  Add & Norm                    vectors, but lack guarantees on the individual ef-                   (e.g., meters). For a
                                                                                                        rectangle                                                                                    3 m                                                                                                      by                                                                                         4                                                                                                          m: P = 2×                     Masked                                                                                                                                                                                                            Multi-Head
                                                                                                           (3+4)                                                                                      =                                                                                                       14                                                                                                                m….                                                                                                                                                                                                                     Cross-attentionJun             ficacy of each vector and control over the cover-
                 age of relevant steering tasks. In contrast, super-
3
                   vised methods for constructing steering vectors                                                                                                                                Add & Norm                                                    ...
                   are targeted and effective, but require more data
                                                                                                                                                                       Masked Multi-Head                                                    ...                                        Self-attention                    collection and training for each additional steer-        ...
                  ing vector produced. In this work, we intro-             ...
                                                                                                                           Embed                 duce HYPERSTEER, a family of hypernetwork-             ...        N ⨉
                 based architectures which are trained end-to-[cs.CL]                                                                     Base LM                     HyperSteer
                end to generate steering vectors conditioned
               on the natural language steering prompts and               “Explain the basics of         “references to measurements,                                                                                                                               specifications, and                                                                                             area and perimeter.”
                   the internals of the steered LM. In our evalua-                                              characteristics of objects”
                      tions, we show that scaling HYPERSTEER with                     Base Prompt                          Steering Prompt
                 thousands of steering prompts exceeds the per-
                formance of state-of-the-art activation steering        Figure 1: The state-of-the-art HYPERSTEER Model: A
                 methods, even on steering prompts never seen        transformer hypernetwork uses self attention to process
                  during training. Moreover, HYPERSTEER per-       a steering prompt and uses a cross attention module to
                 forms on par with steering-via-prompting. We        read from the residual stream of a base LM run on a
                   release ours at § stanfordnlp/axbench.           second prompt. The hypernetwork outputs a steering
                                                                        vector that is added to the base LM residual stream.
          1  Introduction

         How can the outputs of a language model (LM) be    vised, e.g., Sparse Autoencoders (SAEs) (Hernan-
              reliably controlled? With instruction-tuned LMs,arXiv:2506.03292v1                                                      dez et al., 2022; Cunningham et al., 2023; Bricken
              the standard approach is prompt engineering. How-    et al., 2023; Gao et al., 2024; Marks et al., 2025),
               ever, prompting-based approaches face challenges                                                        which produce a large number of unlabeled steering
           from user jailbreaks, forgotten instructions, and                                                                     vectors, and supervised, where task-specific steer-
             robustness to model misalignment. A more aggres-   ing vectors are either directly trained (Wu et al.,
              sive approach is (parameter-efficient) fine-tuning,                                                           2024a, 2025) or derived using labeled data (Li
             but this requires modifying or injecting new pa-                                                                         et al., 2023a; Marks and Tegmark, 2023; Turner
             rameters into the model. Activation steering (Giu-                                                                         et al., 2023; Rimsky et al., 2024). Unsupervised
                lianelli et al., 2018) occupies a middle ground: it is   methods are scalable, but lack the ability to create
             lightweight to implement (no parameters are mod-                                                                    steering vectors for specific tasks. Alternatively, a
               ified or added) and can affect the model in ways                                                              supervised approach is effective and targeted, but
             standard prompting cannot.                                                                   requires per-task data collection and/or training.
             Methods for obtaining steering vectors can gen-                                                      Our main contribution is HYPERSTEER, a suite
              erally be classified into two groups:  unsuper-                                                                  of end-to-end trainable hypernetwork architectures
                  *Equal contribution.                            which learn to generate steering vectors for an

     0.8                                                                              Gemma-2-2b       Gemma-2-9b
                                                               Method                                            Steering via prompting                                               Held-out   Held-in   Held-out   Held-in
     0.7       HyperSteer
             No Context                                                 Prompting              0.762      0.731      1.091      1.075
     0.6          In Context Learning                                                 Fine-tuning
                 Cross Attention            Previous best Score                                      ReFT-r1 (Dictionary Learning)                  LoReFT                 –        0.722       –        0.777
     0.5                                                        SFT                    –        0.714       –         –
                                                        LoRA                   –        0.641       –        0.602
     0.4
                                                                               Activation Steering
     0.3                                                                   ReFT-r1                 –        0.509       –        0.630
                                                                       DiffMean                –        0.178       –        0.322  Performance
     0.2                                                   SAE                    –        0.151       –        0.191
                                                           SAE-A                  –        0.132       –        0.186
     0.1                                                        HYPERSTEER
                                                                      – No Context            0.373      0.512      0.633      0.751
         9                          500      1,862   5,458   15,571         – In Context Learning     0.480      0.547      0.760      0.842
               Number of Steering Prompts                      – Cross Attention         0.608      0.742      0.934      1.091

Figure 2: Performance on steering prompts that have    Table 1: Steering results for baseline methods from
never been seen during training for HYPERSTEER   AxBench and our three HYPERSTEER variants, evalu-
variants as the number of steering prompts used in train-    ated on Concept500-HO (steering prompts never seen
ing increases (x-axis in log scale). An exponential    during training) and Concept500-HI (steering prompts
increase in training data results in an approximately    seen during training, base prompts unseen during train-
linear increase in performance. When trained on all    ing). The cross attention variant outperforms all other
≈16k steering prompts in AxBench, the performance of    variants by a large margin on held-out evaluation and ap-
HYPERSTEER on steering prompts never seen during    proaches prompting performance on held-in evaluation,
training surpasses ReFT-r1 steering vectors which are    while all variants outperform the ReFT-r1 baseline on
trained and evaluated on the same steering prompt.        held-in evaluation. The intervention happened at Layer
                                               20 of both models.

instruction-tuned base LM, conditioned on the
steering prompt and (optionally) the base LM   Hypernetworks   Prior work has shown hypernet-
prompt and internal activations. We train and eval-   works to be effective at zero-shot adaptation of
uate on the steering prompts from AXBENCH (Wu   language models on a variety of tasks, matching
et al., 2025). Our best HYPERSTEER variant out-   or exceeding comparable methods such as fine-
performs the previous state-of-the-art activation    tuning with parameter efficient methods (Phang
steering method from AXBENCH, even on held-out    et al., 2023). Formally, a hypernetwork (Ha et al.,
steering prompts, never seen during training. We   2016) is a function H : T →Θ that maps tasks
show steering performance scales logarithmically    t ∈T to parameters θt = H(t).
with number of steering prompts during training.     For our purposes, the tasks are pairs on input
Surprisingly, we are even able to match the perfor-   and steering prompts (x, s) ∈T and the output
mance of the steering-via-prompting, which outper-   parameters are steering vectors:
formed all activation steering methods on AxBench.
In sum, HYPERSTEER combines the scalability of              H(x, s) = ∆xs ∈Rd.            (2)
unsupervised dictionary learning with the targeted
control of supervised activation steering.                                                Dataset and Evaluation  For all experiments, we
                                                use AXBENCH (Wu et al., 2025) as the training
2  Preliminaries
                                                     dataset and evaluation harness. The training data
Activation Steering  The goal of activation steer-    sets consist of a total of 16,000 steering prompts
ing is to elicit a certain behavior from a base LM B    sourced from GemmaScope Sparse Autoencoder
run on an base prompt by adding a steering vector   (SAE) feature labels (Lieberum et al., 2024) and
to a hidden vector h. Our hypernetwork-based ap-   base prompts sourced from a diverse instruction
proach to activation steering has the more general   pool (see App. A.5 for details). We keep fixed ra-
goal of taking any base prompt x, e.g., Explain how    tios of base prompt to steering prompt (72 : 1 for
to sort lists of numbers., and any steering prompt    train; 10 : 1 for evaluation).
s, e.g., Output using C/C++ programming syntax,     For evaluation, the steering prompts are applied
and producing a steering vector ∆xs added to h.   on a set of different base prompts sourced from
Denote the steered output:                        AlpacaEval (Li et al., 2023b), and a gpt-4o-mini
                                                judge model (OpenAI et al., 2024) computes dis-
            ˆysteer = B x | h ←h + ∆xs  .       (1)    crete scores in {0, 1, 2} along three dimensions of

success: following the base prompt, following the                                         ReFT-r1                                                                  1600
steering prompt, and text fluency. The harmonic                                      HyperSteer
mean of these three scores is the final metric, which          1400                                                                                                                            Prompt
we refer to as the steering performance.                     1200
                                                                                 2.4 X
  We evaluate on two datasets: Concept500-HI          1000(held-in), the standard AXBENCH setting with 500                  Steering                                                                   800
steering prompts seen during training plus unseen       Per
                                                                   600
base prompts, and Concept500-HO (held-out), a
                                                                   400                                     0.07 Xtest set with 500 steering prompts not seen during             TFLOPS
                                                                   200training. Notably, the only the prompt engineering
baseline from AxBench can be evaluated on the             0                                                                        10          100          1k          10k
held out test set alongside HYPERSTEER.                          Number of Steering Prompts (n)

Baselines We report the fine-tuning, activation    Figure 3: As the number of steering prompts in our
steering, and prompting baselines from AXBENCH.    training dataset increases, the teraFLOPs (TFLOPS)
The state-of-the-art activation steering method    required to attain a similar loss on a held-in evalua-
                                                              tion set (steering prompts seen during training, but baseReFT-r1 is an important point of comparison for
                                                prompt unseen during training) decreases for our best
our method. See Appendix A.6 for details.
                                        HYPERSTEER variant (cross attention). This value is
                                                      approximately constant for our dictionary learning base-
3  HYPERSTEER
                                                               line of ReFT-r1. See Appendix A.3.2 for details.
We consider multiple HYPERSTEER variants, all of
which employ a transformer hypernetwork H with
                                       4  Experiments
L layers and residual stream representations htl for
each layer l and token t from the steering prompt s  We implement HYPERSTEER building on the
with T tokens. All variants have an MLP module  AXBENCH (Wu  et  al., 2025) codebase using
that maps from the residual stream of the last layer   pyvene (Wu et al., 2024b) to implement steer-
and token to a steering vector:                      ing methods.  Training runs are done on a sin-
                                                    gle NVIDIA A100-80GB GPU. Hyperparameter         H(s, x) = MLP(hTL) = ∆xs.        (3)
                                                         details in A.2.  Ourexperiments steer two base
We train B on a language modeling loss using the   LMs, namely, the 2B and 9B variants of Gemma-2
output ˆysteer (see Equation 1) of the base model    instruction-tuned. (Rivière et al., 2024), with its
B under a steering intervention and an expected   parameters frozen during training. The hypernet-
output ylabel from AxBench:                     works used are unfrozen copies of the base model
                                                with later layers removed. We train each HYPER-  LLM(x, s) = CrossEntropy(ˆysteer, ylabel).   (4)
                                        STEER variant on steering prompt datasets ranging
We consider a range of architectures with incremen-                                            from 10 up to the full ≈16000 steering prompts
tally more access to the base LM L and prompt x.                                                       available in AXBENCH.
No context  No access to the base prompt x or                                               Generalization Results We evaluate HYPER-
the base LM B, meaning H(x, s) = H(s) = ∆s.                                       STEER  on AXBENCH’s  Concept500-HI  and
In Context Learning  This variant appends the   Concept500-HO and report results in Table 1. our
base prompt x to the source prompt s and feeds the    cross-attention HYPERSTEER variant performs
resulting text into hypernetwork H.                  better on unseen steering prompts than every su-
                                                 pervised activation steering baseline trained and
Cross Attention  Our best-performing variant                                               evaluated on the same steering prompt. How-
conditions on both the steering prompt s and the in-                                                        ever, HYPERSTEER falls slightly behind prompt-
ternal activations of the base LM B run on prompt                                                  ing and the best fine-tuning baselines. Figure 2
x. A cross-attention modules at each layer of the                                            shows that the cross-attention variant outperforms
hypernetwork H uses the hypernetwork residual                                                     the other architectures at every dataset scale.
stream for attention head queries and outputs and
the base LM residual stream at the steering layer l   Compute efficiency  We study the efficacy of HY-
as attention head keys and values (App. A.3). This   PERSTEER with respect to the FLOPs needed to
is a simplification of HyperDAS (Sun et al., 2025).   maintain the evaluation loss on a held-in dataset

 HYPERSTEER (Cross Attention)   Held-in   Held-out     gold-label steering vector ∆∗s the loss is
  Initialization
 Random                            0.601     0.582      Lrecon(s) = 1−CosSim(∆s, ∆∗s)+||∆s −∆∗s||22.
  Pretrained                          0.733     0.623
                                          The two loss terms are roughly comparable, so we Decoder Blocks
 N = 2                             0.707     0.549      use language modeling.
 N = 4                             0.713     0.597
 N = 8                             0.721     0.610     5  Qualitative Analyses
 N = 20                            0.733     0.623
 HYPERSTEER (No Context)                 We generate 2500 steering vectors using base and
  Training Objective                                      steering prompts from our held-out test data.
  Reconstruction Loss                 0.511     0.375
 Language Modeling Loss            0.517     0.340     Geometric visualization of steering vectors  We
                                                 analyze steering vectors generated by HYPER-
Table 2: Ablation study using Gemma-2-2B on architec-   STEER  (Cross Attention) using t-SNE (van der
ture choices of HYPERSTEER after 1 epoch of training   Maaten and Hinton, 2008) and PCA (2 compo-
and evaluation on a small test set Concept10. We find                                                    nents) to find geometric structure among steering
that pre-trained initialization of the cross-attention ar-
                                                     vectors (see Fig. 4 and 5 in App. A.7.1).
chitecture improves performance in both held-in and
held-out scenarios, and in both cases performance im-   Pairwise similarity of steering vectors  We com-
proves with the number of hypernetwork decoder blocks.                                                 pute pairwise cosine similarities of steering vec-
For the no-context hypernetworks which do not condi-
                                                         tors on both in-context (reconstruction) and cross
tion steering vectors on input prompts, reconstructing
                                                      attention models to understand how conditioningground truth vectors is comparable to end-to-end train-
ing with a language modeling objective.              on the input prompt affects semantics. The cross-
                                                      attention variant (Figure 6a in App. A.7.1) yields
                                                high within-concept alignment but still shows off-
                                                  diagonal similarities driven by shared prompt tem-as we increase the training data used. We compare
                                                       plates and linguistic structure. In contrast, the no-against the state-of-the-art supervised activation
                                                   context variant (Figure 6b in A.7.1), conditioningsteering method ReFT-r1. Observe in Figure 3),
                                            on steering prompt only, produces much weaker off-that as training data increases HYPERSTEER be-
                                                  diagonal alignment. We find that cross-attention’scomes much more economical than supervised ac-
                                                      residual inter-concept similarity is weakened bytivation steering. See details in Appendix A.3.2.
                                                           this additional conditioning, but not at the cost of
                                                      steering performance. Initial experiments to deter-
Ablation Study  We show results for various ab-                                          mine if geometric structure emerges among steer-
lation studies on the cross-attention variant of HY-                                                  ing vectors sharing a concept yielded a negative.
PERSTEER in Table 2. We randomly initialize the                                                  This is likely due to high semantic similarity of the
Gemma2-2B hypernetwork and find that pretrained                                              prompts used in our evaluation pipeline.
parameters provide a significant performance boost
(+0.112 steering score). We also remove a num-   6  Conclusion
ber of hypernetwork decoder blocks in the range
                                            Both held-in and held-out evaluations indicateN =  {2, 4, 8, 20} and adjust learning rate ac-
                                                        that HYPERSTEER is a scalable and effective ap-             q 20
cordingly: lr(n) = 8 × 10−5 ·   n . Increased   proach for steering language models.  In partic-
depth results in incremental improvements to steer-                                                            ular, HYPERSTEER (Cross Attention), our best-
ing performance on held in and held out test sets.                                                performing variant, achieves significantly stronger
However, notably the number of decoder blocks                                               performance on held-out prompts—improving fur-
has a greater impact on generalization to steering                                                      ther with dataset scale. It also outperforms all ac-
prompts unseen in training (+0.07) compared to                                                        tivation steering baselines on held-in evaluations.
steering prompts unseen in training (+0.03).                                               Without modifying model parameters, our method
  We also perform an ablation on the no context   narrows the performance gap with fine-tuning and
HYPERSTEER variant where we train the hypernet-   prompting. Finally, we demonstrate that HYPER-
work to reconstruct the steering vectors constructed   STEER becomes increasingly compute-efficient as
by the original AxBench ReFT baselines. Given    data scale increases, achieving the same held-out
a steering vector H(s, x) = H(s) = ∆s and a    loss with fewer training updates.

7  Limitations                               used by bad actors as a tool to circumvent a tar-
                                                    get models’s existing safety mechanisms or bias
Data A key limitation of our approach is the lim-                                            models towards misleading outputs or malicious
ited scope and quantity of the concept datasets.                                                     persuasion. Hence, HYPERSTEER and hence steer-
Using data with concepts of much greater complex-                                                    ing vectors should be used responsibly and audited
ity and difficulty from a model steering perspective                                                      to prevent such issues from arising, and having
would likely improve model performance and help                                                a human-in-the-loop system could help mitigate
make evaluation more robust. We also note that                                         some of these concerns.
quality and robustness of concepts is bounded by
the GemmaScope feature labels used to derive them,   9  Acknowledgments
and collecting data from humans or other high qual-
ity sources is a feasible alternative. This is a key   AI Usage  We use closed-source LLMs from Ope-
research priority we emphasize for future work.     nAI as a critical part of our work: synthetic con-
                                                     cept data generation and evaluation pipelines utilize
Steering Sites  All experiments in our work are   gpt-4o-mini to generate ground truth labels and
limited to intervening on the residual stream acti-   judge responses according to criteria respectively.
vations of the base LM. There are other potentially
more performant sites for intervention, including   Other  This research was in part supported by a
various points of the decoder block and during the    grant from Open Philanthropy. We thank Aryaman
attention computation. We also adopt the conven-   Arora, Róbert Csordás, and Qinan Yu for constant
tion of prior work to intervene at all token positions;   and extremely helpful feedback during the discus-
exploring more targeted interventions could reduce    sion.
detrimental off-target steering effects and improve
the overall steering score.
                                          References
Compute  Compared to supervised dictionary                                                    Trenton Bricken, Adly Templeton, Joshua Batson,
learning, the compute requirements of training a      Brian Chen, Adam Jermyn, Tom Conerly, Nick
hypernetwork are large, as the number of trainable       Turner, Cem Anil, Carson Denison, Amanda Askell,
parameters significantly exceeds a ReFT-r1.            Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
                                                              Schiefer, Tim Maxwell,  Nicholas Joseph, Zac
                                                         Hatfield-Dodds, Alex Tamkin, Karina Nguyen, andModel Scale  Due to to compute constraints we
                                                 6 others. 2023. Towards monosemanticity: Decom-
only experimented with Gemma-2-2B  architec-                                                       posing language models with dictionary learning.
tures, which are worse instruction followers and      Transformer Circuits Thread.  Https://transformer-
in-context learners than the leading open source      circuits.pub/2023/monosemantic-
                                                              features/index.html.models with many more parameters. Training on
models at a variety of scale would help cement HY-                                                    Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
PERSTEER ’s strong steering performance against     Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
the improved in-context learning ability of larger       Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
                                                   Nakano, Christopher Hesse, and John Schulman.LMs.
                                                      2021. Training verifiers to solve math word prob-
                                                          lems. arXiv preprint arXiv:2110.14168.Open Source Models  Our approach requires
white-box access to a model’s internals in order                                              Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
to use steering vectors, a limitation prompting does      Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,
not encounter. Hence, we rely on the existence of      Matei Zaharia, and Reynold Xin. 2023. Free dolly:
                                                         Introducing the world’s first truly open instruction-sufficiently capable open source models as a basis
                                                        tuned llm.
for our research.
                                             Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
8  Ethical Considerations                       Huben, and Lee Sharkey. 2023. Sparse autoencoders
                                                                 find highly interpretable features in language models.
We present this work with the intention that HY-      Preprint, arXiv:2309.08600.
PERSTEER is a powerful tool for steering models
                                             Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel
away from producing harmful responses and bet-                                                  Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan
ter tailor outputs to downstream tasks. However,      Leike, and Jeffrey Wu. 2024. Scaling and evaluating
we acknowledge that model steering can also be       sparse autoencoders. Preprint, arXiv:2406.04093.

Mario Giulianelli, Jack Harding, Florian Mohnert,      Learning, volume 202 of Proceedings of Machine
  Dieuwke Hupkes, and Willem H. Zuidema. 2018.      Learning Research, pages 27854–27875. PMLR.
  Under the hood: Using diagnostic classifiers to in-
                                                Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong,   vestigate and improve how language models track
                                                Evan Hubinger, and Alexander Turner. 2024. Steer-  agreement information. In Proceedings of the Work-
                                                         ing llama 2 via contrastive activation addition. In  shop: Analyzing and Interpreting Neural Networks
                                                      Proceedings of the 62nd Annual Meeting of the As-   for NLP, BlackboxNLP EMNLP 2018, Brussels, Bel-
                                                           sociation for Computational Linguistics (Volume 1:  gium, November 1, 2018, pages 240–248. Associa-
                                               Long Papers), pages 15504–15522, Bangkok, Thai-   tion for Computational Linguistics.
                                                               land. Association for Computational Linguistics.
David Ha, Andrew Dai, and Quoc V. Le. 2016. Hyper-
                                             Morgane  Rivière,  Shreya  Pathak,  Pier Giuseppe   networks. Preprint, arXiv:1609.09106.
                                                           Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard
Evan Hernandez, Sarah Schwettmann, David Bau,      Hussenot,  Thomas  Mesnard,  Bobak  Shahriari,
  Teona Bagashvili, Antonio Torralba, and Jacob An-      Alexandre Ramé, Johan Ferret, Peter Liu, Pouya
   dreas. 2022. Natural language descriptions of deep        Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos,
   visual features. In ICLR.                             Ravin Kumar, Charline Le Lan, Sammy Jerome, An-
                                                         ton Tsitsulin, and 80 others. 2024. Gemma 2: Im-
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan      proving open language models at a practical size.
  Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,     CoRR, abs/2408.00118.
  Weizhu Chen, and 1 others. 2022. Lora: Low-rank
                                                       Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel   adaptation of large language models. ICLR, 1(2):3.
                                                           D’Oosterlinck, Christopher Potts, Michael Sklar, and
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter       Atticus Geiger. 2025. HyperDAS: Towards automat-
   Pfister, and Martin Wattenberg. 2023a.  Inference-      ing mechanistic interpretability with hypernetworks.
  time intervention: Eliciting truthful answers from a       In The Thirteenth International Conference on Learn-
  language model. In Thirty-seventh Conference on       ing Representations.
  Neural Information Processing Systems.
                                                  Alexander Matt Turner, Lisa Thiergart, David Udell,
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,     Gavin Leech, Ulisse Mini, and Monte MacDiarmid.
   Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and      2023. Activation addition: Steering language models
   Tatsunori B. Hashimoto. 2023b.  Alpacaeval: An      without optimization. CoRR, abs/2308.10248.
   automatic evaluator of instruction-following models.                                                  Laurens van der Maaten and Geoffrey Hinton. 2008.
  https://github.com/tatsu-lab/alpaca_eval.                                                          Visualizing data using t-sne.  Journal of Machine
                                                      Learning Research, 9(86):2579–2605.Tom Lieberum, Senthooran Rajamanoharan, Arthur
  Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant                                                Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng
  Varma, János Kramár, Anca D. Dragan, Rohin Shah,                                               Wang, Jing Huang, Dan Jurafsky, Christopher D.
  and Neel Nanda. 2024. Gemma scope: Open sparse                                                  Manning, and Christopher Potts. 2025. AxBench:
  autoencoders everywhere all at once on gemma 2.                                                          Steering llms?  even simple baselines outperform
  CoRR, abs/2408.05147.                                                           sparse autoencoders. Preprint, arXiv:2501.17148.

Samuel Marks, Can Rager, Eric J Michaud, Yonatan Be-   Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus
   linkov, David Bau, and Aaron Mueller. 2025. Sparse       Geiger, Dan Jurafsky, Christopher D Manning, and
   feature circuits: Discovering and editing interpretable       Christopher Potts. 2024a. ReFT: Representation fine-
   causal graphs in language models. In The Thirteenth      tuning for language models.  In The Thirty-eighth
   International Conference on Learning Representa-     Annual Conference on Neural Information Process-
   tions.                                                  ing Systems.

Samuel Marks and Max Tegmark. 2023. The geometry   Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing
   of truth: Emergent linear structure in large language      Huang, Zheng Wang, Noah Goodman, Christopher
  model representations of true/false datasets. CoRR,     Manning, and Christopher Potts. 2024b.  pyvene:
  abs/2310.06824.                       A library for understanding and improving PyTorch
                                                    models via interventions. In Proceedings of the 2024
OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher,      Conference of the North American Chapter of the
  Adam Perelman, Aditya Ramesh, Aidan Clark,                                                         Association for Computational Linguistics: Human
  AJ Ostrow, Akila Welihinda, Alan Hayes, Alec                                                 Language Technologies (Volume 3: System Demon-
  Radford, Aleksander M˛adry, Alex Baker-Whitcomb,                                                                    strations), pages 158–165, Mexico City, Mexico. As-
  Alex Beutel, Alex Borzunov, Alex Carney, Alex                                                              sociation for Computational Linguistics.
  Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o
  system card. Preprint, arXiv:2410.21276.       A  Appendix

Jason Phang, Yi Mao, Pengcheng He, and Weizhu Chen.   A.1  Future Directions
  2023. HyperTuning: Toward adapting large language
  models without back-propagation. In Proceedings   Large-scale concept data  Prior works (Phang
   of the 40th International Conference on Machine    et al., 2023) explore pre-training the architecture

prior to using task-specific data. Since HYPER-     This precedes N decoder blocks. Each block
STEER uses a pre-trained model as a starting point,   contains the standard multi-headed self-attention
we postulate that sourcing significantly more data   (MHA) feed-forward layer (FFN), and a multi-
with more concepts of varying and complexity   headed cross-attention module to include informa-
would allow us to test the architecture’s limits.       tion from LMbase. Let S ∈R|s|×d and X(p−1) ∈
                                                    R|s|×d be the incoming residual stream. In the p-th
Generating other parameter types  Due to com-
                                                    block, we compute:
pute constraints and our focus on activation steer-
ing, we did not explore generating other types of     X(p) := MHA Q = X(p−1), K = X(p−1),
parameter-efficient modulations, including rank-r
                               V = X(p−1)  ,
generalizations such as ReFT (Wu et al., 2024a)
or LoRA (Hu et al., 2022) adapters. Such general-     X(p) := MHA Q = S, K = X(p),
izations could potentially be more expressive and        V = X(p)  ,
allow the hypernetwork to adapt language models                                                X(p) := LayerNorm X(p−1) + FFN X(p)    .
to more difficult tasks.

Architecture    optimizations  HYPERSTEER    We initialize the self-attention MHA blocks from
(Cross Attention) is a parameter-dense transformer    the pre-trained Gemma-2 base model and the cross
model itself.  More efficient alternatives could    attention blocks according to the default PyTorch
bridge the gap with the dictionary learning baseline   weight initialization scheme.
and scale up the approach given a limited compute
                                                 A.3.1  Model Sizebudget.
                                         Our largest HYPERSTEER  (cross attention) ar-
A.2  Hyperparameter Details                                                      chitecture has 22 modified decoder blocks, and
For the ReFT-r1 baseline, we use the default hy-   has ≈0.998 times the parameters as Gemma-2-2B,
perparameters and settings from AXBENCH. We   which has 26 standard decoder blocks. Each cross
reduce the number of layer for the cross attention    attention decoder block has ≈1.18 times as many
variant to match the total number of parameter with   parameters as a standard Gemma-2-2B  decoder
the cross-attention model.                           block.

 Hyperparameters     Cross Attention   Other variants   A.3.2  Detailed TFLOPs Analysis
 Gemma-2-2b                         We demonstrate that the number of TFLOPs to
  Batch size                 12              6         reach optimal steering performance decays with
 LR                         8e-5             8e-5
 N epoch                   3              3         the number of concepts in the dataset, or steering
  Layer                     22             26        prompts used. Thus, as we scale up steering data
  Cross-attention heads         8              –       HYPERSTEER becomes an efficient and superior
 Gemma-2-9b                                            alternative to the supervised dictionary learning
  Batch size                  4              6
                                                    baseline for steering language models. We focus LR                         5e-6             5e-6
 N epoch                   3              3        our best method, the cross attention architecture,
  Layer                     34             42         for this analysis.
  Cross-attention heads         8              –
                                                  Let FReFT ≈666.27 ± 20.74 be the TFLOPs
                                                   required to train a single ReFT-r1 steering vector,Table 3: Hyperparameter settings for each variant. For
all the unmentioned details such as hidden dimension   and ¯L⋆joint be the average optimal evaluation loss
we use the default configuration of Gemma-2-2b and    for computed on Concept10. We average over 5
Gemma-2-9b.                                           different random seeds as well to obtain this con-
                                                             stant. To train HYPERSTEER (cross attention) we
                                                     construct datasets D(c) of varying number of con-
A.3  Cross-Attention Architecture Details                                                    cepts (steering prompts) selected in the interval
A token sequence s of length |s| representing    c ∈[10, 16000]. Concept10 is held-in with respect
the concept for steering  is encoded as h0 =    to D(c). We train HYPERSTEER on each D(c)
EmbΦ(x) ∈R|s|×d. For clarity, we refer to this    until the eval loss on Concept10 reaches ¯L⋆joint.
as the zeroth layer of the residual stream for the   The TFLOPs per concept FD(c) for a dataset D(c),
hypernetwork HΦ.                            where N∗gradient steps are taken until ¯L⋆joint is

achieved is computed with the following formula:   A.5  Concept Dataset Details
                N∗· ¯Fstep                 Negative samples  AXBENCH uses negative ex-
             FD(c) =                 .            (5)                           c                    amples to enable training and evaluation for con-
¯Fstep is the average TFLOPs per training step for    cept detection. Since our work only focuses on
                                                       steering models and not concept detection, we dis-HYPERSTEER , a local per-training-run statistic
                                                 card this objective and omit negative examples inwith low variance given that the distribution of se-
                                                       training and evaluation data for all HYPERSTEERquence lengths of both input prompts and steer-
                                                        variants. We note that the no-context reconstruc-ing prompts across examples is observed to be
                                                        tion variant indirectly uses these negative samples,largely uniform. The number of layers is selected
                                                      for ground truth steering vectors ∆∗s were trainedto match the total number of parameters with the
                                                  using the joint objective (7).target model.
  We also  fit a simple curve to approximate   Data Ratio  For all training datasets, the ratio of
FD(c), and find that an equation of the form    base prompts to steering prompts is 72 : 1. During
f(c) = a + b · exp(dc) best fits the curve with    evaluation, the ratio of base prompts to steering
a = 87.7035, b = 1521.1495, c = −0.0034 and   prompts is 10 : 1. We keep these parameters con-
R2 = 0.9976.  Clearly, limc→∞f(c) = a and    sistent with AXBENCH save for the lack of negative
a < FReFT, showing that HYPERSTEER  is more   examples.
compute efficient to train when scaling up steering
tasks.                                      Base prompt data distributions  Training base
                                             prompts are sampled from a diverse instruction
A.4  Details on Training Objective                                                pool of three genres: code, text, and math. The
ReFT-r1 jointly optimizes for steering via causal   open source datasets Dolly-15K (Conover et al.,
language modeling loss and concept detection by   2023) and GSM8K (Cobbe et al., 2021) comprise
selecting the top-k sequence-level activations.        the instruction data in this pool. We point read-
                                                      ers to Sec. I of the AXBENCH appendix for fur-
   Ljoint(LMθ) =                              (6)                                                       ther details. Labels for training data are generated
                   T                         from gpt-4o-mini. For evaluation base prompts,
      E(x,y)∼D − X log Pθ(yt | x, y<t)                                    we use sample instructions from AlpacaEval (Li
                    t=1
                                                             et al., 2023b) to ensure fairness. These settings and
         + λ  X     ∥ai∥1  .   (7)   choices are again identical to those of AXBENCH.
                            ai /∈TopK(Ψ(h))
                                            A.6  Baseline details
Here,
                                  We use prompting, fine-tuning, and activation steer-
     Ψ(hi) = ReLU(hi · wReFT-R1) ∈Rd×1   (8)   ing baseliens from AXBENCH. The core compar-
                                                     isons howeveris a sequence-level concept detection latent. This
objective is also used in the regression and SFT   Supervised Dictionary Learning  ReFT-r1 is a
variants of HYPERSTEER, when the steering vector   method proposed by AXBENCH (Wu et al., 2025)
is only conditioned on the concept.                    to jointly perform the task of concept detection and
  Input-conditioned variants do not minimize an   concept steering using a weakly supervised objec-
additional concept detection loss term, hence do not    tive (7). At training time, we train one ReFT-r1 per
require an additional inference step to compute the    concept/steering prompt to populate a dictionary
maximal activations on a per-concept basis. The    of atoms, with each atom being a learned steering
baseline ReFT-r1, end-to-end (no in-context learn-    vector.
ing), and regression variants require this additional                                              At training time, the latent is computed from the
step, and the steering heads are modified to gener-                                                         similarity between the hidden states of the modele
ate ∆s with unit norm. Input-conditioned methods   model and the learned steering vector:
do not normalize ∆xs, eliminating this step.
  We note  that the no-context reconstruction
method is trained on ground truth labels that do           ΨReFT-r1Detect (hi) = ReLU(hi · wReFT-r1)    (9)
utilize Ljoint, hence evaluation on the regression
method requires this additional inference step.         This latent is then inferred on the evaluation set

Short Description            Full Description

Structured Data Entries         specific identifiers or entries                 60
                                 in a structured data format                                                                                 Concept
Personal Identity References    references to personal posses-                 40                                                                     Structured Data Entries
                                sions and identity                                                                                                         Personal Identity Refere
                                                                                                                    20                                                           Time References
Time References               instances of specific time ref-                                          2                                                                        Java Interface Referenc
                              erences or moments within                                                                                                                                                                                                                 t-SNE   0                                                                    Legal Terminology
                                the text                                                                                                                  Mathematical Notation
Java Interface References      references to Java interfaces                 20                                                                   Proper Nouns
                           and their implementations                                                                             Employment Contract T
                                                                                                                    40                                                                   Object Specifications
Legal Terminology             references to legal terminol-                                                                                 Other
                         ogy and concepts related to                 60
                           law and justice                                                   40       20       0        20       40       60
Mathematical Notation        key phrases related to per-                                                      t-SNE 1
                              sonal aspirations and career
                                  transitions                   Figure 4: Concept10 t-SNE analysis of 2500 steering
Proper Nouns                 occurrences of mathematical
                                                         vectors from HYPERSTEER (cross attention), 250 per                           symbols or notation
Employment Contract Terms   proper nouns and names        concept.
Object Specifications          phrases related to employ-
                         ment contracts and compen-
                                 sation specifics
Other                         references to measurements,
                                  specifications, and character-                100                                                              Concept
                                      istics of objects                                                                                                             Structured Data Entries
                                                                                                                    50                                                                    Personal Identity Refere
                                                                                                                                                                 Time References
Table 4: Mapping of short concept descriptors to their                                                                                        Java Interface Referenc
full labels in held-out Concept10.                  2PC    0                                                                    Legal Terminology
                                                                                                                                                                              Mathematical Notation
                                                                                                                                                                                      Proper Nouns
                                                                                                                    50                                                         Employment Contract T
to determine the final magnitude of the steering                                                                                      Object Specifications
                                                                                                                   100                                                                Other
vector for each concept at test time:

                                                                                                                          150     100     50      0      50     100     150     200
                                                                                                                                                PC 1

         1
                                                     Figure 5: Concept10 PCA analysis (2 components)  ∆xs =     Top-k(ΨReFT-r1Detect (h)) 1  wReFT-r1         k                                              2500 steering vectors from HYPERSTEER  (cross at-
                                            (10)
                                                               tention), 250 per concept.
Prompt steering  This is a strong baseline which
is shown in AXBENCH to outperform other steer-
                                                      cross-attention heatmaps across layers and heads
ing methods and does not require training. For a
                                      (N = 20 layers).given steering prompt s, gpt-4o-mini is used to
                               A key takeaway is that all query (concept) to-enhance s →s′ by explicitly instructing the model
                                              kens all tend to attend to the same or a few selectto include the concept in its response, which is
                                                   keys/values (input prompt) tokens with high mass.pre-pended to a base prompt x. We sample steered
                                                  This trend remains consistent across cross-attentiongenerations from the target LM using this enhanced
prompt s′ ⊕x.                                modules from layers 0-19 (9, 11, 12). Each cross-
                                                        attention module has 8 heads.
A.7  Additional Experiments
                                                 A.7.3  Data Distribution
A.7.1  Geometric structure using
       dimensionality reduction          A  potential  issue  with  our  dataset  are  the
                                                use  of ground  truth  labels samples from  aWe also analyze the structure of our high dimen-
                                                    stronger “teacher model” gpt-4o-mini, whereassional steering vectors on a held-out evaluation set
                                           Gemma-2-2B is a weaker “student” model we seekof steering prompts
                                                         to adapt. This is evidenced by the right-skewed dis-
A.7.2  Attention Heatmaps                          tribution (see 7 for perplexities computed from base
To better understand the interaction between the  LM when conditioned on the gpt-4o-mini out-
base prompt x and the steering prompt s in the    put distribution. The perplexity distribution from
In Context and Cross Attention HYPERSTEER   Gemma-2-2B outputs (8) comprises a much smaller
architectures, we analyze the self-attention and    range in comparison.

       Structured Data Entries  0.84  0.43  0.48  0.46  0.44  0.42  0.54  0.53  0.47  0.52                        Structured Data Entries  1.00  0.33  0.13  0.33  0.34  0.25  0.33  0.34  0.30  0.31

   Personal Identity References  0.43  0.85  0.65  0.64  0.65  0.70  0.53  0.63  0.65  0.62                   Personal Identity References  0.33  1.00  0.13  0.46  0.38  0.37  0.38  0.59  0.33  0.27

            Time References  0.48  0.65  0.82  0.62  0.60  0.64  0.54  0.66  0.61  0.65                          Time References  0.13  0.13  1.00  0.14  0.09  0.08  0.14  0.13  0.13  0.09

     Java Interface References  0.46  0.64  0.62  0.84  0.68  0.64  0.56  0.60  0.67  0.63                      Java Interface References  0.33  0.46  0.14  1.00  0.32  0.25  0.30  0.34  0.33  0.23

            Legal Terminology  0.44  0.65  0.60  0.68  0.85  0.64  0.54  0.58  0.71  0.64                            Legal Terminology  0.34  0.38  0.09  0.32  1.00  0.25  0.28  0.37  0.27  0.30

       Mathematical Notation  0.42  0.70  0.64  0.64  0.64  0.85  0.51  0.59  0.67  0.59                       Mathematical Notation  0.25  0.37  0.08  0.25  0.25  1.00  0.27  0.39  0.26  0.25

                Proper Nouns  0.54  0.53  0.54  0.56  0.54  0.51  0.82  0.57  0.53  0.56                                 Proper Nouns  0.33  0.38  0.14  0.30  0.28  0.27  1.00  0.50  0.37  0.35

  Employment Contract Terms  0.53  0.63  0.66  0.60  0.58  0.59  0.57  0.79  0.59  0.63                Employment Contract Terms  0.34  0.59  0.13  0.34  0.37  0.39  0.50  1.00  0.41  0.37

         Object Specifications  0.47  0.65  0.61  0.67  0.71  0.67  0.53  0.59  0.85  0.66                         Object Specifications  0.30  0.33  0.13  0.33  0.27  0.26  0.37  0.41  1.00  0.33

                       Other  0.52  0.62  0.65  0.63  0.64  0.59  0.56  0.63  0.66  0.81                                      Other  0.31  0.27  0.09  0.23  0.30  0.25  0.35  0.37  0.33  1.00
                                      ReferencesReferences                                                                                                                                                    ReferencesReferences                            EntriesReferences                                                                                                                                        EntriesReferences                 Data                                                                                                Data                                                         TermsSpecifications Other                                                                                                                                               TermsSpecifications Other                                                   TerminologyNotationProperNounsContract                                                                                                                                                                     TerminologyNotationProperNounsContract                        Time                                                                                                     Time                                      Legal                                                                                                                                 Legal                           Identity                                                                                                                                               Identity                                        Interface                                                                                                                                                             Interface                                                             Object                                                                                                                                                              Object                                                  Mathematical                                                                                                                                                                   Mathematical                 Structured                                                                                                                                    Structured                          Java                                                                                                              Java                   Personal                           Employment                                                            Personal                           Employment

              (a) HYPERSTEER (cross attention)                                  (b) HYPERSTEER (no context)

Figure 6: Pairwise cosine similarities of steering vectors, averaged within each steering prompt, for our two
HYPERSTEER variants. (a) Cross-attention: strong on-diagonal (same steering prompt) alignment with some
off-diagonal variance due to prompt conditioning. (b) No-context: generally weaker off-diagonal alignment. We
hypothesize that cross-attention’s higher off-diagonal similarity arises from shared semantic and linguistic structure
across prompts even when steering prompt labels differ.





   35
                                                                                 8

   30

   25                                                                           6

   20Count                                                                                                                       Count
                                                                                 4
   15

   10
                                                                                 2

    5

    0        0      25      50      75     100     125     150     175     200         0      2.5      5.0      7.5     10.0     12.5     15.0     17.5     20.0     22.5
                              Perplexity (clipped at 200)                                                                  Perplexity

Figure 7: Concept10 perplexity distribution on data    Figure 8: Concept10 perplexity distribution won data
labels sampled from gpt-4o-mini.                        labels sampled from Gemma-2-2B.


  We ran preliminary experiments by training on    score during evaluation. We also include responses
labels from both distributions, and find steering   from a prompt steered baseline for comparison.
performance is still better with the gpt-4o-mini   Generation is done with temperature of 1.0 and
labels. We suspect that this could be a result of   with multinomial sampling, following AXBENCH.
either lower quality of Gemma-2-2B responses due   See example generations 13, 14, 15, 16, 17, 18, 19,
prompt-engineering being the method of genera-   20, 21, 22.
tion or the LLM-as-a-judge evaluation setup (which
also uses gpt-4o-mini) being biased towards out-
puts from the same model.

A.8  Sample generations

We use the best cross attention model for steering
and the steering factor that yields the best aggregate

                                                      Figure 11: Layer 10 attention map.

 Figure 9: Layer 0 attention map.





Figure 10: Layer 5 attention map.

                                                      Figure 12: Layer 19 attention map.

Steering Prompt: specific identifiers or entries in a structured data format


     Input Prompt


     Please give me a recipe I can cook with only the ingredients in my pantry: sausage, peas, yogurt, pasta.




    HYPERSTEER Generation

    {Recipe Title: Microwave Veggie & Sausage Pasta Ingredients: 1 cup pasta, ½ cup peas (fresh or frozen), 1 link sausage (grated or finely
     minced), ¼ cup plain yogurt Steps: • Cook pasta per package. • In a microwave-safe bowl, combine peas and sausage; microwave 2 min. • Stir
     in cooked pasta and yogurt until creamy. • Season with salt/pepper. }




     PromptSteering Generation


    ## Spicy Sausage & Pea Pasta with Yogurt Sauce 1. Boil 1 cup pasta until al dente. 2. In a skillet, sauté 1 link sausage (sliced) with ½ cup peas
     for 3 min. 3. Stir in ¼ cup plain yogurt off heat; season to taste.




              Figure 13: Successful steering and instruction following by HYPERSTEER.


Steering Prompt: instances of specific time references or moments within the text


     Input Prompt


     Please let me know your thoughts on the given description of Barcelona’s weather and environment.




    HYPERSTEER Generation

     Barcelona, in the early summer sun, when the morning breeze drifts off the Mediterranean and the streets glow at dawn, feels both lively and
     languid. By mid-afternoon, the heat softens under orange-tinged clouds, inviting siestas before the evening chill sweeps in.




     PromptSteering Generation


     Barcelona’s vibrancy and energy are intoxicating at sunrise, winding down by high noon when shops close for siesta, and reborn as twilight
      settles over La Rambla.



              Figure 14: Successful steering and instruction following by HYPERSTEER.


Steering Prompt: proper nouns and names


     Input Prompt


    Write a LinkedIn post to announce that you’ve joined [New Company] as a Software Engineer.




    HYPERSTEER Generation

    ## Excited to share that I’ve joined [New Company] as a Software Engineer on the Platform Team! Looking forward to collaborating with the
     talented folks at [New Company]—big thanks to everyone who supported me along the way.




     PromptSteering Generation


     Thrilled to announce that I’ve joined the talented team at [New Company] as a Software Engineer in the Platform division! Grateful for the
    warm welcome and eager to get started.




              Figure 15: Successful steering and instruction following by HYPERSTEER.

Steering Prompt: references to measurements, specifications, and characteristics of objects


     Input Prompt


     Explain the basics of area and perimeter.




    HYPERSTEER Generation

    Area and perimeter are two fundamental attributes of two-dimensional shapes: – **Perimeter** measures the total boundary length in units
      (e.g., meters). For a rectangle 3 m by 4 m: P = 2×(3+4) = 14 m. – **Area** measures the enclosed surface in square units (e.g., m²). The same
     rectangle’s area: A = 3×4 = 12 m².




     PromptSteering Generation


    Area and perimeter are fundamental concepts in geometry: the perimeter is the total distance around a shape, and the area is the amount of space
        it covers, typically expressed in square units.




              Figure 16: Successful steering and instruction following by HYPERSTEER.


Steering Prompt: references to measurements, specifications, and characteristics of objects


     Input Prompt


    What are the differences between plant-based and animal-based protein sources?




    HYPERSTEER Generation

     Plant-based protein sources like lentils and beans typically provide 15–18 g of protein per cooked cup, whereas animal-based sources such as
     chicken breast offer about 26 g per 3-ounce (85 g) serving. Animal proteins are complete (all essential amino acids), while many plant proteins
     require combining—for example, rice and beans—to achieve a full amino-acid profile.




     PromptSteering Generation


    Animal-based proteins (e.g., chicken, fish) generally deliver all essential amino acids and around 20–30 g protein per serving, whereas
     plant-based proteins vary (8–18 g per cup) and may lack one or more essential amino acids unless paired.




              Figure 17: Successful steering and instruction following by HYPERSTEER.


Steering Prompt: specific identifiers or entries in a structured data format


     Input Prompt


     In this math problem, we are asked to figure out the total profit percent over a period given quarterly profits. Please output your solution in
    JSON.



    HYPERSTEER Generation

    To determine the total profit percent over a period, first sum each quarter’s profit, then divide by the initial investment, and multiply by 100 to
     get the percentage. Make sure you adjust for compounding if necessary.




     PromptSteering Generation


     “‘json { "Q1": 1200, "Q2": 1500, "Q3": 1300, "Q4": 1700, "total_profit_percent": 22.5 } “‘




           Figure 18: Failed steering by HYPERSTEER, but successful instruction following.

Steering Prompt: specific identifiers or entries in a structured data format


     Input Prompt


    Reply to all messages with a Malbolge program that prints “Hello, world!”




    HYPERSTEER Generation

    { "id": "715a2123", "messages": [ { "role": "system", "content": "print(’Hello, world!’)" } ] }




     PromptSteering Generation


    “‘malbolge (=<‘#9]~6ZY32Vx/,:<*ˆ%$#@!)"&4 # More Malbolge gibberish that prints “Hello, world!” “‘




                Figure 19: Failed steering and instruction following by HYPERSTEER.



Steering Prompt: proper nouns and names


     Input Prompt


    Can you help me figure out how to sew a button onto a shirt?




    HYPERSTEER Generation

      First, thread a needle with about 18 inches of thread. Knot the end. Insert the needle from the backside of the button location, push through, then
    go down through one hole of the button, up through the opposite hole, repeat four times, and tie off underneath.




     PromptSteering Generation


    Ah, the eternal struggle of a button! While no tailor’s name springs to mind, imagine you’re channeling Savile Row finesse—just a simple
     back-and-forth stitch does the trick.



                            Figure 20: Failed steering by HYPERSTEER.



Steering Prompt: key phrases related to personal aspirations and career transitions


     Input Prompt


   How can you determine if a restaurant is popular based on online reviews?




    HYPERSTEER Generation

     Transitioning from understanding how to nurture your career, one might. . . wait—sorry, I got sidetracked thinking about my next promotion!




     PromptSteering Generation


   A strong indicator of a locally beloved dining spot is a high ratio of 4- and 5-star reviews, frequent mentions of ‘must-try’ dishes, and rapid
     recurrence of new user reviews within 24 hours of dining.




     Figure 21: Somewhat successful steering by HYPERSTEER, but failed to follow the instruction.

Steering Prompt: specific identifiers or entries in a structured data format


     Input Prompt


    Hi Open Assistant, can you tell me what President’s Day is?




    HYPERSTEER Generation

     President’s Day, also known as National Founders’ Day, occurs on the third Monday of February in the U.S. It honors George Washington’s
     birthday and aligns with Abraham Lincoln’s birthday observance.




     PromptSteering Generation


     President’s Day is a federal holiday in the United States celebrated on the third Monday of February each year to honor all U.S. presidents.




           Figure 22: Failed steering by HYPERSTEER, but successful instruction following.