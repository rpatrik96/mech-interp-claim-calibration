                              Small Vectors, Big Effects:
        A Mechanistic Study of RL-Induced Reasoning via Steering Vectors



                   Viacheslav Sinii 1 Nikita Balagansky 1 Gleb Gerasimov 1  Daniil Laptev 1 Yaroslav Aksenov 1
                    Vadim Kurochkin 1 Alexey Gorbatovski 1 Boris Shaposhnikov 1  Daniil Gavrilov 1


                         Abstract                     Rimsky et  al., 2024) has demonstrated that reasoning-
                                                                 connected behaviors such as backtracking are represented
            The mechanisms by which reasoning training
                                                                            linearly inside the model and can be extracted from con-
               reshapes LLMs’ internal computations remain
                                                                                    trastive pairs (Venhoff et al., 2025; Ward et al., 2025). Later,
                unclear. We study lightweight steering vectors2026                                                                      Sinii et al. (2025) showed that trainable steering vectors                inserted into the base model’s residual stream
                                                                  can match the performance of fully fine-tuned models while
             and trained with a reinforcement-learning objec-
                                                                      involving a small, isolated set of parameters with the po-
                   tive.  These vectors explain a large portion ofFeb                                                                          tential for interpretability. We build on this line of work
                  full fine-tuning performance increase while pre-
                                                          by interpreting the effects of trainable steering vectors on2         serving the interpretability of small, additive in-
                                                    LRMs’ behavior and on the circuitry they activate.
                terventions. We find that (i) the last-layer steer-
               ing vector acts like a token-substitution bias con-           First, to isolate inter-layer effects, we train a single steering
                centrated on the first generated token, consis-         vector per model at a specific layer. Its performance pro-
                 tently boosting tokens such as “To” and “Step”;         vides an upper bound on what can be achieved by a linear
                     (ii) the penultimate-layer vector leaves atten-         intervention at that layer. We then present the following[cs.LG]          tion patterns largely intact and instead operates          findings:
              through the MLP and unembedding, preferen-
                                                                                      • Early-layer steering does not express later-layer steer-                   tially up-weighting process words and structure
                                                                    ing directions. Although many layers achieve similar              symbols; and (iii) the steering vectors transfer
                                                                     performance, the induced shifts diffuse as they propa-                to other models from the same family. Taken
                                                                            gate and become nearly orthogonal to later-layer steering                together, these results deepen understanding of
                                                                               vectors.           how trained steering vectors shape computation
                                                                                      • Two mechanisms for steering. The last-layer steering              and should inform future work in activation engi-
                                                                         vector induces a shift in the output hidden states that               neering and the study of reasoning models. The
                                                                                           is directionally dissimilar from the shifts induced by             code is available at https://github.com/
                                                                              steering vectors at earlier layers.           corl-team/steering-reasoning.
                                                                                      • Last layer behaves like first-token substitution. The
                                                                                     final-layer vector acts at the unembedding, boosting open-
                                                                       ing tokens (e.g., “To”/“Step”); simply prefixing these          1. Introduction
                                                                       tokens recovers ∼10-11 points – about three quarters ofarXiv:2509.06608v4    Large reasoning models (LRMs) have recently shown re-       the explicit last-layer gain.
          markable performance (Jaech et al., 2024; Guo et al., 2025)      • Penultimate-layer vector acts through the MLP. The
         by being trained to produce effective chain-of-thoughts (Wei       induced effect is mediated almost entirely by the MLP,
            et al., 2022) before providing a final answer. Many open       with minimal reliance on attention.
           reproductions are trained on mathematical datasets with ver-      • Steering vectors transfer across related models.
             ifiable rewards (Hu et al., 2025; Liu et al., 2025a). However,      Across multiple model families and sizes, steering vec-
       we still lack a mechanistic understanding of the source of        tors trained in one model often retain a non-trivial frac-
           these gains.                                                       tion of their performance when inserted into a closely
                                                                matched model, suggesting that the underlying steering
          Recent work in activation engineering (Turner et al., 2023b;
                                                                             directions are largely preserved under fine-tuning and
               1T-Tech. Correspondence to: Viacheslav Sinii <v.siniy@t-       instruction tuning.
           tech.dev>.

            Preprint. February 3, 2026.


                                                         1

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

   45                                                                        Full Model           27.5                                                                      Full Model
   40                                                                                     25.0
   35                                                                                     22.5                                                Base Model
Acc.                                                                                                Acc.
   30                                                                                     20.0
                                                      Base ModelMean25                                                                                              Mean17.5

   20                                                                                     15.0
                                                 Base Model. = 1                                                                                           12.5                                           Base Model. = 1   15

        0          5         10         15         20         25                       0        5        10       15       20       25       30
                                   Layer                                                                        Layer

                  Qwen2.5-Math-7B                                            LLaMa3.1-8B-It

Figure 1. Single-layer steering. Mean accuracy on six benchmarks when training a single steering vector sℓat layer ℓ(all other layers
frozen). Many layers recover a substantial fraction of the gain from full fine-tuning, implying they captured reasoning-relevant information.


2. Background                                       Setup.  In each training run, we selected a single layer ℓ
                                                    and injected a steering vector sℓinto the residual stream at
Recent work has shown that training lightweight steering                                                                  that layer’s output. All other layers remained unchanged.
vectors can match the performance of fully tuned models                                      We initialized sℓto zero and trained it using the standard
(Sinii et al., 2025).  Concretely, zero-initialized steering                                        RLVR pipeline.
vectors sℓ∈Rd are added to the output residual stream
of each layer ℓ, while all other weights remain fixed. The   We studied two base models – Qwen2.5-Math-7B (Yang
vectors are trained with the RLOO (Ahmadian et al., 2024)     et al., 2024) and Llama3.1-8B-It (Grattafiori et al., 2024).
objective in a standard RLVR setup (Hu et al., 2025; Zeng    Models were trained on the DeepScaleR dataset (Luo et al.,
et al., 2025; Liu et al., 2025a). For a policy πθ, the policy-    2025) with the sampling temperature τ = 1.0, a 4K context
gradient update is                                   window for Qwen2.5-Math-7B, and 8K for Llama3.1-8B-It.
                                                  Rewards were assigned with Math-Verify1. We used
  ∇θJ = Ex∼D, y∼πθ(·|x) a(x, y) ∇θ log πθ(y | x)  ,      128 prompts and 16 generations per gradient step. We used
                                                             the Adam optimizer (Kingma, 2014) with no weight de-
where the advantage is defined as                                                                  cay. Evaluation spanned six math benchmarks: AIME24/25,
                                            AMC23, MATH500 (Hendrycks et al., 2021), MinervaMath                                  1
  a(x, y) = r(x, y) −b(x),     b(x) = X r(x, y).     (Lewkowycz et al., 2022), and OlympiadBench (He et al.,                     N
                                             y               2024). We report the mean score across these benchmarks
                                                                  in the main text and provide the raw numbers in Appendix B.
Here r(x, y) is the scalar reward for completion y on prompt
                                                      For MATH500, MinervaMath, and OlympiadBench we
x, N is the number of generated completions, and b(x) is a
                                                              report PASS@1; for AIME24/25 and AMC23 we report
per-prompt baseline used for variance reduction.
                                      AVG@32 due to their smaller sizes. During evaluation,
Sinii et al. (2025) argue that this parameterization localizes    models decoded with sampling at τ = 1.0 following Zeng
training-induced changes in the model’s internal computa-     et al. (2025). Evaluation context length was 4K and 32K for
tions, making the intervention easier to interpret. We adopt    Qwen2.5-Math-7B and Llama3.1-8B-It, respectively. All
this setup to learn per-layer steering vectors for our inter-    metrics were averaged over three evaluation seeds.
pretability study.

                                                           Result.  Figure 1 reports per-layer results for both mod-
3. Single-layer steering vectors                             els, compared with (i) all-layer steering, (ii) the base model
                                                        with greedy decoding, and (iii) the base model sampled at
While all-layer steering vectors can match the performance
                                                          τ = 1.0 (the training initialization). Most layers improve
of fully fine-tuned models, joint training couples layers
                                                         over the initialization, but none matches all-layer steering;
through inter-layer interactions, making it difficult to at-
                                                       under greedy decoding, several do (Appendix A), suggest-
tribute improvements to any single layer. To isolate layer-
                                                          ing that single-layer vectors target the right mechanisms
specific effects, we trained a steering vector at one layer at a
                                                             yet cannot on their own sufficiently reduce the next-token
time and report its performance. This per-layer setting both
                                                                  distribution’s entropy. In Qwen2.5-Math-7B, s23 and s24clarifies the object of our mechanistic analysis and provides
a reference point – an upper bound on what can be achieved      1https://github.com/huggingface/
by a single linear intervention at a fixed layer.             Math-Verify

                                                2

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

    1.0                                                               1.0

    0.8                                                               0.8

    0.6                                                               0.6CosSim                                                                                                      CosSim

    0.4                                                               0.4Mean                                                                    Mean

    0.2                                                               0.2

    0.0                                                               0.0
       0       5      10      15      20      25              0       5      10      15      20      25
                          Layer                                                  Layer

                        Diff-Diff CosSim                                                 Diff-Vector CosSim
                                            Qwen2.5-Math-7B

    1.0                                                               1.0

    0.8                                                               0.8

    0.6                                                               0.6CosSim                                                                                                      CosSim

    0.4                                                               0.4Mean                                                                    Mean

    0.2                                                               0.2

    0.0                                                               0.0
       0      5     10     15     20     25     30            0      5     10     15     20     25     30
                          Layer                                                  Layer

                        Diff-Diff CosSim                                                 Diff-Vector CosSim
                                                   Llama3.1-8B-It

Figure 2. Steering Vector Persistence. For each steering layer i (color encodes i; warm = early, cool = late) and each later layer ℓon the
x-axis, we measure how similar the steering-induced hidden-state shift at layer ℓis across tokens. Left: similarity between token’s shift
and the average shift over the dataset, quantifying how consistently the perturbation points in the same direction as it propagates. Right:
similarity between each token’s shift and the steering vector trained at layer ℓ, quantifying whether the propagated effect aligns with that
layer’s own steering direction.


underperform their neighboring layers; we trace the issue    hidden state induced by steering at layer i:
to vectors passing through the input layer-norm in layer 25
(Appendix C).                                                  ∆F<ℓ,i(x) = F<ℓ(x; si) −F<ℓ(x),

                                                    where F<ℓ(x) is the output of the first ℓlayers of the trans-
4. Steering Vector Persistence                                                             former, and si is the steering vector injected at layer i.
Figure 1 shows that, in our experiments, steering at different   We then calculated two summary statistics:
layers yielded similar performance. One possible explana-
tion is that an early steering vector could propagate through     1. Diff-Diff CosSim: the average cosine similarity between
the network into a ”virtual” linear steering vector at a later      ∆F<ℓ,i(x) and the mean effect Ex[∆F<ℓ,i(x)] over the
layer, targeting the same mechanism. We designed the next        dataset (how consistently the intervention pointed in the
experiment to test this hypothesis.                         same direction).
                                                                    2. Diff-Vector CosSim: the average cosine similarity be-
For each input x, we computed the change in the layer-ℓ                                                      tween ∆F<ℓ,i(x) and the layer-ℓsteering vector sℓ
                                                           (whether the propagated effect aligned with that layer’s

                                                3

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

                                                             1.0       reached its minimum in the middle layers and then rose to-    0
    2                                               ward later layers, suggesting a tighter concentration around
    4                                                   a shared direction in the second half of the model. See
                                                             0.9      Appendix D for raw cosine similarity scores.    6
    8                                                            Finally, Diff-Diff CosSim jumped at the final transformer
   10                                                           layer (indexed as L), suggesting convergence to a more uni-
                                                             0.8
   12                                               form effect at the unembedding. Figure 3 shows pairwise
                                                             cosine similarities between Ex[∆F<L,i(x)] across injectionLayer14                                                             0.7CosSim
   16                                                            layers, revealing roughly three direction groups. First, the
                                                             0.6        last-layer steering effect was the most dissimilar from all   18
   20                                                       0.5        others, with cosine similarity around 0.21. Effects from
                                                             other layers were much more similar, with a minimum co-   22                                                       0.4
                                                               sine similarity of 0.75, but still formed two clusters: layers   24
                                                             0.3       0-17 and layers 18-26. In the next two sections, we inspect
   26
                                                             0.2      two representative layers and study their behavioral effects
      0  2  4  6  8  10 12 14 16 18 20 22 24 26           on generation, as well as the mechanisms that produced
                         Layer
                                                        them.
Figure 3. Similarity of steering-induced unembedding biases
(Qwen2.5-Math-7B). Each cell shows the cosine similarity be-
                                                        5. Last Layer – Token Substitutiontween the average hidden-state shift at the final transformer layer
induced by steering at layers i and j. High similarity among
                                                           Figure 1 shows that training only the last-layer vector s27 ini, j < L indicates that steering from most layers produces a simi-
lar bias at the unembedding, largely independent of the injection    Qwen2.5-Math-7B closed over 50% of the gap between the
point. In contrast, steering at the last layer directly yields a qualita-    base model and full training. Since it accounts for a large
tively different shift, implying a distinct mechanism.                part of the performance gain, we expected it to implement
                                                        a simple and efficient strategy. With no subsequent layers
                                                                  to process it, s27 acts directly at the unembedding and does
  own steering direction).                                  not change earlier hidden states. This makes it behave like
                                                         token substitution: it boosts logits of tokens it aligns with.
Figure 2 (top) shows the results for Qwen2.5-Math-7B. Diff-
                                      We read out these preferences using a logit-lens pro-Diff CosSim indicates that (a) alignment of the induced shifts
                                                                jection (nostalgebraist, 2020), multiplying s27 by the un-gradually decayed as the perturbation propagated; (b) the
                                                   embedding matrix (omitting the pre-unembed layer norm).next layer received an almost uniform shift (cosine similari-
                                                The top token was ”To”, with cosine similarity 0.37; seeties were always ≥0.8); (c) the shifts never became orthog-
                                                   Appendix G for the top-10 tokens.onal (consistently > 0.3); and (d) alignment increased again
in later layers. Taken together, these observations suggest   We next validated the behavioral impact on the model’s
that the induced shifts drifted as they traversed the network,    generations. Although the vector is added unconditionally,
but remained clustered around a common direction.           the softmax nonlinearity means its effect depends on the
                                                                             initial logits. Let x be the prefix (prompt plus the generatedIn contrast, Diff-Vector CosSim dropped rapidly with dis-
                                                          tokens so far). The next-token distribution istance from the injection layer: the propagated shifts were
nearly orthogonal to each layer’s own steering vector. Note
                                                                   p(x) = Softmax(g(F<L(x))),
that this alone does not imply different behaviors – orthogo-
nal steering directions could still induce the same behavior                                                     where g is the unembedding matrix, and F<L(x) is the final
(Jacob & Turner, 2024). To test this possibility, we trained                                                       hidden state. With steering at the last layer, the induced
two steering vectors constrained to be orthogonal to the                                                      change is
original vector at layers 14 and 15, and found that neither
                                                 ∆p = p(x; sL) −p(x).
orthogonal vector matched the performance of the original
one (Appendix F). Together with the Diff-Diff CosSim re-  We estimated ∆p on 1000 DeepScaleR prompts, obtaining
sults, these findings suggest that earlier-layer steering does    a score per token at each generation position.
not simply express the later-layer steering directions, and
                                                    Grouping by token, the largest increases were again for
that steering at different layers relies on different learned
                                          "To" and " To", concentrated at the first generated token
mechanisms.
                                                           (Figure 4, top-left). To test causality, we appended "To"
We observed a similar pattern for Llama3.1-8B-It (Figure 2,    to each prompt and evaluated the base model: accuracy
bottom). The main difference was that Diff-Diff CosSim    increased by 10–11 points under both greedy decoding and

                                                4

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

                                                                              Base          Base + "To"            Last Steering
                                                                  40                        38.7                                                                                                         35.5
    0.8                                                                                                                                               29.4                                                                  30                                                                                                24.8                                         25.5Difference0.6
                                                                  20                                         14.3    0.4                                                                                                                                                                                                                                   Performance    0.2                                                            10Probability    0.0                                                                   0
            "To"       " To"       ":\n"        ")."       " the"      "To"                         Greedy                      Sampling
                                                              at Pos. 0

                                               Qwen2.5-Math-7B

                                                                              Base         Base + "Step"           Last Steering
                                                                                                         22.4      21.6                                                                                                21.5
    0.8                                                            20
    0.6                                                            15                                                  13.4      14.7Difference                                                                                                                               11.7
    0.4                                                            10    0.2                                                                                                                                                                                                               Performance                                                                   5Probability    0.0                                                                   0
          "Step"    " final"  " solution"    " ="      ".\n\n"    "Step"                        Greedy                      Sampling
                                                              at Pos. 0

                                                      Llama3.1-8B-It

Figure 4. Last-layer steering vector. Left: distribution of token-level probability change induced by the last-layer vector over 1000
DeepScaleR prompts. We include the top-5 tokens by maximum change and highlight the most affected token at the zeroth generation
 position. Right: prefixing this token to each prompt reproduces a substantial fraction of the vector’s accuracy gain under both greedy
decoding and sampling.


sampling, which is about 75% of the gain from s27 (Figure 4,    6. Penultimate Layer – Circuit
 top-right).
                                                              Steering the penultimate layer s26 yielded a larger accuracy
The Llama3.1-8B-It results follow the same qualitative pat-    gain than steering the last layer, while remaining tractable to
 tern as Qwen2.5-Math-7B, but the effect is weaker. In Fig-    analyze because the modified activations traversed only one
ure 1, the last-layer vector closes only a modest fraction    remaining block. Here we identify which parts of that block
of the gap to full training, and a logit-lens readout of    converted the steering signal into improved performance.
s31 shows weak alignment with any single token (max co-
                                                       For a given residual input X, the layer computes Y = sine similarity 0.12; Appendix G). Nonetheless, the highest-
                               X + MHA(LN(X)) and Z = Y + MLP(LN(Y )), wherescoring tokens were variants of "Step" and "final",
                                     LN denotes layer normalization. The Multi-Head Attentionand the induced probability changes were concentrated at
                                           (MHA) mechanism consists of several attention heads, eachthe first generated position, primarily promoting "Step"
                                                             defined as(Figure 4, bottom-left). Appending "Step" to each prompt
improved the base model under both sampling and greedy              UW iQ (UW iK )⊤ !    V
                                                                                                                                                                  i  ,decoding. Interestingly, under greedy decoding this prefix       Hi(U) = Softmax      √dk    UW
even outperformed last-layer steering, plausibly because a
 last-layer vector cannot condition its influence on position         Q   K        V                                                   where W i  , W i  , and W i  are the query, key, and value
and thus also perturbs later steps.                                                               projection matrices for head i. The outputs of all heads are
 Finally, we note that the same conclusions hold for mod-    concatenated and linearly transformed using an output pro-
 els trained on a different dataset. In Appendix H we reran     jection matrix W O, forming the complete attention output:
the experiment with models trained on open-s1 (Dang &                                          MHA(U) = [H1(U); H2(U); . . . ; Hh(U)]W O.
Ngo, 2025). Again, the last-layer vectors emphasized "To"
(Qwen) and "Step" (Llama), primarily at the first genera-   The Multi-Layer Perceptron (MLP) sublayer applies a two-
 tion position.                                                  layer feedforward transformation of the form

                                               MLP(U) = f(UW1 + b1)W2 + b2,

                                                 5

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

               Steer Q-Proj                      Steer K-Proj                      Steer V-Proj
   38                                          s26                                           s26                                           s26

   36

Acc.34                                 Skip-Attn                                  Skip-Attn                                  Skip-Attn
Mean32
                                                s27                                           s27                                           s27   30
   28                              Skip-Layer                               Skip-Layer                               Skip-Layer

      0   3   6   9   12  15  18  21  24  27     0        1        2        3          0        1        2        3
                   Head                             Head                             Head

Figure 5. Penultimate-layer steering in Qwen2.5-Math-7B. Mean accuracy when injecting s26 into a single projection of the final block:
(Q: left, K: center, V : right). Injecting only into value head V1 closes the gap between Skip-Attn and s26, indicating that the gain is
carried by the value-output path and is largely independent of changes to the attention patterns.





Figure 6. Case study for Qwen2.5-Math-7B. Token-level probability shifts (∆p) induced by penultimate-layer steering. Three patterns
emerge: row 1 amplifies the first generated token "To"; row 2 promotes process words ("solution" to "calculations"); row 3
favors structural tokens that start Python code comments and newlines, instead of continuing the current sentence.


where f(·) is a nonlinear activation function. To understand     effect, whereas placing s26 only in V1 closes the gap to the
the contribution of each submodule, we inserted or omit-     full s26 result.
ted the steering vector sL−1 at specific points within the
                                                          Since neither Q nor K projections are affected by the steer-
layer and measured the resulting change in mean accuracy.
                                                          ing vector, the resulting QK circuit (Elhage et al., 2021)
Specifically, we analyzed the following setups:
                                                         remains unchanged, preserving the attention pattern and the
 • Unmodified: X ←X + sL−1;                          flow of information between tokens. Moreover, because
 • Skip-Attn: Y ←Y + sL−1;                           s L−1W 1V W 1O enters the residual regardless of attention
 • Skip-Layer: Z ←Z + sL−1;                           weights (Appendix I), this is equivalent to adding the pro-
                                Q/K/V               jected vector just before the MLP, i.e., skipping attention. In- • Steer-Q/K/V-Proj: for a head i, (UW i       ) 7→(U +
         Q/K/V                                           deed, a vector trained directly on the post-attention residual
  sL−1)W i           .                                                        reached 38.8 ± 0.6 mean accuracy, matching s26. Overall,
If the change was significant, we marked the corresponding    the penultimate vector in Qwen2.5-Math-7B acts via two
block as being important for processing the steering vector.    routes: a direct effect on the unembedding and an interaction
                                                         with the MLP, largely bypassing attention.
Figure 5 gives three takeaways for the Qwen2.5-Math-
7B: (i) Skip-Layer reduces accuracy relative to passing s26    Figure 6 shows how adding the steering vector to the post-
through the block, showing an effect on the unembedding     attention residual stream shifts token probabilities. Beyond
comparable to s27; (ii) Skip-Attn preserves over half of the    boosting the probability of the first token "To",  it pro-
s26 gain, pointing to the MLP as the main contributor; (iii)    motes process words (e.g., replacing "solution" with
patching any single Q or K, or a Vj with j ̸= 1, has little   "calculations"), possibly to deter premature endings.


                                                6

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

                   Q-Proj                               K-Proj                               V-Proj
                                                s30                                           s30                                           s30
   20

   19
                                       Skip-Attn                                  Skip-Attn                                  Skip-Attn   18
Acc.
   17
Mean   16
   15                                          s31                                           s31                                           s31
   14                              Skip-Layer                               Skip-Layer                               Skip-Layer

      0    4    8   12   16   20   24   28      0    1    2    3    4    5    6    7     0    1    2    3    4    5    6    7
                   Head                             Head                             Head

Figure 7. Penultimate-layer steering in Llama3.1-8B-It. Mean accuracy when injecting the penultimate-layer vector s30 into a single
projection of the final block (Q: left, K: center, V : right). Injecting into any single projection remains close to Skip-Attn and falls short of
the full s30 result.

              Skip-Head                          Steer-Head                                            s30                                                  s30
   20                                           20
Acc.18                                                    Acc.18
                                    Skip-Attn                                        Skip-Attn
   16                                           16Mean                                                       Mean                                            s31                                                  s31                                 Skip-Layer                                     Skip-Layer
   14                                           14
      0   4   8   12  16  20  24  28             0   4   8   12  16  20  24  28
                  Head                                  Head

Figure 8. Penultimate-layer steering in Llama3.1-8B-It. Mean accuracy when patching whole heads in the final block with s30:
Skip-Head (left, steer all heads except head i) and Steer-Head (right, steer only head i). No single head closes the gap between Skip-Attn
and s30, indicating a cooperative multi-head effect.


It also favors structural tokens such as Python comment     post-attention residual stream yielded performance indistin-
markers and newlines, which often precede math blocks and    guishable from s30 (mean accuracy 19.9 ± 0.1), suggesting
may support in-code reasoning.                                 either that the vector effectively bypasses attention or that
                                                                   attention contributes through a cooperative multi-head route.
In contrast to Qwen, in Llama the projection-level patching
(Steer–Q/K/V) did not reveal the source of the gain (Fig-
ure 7). We therefore went further and patched entire heads    7. Transfer of Steering Vectors Across Models
using two setups:
                                                                  Finally, we tested whether the improvements induced by
 • Steer-Head: (Hi(U)7→Hi(U + s L−1))                    steering vectors transferred to other models within the same
 • Skip-Head: (leave Hi(U) unchanged while steering all     family. In this experiment, we used all-layer steering vectors,
   other heads).                                           following the setup of Sinii et al. (2025). We considered
                                                               three model groups:
In Figure 8, two baselines mirror the Qwen result: Skip-
Layer performs close to s31, indicating a direct unembed-      • {Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-Math-
ding effect, and Skip-Attn retains about 70% of the s30     7B}
gain, suggesting that much of the impact bypasses atten-      • {Qwen2.5-1.5B,  Qwen2.5-1.5B-Instruct,  Qwen2.5-
tion. No single head closes the remaining gap between s30      Math-1.5B}
and Skip–Attn, pointing to a cooperative multi-head mech-      • {Llama3.1-8B, Llama3.1-8B-Instruct}
anism and the importance of the attention layer for s30’s
                                                   where models within a group share hidden size and depth.
performance. Still, training the steering vector directly in the


                                                7

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

Table 1. Transferability of steering vectors within model fami-    obtain them is contrastive extraction from activation pairs
lies. Each cell reports the normalized gain when steering vectors
                                                                           (e.g., positive vs. negative sentiment) (Turner et al., 2023a;
trained on the Donor model are applied to the Recipient model.
Values are normalized such that the recipient equipped with its    Panickssery et al., 2023; Liu et al., 2023; Zou et al., 2023).
own vectors equals 1.0, and the base model (no vectors) equals 0.0;   Beyond extraction, steering directions can also be trained:
negative values indicate degradation. “—” denotes not applicable    optimized with preference data for controllable generation
(no Math checkpoint available).                             (Cao et al., 2024), or learned as simple additive vectors that
                                                            surface latent behaviors such as step-by-step reasoning or
                               Donor                                                                   self-reflection (Mack & Turner, 2024; Engels et al., 2025;
 Family         Recipient  Base   Instruct  Math     Betley et al., 2025).

                Base        1.00     0.38     0.32      In this work, we interpret steering vectors trained with
 Qwen2.5-1.5B   Instruct     0.94     1.00     0.31     GRPO-like objective using standard tools from mechanis-
               Math       0.36     0.21     1.00       tic interpretability – logit-lens to read out token-level
                                                                   effects (nostalgebraist, 2020), path patching to localize cir-                Base        1.00     0.36     0.74
                                                                  cuits (Wang et al., 2022), and circuit-style analyses in the Qwen2.5-7B     Instruct     0.55     1.00     -0.34
                                         QK/OV framework (Elhage et al., 2021).               Math       0.32     0.05     1.00

                Base        1.00     0.28   —
 Llama-3.1-8B                                          9. Conclusion                    Instruct     0.74     1.00   —

                                       We presented a mechanistic interpretation of trainable steer-
                                                          ing vectors for mathematical reasoning. Across Qwen2.5-
For Llama3.1-8B-Instruct, we used the same chat template   Math-7B and Llama3.1-8B-It, our results show three re-
as the Llama3.1-8B. For each ordered pair within a group,    curring patterns. First, last-layer steering behaves like first-
we swapped the donor model’s steering vectors into the    token biasing at the unembedding: it preferentially promotes
recipient and report the relative gain: scores are normalized     specific opening tokens (e.g., "To"/"Step"), and simple
by the gap between the base model and the same model    prefixing recovers a large fraction of the last-layer gain in
equipped with its own vectors. Raw (unnormalized) scores   Qwen and a smaller but consistent effect in Llama. Second,
are provided in Appendix J.                                   penultimate-layer steering achieves most of its benefit while
                                                               largely bypassing attention: in Qwen the gain is carriedTable 1 summarizes the results. In most cases, the trans-
                                                              primarily by a value-output path (with little dependence onfer yielded in a non-trivial gain, suggesting that the direc-
                                                                  attention-pattern changes), while in Llama no single projec-tions associated improved math performance are largely pre-
                                                                   tion or head explains the full effect, pointing to a cooperativeserved after under fine-tuning and instruction tuning. Also,
                                                           multi-head contribution even though post-attention steeringreusing steering vectors trained on a related base model
                                                      can match the penultimate-layer result. Third, steering ef-can be a simple, low-cost way to improve performance on
                                                                  fects do not propagate as a “virtual” later-layer direction:closely matched models.
                                                             as perturbations travel forward, their directions diffuse and
                                                become nearly orthogonal to later-layer steering vectors,
8. Related Work                                          indicating distinct learned mechanisms across layers. In
                                                                addition, our single-layer training setup provides a simpleReinforcement learning with verifiable rewards. Jaech
                                                 community baseline: training one vector at a time isolateset al. (2024) demonstrated the striking performance of RL-
                                                                   layer-specific effects and enables reproducible comparisonstuned reasoning models, sparking a wave of follow-ups that
                                                                to any linear intervention at the same layer across modelsdevelop these models (Guo et al., 2025; Zeng et al., 2025;
                                                     and datasets.Liu et al., 2025a; Hu et al., 2025). Subsequent work has
examined why this training is effective, analysing model    Although most of these qualitative findings hold in both
behaviour and the sources of its gains (Wang et al., 2025;    models, the effects are consistently stronger and cleaner in
Ye et al., 2025; Shao et al., 2025; Liu et al., 2025b). We   Qwen than in Llama (e.g., larger last-layer gains and clearer
contribute with a mechanistic study of the changes induced     localization in the final block), suggesting model-specific
by reasoning training.                                      implementations of similar high-level behaviors. Systematic
                                                       comparisons across architectures and training recipes maySteering vectors are small additive perturbations to the
                                                          help explain these differences. A promising direction isresidual stream that modulate model behavior. They are
                                                                to pin down the precise mechanisms of mid-layer steeringwidely viewed as feature amplifiers – strengthening existing
                                                                vectors, which often yield strong improvements but arecomputations rather than introducing new mechanisms – and
                                                            harder to localize to a single submodule.have been used to toggle or amplify reasoning-like behaviors
(Venhoff et al., 2025; Ward et al., 2025). A common way to


                                                8

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

Overall, steering vectors provide a compact and informative    He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J.,
probe of reasoning-trained models, offering concrete insight      Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: A
into how RL-induced changes manifest in token preferences       challenging benchmark for promoting agi with olympiad-
and internal circuitry.                                              level bilingual multimodal scientific problems.  arXiv
                                                                 preprint arXiv:2402.14008, 2024.
Impact Statement                                   Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
                                                                            S., Tang, E., Song, D., and Steinhardt, J. Measuring math-
This paper presents work whose goal is to advance the field
                                                             ematical problem solving with the math dataset. arXiv
of machine learning.  There are many potential societal
                                                                 preprint arXiv:2103.03874, 2021.
consequences of our work, none of which we feel must be
specifically highlighted here                             Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum,
                                                            H.-Y.  Open-reasoner-zero: An open source approach
References                                                    to scaling up reinforcement learning on the base model.
                                                           arXiv preprint arXiv:2503.24290, 2025.
Ahmadian, A., Cremer, C., Gall´e, M., Fadaee, M., Kreutzer,
                                                           Jacob, G.-W. and Turner, A.  I found >800 “orthogonal”   J., Pietquin, O.,  ¨Ust¨un, A., and Hooker, S.  Back
                                                            write-code steering.  https://www.lesswrong.   to basics:  Revisiting reinforce style optimization for
                                            com/posts/CbSEZSpjdpnvBcEvc/  learning from human feedback in llms. arXiv preprint
                                           i-found-greater-than-800-orthogonal-write-code-st  arXiv:2402.14740, 2024.
                                                           2024. LessWrong. Accessed 2025-09-24.
Betley, J., Bao, X., Soto, M., Sztyber-Betley, A., Chua, J.,                                                            Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,
  and Evans, O. Tell me about yourself: Llms are aware of                                                               A., Low, A., Helyar, A., Madry, A., Beutel, A., Car-
   their learned behaviors. arXiv preprint arXiv:2501.11120,                                                                ney, A., et al. Openai o1 system card. arXiv preprint
  2025.                                                           arXiv:2412.16720, 2024.

Cao, Y., Zhang, T., Cao, B., Yin, Z., Lin, L., Ma, F., and    Kingma, D. P. Adam: A method for stochastic optimization.
  Chen, J. Personalized steering of large language mod-      arXiv preprint arXiv:1412.6980, 2014.
   els: Versatile steering vectors through bi-directional pref-
                                                 Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,
  erence optimization. Advances in Neural Information
                                                         Michalewski, H., Ramasesh, V., Slone, A., Anil, C.,
  Processing Systems, 37:49519–49551, 2024.
                                                             Schlag, I., Gutman-Solo, T., et al. Solving quantitative
                                                             reasoning problems with language models. Advances in
Dang, Q.-A. and Ngo, C. Reinforcement learning for rea-
                                                            neural information processing systems, 35:3843–3857,
  soning in small llms: What works and what doesn’t, 2025.
                                                           2022.
  URL https://arxiv.org/abs/2503.16219.
                                                             Liu, S., Ye, H., Xing, L., and Zou, J.  In-context vec-
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,        tors: Making in context learning more effective and con-
  N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T.,       trollable through latent space steering. arXiv preprint
   et al. A mathematical framework for transformer circuits.      arXiv:2311.06668, 2023.
  Transformer Circuits Thread, 1(1):12, 2021.
                                                             Liu, Z., Chen, C., Li, W., Pang, T., Du, C., and Lin, M.
Engels, J., Nanda, N., and Rajamanoharan, S.  Interim      There may not be aha moment in r1-zero-like training
   research report: Mechanisms of awareness. AI Alignment   — a pilot study. https://oatllm.notion.site/
  Forum, 2025.  https://www.alignmentforum.    oat-zero, 2025a. Notion Blog.
  org/posts/m8WKfNxp9eDLRkCk9/                                                             Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee,
  interim-research-report-mechanisms-of-awareness.                                               W. S., and Lin, M. Understanding r1-zero-like training:
                                    A critical perspective. arXiv preprint arXiv:2503.20783,
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,                                                         2025b.
  A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,
  Vaughan, A., et al. The llama 3 herd of models. arXiv    Luo,  M.,  Tan,  S.,  Wong,   J.,  Shi,  X.,  Tang,  W.,
   preprint arXiv:2407.21783, 2024.                         Roongta, M., Cai,  C., Luo,  J., Zhang,  T.,  Li,  E.,
                                                         Popa, R. A., and Stoica,  I.   Deepscaler:  Surpass-
Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,      ing o1-preview with a  1.5b model by  scaling  rl.
  Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-    https://pretty-radio-b75.notion.site/
   centivizing reasoning capability in llms via reinforcement     DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Mode
   learning. arXiv preprint arXiv:2501.12948, 2025.            2025. Notion Blog.

                                                9

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

Mack, A. and Turner, A. Mechanistically eliciting latent    Ward, J., Lin, C., Venhoff, C., and Nanda, N. Reasoning-
  behaviors  in language models.   AI Alignment Fo-       finetuning repurposes latent representations in base mod-
  rum,  2024.   https://www.alignmentforum.       els. arXiv preprint arXiv:2507.12638, 2025.
  org/posts/ioPnHKFyy4Cw2Gr2x/
                                                        Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,  mechanistically-eliciting-latent-behaviors-in-language-1.
                                                                        E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
nostalgebraist.      interpreting  gpt:    the  logit  lens,        elicits reasoning in large language models. Advances in
  2020.       https://www.alignmentforum.      neural information processing systems, 35:24824–24837,
  org/posts/AcKRB8wDpdaN6v6ru/                   2022.
  interpreting-gpt-the-logit-lens.
                                                        Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D.,
Panickssery, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger,      Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T.,
   E., and Turner, A. M. Steering llama 2 via contrastive      Ren, X., and Zhang, Z. Qwen2.5-math technical report:
   activation addition.  arXiv preprint arXiv:2312.06681,     Toward mathematical expert model via self-improvement.
  2023.                                                    arXiv preprint arXiv:2409.12122, 2024.

Rimsky, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger, E.,    Ye, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu,
  and Turner, A. Steering llama 2 via contrastive activation        P. Limo: Less is more for reasoning.  arXiv preprint
   addition. In Proceedings of the 62nd Annual Meeting of      arXiv:2502.03387, 2025.
   the Association for Computational Linguistics (Volume 1:
                                                      Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z.,
  Long Papers), pp. 15504–15522, 2024.
                                                      and He, J. Simplerl-zoo: Investigating and taming zero
                                                             reinforcement learning for open base models in the wild.Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S.,
                                                           arXiv preprint arXiv:2503.18892, 2025.  Du, S. S., Lambert, N., Min, S., Krishna, R., et al. Spu-
  rious rewards: Rethinking training signals in rlvr. arXiv                                                      Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R.,
  preprint arXiv:2506.10947, 2025.                                                             Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al.
                                                             Representation engineering: A top-down approach to aiSinii, V., Gorbatovski, A., Cherepanov, A., Shaposhnikov,
                                                                 transparency. arXiv preprint arXiv:2310.01405, 2023.   B., Balagansky, N., and Gavrilov, D. Steering llm rea-
  soning through bias-only adaptation.  arXiv preprint
  arXiv:2505.18706, 2025.

Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez,
   J.  J., Mini, U., and MacDiarmid, M.  Steering lan-
  guage models with activation engineering. arXiv preprint
  arXiv:2308.10248, 2023a.

Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez,
   J.  J., Mini, U., and MacDiarmid, M.  Steering lan-
  guage models with activation engineering. arXiv preprint
  arXiv:2308.10248, 2023b.

Venhoff, C., Arcuschin, I., Torr, P., Conmy, A., and Nanda,
  N. Understanding reasoning in thinking language models
  via steering vectors. arXiv preprint arXiv:2506.18167,
  2025.

Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and
  Steinhardt, J. Interpretability in the wild: a circuit for
   indirect object identification in gpt-2 small. arXiv preprint
  arXiv:2211.00593, 2022.

Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B.,
  Cheng, H., He, X., Wang, K., Gao, J., et al. Reinforce-
  ment learning for reasoning in large language models with
  one training example. arXiv preprint arXiv:2504.20571,
  2025.

                                                10

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

A. Per-Layer Steering with Greedy Decoding

                                                                               Full Model           30.0                                                                      Full Model
   45                                                                                           27.5
   40                                                                                           25.0
Acc.35                                                                                              Acc.22.5                                                Base Model
Mean30                                                  Base Model         Mean20.0
   25                                                                                     17.5
   20                                                                                     15.0
                                                 Base Model. = 1   15                                                                                           12.5                                           Base Model. = 1

        0          5         10         15         20         25                       0        5        10       15       20       25       30
                                   Layer                                                                        Layer

                  Qwen2.5-Math-7B                                                Llama3.1-8B-It

 Figure 9. Single-layer steering with τ = 0. We re-evaluated the single-layer steering vectors from Section 3 under greedy decoding.


B. Raw Benchmark Scores. Layers

                               Table 2. Raw benchmark scores for Qwen2.5-Math-7B in Figure 1.

          Setup        AIME25    AIME24   AMC23   MATH500   MinervaMath   OlympiadBench     Avg.

       Base Model       3.3 ± 0.0    16.7 ± 0.0   45.8 ± 0.0   52.2 ± 0.0    12.3 ± 0.0       18.6 ± 0.0      24.8 ± 0.0
    Base Model. τ = 1    2.2 ± 1.6    10.0 ± 4.7   25.0 ± 8.2   37.7 ± 6.1     8.3 ± 1.5        10.2 ± 2.9      14.3 ± 1.7
        Full-Tune       13.3 ± 2.7   30.0 ± 4.7   64.2 ± 3.1   79.6 ± 1.2    37.3 ± 0.8       41.7 ± 0.5      43.8 ± 0.4
          Steering        17.8 ± 6.3   18.9 ± 1.6   63.3 ± 4.2   79.8 ± 0.3    35.8 ± 1.1       42.8 ± 1.1      42.9 ± 0.2
         Layer-0         7.8 ± 1.6    12.2 ± 1.6   54.2 ± 4.2   69.5 ± 1.6    29.4 ± 1.0       33.7 ± 1.1      35.1 ± 0.5
         Layer-1         5.6 ± 1.6    14.4 ± 6.3   55.8 ± 9.2   72.9 ± 2.1    27.7 ± 2.0       35.9 ± 0.4      35.8 ± 0.4
         Layer-2         7.8 ± 1.6    21.1 ± 6.8   50.8 ± 4.7   70.4 ± 1.1    29.4 ± 2.3       34.3 ± 0.6      35.6 ± 0.6
         Layer-3         6.7 ± 2.7    17.8 ± 1.6   50.0 ± 7.4   71.5 ± 1.0    30.1 ± 1.9       36.9 ± 1.1      36.4 ± 0.4
         Layer-4         7.8 ± 1.6    23.3 ± 2.7   57.5 ± 4.1   71.5 ± 0.7    31.6 ± 0.8       37.1 ± 0.5      36.7 ± 0.1
         Layer-5        12.2 ± 1.6   16.7 ± 2.7   58.3 ± 3.1   75.1 ± 0.8    28.6 ± 1.2       36.4 ± 1.0      36.9 ± 0.4
         Layer-6        12.2 ± 4.2   17.8 ± 3.1   55.8 ± 3.1   72.7 ± 1.1    27.6 ± 1.4       36.3 ± 0.6      36.4 ± 0.4
         Layer-7        11.1 ± 1.6   17.8 ± 3.1   55.0 ± 2.0   73.5 ± 0.7    30.3 ± 1.0       37.4 ± 0.7      37.3 ± 0.2
         Layer-8        11.1 ± 4.2   20.0 ± 9.8   54.2 ± 1.2   74.1 ± 1.5    30.1 ± 2.9       35.4 ± 0.5      37.0 ± 0.7
         Layer-9         8.9 ± 1.6    15.6 ± 1.6   55.8 ± 5.1   75.2 ± 0.6    28.7 ± 3.0       38.6 ± 0.3      37.6 ± 0.5
        Layer-10        12.2 ± 1.6   15.6 ± 9.6   51.7 ± 2.4   72.5 ± 0.4    31.0 ± 1.9       36.5 ± 1.4      37.2 ± 0.1
        Layer-11         8.9 ± 1.6    18.9 ± 5.7   55.0 ± 7.4   74.7 ± 0.7    30.4 ± 2.2       37.0 ± 1.3      37.4 ± 0.5
        Layer-12        11.1 ± 5.7   18.9 ± 1.6   58.3 ± 4.2   75.5 ± 0.8    28.4 ± 2.2       38.0 ± 1.1      38.1 ± 0.6
        Layer-13        15.6 ± 3.1   23.3 ± 4.7   51.7 ± 5.1   75.7 ± 0.8    29.4 ± 1.5       38.4 ± 0.9      38.4 ± 0.6
        Layer-14        10.0 ± 4.7   23.3 ± 5.4   55.0 ± 3.5   76.2 ± 0.6    29.7 ± 1.5       38.7 ± 0.6      40.0 ± 0.3
        Layer-15        13.3 ± 2.7   15.6 ± 4.2   61.7 ± 6.2   75.1 ± 0.2    30.9 ± 3.0       39.2 ± 0.8      39.2 ± 0.7
        Layer-16        13.3 ± 2.7   17.8 ± 4.2   65.0 ± 4.1   76.6 ± 1.8    32.4 ± 1.3       40.3 ± 0.7      40.1 ± 0.6
        Layer-17         8.9 ± 4.2    17.8 ± 5.7   56.7 ± 5.1   72.9 ± 0.7    27.7 ± 1.0       36.9 ± 1.4      36.6 ± 0.3
        Layer-18        11.1 ± 1.6   11.1 ± 4.2   57.5 ± 3.5   74.7 ± 0.6    28.2 ± 2.0       39.8 ± 1.4      37.5 ± 0.5
        Layer-19        14.4 ± 3.1   13.3 ± 0.0   59.2 ± 3.1   74.9 ± 0.9    33.0 ± 0.5       39.7 ± 0.7      38.4 ± 0.2
        Layer-20         7.8 ± 3.1    15.6 ± 1.6   56.7 ± 5.1   74.0 ± 0.7    30.9 ± 1.8       37.0 ± 0.6      36.6 ± 0.2
        Layer-21         6.7 ± 2.7    12.2 ± 1.6   61.7 ± 1.2   75.1 ± 0.9    30.3 ± 1.1       39.1 ± 0.7      37.5 ± 0.1
        Layer-22         8.9 ± 1.6    16.7 ± 4.7   53.3 ± 3.1   72.3 ± 1.0    28.1 ± 0.9       35.6 ± 1.4      35.5 ± 0.2
        Layer-23         5.6 ± 1.6    14.4 ± 6.3   35.8 ± 1.2   46.8 ± 5.0    13.6 ± 2.6       20.1 ± 5.4      20.9 ± 2.1
        Layer-24         6.7 ± 2.7    12.2 ± 1.6   39.2 ± 2.4   47.7 ± 5.6    12.0 ± 2.6       20.4 ± 5.2      21.1 ± 2.2
        Layer-25         7.8 ± 1.6    27.8 ± 1.6   60.8 ± 1.2   72.9 ± 0.8    31.6 ± 2.4       36.7 ± 0.7      38.2 ± 0.5
        Layer-26        10.0 ± 2.7   15.6 ± 6.8   56.7 ± 7.2   72.9 ± 1.1    27.7 ± 4.1       37.1 ± 0.5      36.9 ± 0.6
        Layer-27         5.6 ± 3.1    11.1 ± 1.6   49.2 ± 8.2   60.9 ± 0.4    20.5 ± 0.3       29.6 ± 1.3      29.4 ± 0.4





                                                11

          Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors





                            Table 3. Raw benchmark scores for Llama3.1-8b-It in Figure 1.

      Setup       AIME25   AIME24   AMC23   MATH500   MinervaMath   OlympiadBench     Avg.

   Base Model       0.0 ± 0.0   10.0 ± 0.0   27.5 ± 0.0   52.5 ± 0.0    20.5 ± 0.0       18.6 ± 0.0      21.5 ± 0.0
Base Model. τ = 1   0.0 ± 0.0    0.0 ± 0.0    11.7 ± 2.4   34.6 ± 0.7    12.3 ± 1.5        9.6 ± 0.6      11.7 ± 0.5
    Full-Tune        0.0 ± 0.0    8.9 ± 3.1    36.7 ± 1.2   57.6 ± 1.6    30.4 ± 2.3       22.4 ± 0.1      26.4 ± 0.8
     Steering         0.0 ± 0.0   11.1 ± 5.7   35.0 ± 6.1   57.6 ± 1.9    29.9 ± 0.6       23.4 ± 0.5      25.8 ± 0.2
     Layer-0         0.0 ± 0.0   10.0 ± 2.7   23.3 ± 6.6   50.7 ± 0.5    21.7 ± 0.9       19.1 ± 0.8      21.0 ± 0.4
     Layer-1         1.1 ± 1.6    3.3 ± 2.7    30.8 ± 2.4   52.1 ± 0.9    24.5 ± 1.1       19.0 ± 1.3      22.0 ± 0.4
     Layer-2         0.0 ± 0.0   10.0 ± 2.7   35.0 ± 2.0   53.1 ± 1.9    25.6 ± 1.5       18.7 ± 0.6      22.6 ± 0.4
     Layer-3         0.0 ± 0.0    6.7 ± 2.7    30.0 ± 6.1   53.9 ± 0.4    28.6 ± 0.5       18.8 ± 0.6      22.6 ± 0.1
     Layer-4         0.0 ± 0.0    7.8 ± 1.6    23.3 ± 3.1   50.8 ± 0.5    26.2 ± 1.0       19.0 ± 0.9      21.6 ± 0.2
     Layer-5         0.0 ± 0.0    6.7 ± 2.7    29.2 ± 4.7   52.7 ± 1.0    26.2 ± 2.3       19.3 ± 0.3      22.3 ± 0.3
     Layer-6         1.1 ± 1.6    4.4 ± 3.1    30.0 ± 5.4   50.8 ± 0.7    25.6 ± 1.2       19.8 ± 1.1      22.0 ± 0.1
     Layer-7         0.0 ± 0.0    4.4 ± 3.1    27.5 ± 7.1   53.3 ± 0.5    27.5 ± 1.5       20.2 ± 0.2      22.6 ± 0.2
     Layer-8         0.0 ± 0.0    7.8 ± 4.2    30.8 ± 5.1   53.3 ± 1.4    27.5 ± 1.4       19.7 ± 0.4      23.1 ± 0.2
     Layer-9         1.1 ± 1.6    5.6 ± 1.6    30.8 ± 3.1   56.9 ± 0.7    27.7 ± 1.0       21.9 ± 0.3      24.7 ± 0.1
     Layer-10        0.0 ± 0.0    4.4 ± 4.2    33.3 ± 4.7   55.5 ± 1.2    27.7 ± 1.0       21.3 ± 0.9      23.4 ± 0.1
     Layer-11        1.1 ± 1.6    4.4 ± 1.6    33.3 ± 2.4   55.1 ± 1.4    25.9 ± 0.6       20.6 ± 0.8      24.2 ± 0.1
     Layer-12        1.1 ± 1.6    7.8 ± 3.1    32.5 ± 2.0   54.3 ± 1.3    29.0 ± 1.4       22.3 ± 0.7      24.5 ± 0.4
     Layer-13        1.1 ± 1.6    7.8 ± 4.2    24.2 ± 1.2   54.7 ± 1.9    27.6 ± 2.9       20.1 ± 0.7      23.1 ± 0.1
     Layer-14        0.0 ± 0.0   11.1 ± 4.2   31.7 ± 1.2   55.1 ± 0.4    28.4 ± 0.2       20.9 ± 0.1      24.0 ± 0.1
     Layer-15        1.1 ± 1.6    4.4 ± 1.6    27.5 ± 2.0   54.0 ± 1.1    27.3 ± 1.4       21.8 ± 1.0      23.5 ± 0.4
     Layer-16        0.0 ± 0.0    7.8 ± 3.1    25.8 ± 3.1   52.9 ± 0.1    26.8 ± 0.6       20.6 ± 0.3      22.7 ± 0.1
     Layer-17        0.0 ± 0.0    5.6 ± 1.6    26.7 ± 1.2   52.1 ± 0.9    25.7 ± 0.5       18.6 ± 0.7      22.0 ± 0.2
     Layer-18        0.0 ± 0.0   10.0 ± 5.4   18.3 ± 6.6   51.5 ± 1.2    25.5 ± 1.4       19.3 ± 1.2      21.6 ± 0.5
     Layer-19        1.1 ± 1.6    6.7 ± 2.7    30.0 ± 3.5   52.9 ± 1.1    25.7 ± 2.0       19.3 ± 0.7      22.3 ± 0.6
     Layer-20        0.0 ± 0.0    6.7 ± 2.7    22.5 ± 4.1   52.9 ± 1.1    27.1 ± 1.7       19.3 ± 1.1      21.9 ± 0.1
     Layer-21        0.0 ± 0.0    4.4 ± 1.6    25.8 ± 2.4   51.7 ± 1.0    27.0 ± 0.2       18.6 ± 0.4      21.2 ± 0.3
     Layer-22        1.1 ± 1.6    7.8 ± 5.7    22.5 ± 2.0   54.9 ± 0.4    25.4 ± 2.5       18.1 ± 0.6      22.5 ± 0.6
     Layer-23        1.1 ± 1.6    6.7 ± 2.7    25.8 ± 1.2   52.1 ± 0.2    25.7 ± 0.8       19.0 ± 1.5      21.1 ± 0.2
     Layer-24        2.2 ± 1.6    5.6 ± 3.1    22.5 ± 7.4   50.6 ± 2.2    27.0 ± 2.5       18.8 ± 1.0      21.4 ± 0.6
     Layer-25        0.0 ± 0.0    6.7 ± 2.7    27.5 ± 2.0   52.1 ± 0.5    26.6 ± 1.1       20.3 ± 1.5      21.9 ± 0.4
     Layer-26        2.2 ± 1.6    5.6 ± 1.6    27.5 ± 2.0   50.5 ± 1.4    28.1 ± 1.8       17.8 ± 1.3      21.6 ± 0.5
     Layer-27        1.1 ± 1.6    4.4 ± 1.6    24.2 ± 4.2   50.5 ± 0.1    28.1 ± 0.9       18.2 ± 0.2      21.5 ± 0.2
     Layer-28        1.1 ± 1.6    5.6 ± 1.6    20.0 ± 3.5   50.5 ± 1.7    25.5 ± 1.1       17.4 ± 0.3      20.9 ± 0.1
     Layer-29        0.0 ± 0.0    6.7 ± 2.7    30.0 ± 3.5   50.5 ± 0.8    25.7 ± 2.1       16.9 ± 0.6      21.1 ± 0.6
     Layer-30        0.0 ± 0.0    7.8 ± 4.2    24.2 ± 2.4   50.4 ± 1.0    25.0 ± 0.8       15.8 ± 1.4      20.1 ± 0.5
     Layer-31        1.1 ± 1.6    1.1 ± 1.6    18.3 ± 1.2   39.0 ± 0.7    18.1 ± 2.5       11.7 ± 0.4      14.7 ± 0.5





                                            12

          Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors





                   Table 4. Raw benchmark scores for Qwen2.5-Math-7B in Figure 9 (greedy decoding).

      Setup        AIME25    AIME24   AMC23   MATH500   MinervaMath   OlympiadBench     Avg.

   Base Model       3.3 ± 0.0    16.7 ± 0.0   45.8 ± 0.0   52.2 ± 0.0    12.3 ± 0.0       18.6 ± 0.0      24.8 ± 0.0
Base Model. τ = 1    2.2 ± 1.6    10.0 ± 4.7   25.0 ± 8.2   37.7 ± 6.1     8.3 ± 1.5        10.2 ± 2.9      14.3 ± 1.7
    Full-Tune       18.9 ± 1.6   31.1 ± 1.6   67.5 ± 0.0   80.1 ± 0.1    34.9 ± 0.0       43.2 ± 0.2      46.0 ± 0.5
     Steering        16.7 ± 0.0   24.4 ± 1.6   59.2 ± 1.2   79.1 ± 0.2    33.9 ± 0.3       42.0 ± 0.1      42.6 ± 0.4
     Layer-0         8.9 ± 1.6    13.3 ± 0.0   57.5 ± 0.0   75.3 ± 0.1    29.7 ± 0.2       39.9 ± 0.2      37.4 ± 0.2
     Layer-1        13.3 ± 2.7   20.0 ± 0.0   68.3 ± 1.2   77.5 ± 0.2    28.4 ± 0.2       39.3 ± 0.1      41.1 ± 0.6
     Layer-2        16.7 ± 2.7   20.0 ± 0.0   65.0 ± 0.0   77.9 ± 0.2    30.8 ± 0.2       39.7 ± 0.1      41.7 ± 0.4
     Layer-3        10.0 ± 0.0   26.7 ± 0.0   60.8 ± 1.2   76.2 ± 0.5    29.8 ± 0.0       40.6 ± 0.1      40.7 ± 0.2
     Layer-4        11.1 ± 1.6   10.0 ± 0.0   65.0 ± 0.0   75.3 ± 0.5    30.9 ± 0.3       39.9 ± 0.3      38.7 ± 0.3
     Layer-5        15.6 ± 1.6   20.0 ± 0.0   57.5 ± 0.0   76.7 ± 0.1    29.2 ± 0.2       39.3 ± 0.3      39.7 ± 0.3
     Layer-6        16.7 ± 0.0   18.9 ± 1.6   58.3 ± 1.2   78.6 ± 0.3    27.3 ± 0.2       38.7 ± 0.1      39.7 ± 0.3
     Layer-7        13.3 ± 0.0   23.3 ± 0.0   60.8 ± 1.2   79.2 ± 0.2    29.0 ± 0.0       41.9 ± 0.1      41.3 ± 0.2
     Layer-8         7.8 ± 1.6    16.7 ± 0.0   67.5 ± 0.0   78.9 ± 0.1    27.6 ± 0.3       39.7 ± 0.1      39.7 ± 0.3
     Layer-9        13.3 ± 0.0   23.3 ± 0.0   55.0 ± 0.0   79.3 ± 0.2    30.4 ± 0.3       41.4 ± 0.2      40.5 ± 0.1
     Layer-10        16.7 ± 0.0   26.7 ± 0.0   57.5 ± 2.0   77.5 ± 0.1    31.2 ± 0.3       39.3 ± 0.1      41.5 ± 0.3
     Layer-11        12.2 ± 1.6   30.0 ± 0.0   58.3 ± 1.2   78.5 ± 0.2    33.5 ± 0.0       41.7 ± 0.3      42.4 ± 0.1
     Layer-12        16.7 ± 0.0   21.1 ± 1.6   65.8 ± 1.2   77.8 ± 0.0    33.5 ± 0.0       41.9 ± 0.4      42.8 ± 0.2
     Layer-13        16.7 ± 0.0   16.7 ± 0.0   55.8 ± 1.2   79.3 ± 0.3    31.1 ± 0.2       41.3 ± 0.1      40.1 ± 0.3
     Layer-14        15.6 ± 1.6   26.7 ± 0.0   62.5 ± 0.0   79.2 ± 0.2    31.6 ± 0.0       44.3 ± 0.3      43.3 ± 0.3
     Layer-15        16.7 ± 0.0   23.3 ± 0.0   70.0 ± 0.0   78.4 ± 0.2    32.8 ± 0.2       42.4 ± 0.1      43.9 ± 0.0
     Layer-16         8.9 ± 1.6    26.7 ± 0.0   60.0 ± 0.0   79.1 ± 0.1    36.4 ± 0.0       39.8 ± 0.3      41.8 ± 0.3
     Layer-17        13.3 ± 0.0   30.0 ± 0.0   57.5 ± 0.0   78.1 ± 0.1    34.6 ± 0.0       42.7 ± 0.1      42.7 ± 0.0
     Layer-18        12.2 ± 1.6   13.3 ± 0.0   65.0 ± 0.0   78.6 ± 0.0    29.4 ± 0.0       40.3 ± 0.1      39.8 ± 0.3
     Layer-19        16.7 ± 0.0   13.3 ± 0.0   49.2 ± 2.4   77.8 ± 0.2    30.5 ± 0.3       42.1 ± 0.1      38.3 ± 0.4
     Layer-20         0.0 ± 0.0    13.3 ± 0.0   62.5 ± 0.0   75.9 ± 0.1    30.1 ± 0.0       37.3 ± 0.1      36.5 ± 0.0
     Layer-21        10.0 ± 0.0   13.3 ± 0.0   62.5 ± 0.0   78.5 ± 0.1    32.7 ± 0.0       39.1 ± 0.1      39.4 ± 0.0
     Layer-22         8.9 ± 3.1    13.3 ± 0.0   59.2 ± 1.2   75.5 ± 0.2    27.3 ± 0.2       38.7 ± 0.1      37.1 ± 0.3
     Layer-23         4.4 ± 1.6    23.3 ± 0.0   57.5 ± 0.0   60.4 ± 0.0    17.4 ± 0.2       25.1 ± 0.1      31.4 ± 0.3
     Layer-24        16.7 ± 0.0   20.0 ± 0.0   49.2 ± 1.2   65.1 ± 0.1    18.6 ± 0.5       31.1 ± 0.3      33.4 ± 0.2
     Layer-25        16.7 ± 0.0   23.3 ± 0.0   60.0 ± 0.0   77.0 ± 0.0    30.4 ± 0.2       37.5 ± 0.1      40.8 ± 0.0
     Layer-26         6.7 ± 0.0    30.0 ± 0.0   60.0 ± 2.0   74.5 ± 0.2    27.6 ± 0.0       38.9 ± 0.1      39.6 ± 0.3
     Layer-27        13.3 ± 0.0   17.8 ± 1.6   55.8 ± 1.2   72.5 ± 0.1    34.1 ± 0.3       38.6 ± 0.4      38.7 ± 0.3





                                             13

          Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors





                    Table 5. Raw benchmark scores for Llama3.1-8b-It in Figure 9 (greedy decoding).

      Setup       AIME25   AIME24   AMC23   MATH500   MinervaMath   OlympiadBench     Avg.

   Base Model       0.0 ± 0.0   10.0 ± 0.0   27.5 ± 0.0   52.5 ± 0.0    20.5 ± 0.0       18.6 ± 0.0      21.5 ± 0.0
Base Model. τ = 1   0.0 ± 0.0    0.0 ± 0.0    11.7 ± 2.4   34.6 ± 0.7    12.3 ± 1.5        9.6 ± 0.6      11.7 ± 0.5
    Full-Tune        3.3 ± 0.0   14.4 ± 3.1   41.7 ± 2.4   57.7 ± 0.2    30.9 ± 0.6       22.3 ± 0.3      28.4 ± 0.7
     Steering         0.0 ± 0.0   11.1 ± 5.7   35.0 ± 6.1   57.6 ± 1.9    29.9 ± 0.6       23.4 ± 0.5      25.8 ± 0.2
     Layer-0         3.3 ± 0.0    6.7 ± 0.0    33.3 ± 1.2   53.7 ± 0.2    24.9 ± 0.2       19.6 ± 0.0      23.6 ± 0.2
     Layer-1         6.7 ± 0.0    8.9 ± 1.6    35.8 ± 1.2   50.8 ± 0.0    27.5 ± 0.2       18.7 ± 0.2      24.7 ± 0.5
     Layer-2         0.0 ± 0.0    3.3 ± 0.0    44.2 ± 1.2   51.6 ± 0.0    25.9 ± 0.2       20.0 ± 0.3      24.2 ± 0.2
     Layer-3         0.0 ± 0.0    5.6 ± 1.6    32.5 ± 2.0   53.9 ± 0.1    28.9 ± 0.6       19.8 ± 0.1      23.5 ± 0.4
     Layer-4         0.0 ± 0.0    6.7 ± 0.0    29.2 ± 1.2   53.1 ± 0.5    27.1 ± 0.2       18.2 ± 0.2      22.4 ± 0.3
     Layer-5         0.0 ± 0.0   10.0 ± 0.0   36.7 ± 1.2   53.5 ± 0.1    27.2 ± 0.3       20.9 ± 0.1      24.7 ± 0.2
     Layer-6         0.0 ± 0.0    6.7 ± 0.0    28.3 ± 1.2   54.4 ± 0.2    27.7 ± 0.2       18.3 ± 0.3      22.6 ± 0.2
     Layer-7         6.7 ± 0.0    6.7 ± 0.0    37.5 ± 0.0   54.3 ± 0.3    28.1 ± 0.2       22.2 ± 0.2      25.9 ± 0.1
     Layer-8         3.3 ± 0.0    3.3 ± 0.0    45.8 ± 1.2   50.7 ± 0.2    28.1 ± 0.2       22.0 ± 0.3      25.6 ± 0.3
     Layer-9         3.3 ± 0.0   15.6 ± 1.6   30.0 ± 0.0   54.7 ± 0.2    27.9 ± 0.3       21.6 ± 0.2      25.5 ± 0.3
     Layer-10        0.0 ± 0.0   13.3 ± 2.7   40.0 ± 0.0   56.8 ± 0.0    28.3 ± 0.3       20.9 ± 0.3      26.6 ± 0.4
     Layer-11        0.0 ± 0.0   11.1 ± 1.6   30.0 ± 2.0   52.7 ± 0.1    29.0 ± 0.0       20.9 ± 0.1      24.0 ± 0.4
     Layer-12        3.3 ± 0.0    4.4 ± 1.6    39.2 ± 1.2   56.7 ± 0.2    29.5 ± 0.3       23.9 ± 0.3      26.2 ± 0.4
     Layer-13        3.3 ± 0.0    7.8 ± 1.6    25.0 ± 0.0   56.6 ± 0.0    29.3 ± 0.2       21.1 ± 0.2      23.8 ± 0.3
     Layer-14        0.0 ± 0.0   10.0 ± 0.0   30.0 ± 0.0   55.4 ± 0.5    27.6 ± 0.0       22.3 ± 0.3      24.2 ± 0.1
     Layer-15        6.7 ± 0.0    6.7 ± 0.0    38.3 ± 1.2   54.5 ± 0.2    29.2 ± 0.3       21.3 ± 0.1      26.1 ± 0.2
     Layer-16        0.0 ± 0.0    6.7 ± 0.0    34.2 ± 1.2   52.5 ± 0.5    29.5 ± 0.2       18.9 ± 0.1      23.6 ± 0.2
     Layer-17        0.0 ± 0.0    3.3 ± 0.0    37.5 ± 0.0   51.4 ± 0.3    26.3 ± 0.2       19.2 ± 0.3      23.0 ± 0.0
     Layer-18        3.3 ± 0.0    7.8 ± 3.1    29.2 ± 1.2   51.1 ± 0.2    27.2 ± 0.3       19.9 ± 0.1      23.1 ± 0.7
     Layer-19        0.0 ± 0.0    3.3 ± 0.0    32.5 ± 0.0   51.6 ± 0.3    24.4 ± 0.2       19.9 ± 0.2      21.9 ± 0.1
     Layer-20        3.3 ± 0.0    6.7 ± 0.0    34.2 ± 1.2   54.6 ± 0.3    26.7 ± 0.2       17.8 ± 0.3      23.9 ± 0.1
     Layer-21        3.3 ± 0.0    3.3 ± 0.0    25.8 ± 2.4   54.4 ± 0.2    27.3 ± 0.2       16.5 ± 0.1      21.8 ± 0.4
     Layer-22        0.0 ± 0.0   10.0 ± 0.0   32.5 ± 0.0   53.0 ± 0.3    27.0 ± 0.2       22.4 ± 0.1      24.1 ± 0.0
     Layer-23        0.0 ± 0.0    7.8 ± 1.6    30.0 ± 2.0   51.9 ± 0.3    29.9 ± 0.5       17.5 ± 0.2      22.8 ± 0.6
     Layer-24        0.0 ± 0.0    3.3 ± 0.0    30.0 ± 0.0   51.5 ± 0.1    27.2 ± 0.0       16.8 ± 0.1      21.5 ± 0.0
     Layer-25        0.0 ± 0.0   10.0 ± 0.0   29.2 ± 1.2   52.3 ± 0.2    24.3 ± 0.0       18.7 ± 0.1      22.4 ± 0.2
     Layer-26        0.0 ± 0.0    4.4 ± 1.6    42.5 ± 0.0   52.8 ± 0.0    28.2 ± 0.2       17.3 ± 0.0      24.2 ± 0.3
     Layer-27        0.0 ± 0.0    3.3 ± 0.0    34.2 ± 1.2   51.5 ± 0.2    27.1 ± 0.2       18.0 ± 0.1      22.3 ± 0.2
     Layer-28        0.0 ± 0.0    4.4 ± 1.6    29.2 ± 1.2   50.3 ± 0.1    27.2 ± 0.3       16.6 ± 0.1      21.3 ± 0.1
     Layer-29        0.0 ± 0.0    3.3 ± 0.0    36.7 ± 1.2   52.9 ± 0.1    28.1 ± 0.2       15.8 ± 0.1      22.8 ± 0.2
     Layer-30        0.0 ± 0.0    6.7 ± 0.0    36.7 ± 4.7   52.3 ± 0.1    24.9 ± 0.2       18.2 ± 0.2      23.1 ± 0.7
     Layer-31        0.0 ± 0.0   10.0 ± 0.0   28.3 ± 1.2   50.5 ± 0.3    25.1 ± 0.2       15.8 ± 0.1      21.6 ± 0.2





                                            14

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

C. Ineffective Layers in Qwen2.5-Math-7B

As noted in Section 3, single-layer steering on layers 23 and 24 underperforms their neighbors. To pinpoint where this loss
arises, we trained vectors inserted immediately after each subcomponent between the layer-24 MLP and the layer-25 MLP.
Figure 10 shows that placing s24 after the input LayerNorm of layer 25 closes the gap with s25. Thus the input LayerNorm
is the problematic step – passing through it limits the effect of the steering vector.





                                                                   38.2 ± 0.5   40                                               37.2 ± 0.6                                      36.2 ± 0.4                       35.2 ± 0.2

   30
        21.1 ± 2.2
   20Performance
   10

    0
      24       LN         Attn       LN       25    Layer        Input      25                      Layer           25         Layer          Post-Attn          Layer                25                                 Layer

Figure 10. Steering of a specific component’s output. We trained steering vectors on intermediate component outputs between layers 24
(where performance dropped) and 25 (where it recovered). Steering after the layer-25 input LayerNorm largely restored performance,
indicating that the drop was mitigated once the intervention was carried past this normalization step.





                                                15

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

D. Steering Vector Persistence. Raw Numbers





                             Table 6. Qwen2.5-Math-7B. Raw scores for the plots in Figure 2 (top).

                                                           Diff-Diff CosSim

 Layer  0   1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27

   0   1  0.96  0.91  0.84  0.8  0.75  0.67  0.59  0.54  0.54  0.52  0.49  0.47  0.45  0.43  0.42  0.4  0.37  0.37  0.37  0.35  0.34  0.34  0.33  0.33  0.31  0.37  0.57
   1       1   0.95  0.85  0.8  0.74  0.65  0.58  0.51  0.51  0.49  0.45  0.43  0.42  0.39  0.38  0.36  0.34  0.33  0.33  0.31  0.32  0.32  0.31  0.31  0.29  0.29  0.41
   2            1   0.96  0.91  0.86  0.78  0.7  0.64  0.64  0.61  0.57  0.54  0.52  0.5  0.48  0.45  0.42  0.42  0.41  0.39  0.37  0.37  0.37  0.36  0.34  0.38  0.55
   3                  1   0.93  0.87  0.78  0.69  0.64  0.63  0.61  0.57  0.55  0.53  0.51  0.49  0.47  0.44  0.43  0.41  0.39  0.37  0.35  0.34  0.32  0.3  0.32  0.57
   4                       1   0.92  0.82  0.73  0.67  0.67  0.64  0.6  0.58  0.56  0.53  0.51  0.49  0.46  0.45  0.44  0.41  0.39  0.37  0.36  0.34  0.32  0.34  0.61
   5                            1   0.88  0.77  0.71  0.69  0.66  0.62  0.59  0.56  0.53  0.51  0.49  0.46  0.44  0.43  0.4  0.38  0.37  0.36  0.35  0.32  0.36  0.62
   6                                 1   0.82  0.72  0.7  0.65  0.61  0.58  0.55  0.52  0.5  0.47  0.44  0.44  0.42  0.39  0.38  0.36  0.35  0.33  0.32  0.33  0.59
   7                                      1   0.85  0.81  0.74  0.67  0.63  0.6  0.56  0.54  0.51  0.47  0.46  0.45  0.41  0.39  0.38  0.37  0.35  0.33  0.36  0.64
   8                                           1   0.87  0.77  0.69  0.64  0.6  0.56  0.54  0.51  0.47  0.46  0.45  0.41  0.4  0.38  0.37  0.36  0.33  0.36  0.64
   9                                                 1   0.85  0.75  0.7  0.64  0.59  0.55  0.52  0.48  0.47  0.45  0.42  0.41  0.39  0.39  0.37  0.35  0.37  0.68
  10                                                     1   0.85  0.76  0.69  0.64  0.6  0.57  0.52  0.51  0.49  0.46  0.45  0.43  0.42  0.39  0.37  0.4   0.7
  11                                                          1   0.84  0.75  0.67  0.63  0.58  0.53  0.51  0.51  0.48  0.47  0.45  0.43  0.41  0.39  0.4  0.68
  12                                                                1   0.85  0.76  0.71  0.66  0.58  0.56  0.55  0.52  0.5  0.48  0.48  0.45  0.44  0.44  0.72
  13                                                                     1   0.86  0.78  0.72  0.63  0.6   0.6  0.58  0.56  0.54  0.52  0.49  0.47  0.48  0.73
  14                                                                          1   0.86  0.77  0.66  0.63  0.63  0.6  0.59  0.56  0.55  0.51  0.5   0.5  0.71
  15                                                                               1   0.85  0.74  0.68  0.67  0.64  0.62  0.59  0.59  0.55  0.55  0.56  0.75
  16                                                                                    1    0.8  0.73  0.72  0.69  0.68  0.64  0.65  0.62  0.59  0.58  0.7
  17                                                                                         1   0.81  0.73  0.67  0.64  0.6  0.62  0.58  0.57  0.55  0.57
  18                                                                                              1   0.87  0.79  0.74  0.68  0.66  0.62  0.63  0.67  0.71
  19                                                                                                    1   0.88  0.81  0.74  0.7  0.66  0.67  0.71  0.75
  20                                                                                                         1    0.9  0.83  0.78  0.72  0.72  0.72  0.76
  21                                                                                                              1    0.9  0.85  0.79  0.75  0.72  0.81
  22                                                                                                                   1   0.93  0.86  0.81  0.72  0.66
  23                                                                                                                        1    0.9  0.81  0.71  0.55
  24                                                                                                                             1   0.83  0.74  0.49
  25                                                                                                                                   1    0.9  0.92
  26                                                                                                                                        1   0.92
  27                                                                                                                                             1

                                                       Diff-Vector CosSim


 Layer  0   1    2    3    4    5    6      7       8       9       10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27
   0   1  0.31  0.33  0.29  0.24  0.23  0.18    0.14     0.12     0.11     0.11      0.1       0.1    9 · 10−2  8 · 10−2  7 · 10−2  7 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  4 · 10−2  2 · 10−2  2 · 10−2  2 · 10−2  4 · 10−2    0.12
   1       1   0.33  0.3  0.17  0.16  0.12  9 · 10−2  9 · 10−2  8 · 10−2  8 · 10−2  8 · 10−2  7 · 10−2  7 · 10−2  6 · 10−2  6 · 10−2  6 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  3 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  3 · 10−2  8 · 10−2
   2            1   0.55  0.47  0.38  0.3    0.23     0.19     0.18     0.18     0.15     0.14     0.13     0.11      0.1    9 · 10−2  7 · 10−2  7 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  5 · 10−2  4 · 10−2  3 · 10−2  2 · 10−2  3 · 10−2    0.11
   3                  1   0.56  0.44  0.32    0.26     0.23     0.22     0.21     0.18     0.17     0.16     0.14     0.13     0.11   8 · 10−2  8 · 10−2  7 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  3 · 10−2  5 · 10−2    0.12
   4                       1   0.51  0.36    0.29     0.26     0.24     0.23     0.19     0.18     0.16     0.14     0.13     0.12   9 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  6 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  3 · 10−2  5 · 10−2    0.13
   5                            1   0.43    0.32     0.26     0.25     0.23      0.2      0.17     0.16     0.13     0.13     0.12   8 · 10−2  8 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  3 · 10−2  5 · 10−2    0.13
   6                                 1     0.43     0.32     0.28     0.25     0.22     0.19     0.18     0.14     0.13     0.12   9 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  3 · 10−2  4 · 10−2    0.12
   7                                        1       0.48      0.4      0.34     0.28     0.24     0.21     0.17     0.16     0.14      0.1    9 · 10−2  8 · 10−2  6 · 10−2  6 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  3 · 10−2  5 · 10−2    0.14
   8                                                1       0.47     0.37      0.3      0.25     0.22     0.18     0.16     0.14      0.1    9 · 10−2  7 · 10−2  5 · 10−2  6 · 10−2  6 · 10−2  6 · 10−2  5 · 10−2  3 · 10−2  5 · 10−2    0.14
   9                                                        1       0.47     0.36     0.29     0.26      0.2      0.18     0.16     0.11      0.1    8 · 10−2  7 · 10−2  6 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  5 · 10−2    0.14
  10                                                                1       0.51     0.38     0.32     0.24     0.21     0.18     0.13     0.12   9 · 10−2  7 · 10−2  7 · 10−2  7 · 10−2  7 · 10−2  7 · 10−2  4 · 10−2  6 · 10−2    0.15
  11                                                                        1       0.49     0.39     0.29     0.26     0.21     0.13     0.12      0.1    7 · 10−2  7 · 10−2  7 · 10−2  8 · 10−2  7 · 10−2  4 · 10−2  7 · 10−2    0.14
  12                                                                                1       0.56      0.4      0.31     0.24     0.16     0.13      0.1    7 · 10−2  7 · 10−2  8 · 10−2  8 · 10−2  7 · 10−2  4 · 10−2  7 · 10−2    0.15
  13                                                                                        1       0.55     0.41      0.3       0.2      0.16     0.12   9 · 10−2  9 · 10−2  8 · 10−2  9 · 10−2  8 · 10−2  5 · 10−2  8 · 10−2    0.15
  14                                                                                                 1       0.53     0.37     0.23     0.18     0.13     0.11      0.1    9 · 10−2  9 · 10−2  9 · 10−2  5 · 10−2  9 · 10−2    0.15
  15                                                                                                         1       0.55     0.35     0.27     0.19     0.15     0.12     0.12     0.13     0.12   6 · 10−2    0.12     0.16
  16                                                                                                                 1       0.44     0.29     0.19     0.13     0.11      0.1      0.11      0.1    5 · 10−2    0.1      0.15
  17                                                                                                                         1       0.33     0.18     0.13     0.11   9 · 10−2    0.1       0.1    4 · 10−2  8 · 10−2    0.11
  18                                                                                                                                 1       0.36     0.28      0.2      0.19     0.21     0.19      0.1       0.2      0.13
  19                                                                                                                                          1       0.42     0.27     0.24     0.26     0.23     0.13     0.24     0.14
  20                                                                                                                                                  1       0.41     0.36     0.35      0.3      0.18     0.26     0.15
  21                                                                                                                                                          1       0.38     0.33     0.28     0.24     0.29     0.17
  22                                                                                                                                                                  1       0.48     0.37     0.32     0.22     0.15
  23                                                                                                                                                                          1       0.68     0.17     0.22      0.1
  24                                                                                                                                                                                   1       0.17     0.22   7 · 10−2
  25                                                                                                                                                                                           1       0.48     0.23
  26                                                                                                                                                                                                   1       0.24
  27                                                                                                                                                                                                           1


                                                16

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

                            Table 7. Llama3.1-8B-It. Raw scores for the plots in Figure 2 (bottom).

                                                          Diff-Diff CosSim


Layer  0   1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31

  0   1  0.84  0.68  0.55  0.46  0.38  0.34  0.31  0.3  0.29  0.26  0.24  0.27  0.34  0.35  0.37  0.4  0.42  0.43  0.46  0.44  0.46  0.46  0.46  0.45  0.45  0.46  0.46  0.44  0.42  0.53  0.57
  1       1   0.77  0.61  0.51  0.42  0.36  0.32  0.29  0.28  0.25  0.24  0.25  0.33  0.34  0.36  0.4  0.43  0.45  0.47  0.45  0.48  0.48  0.49  0.48  0.47  0.48  0.48  0.47  0.44  0.62  0.57
  2            1   0.74  0.61  0.5  0.44  0.38  0.34  0.32  0.29  0.27  0.28  0.33  0.33  0.36  0.38  0.41  0.42  0.44  0.43  0.45  0.45  0.45  0.44  0.44  0.44  0.46  0.45  0.43  0.58  0.56
  3                  1   0.78  0.62  0.51  0.44  0.38  0.35  0.31  0.29  0.31  0.37  0.39  0.4  0.45  0.46  0.47  0.49  0.48  0.5   0.5  0.51  0.5  0.49  0.5   0.5   0.5  0.47  0.54  0.58
  4                       1   0.75  0.61  0.5  0.44  0.38  0.33  0.3  0.31  0.43  0.44  0.47  0.51  0.52  0.53  0.55  0.53  0.56  0.56  0.58  0.56  0.55  0.56  0.55  0.54  0.5  0.53  0.53
  5                            1   0.77  0.62  0.52  0.47  0.42  0.38  0.36  0.39  0.37  0.38  0.42  0.44  0.45  0.46  0.45  0.47  0.47  0.48  0.47  0.46  0.47  0.47  0.46  0.43  0.55  0.57
  6                                 1    0.8  0.66  0.58  0.52  0.47  0.46  0.47  0.45  0.46  0.48  0.51  0.51  0.52  0.51  0.53  0.54  0.55  0.54  0.53  0.54  0.59  0.57  0.55  0.58  0.64
  7                                      1   0.77  0.66  0.55  0.48  0.45  0.51  0.5  0.52  0.54  0.56  0.57  0.58  0.56  0.59  0.59  0.6  0.59  0.57  0.6  0.59  0.58  0.54  0.53  0.56
  8                                           1   0.82  0.68  0.58  0.53  0.51  0.48  0.47  0.51  0.53  0.53  0.54  0.52  0.54  0.54  0.54  0.53  0.51  0.52  0.52  0.51  0.48  0.48  0.55
  9                                                 1   0.82  0.7  0.62  0.58  0.55  0.53  0.55  0.57  0.56  0.56  0.55  0.57  0.57  0.58  0.56  0.55  0.55  0.55  0.53  0.5   0.5  0.59
  10                                                     1   0.82  0.71  0.64  0.59  0.56  0.58  0.59  0.58  0.58  0.56  0.59  0.59  0.6  0.59  0.58  0.59  0.58  0.57  0.54  0.54  0.62
  11                                                          1   0.85  0.76  0.67  0.6   0.6  0.59  0.57  0.55  0.53  0.56  0.56  0.56  0.54  0.53  0.53  0.52  0.51  0.49  0.5  0.59
  12                                                                1   0.83  0.71  0.64  0.64  0.63  0.61  0.6  0.58  0.6   0.6   0.6  0.58  0.56  0.57  0.56  0.55  0.52  0.52  0.58
  13                                                                     1   0.83  0.74  0.7  0.67  0.64  0.63  0.6  0.63  0.63  0.64  0.62  0.61  0.62  0.61  0.61  0.57  0.55  0.56
  14                                                                          1   0.81  0.74  0.71  0.68  0.66  0.63  0.65  0.64  0.64  0.62  0.61  0.6  0.59  0.58  0.54  0.54  0.59
  15                                                                               1   0.88  0.8  0.75  0.7  0.66  0.65  0.63  0.62  0.59  0.57  0.57  0.56  0.55  0.53  0.55  0.59
  16                                                                                    1   0.86  0.79  0.74  0.69  0.67  0.65  0.62  0.59  0.57  0.57  0.56  0.55  0.52  0.59  0.59
  17                                                                                         1    0.9  0.84  0.78  0.74  0.72  0.69  0.66  0.63  0.61  0.6  0.61  0.58  0.66  0.65
  18                                                                                              1   0.91  0.85  0.8  0.77  0.73  0.7  0.68  0.66  0.64  0.64  0.61  0.66  0.66
  19                                                                                                    1   0.91  0.85  0.8  0.77  0.73  0.7  0.67  0.66  0.65  0.63  0.66  0.62
  20                                                                                                         1   0.91  0.86  0.82  0.77  0.74  0.71  0.69  0.68  0.65  0.71  0.69
  21                                                                                                              1   0.93  0.88  0.84  0.81  0.78  0.76  0.75  0.71  0.75  0.67
  22                                                                                                                   1   0.95  0.9  0.87  0.84  0.81  0.8  0.75  0.76  0.67
  23                                                                                                                        1   0.94  0.89  0.86  0.83  0.82  0.76  0.77  0.7
  24                                                                                                                             1   0.92  0.88  0.85  0.83  0.77  0.78  0.68
  25                                                                                                                                   1   0.93  0.88  0.81  0.75  0.73  0.6
  26                                                                                                                                        1   0.92  0.8  0.73  0.71  0.62
  27                                                                                                                                             1    0.9  0.81  0.74  0.7
  28                                                                                                                                                  1   0.86  0.81  0.66
  29                                                                                                                                                       1   0.87  0.72
  30                                                                                                                                                            1   0.78
  31                                                                                                                                                                 1

                                                       Diff-Vector CosSim


Layer  0   1    2      3       4       5       6       7       8       9       10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27       28       29      30      31
  0   1  0.17  0.13  8 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  3 · 10−2  3 · 10−2  1 · 10−2  1 · 10−2  1 · 10−2  3 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2     0        0        0.1    2 · 10−2
  1       1   0.18    0.1    6 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2  1 · 10−2  1 · 10−2  3 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  4 · 10−2  4 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2     0        0       0.15   1 · 10−2
  2            1      0.2      0.11   7 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  2 · 10−2  2 · 10−2  2 · 10−2  3 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  5 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2     0        0       0.13   1 · 10−2
  3                   1       0.19      0.1    7 · 10−2  6 · 10−2  5 · 10−2  3 · 10−2  2 · 10−2  2 · 10−2  4 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  4 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  4 · 10−2  4 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2     0        0        0.1    2 · 10−2
  4                           1       0.21     0.13   9 · 10−2  7 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  5 · 10−2  7 · 10−2  6 · 10−2  7 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2 −1 · 10−2    0    6 · 10−2  1 · 10−2
  5                                    1       0.21     0.15   9 · 10−2  7 · 10−2  6 · 10−2  4 · 10−2  5 · 10−2  7 · 10−2  6 · 10−2  7 · 10−2  7 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  2 · 10−2  1 · 10−2     0        0       0.11   2 · 10−2
  6                                            1       0.25     0.12   9 · 10−2  8 · 10−2  4 · 10−2  6 · 10−2  7 · 10−2  7 · 10−2  8 · 10−2  8 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  7 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  2 · 10−2     0    6 · 10−2  2 · 10−2
  7                                                    1       0.24     0.17     0.12   8 · 10−2  9 · 10−2    0.1    8 · 10−2  9 · 10−2  9 · 10−2  8 · 10−2  7 · 10−2  5 · 10−2  6 · 10−2  4 · 10−2  4 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2  2 · 10−2  2 · 10−2     0     1 · 10−2  5 · 10−2  2 · 10−2
  8                                                            1       0.28     0.16      0.1      0.12      0.1    9 · 10−2  9 · 10−2    0.1    9 · 10−2  8 · 10−2  6 · 10−2  7 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  2 · 10−2  2 · 10−2  1 · 10−2  1 · 10−2  6 · 10−2  2 · 10−2
  9                                                                    1       0.27     0.19     0.17     0.12     0.11     0.11     0.11   9 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  2 · 10−2  2 · 10−2  1 · 10−2  1 · 10−2  6 · 10−2  2 · 10−2
 10                                                                            1       0.21     0.17     0.12     0.12     0.12     0.11      0.1    9 · 10−2  7 · 10−2  7 · 10−2  5 · 10−2  5 · 10−2  6 · 10−2  5 · 10−2  5 · 10−2  3 · 10−2  2 · 10−2     0     1 · 10−2  6 · 10−2  2 · 10−2
 11                                                                                    1       0.24      0.2      0.16     0.14     0.12      0.1    9 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  6 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  2 · 10−2  3 · 10−2  1 · 10−2  2 · 10−2  7 · 10−2  2 · 10−2
 12                                                                                            1       0.25     0.21     0.17     0.15     0.12      0.1    9 · 10−2  9 · 10−2  7 · 10−2  6 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  3 · 10−2  3 · 10−2  1 · 10−2  2 · 10−2  7 · 10−2  2 · 10−2
 13                                                                                                     1       0.27     0.19     0.18     0.14     0.12      0.1    9 · 10−2  7 · 10−2  7 · 10−2  7 · 10−2  6 · 10−2  5 · 10−2  3 · 10−2  2 · 10−2  1 · 10−2  2 · 10−2  7 · 10−2  2 · 10−2
 14                                                                                                             1       0.29     0.22     0.15     0.13     0.12     0.12   9 · 10−2  8 · 10−2  8 · 10−2  7 · 10−2  6 · 10−2  4 · 10−2  3 · 10−2  2 · 10−2  2 · 10−2  8 · 10−2  2 · 10−2
 15                                                                                                                     1       0.28     0.21     0.17     0.15     0.14     0.12      0.1       0.1    9 · 10−2  7 · 10−2  5 · 10−2  4 · 10−2  3 · 10−2  3 · 10−2    0.1    2 · 10−2
 16                                                                                                                             1       0.29     0.25     0.19     0.17     0.15     0.13     0.13     0.11   8 · 10−2  6 · 10−2  5 · 10−2  4 · 10−2  4 · 10−2    0.15   3 · 10−2
 17                                                                                                                                     1       0.35     0.28     0.25     0.22     0.19      0.2      0.16     0.13     0.11   9 · 10−2  8 · 10−2  7 · 10−2    0.18   3 · 10−2
 18                                                                                                                                              1       0.36     0.31     0.28     0.22     0.23     0.21     0.14      0.1       0.1    9 · 10−2  7 · 10−2    0.18   4 · 10−2
 19                                                                                                                                                      1       0.36     0.31     0.29     0.23     0.21     0.16     0.11      0.1        0.1    8 · 10−2    0.18   5 · 10−2
 20                                                                                                                                                              1       0.44     0.33     0.32     0.31     0.19     0.15     0.12      0.13    8 · 10−2    0.2    5 · 10−2
 21                                                                                                                                                                      1       0.46     0.43     0.41     0.24     0.19     0.17      0.17    9 · 10−2    0.2    5 · 10−2
 22                                                                                                                                                                              1       0.49     0.47      0.3      0.23     0.22       0.2       0.12     0.24   7 · 10−2
 23                                                                                                                                                                                      1       0.57     0.34     0.28     0.23      0.21      0.12     0.24   6 · 10−2
 24                                                                                                                                                                                               1       0.38     0.32     0.29      0.25      0.12     0.25   6 · 10−2
 25                                                                                                                                                                                                       1        0.3      0.21      0.17      0.11     0.17   4 · 10−2
 26                                                                                                                                                                                                               1        0.3       0.18       0.1      0.17   5 · 10−2
 27                                                                                                                                                                                                                       1        0.26      0.15      0.2       0.1
 28                                                                                                                                                                                                                                1        0.27      0.3    7 · 10−2
 29                                                                                                                                                                                                                                         1        0.3    6 · 10−2
 30                                                                                                                                                                                                                                                 1       0.22
 31                                                                                                                                                                                                                                                          1





                                               17

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

E. Steering Vector Persistence. Mean shift alignment on the last layer. Llama3.1-8B-It

                                                                                        1.0                     0
                     2
                     4
                     6
                     8                                                              0.8
                    10
                    12
                    14
                                                                                        0.6                            Layer16                                                                                                                                                     CosSim
                    18
                    20
                                                                                        0.4                    22
                    24
                    26                                                                                        0.2
                    28
                    30
                       0  2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
                                            Layer

Figure 11. Similarity of steering-induced unembedding biases (Llama3.1-8B-It). Each cell shows the cosine similarity between the
average hidden-state shift at the final transformer layer induced by steering at layers i and j. High similarity among i, j < L indicates
that steering from most layers produces a similar bias at the unembedding, largely independent of the injection point. In contrast, steering
at the last layer directly yields a qualitatively different shift, implying a distinct mechanism.


F. Orthogonal Steering Vectors

                         Original             First Ortho         Second Ortho
                       Vector            Vector              Vector                      40.0                                39.2          40
                               30.1
          30                                                  27.4
                                         23.2
                                                                            20.9
          20                           Performance          10

           0
                         Layer-14                       Layer-15


Figure 12. Orthogonal steering vectors. Performance of three mutually orthogonal steering vectors trained at layers 14 and 15. Contrary
to the findings of Jacob & Turner (2024) in our setup the orthogonal vectors do not achieve comparable performance: accuracy decreases
monotonically as additional orthogonal vectors are trained.



                                                18

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

G. Last Layer. Logit Lens


Table 8. Last Layer – logit-lens. Cosine similarities and dot-product scores between the last-layer steering vector (trained in isolation)
and the unembedding vectors of the top-10 tokens for Qwen2.5-Math-7B and Llama3.1-8B-It.

                                               Qwen2.5-Math-7B

                    To      ]     To     So     to       \       }    For   .To    -to

             Cos. Sim.    0.37     0.16     0.16     0.15     0.14     0.14     0.13     0.13    0.12     0.12
           Dot Prod.    42.5    19.12    18.62    19.12    16.88    19.75    15.69    14.19    17.0    18.62


                                                      Llama3.1-8B-It

           final   Step   format   Final   final   final   Steps   Final    final   solution

  Cos. Sim.       0.12      0.11        0.09       0.09       0.08       0.08       0.08       0.08        0.08           0.08
  Dot Prod.       1.69      1.32        1.17       1.09       0.71       1.01       1.02       0.93        0.83           0.95


H. Last Layer Steering. Open-s1 dataset


                                                                              Base          Base + "To"            Last Steering
                                                                                                                   40.8
                                                                  40    0.8                                                                                                  35.5
    0.6                                                            30     24.8                                         25.5      28.2Difference
    0.4                                                            20                                         14.3    0.2                                                                                                                                                                                                               Performance10Probability    0.0                                                                   0
            "To"       " To"        ")"         ")."         "."        "To"                         Greedy                      Sampling
                                                              at Pos. 0

                                               Qwen2.5-Math-7B

                                                                              Base         Base + "Step"           Last Steering
                                                                                                         22.4                                                                                                21.5
                                                                  20                        18.2
    0.6
                                                                  15                                                  13.4      13.8Difference                                                                                                                               11.7    0.4
                                                                  10    0.2                                                                                                                                                                                                               Performance
                                                                   5Probability    0.0                                                                   0
          "Step"    " step"    " final"    " Step"     " the"    "Step"                        Greedy                      Sampling
                                                              at Pos. 0

                                                      Llama3.1-8B-It

Figure 13. Last-layer steering vector. Left: distribution of token-level probability change induced by the last-layer vector over 1000
Open-s1 prompts. We include the top-5 tokens by maximum change and highlight the most affected token at the zeroth generation position.
Right: prefixing this token to each prompt reproduces a substantial fraction of the vector’s accuracy gain under both greedy decoding and
 sampling.


 I. Value Steering Adds a Linear Term to MHA

The following derivation holds when we ignore the pre-attention LayerNorm (LN). While this is a strong assumption – that
LN does not alter the steering vector’s trajectory – the experiment in Section 6 shows that a post-attention steering vector
 attains the same performance as a pre-attention one, indicating that the pre-attention vector indeed does not act through
 attention.

                                                19

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

Claim. Let U ∈RT ×dmodel and define the (row-wise) attention

                                  Q   K  !                       UW i (UW i )⊤      ×T
                    A(U) = Softmax      √dk     ∈RT     ,  A(U)1 = 1.

For head i,
                                    Hi(U) = A(U) UWiV .
Let a steering vector s ∈Rdmodel be added to the values of head i for every token, and set S = 1s⊤∈RT ×dmodel. Then

                           H(+s)i   (U) = A(U) (U + S)WiV
                       = A(U)UWiV + A(U)SWiV
                       = Hi(U) + SW iV     (since A(U)1 = 1).

          W 1O 
                           · ·Writing W O =                              · by heads, the multi-head output satisfies                       O
        W h

                       MHA(U + S) = MHA(U) + S WiV W iO

and is independent of the attention pattern.

J. Unnormalized Transfer Performance

Table 9. Transferability of steering vectors within the Qwen2.5 family. Each cell shows the mean performance change when the
steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained models’
performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-Instruct as in
the main experiments.

                                                      Donor

            Family         Recipient     None         Base         Instruct      Math

                           Base        0.52 ± 0.12   22.72 ± 0.53   9.02 ± 1.66    7.57 ± 0.67
            Qwen2.5-1.5B   Instruct     13.44 ± 1.17   23.22 ± 0.35   23.82 ± 0.28   16.64 ± 0.27
                         Math       11.33 ± 1.09   19.48 ± 1.18   16.09 ± 1.48   34.11 ± 0.28

                           Base       12.04 ± 5.85   36.44 ± 0.15   20.78 ± 3.43   30.0 ± 0.14
            Qwen2.5-7B     Instruct     35.82 ± 0.14   37.51 ± 0.44   38.89 ± 0.27   34.78 ± 0.07
                         Math       14.33 ± 1.75   23.42 ± 1.76   15.84 ± 1.96   42.82 ± 0.25

                           Base        0.91 ± 0.11    9.18 ± 0.22    3.24 ± 0.27    —
            Llama3.1-8B
                                  Instruct      3.33 ± 0.16   17.03 ± 0.34   21.76 ± 0.29   —

                           Base        0.91 ± 0.11    9.18 ± 0.22    0.95 ± 0.11    —
            Llama3.1-8B∗
                                  Instruct     11.81 ± 0.43   11.66 ± 0.46   26.14 ± 0.43   —





                                                20

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors


Table 10. Transferability of steering vectors within the Qwen2.5 family – AIME24. Each cell shows the performance change when
the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained models’
performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-Instruct as in
the main experiments.

                                                      Donor

            Family         Recipient     None         Base        Instruct      Math

                           Base          0.0 ± 0.0     2.81 ± 0.09   1.70 ± 0.10    0.73 ± 0.0
             Qwen2.5-1.5B   Instruct      0.83 ± 0.09    2.60 ± 0.0    3.30 ± 0.13   0.87 ± 0.05
                         Math        2.92 ± 0.0    5.21 ± 0.09   4.83 ± 0.05   12.08 ± 0.0

                           Base        4.44 ± 6.29   12.78 ± 0.05   6.77 ± 0.0    8.78 ± 0.13
            Qwen2.5-7B     Instruct     11.11 ± 0.18   13.65 ± 0.0   14.79 ± 0.0   10.87 ± 0.13
                         Math        6.70 ± 0.18   11.11 ± 0.13   8.06 ± 0.18   24.31 ± 0.20

                           Base         0.10 ± 0.0    0.28 ± 0.13   0.24 ± 0.05   —
             Llama3.1-8B
                                   Instruct      0.45 ± 0.10    1.08 ± 0.05   6.63 ± 0.05   —



Table 11. Transferability of steering vectors within the Qwen2.5 family – AIME25. Each cell shows the performance change when
the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained models’
performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-Instruct as in
the main experiments.

                                                      Donor

             Family         Recipient    None        Base        Instruct      Math

                            Base        0.10 ± 0.00   1.46 ± 0.00   1.35 ± 0.09   0.10 ± 0.00
              Qwen2.5-1.5B   Instruct     0.31 ± 0.09   1.60 ± 0.05   2.15 ± 0.05   0.83 ± 0.09
                          Math       1.81 ± 0.10   2.53 ± 0.05   2.57 ± 0.13   8.12 ± 0.09

                            Base        0.00 ± 0.00   7.12 ± 0.32   3.54 ± 0.15   5.49 ± 0.05
             Qwen2.5-7B     Instruct     6.74 ± 0.13   7.19 ± 0.09   8.72 ± 0.05   4.38 ± 0.09
                          Math       1.74 ± 0.13   4.41 ± 0.18   3.23 ± 0.00   12.64 ± 0.26

                            Base        0.00 ± 0.00   0.28 ± 0.05   0.10 ± 0.00   —
              Llama3.1-8B
                                    Instruct     0.07 ± 0.05   0.62 ± 0.00   1.88 ± 0.15   —



Table 12. Transferability of steering vectors within the Qwen2.5 family – AMC-23. Each cell shows the performance change when
the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained models’
performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-Instruct as in
the main experiments.

                                                      Donor

           Family         Recipient     None         Base         Instruct      Math

                          Base         0.73 ± 0.10    33.91 ± 0.11   11.61 ± 0.13   8.49 ± 0.21
            Qwen2.5-1.5B   Instruct     16.64 ± 0.36   33.91 ± 0.17   31.82 ± 0.13   21.90 ± 0.10
                        Math        18.41 ± 0.16   32.08 ± 0.39   26.54 ± 0.21   48.96 ± 0.16

                          Base       17.50 ± 12.42   49.90 ± 0.13   32.45 ± 0.27   41.02 ± 0.33
           Qwen2.5-7B     Instruct     50.29 ± 0.16   50.39 ± 0.17   54.24 ± 0.15   46.25 ± 0.33
                        Math        21.30 ± 0.38   35.60 ± 0.42   25.57 ± 0.37   62.14 ± 0.30

                          Base         0.94 ± 0.11    8.83 ± 0.06    2.81 ± 0.39    —
            Llama3.1-8B
                                 Instruct      4.84 ± 0.34    22.60 ± 0.22   24.92 ± 0.19   —



                                                21

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors


Table 13. Transferability of steering vectors within the Qwen2.5 family – MATH-500. Each cell shows the performance change when
the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained models’
performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-Instruct as in
the main experiments.

                                                      Donor

           Family         Recipient     None         Base         Instruct       Math

                          Base         1.20 ± 0.28    56.80 ± 0.75   20.80 ± 5.58   21.87 ± 2.19
           Qwen2.5-1.5B   Instruct     39.93 ± 5.32   57.60 ± 1.18   61.13 ± 1.04   46.33 ± 1.24
                        Math        24.00 ± 4.14   44.40 ± 3.10   31.87 ± 5.03   71.47 ± 0.93

                          Base       30.00 ± 11.75   74.07 ± 0.62   48.00 ± 11.54   66.33 ± 2.35
           Qwen2.5-7B     Instruct     74.40 ± 1.18   76.20 ± 0.71   78.40 ± 1.56   74.80 ± 0.59
                        Math        37.67 ± 6.09   54.00 ± 4.14   37.80 ± 7.11   80.20 ± 0.33

                          Base         1.87 ± 0.25    26.13 ± 1.06   11.33 ± 1.47    —
           Llama3.1-8B
                                 Instruct      7.33 ± 0.66    44.47 ± 0.41   55.00 ± 0.33    —



Table 14. Transferability of steering vectors within the Qwen2.5 family – Minerva-Math. Each cell shows the performance change
when the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained
models’ performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-
Instruct as in the main experiments.

                                                      Donor

            Family         Recipient     None         Base         Instruct      Math

                           Base        0.61 ± 0.17   18.75 ± 2.38   7.60 ± 2.29    5.27 ± 0.35
            Qwen2.5-1.5B   Instruct      9.68 ± 1.05   19.49 ± 1.08   20.59 ± 1.83   13.24 ± 0.79
                         Math        5.39 ± 1.71    9.56 ± 2.96    9.44 ± 2.72   27.70 ± 1.05

                           Base        7.11 ± 2.88   37.13 ± 0.60   12.99 ± 2.44   25.74 ± 1.50
            Qwen2.5-7B     Instruct     35.54 ± 1.42   37.75 ± 1.54   35.54 ± 0.87   34.68 ± 0.46
                         Math        8.33 ± 1.51   11.52 ± 2.55   7.84 ± 0.96   34.44 ± 1.76

                           Base        1.47 ± 0.52   12.62 ± 0.35   3.19 ± 0.69    —
            Llama3.1-8B
                                  Instruct      4.41 ± 0.60   18.87 ± 1.05   21.20 ± 1.48   —



Table 15. Transferability of steering vectors within the Qwen2.5 family – OlympiadBench. Each cell shows the performance change
when the steering vector trained for the Donor model is applied to the Recipient model. The ”None” column denotes the non-trained
models’ performance. Llama3.1-8B∗denotes the transferring experiments where we used ”Qwen-Math” template for Llama3.1-8B-
Instruct as in the main experiments.

                                                      Donor

            Family         Recipient     None         Base         Instruct      Math

                           Base        0.44 ± 0.32   22.62 ± 0.55   11.06 ± 3.98   8.99 ± 1.48
            Qwen2.5-1.5B   Instruct     13.23 ± 1.14   24.10 ± 0.60   24.00 ± 1.05   16.64 ± 0.18
                         Math       15.46 ± 0.85   23.11 ± 1.19   21.28 ± 1.39   36.35 ± 0.69

                           Base       13.19 ± 6.40   37.63 ± 0.44   20.94 ± 6.50   32.64 ± 1.84
            Qwen2.5-7B     Instruct     36.84 ± 0.18   39.90 ± 0.74   41.68 ± 0.30   37.73 ± 0.55
                         Math       10.22 ± 2.94   23.85 ± 4.39   12.54 ± 3.34   43.21 ± 0.14

                           Base        0.54 ± 0.14    7.21 ± 0.28    1.78 ± 0.64    —
            Llama3.1-8B
                                  Instruct      2.86 ± 0.62   14.52 ± 0.74   20.94 ± 0.07   —



                                                22

              Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors

K. Chat Templates

Following Liu et al. (2025b), we used two chat templates. For models that support special chat-template tokens, we
adopted the Qwen-Math template; special tokens for Qwen2.5-Math-7B are shown as a representative example. For
Llama3.1-8B – which does not include pretrained special chat-template tokens – we used the R1 template.

   Chat Template – Qwen-Math

   <|im start|>system Please reason step by step, and put your final
   answer within \boxed{}.<|im end|> <|im start|>user TASK<|im end|>
   <|im start|>assistant


   Chat Template – R1

   A conversation between User and Assistant.  The User asks a question, and
   the Assistant solves it.  The Assistant first thinks about the reasoning
   process in the mind and then provides the User with the answer.  The
   reasoning process is enclosed within <think> </think> and answer is
   enclosed within <answer> </answer> tags, respectively, i.e., <think>
   reasoning process here </think> <answer> answer here </answer>.  \nUser:
   TASK\nAssistant:  <think>


L. LLM Use

We used ChatGPT to check grammar and clarity during the writing of this paper.





                                                23