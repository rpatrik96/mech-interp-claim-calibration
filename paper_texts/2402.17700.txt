                 RAVEL: Evaluating Interpretability Methods on
                      Disentangling Language Model Representations

                    Jing Huang            Zhengxuan Wu           Christopher Potts
                  Stanford University           Stanford University           Stanford University
               hij@stanford.edu      wuzhengx@stanford.edu    cgpotts@stanford.edu

                     Mor Geva                          Atticus Geiger
                            Tel Aviv University                     Pr(Ai)2R Group
                    morgeva@tauex.tau.ac.il             atticusg@gmail.com

                          Abstract                                             Inputs        Task: Localize the    Expected
                                                                                                                 Continent Attribute    Outputs

                                                                                                         is in                                                                                                               the                   Individual neurons participate in the represen-             Cause Paris                                                                                                                                           Asia                                                                                                    continent                                                                                                              of
                     tation of multiple high-level concepts. To what                           ❄LM
                   extent can different interpretability methods                Isolate Paris is in the                        France                                                                                                     country of
                   successfully disentangle these roles? To help                                                                        Continent Subspace
                                                                                                                                                                                                Tokyo2024            address this question, we introduce RAVEL              Interchange Intervention                                Paris
                                                                                              with Bijective Featurizer F
                 (Resolving Attribute–Value Entanglements in                                                                          Other Attributes
               Language Models), a dataset that enables
                                                                                                     Tokyo is a         ❄LMAug             tightly controlled, quantitative comparisons                     large city.
                between a variety of existing interpretabil-
                      ity methods. We use the resulting concep-26
                    tual framework to define the new method        Figure 1: An overview of the RAVEL benchmark, which
                  of Multi-task Distributed Alignment Search        evaluates how well an interpretability method can find
              (MDAS), which allows us to find distributed        features that isolate the causal effect of individual at-
                   representations satisfying multiple causal cri-         tributes of an entity.
                      teria. With Llama2-7B as the target language[cs.CL]           model, MDAS achieves state-of-the-art results        To facilitate these method comparisons, we in-
               on RAVEL, demonstrating the importance of       troduce a diagnostic benchmark, RAVEL (Resolv-
                 going beyond neuron-level analyses to identify                                                              ing Attribute–Value Entanglements in Language
                   features distributed across activations. We re-
                                                           Models). RAVEL evaluates interpretability meth-
                    lease our benchmark at https://github.com/
                                                          ods on their ability to localize and disentangle the                explanare/ravel.
                                                                        attributes of different types of entities encoded as
          1  Introduction                                     text inputs to language models (LMs). For exam-
                                                                        ple, the entity type “city” has instances such as       A central goal of interpretability is to localize an
                                                                 “Paris” or “Tokyo”, which each have attributes for              abstract concept to a component of a deep learning
                                                                 “continent”, namely “Europe” and “Asia”. An in-           model that is used during inference. However, this
                                                                          terpretability method must localize this attribute to                is not as simple as identifying a neuron for each
                                                             a group of neurons N, learn a featurizer F (e.g., a             concept, because neurons are polysemantic – they
                                                                       rotation matrix or sparse autoencoder), and identify              represent multiple high-level concepts (Smolensky,arXiv:2402.17700v2                                                       a feature F (e.g., a linear subspace of the residual            1988; Rumelhart et al., 1986; McClelland et al.,
                                                            stream in a Transformer) for the attribute. RAVEL            1986; Olah et al., 2020; Cammarata et al., 2020;
                                                                  contains five types of entities (cities, people names,            Bolukbasi et al., 2021; Gurnee et al., 2023).
                                                                     verbs, physical objects, and occupations), each with               Several recent interpretability works (Bricken
                                                                         at least 500 instances, at least 4 attributes, and at                et al., 2023; Cunningham et al., 2024; Geiger et al.,
                                                                          least 50 prompt templates per entity type.            2023b; Wu et al., 2023) tackle this problem us-
             ing a featurizer that disentangles the activations of     The metric we use to assess interpretability meth-
            polysemantic neurons by mapping to a space of   ods is based on interchange interventions (also
           monosemantic features that each represent a dis-   known as activation patching). This operation has
               tinct concept.  Intuitively, these methods should   emerged as a workhorse in interpretability, with
            have a significant advantage over approaches that   a wide swath of research applying the technique
              identify concepts with sets of neurons. However,    to test if a high-level concept is stored in a model
             these methods have not been benchmarked.          representation and used during inference (Geiger


                                                    1

et al., 2020; Vig et al., 2020; Geiger et al., 2021;         Entity         Attributes            # Entities    # Prompt
                                                                 Type                                           Templates
Li et al., 2021; Finlayson et al., 2021; Meng et al.,
                                                                           City          Country, Language,        3552         150
2022; Chan et al., 2022; Geva et al., 2023; Wang                        Latitude, Longitude,
et al., 2023; Hanna et al., 2023; Conmy et al., 2023;                    Timezone, Continent
Goldowsky-Dill et al., 2023; Hase et al., 2023;       Nobel       Award Year, Birth Year,      928         100
                                                                          Laureate     Country of Birth, Field,
Todd et al., 2024; Feng and Steinhardt, 2024; Cun-                   Gender
ningham et al., 2024; Huang et al., 2023; Tigges        Verb          Definition, Past Tense,       986          60
et al., 2023; Lieberum et al., 2023; Davies et al.,                      Pronunciation, Singular
2023; Hendel et al., 2023; Ghandeharioun et al.,        PhysicalObject        BiologicalColor, Size,Category,Texture         563          60
2024).                                                               Occupation   Duty, Gender Bias,          799          50
   Specifically, we use the LM to process a prompt                        Industry, Work Location
like “Paris is in the continent of” and then
                                                        Table 1: Types of entities and attributes in RAVEL.
intervene on the neurons N to fix the feature F to
be the value it would have if the LM were given a    2. Causal: Interpretability methods should ana-
prompt like “Tokyo is a large city.” If this       lyze the causal effects of model components on
leads the LM to output “Asia” instead of “Europe”,     model input–output behaviors.
then we have evidence that the feature F encodes                                                          3. Generalizable: The causal effects of the iden-
the attribute “continent”. Then, we perform the                                                                  tified components should generalize to similar
same intervention when the LM processes a prompt                                                        inputs that the underlying model makes correct
like “People in Paris speak”. If the LM outputs                                                         predictions for.
“French” rather than “Japanese’, then we have
                                                          4. Isolating individual concepts: Interpretability
evidence that the feature F has disentangled the
                                                methods should isolate causal effects of individ-
attributes “continent” and “language”.
                                                        ual concepts involved in model behaviors.
  A variety of existing interpretability methods are
easily cast in the terms needed for RAVEL evalu-   The goal of RAVEL is to assess the ability of meth-
ations, including supervised probes (Peters et al.,   ods to isolate individual explanatory factors in
2018; Hupkes et al., 2018; Tenney et al., 2019;   model representations (desideratum 4), and do so
Clark et al., 2019), Principal Component Analysis    in a way that is faithful to how the target models
(Tigges et al., 2023; Marks and Tegmark, 2023),   work (desideratum 1). The dataset train/test struc-
Differential Binary Masking (DBM: Cao et al.    ture seeks to ensure that methods are evaluated by
2020; Csordás et al. 2021; Cao et al. 2022; Davies   how well their explanations generalize to new cases
et al. 2023), sparse autoencoders (Bricken et al.,   (desideratum 3), and RAVEL is designed to support
2023; Cunningham et al., 2024), and Distributed    intervention-based metrics that assess the extent
Alignment Search (DAS: Geiger et al. 2023b; Wu    to which methods have found representations that
et al. 2023). Our apples-to-apples comparisons re-   causally affect the model behavior (desideratum 2).
veal conceptual similarities between the methods.     RAVEL  is carefully curated as a diagnostic
   In addition, we propose multi-task training ob-   dataset for the attribute disentanglement problem.
jectives for DBM and DAS. These objectives allow   RAVEL has five types of entity, where every in-
us to find representations satisfying multiple causal    stance has every attribute associated with its type.
criteria, and we show that Multi-task DAS is the    Table 1 provides an overview of RAVEL’s structure.
most effective of all the methods we evaluate at
                                        The Attribute Disentanglement Task  We beginidentifying disentangled features. This contributes
to the growing body of evidence that interpretabil-   with a set of entities E = {E1, . . . , En}, each with
ity methods need to identify features that are dis-    attributes A = {A1, . . . , Ak}, where the correct
tributed across neurons.                             value of A for E is given by AE. Our interpretabil-
                                                             ity task asks whether we can find a feature F that
2  The RAVEL Dataset                       encodes the attribute A separately from the other
                                                            attributes A \ {A}. For Transformer-based models
The design of RAVEL is motivated by four high-                                               (Vaswani et al., 2017), a feature might be a dimen-
level desiderata for interpretability methods:                                                   sion in a hidden representation of an MLP or a
1. Faithful: Interpretability methods should accu-    linear subspace of the residual stream.
    rately represent the model to be explained.       We do not know a priori the degree to which it is


                                         2

possible to disentangle a model’s representations.   across unseen entities and contexts. Each setting
However, our benchmark evaluates interpretability    has a predefined train/dev/test structure. In Entity,
methods according to the desiderata given above    for each entity type, we randomly split the entities
and so methods will need to be faithful to the    into 50%/25%/25% for train/dev/test, but use the
model’s underlying structure to succeed. In other   same set of prompt templates across the three splits.
words, assuming methods are faithful, we can favor    In Context, for each attribute, we randomly split
methods that achieve more disentanglement.         the prompt templates into 50%/25%/25%, but use
                                                     the same set of entities across the three splits.
2.1  Data Generation
                                                     Filtering for a Specific Model  When evaluatingSelecting Entity Types and Attributes  We first
                                                         interpretability methods that analyze a model M,identify entity types from existing datasets that
                                    we generally focus on a subset of the instancespotentially have thousands of instances (see Ap-
                                           where M correctly predicts the values of the at-pendix A.1), such as cities or famous people. More-
                                                         tributes (see Appendix A.2).  This allows us toover, each entity type has multiple attributes with
                                                 focus on understanding why models succeed, anddifferent degrees and types of associations. For
                                                                             it means that we don’t have to worry about howexample, for attributes related to city, “country” en-
                                            methods might have different biases for incorrecttails “continent”, but not the reverse; “country” is
                                                       predictions.predictable from “timezone” but non-entailed; and
“latitude” and “longitude” are the least correlated
                                                    2.2  Interpretability Evaluation
compared with the previous two pairs, but have
identical output spaces. These entity types together   Interchange Interventions A central goal of
cover a diverse set of attributes such that predicting   RAVEL is to assess methods by the extent to which
the value of the attribute uses factual, linguistic, or    they provide causal explanations of model behav-
commonsense knowledge.                             iors (desideratum 2). To build such analyses, we
                                             need to put models into counterfactual states that
Constructing Prompts We consider two types                                                 allow us to isolate the causal effects of interest.
of prompts: attribute prompts and entity prompts.
                                            The fundamental operation for achieving this is
Attribute prompts PAE  contain mentions of E                                                    the intervention (Spirtes et al., 2000; Pearl, 2001,
and instruct the model to output the attribute
                                                 2009): we change the value of a model-internal
value AE.   For example, E = Paris  is an                                                          state and study the effects this has on the model’s
instance of the type “city”, which has an  at-
                                                    input–output behavior. In more detail: let M(x) be
tribute A = Continent that can be queried with
                                                      the entire state of the model when M receives input
prompts “Paris  is  in  the  continent  of”.
                                                     x, i.e., the set of all input, hidden, and output rep-
Prompts can also be JSON-format, e.g., “{"city":
                                                       resentations created during inference. Let MN←n
"Paris", "continent":"”, which reflects how
                                               be the model where neurons N are intervened upon
entity–attribute association might be encoded in
                                             and fixed to take on the value n ∈Values(N).
training data. For each format, we do zero- and few-
                                                   Geiger et al. (2023b) generalize this operation to
shot prompting. In addition to attribute prompts,
                                                    intervene upon features that are distributed across
entity prompts WE contain mentions of the E, but
                                                 neurons using a bijective featurizer F. Let MF←f
does not query any A ∈A. For example, “Tokyo
                                               be the model where neurons N are projected into a
is a large city”. We sample entity prompts
                                                        feature space using F, the feature F is fixed to take
from the Wikipedia corpus.1
                                            on value f, and then the features are projected back
  For a set of entities E and a set of attributes to                                                       into the space of neural activations using F−1. If
disentangle A, the full set of prompts is
                                      we let τ(M(x)) be the token that a model predicts
                                                       for a given prompt x ∈D, then comparisons be-  D = {x : x ∈PAE ∪WE, E ∈E, A ∈A}
                                             tween τ(M(x)) and τ(MF←f(x)) yield insights
Generating Splits  RAVEL offers two settings,    into the causal role that F plays in model behavior.
Entity and Context, to evaluate the generalizabil-     However, most conceivable interventions fix
ity (desideratum 3) of an interpretability method   model representations to be values that are never re-
                                                       alized by any input. To characterize the high-level
   1We  use  the  20220301.en  version  pre-processed
                                                   conceptual role of a model representation, we needby HuggingFace at https://huggingface.co/datasets/
wikipedia                                         a data-driven intervention that sets a representation


                                         3

to values it could actually take on. This is achieved     To balance these two objectives, we define the
by the interchange intervention, which fixes a fea-   Disentangle score as a weighted average between
ture F to the value it would take if a different input   Cause and Iso.
x′ were provided:
                        def                            Disentangle(A, F, M, D) =
  II(M, F, x, x′) =
                                              1                                                  h                                   i                                                 Cause(A, F, M, D) + Iso(A, F, M, D)
            τ  MF←GetFeature(M(x′),F)(x)    (1)     2
where GetFeature(M(x′), F) is the value of F in     The score on RAVEL for an entity type is its
M(x′). Interchange interventions represent a very   average Disentangle score over all attributes.
general technique for identifying abstract causal                                                       In practice, two attributes might not be fully dis-
processes that occur in complex black-box systems    entanglable in the model M so there is no guar-
(Beckers and Halpern, 2019; Beckers et al., 2020;                                                  antee that it is possible to find a feature F that
Geiger et al., 2023a).                                                   achieves Cause = 1 and Iso = 1 at the same time.
                                            However, evidence that two attributes might notEvaluation Data  For evaluation, each interven-
tion example consists of a tuple: an input x ∈PAE,   be separable is an insight into how knowledge is
an input x′ ∈PA′E′ ∪WE′, a target attribute A∗,    structured in the model.
and an intervention label y.  If A∗= A, then
                                       3  Interpretability Methodsy  is AE′ and otherwise y  is AE.  For exam-
ple, if the set of “city” entities to evaluate on is                                  We use RAVEL to evaluate a variety of interpretabil-
{"Paris", "Tokyo"} and the goal is to disentan-                                                               ity methods on their ability to disentangle attributes
gle the “country” attribute from the “continent” at-                                                while generalizing to novel templates and enti-
tribute, then the set of test examples becomes the                                                                 ties.  Each method uses data from the training
one shown in Figure 1.                                                             split to find a set of neurons N, learn a featur-
Metrics   If M achieves behavioral success on    izer F, and find a feature FA that captures an
a dataset, we can use that dataset to evaluate an    attribute A ∈A independent from the other at-
interpretability method on its ability to identify a    tributes.  In Section 4, we describe the baseline
collection of neurons N, a featurizer F for those   procedure we use for considering different sets of
neurons, and a feature F that represents an attribute    neurons. In this section, we define methods for
A separately from all others attributes A \ {A}.      learning a featurizer and identifying a feature given
   If F encodes A, then interventions on F should   a set of neurons. For each method, the core inter-
change the value of A. When M is given a prompt    vention for A is given by II(M, FA, x, x′) where
x ∈PAE, we can intervene on F to set the value to   FA is defined by the method. In this section, we
what it would be if a second prompt x′ ∈PA′E′ ∪    use GetVals(M(x), N) to mean the activations of
WE′ were provided. The token predicted by M   neurons N when M processes input x.
should change from AE to AE′:
                                                    3.1 PCA
                              def
  Cause(A, F, M, D) =
                                                     Principal Component Analysis (PCA) is a dimen-
            ED II(M, F, x, x′) = AE′        sionality reduction method that minimizes informa-
If F isolates A, then interventions on F should not    tion loss. In particular, given a set of real valued
cause the values of other attributes A∗∈A \ {A}    vectors V ⊂Rn, |V| > n, the principal compo-
to change. When M is given a prompt x∗∈PA∗E  ,   nents are n orthogonal unit vectors p1, . . . , pn that
we can again intervene on F to set the value to what   form an n × n matrix:
it would be if a second prompt x′ ∈PA′E′ ∪WE′
                                              PCA(V) = p1   . . .  pnwere provided. The token predicted by M should
remain A∗E:
                                                For our purposes, the orthogonal matrix formed by
                           def
  Iso(A, F, M, D)           =                               the principal components serves as a featurizer that
    1                                    maps neurons N into a more interpretable space
    X ED II(M, F, x∗, x′) = A∗E      (Chormai et al., 2022; Marks and Tegmark, 2023;|A \ {A}|
         A∗∈A\{A}                               Tigges et al., 2023). Given an attribute A, a training


                                         4

dataset D from RAVEL for a particular entity type,   3.3  Relaxed Linear Adversarial Probe
a model M, and a set of neurons N, we define
                                                Supervised probes are a popular interpretability
  FA(n) =                                        technique for analyzing how neural activations cor-
                                                           relate with high-level concepts (Peters et al., 2018;     nT PCA({GetVals(M(x), N) : x ∈D})
                                          Hupkes et al., 2018; Tenney et al., 2019; Clark
  PCA is an unsupervised method, so there is no    et al., 2019). When probes are arbitrarily powerful,
easy way to tell what information is encoded in    this method is equivalent to measuring the mutual
each principal component. To solve this issue, for    information between the neurons and the concept
each attribute A ∈A we train a linear classifier   (Pimentel et al., 2020; Hewitt et al., 2021). How-
with L1 regularization to predict the value of A    ever, probes are typically simple linear models in
from the featurized neural representations. Then,   order to capture how easily the information about
we define the feature FA to be the set of dimensions   a concept can be extracted. Probes have also been
assigned a weight by the classifier that is greater   used to great effect on the task of concept erasure
than a hyperparameter ϵ.                           (Ravfogel et al., 2020; Elazar et al., 2021; Ravfogel
                                                            et al., 2022).
3.2  Sparse Autoencoder
                                                  Following the method of Ravfogel et al. (2022),
A recent approach to featurization is to train an                                     we train a relaxed linear adversarial probe (RLAP)
autoencoder to project neural activations into a                                                         to learn a linear subspace parameterized by a set of
higher dimensional sparse feature space and then                                         k orthonormal vectors W ∈Rk×n that captures an
reconstruct the neural activations from the features                                                           attribute A, using the following loss objective:
(Bricken et al., 2023; Cunningham et al., 2024).
We train a sparse autoencoder on the loss
                                               min maxX CE θT f, AEx
X ||GetVals(M(x), N)− W2f +b2  ||2+||f||1                   θ  W x∈D
x∈D
 f = ReLU(W1(GetVals(M(x), N) −b2) + b1)           f = (I −W T W) GetVals(M(x), N)
with W1 ∈Rn×m, W2 ∈Rm×n, b1 ∈Rm, and
b2 ∈Rn. To construct a training datset, we sam-   where f is the representation of the entity with
ple 100k sentences from the Wikipedia corpus for    the attribute information erased, and θ is a linear
each entity type, each containing a mention of an    classifier that tries to predict the attribute value
entity in the training set. We extract the 4096-  AEx from the erased entity representation.
dimension hidden states of Llama2-7B at the target    We define the F using the set of k orthonormal
intervention site as the input for training a sparse    vectors that span the row space of W and the set of
autoencoder with 16384 features.              n −k orthonormal vectors that span the null space:
  We use the autoencoder to define a featurizer
                                         FA(n) = n r1   . . .  rk  uk+1   . . .  un     FA(n) = ReLU(W1(n −b2) + b1)
and an inverse F−1A (n) = W2n + b2.                                           Our feature FA is the first k dimensions of the
  An important caveat to this method is that the
                                                      feature space, i.e. the row space of W. Intuitively,
featurizer is only truly invertible if the autoencoder
                                                    since the linear probe was trained to extract the
has a reconstruction loss of 0. The larger the loss
                                                          attribute A, the rowspace is the linear subspace of
is, the more unfaithful this interpretability method
                                                    neural activations that the probe is “looking at” to
is to the model being analyzed. All other methods
                                        make predictions.
considered use an orthogonal matrix, which is truly
invertible up to floating point precision.
                                                    3.4  Differential Binary Masking
  Similar to PCA, sparse autoencoders are an un-
supervised method that does not produce features    Differential Binary Masking (DBM) learns a binary
with obvious meanings. Again, to solve this issue,   mask to select a set of neurons that causally rep-
for each attribute A ∈A we train a linear classifier    resents a concept (Cao et al., 2020; Csordás et al.,
with L1 regularization and define the feature FA to   2021; Cao et al., 2022; Davies et al., 2023). The
be the set of dimensions assigned a weight that is    loss objective used to train the mask is a combina-
greater than a hyperparameter ϵ.                      tion of matching the counterfactual behavior and


                                         5

    Method      Supervision    Entity   Context

     Full Rep.      None         40.5      39.5
   PCA         None         39.5      39.1
   SAE         None         48.6      46.8
   RLAP         Attribute       48.8      50.9
   DBM       Counterfactual    52.2      49.8
   DAS       Counterfactual    56.5      57.3
   MDBM    Counterfactual    53.7      53.9
   MDAS     Counterfactual    60.1      65.6

Table 2: The disentanglement score on RAVEL for each
interpretability method. Numbers are represented in %.

forcing the mask to be sparse with coefficient λ:

  LCause = CE(τ(MN←n(x)), AE′) + λ||m||1
                                                      Figure 2: Cause and Iso scores for each method when
  n = 1 −σ(m/T) ◦GetVals(M(x), N)       using different feature sizes, shown as the ratio (%)
       + σ(m/T) ◦GetVals(M(x′), N)      between the dimension of FA and the dimension of the
                                                      output space of F. Each method has three data points
where the intervention is determined by inputs x, x′     that vary from using very few (≈1%) to half (≈50%) of
and learnable parameter m ∈Rn, where ◦is ele-    the dimensions. Increasing feature dimensions generally
ment wise multiplication and T ∈R is a tempera-    leads to higher Cause score, but lower Iso score. Figure
                                                           best viewed in color.
ture annealed throughout training.
  The feature space is the original space of neural   without considering the impact on the Iso score.
activations, i.e., featurizer FA(n) = n. The feature  We introduce the Iso aspect into the training objec-
FA is the set of dimensions i where 1−σ(mi/T) <    tive through multitask learning. For each attribute
ϵ for a (small) hyperparameter ϵ.           A∗∈A \ {A}, we define the Iso objective as
3.5  Distributed Alignment Search               LIso(A, FA, M) = CE(II(M(x), FA, x′), A∗E)

Distributed Alignment Search (DAS) (Geiger et al.,  We minimize a linear combination of losses from
2023b) learns a linear subspace of a model repre-   each task:
sentation with a training objective defined using
interchange interventions. In the original work, the      LDisentangle(A, FA, M) =
linear subspace learned by DAS is parameterized                              LIso(A∗, FA, M)
                                                 LCause(A, FA, M)+ Xas an n × n orthogonal matrix Q = [u1 . . . un],                                                                  |A \ {A}|
which rotates the representation into a new coor-                    A∗∈A\{A}
dinate system, i.e., FA(n) = Q⊤n. The set of
feature FA is the first k dimensions of the rotated   4  Experiments
subspace, where k is a hyperparameter. The matrix
                                 We evaluate the methods described in Section 3 on
Q is learned by minimizing the following loss:
                                     RAVEL with Llama2-7B (Touvron et al., 2023), a
                                                   32-layer decoder-only Transformer model, as theLCause(A, FA, M) = CE(II(M, FA, x, x′), AE′)
                                                          target LM. Implementation details of each method
Computing Q is expensive, as it requires comput-   are provided in Appendix B.
ing n orthogonal vectors. To avoid instantiating
                                                    4.1  Setupthe full rotation matrix, we use an alternative pa-
rameterization where we only learn the k ≪n  We consider the residual stream representations
orthogonal vectors that form the feature FA (see    at the last token of the entity as our potential in-
Appendix B.4).                                     tervention sites. For autoregressive LMs, the last
                                                token of the entity tE (e.g., the token “is” in the
3.6  Multi-task DBM and DAS                                                 case of “Paris”) likely aggregates information of
To address the disentanglement problem, we pro-   the entity (Meng et al., 2022; Geva et al., 2023).
pose a multitask extension of DBM (MDBM) and   For Transformer-based LMs like Llama2, an ac-
DAS (MDAS). The original training objective of    tivation vector NLt  in the residual stream is cre-
DBM and DAS only optimizes for the Cause score,   ated for each token t at each Transformer layer


                                         6

(a) Cause score for all attributes when intervening on the attribute features iden-  (b) The Cause, Iso, and Disentangle score
tified by DAS (left) and MDAS (right). A Cause score of 0.62 for column  on the Entity split for the “country” feature
Continent, row Timezone (bottom left corner), means that, when intervening on  found by MDAS. The attributes of cities be-
the Continent feature, the same subspace changes Timezone 62% of the time.    come more disentangled across layers.

                             Figure 3: Additional results for the MDAS method.


L. As the contributions of the MLP and attention    levels of disentanglement. Figure 3b shows how
heads must pass through the residual stream, it    the Cause, Iso, and Disentangle scores change
serves as a bottleneck. Therefore, we will limit our    for the “country” attribute across model layers.
methods to examining the set of representations
N = {NLtE : L ∈{1, . . . , 32}}.                Methods  with  counterfactual  supervision
                                               achieve strong  results while methods with  This simplification is only to establish baseline
                                             unsupervised featurizers struggle. MDAS isresults on the RAVEL benchmark. We expect the
                                                    the state-of-the-art method on RAVEL, being ablebest methods will consider other token representa-
                                                      to achieve high Disentangle scores while onlytions, such as the remainder of the token sequence
                                                    intervening on a feature FA with a dimensionalitythat realizes the entity.
                                                         that is 4% of |N| where N are the neurons the
4.2  Results                                        feature is distributed across (Figure 2). DBM,
                               MDBM, and DAS, the other methods that are
We evaluate each method on every representation                                                     trained with interventions using counterfactual
NLtE and report the highest disentanglement score    labels as supervision, achieve the next best perfor-
on test splits in Table 2. We additionally include a                                             mance. PCA and Sparse Autoencoder achieve the
baseline that simply replaces the full representation                                                   lowest Disentangle scores, which aligns with the
NLtE regardless of what attribute is being target-    prior finding that disentangled representations are
ted (see Full Rep. in Table 2). A breakdown of                                                             difficult to learn without supervision (Locatello
the results with per-attribute Cause and Iso is in                                                           et al., 2018).  Unsurprisingly, more supervision
Appendix C.                                                          results in higher performance.
  In Figure 2, we show for each method, how the
Iso and Cause scores vary as we change the dimen-   Multi-task supervision is better at isolating at-
sionality of FA, the feature targeted for interven-   tributes.  Adding multitask objectives to DBM
tion. For RLAP, DAS, and MDAS, the dimension-   and DAS increases the overall disentanglement
ality of FA is a hyperparameter we vary directly.   score by 1.5%/4.1% and 3.6%/8.3% on  the
For other methods, we vary the coefficient of L1   Entity/Context split respectively. To further il-
penalty to vary the size of FA. Details are given in    lustrate the differences, we compare DAS with
Appendix B.                       MDAS in Figure 3a. On the left, attributes such
  In Figure 3, we focus on using MDAS, the best    as “continent” and “timezone” are naturally entan-
performing method, to understand how attributes    gled with all other attributes; intervening on the
are disentangled in Llama2-7B. Figure 3a shows    feature learned by DAS for any city attribute will
two heat maps summarizing the performance of    also change these two attributes. In contrast, in
DAS and MDAS on the entity type “city”. These    Figure 3a right, MDAS is far more successful at
heat maps also show how attributes have different    disentangling these attributes, having small Cause


                                         7

scores in all off-diagonal entries.                    concepts in a overlapping sets of neurons (Smolen-
                                                    sky, 1988; Olah et al., 2020; Elhage et al., 2022).
Some groups of attributes are more difficult
                                                 Various methods have been proposed to find com-
to disentangle than others.  As show in Fig-
                                                ponents that capture a concept, such as finding a
ure 3a, the attribute pairs “country–language” and
                                                        linear subspace that modifies a concept (Ravfo-
“latitude–longitude” are difficult to disentangle.
                                                    gel et al., 2020, 2022; Belrose et al., 2023; Cao
When we train DAS to find a feature for either
                                                           et al., 2020; Geiger et al., 2023b) and generating
of these attributes (Figure 3a left), the same feature
                                                a sparse feature space where each direction cap-
also has causal effects on the other attribute. Even
                                                       tures a word sense or is more interpretable (Arora
with the additional supervision (Figure 3a right),
                                                             et al., 2018; Bricken et al., 2023; Cunningham et al.,
MDAS cannot isolate these attributes. Changing
                                               2024; Tamkin et al., 2023). However, these meth-
one of these entangled attributes has seemingly un-
                                               ods have not been evaluated against each other on
avoidable ripple effects (Cohen et al., 2024) that
                                                          their ability to isolate concepts. Isolating an indi-
change the other.  In contrast, the attribute pair
                                                    vidual concept is also related to the goal of “disen-
“language–continent” can be disentangled. More-
                                                 tanglement” in representation learning (Schölkopf
over, the pairs that are difficult to disentangle are
                                                           et al., 2021), where each direction captures a sin-
consistent across all five supervised methods in
                                                    gle generative factor. In this work, we focus on
our experiment, despite these methods using dif-
                                                         isolating the causal effect of a representation.
ferent training objectives. We include additional
visualizations in Appendix C.2.                 Knowledge Representation  in LMs  Under-
                                                  standing knowledge representation in LMs startsAttributes are gradually disentangled across lay-
                                                  with probing structured linguistic knowledge (Con-
ers.  The representations of different attributes
                                             neau et al., 2018; Tenney et al., 2019; Manning
gradually disentangle as we move towards later lay-
                                                             et al., 2020). Recent work expands to factual knowl-
ers, as shown in Figure 3b. Early layer features
                                               edge stored in Transformer MLP layers (Geva et al.,
identified by MDAS fail to generalize to unseen
                                                2021; Dai et al., 2022; Meng et al., 2022), associa-entities, hence low Cause score. While MDAS is
                                                         tions represented in linear structures (Merullo et al.,able to identify a feature with relatively high Cause
                                                 2023; Hernandez et al., 2024; Park et al., 2023), andstarting at layer 8, the Iso increases from 0.5 to 0.8
                                                  deeper study of the semantic enrichment of subject
from layer 8 to layer 16. It is not until layer 16 that
                                                     representation (Geva et al., 2023). These findingsthe highest Disentangle score is achieved.
                                                  suggest LMs store knowledge modularly, motivat-
5  Related Work                               ing the disentanglement objective in our work.

Intervention-based Interpretability Methods   Benchmarking Interpretability Methods   Test-
Intervention-based techniques, branching off from    ing the faithfulness of interpretability method relies
interchange intervention (Vig et al., 2020; Geiger   on counterfactuals. Existing counterfactual bench-
et al., 2020) or activation patching (Meng et al.,   marks use behavioral testing (Atanasova et al.,
2022), have shown promising results in uncovering   2023; Schwettmann et al., 2023; Mills et al., 2023),
causal mechanisms of LMs. They play important    interventions (Abraham et al., 2022), or a combi-
roles in recent interpretability research of LMs such    nation of both (Huang et al., 2023). Recent model
as causal abstraction (Geiger et al., 2021, 2023b),    editing benchmarks (Meng et al., 2022; Zhong
causal tracing to locate factual knowledge (Meng    et al., 2023; Cohen et al., 2024) also provide coun-
et al., 2022; Geva et al., 2023), path patching or    terfactuals that have potential for evaluating in-
causal scrubbing to find causal circuits (Chan et al.,    terpretability methods. MQUAKE (Zhong et al.,
2022; Conmy et al., 2023; Goldowsky-Dill et al.,   2023) and RIPPLEEDITS (Cohen et al., 2024), in
2023), and Distributed Alignment Search (Geiger    particular, consider entailment relationships of at-
et al., 2023b). Previous works suggest that activa-    tributes, while we focus on disentanglement.
tion interventions that result in systematic counter-
factual behaviors provide clear causal insights into   6  Conclusion
model components.
                                We present RAVEL a benchmark for evaluating
Isolating Individual Concepts  LMs learn highly    the ability of interpretability methods to localize
distributed representations that encode multiple   and disentangle entity attributes in LMs in a causal,


                                         8

generalizable manner. We show how RAVEL can   used for manipulating models in undesirable ap-
be used to evaluate five different families of in-    plications such as triggering toxic outputs. These
terpretability methods that are commonly used in    interpretability methods should be studied and used
the community. We benchmark several strong in-    in a responsible manner.
terpretability methods on RAVEL with Llama2-7B
                                      Acknowledgementsmodel as baselines, and we introduce a multi-task
objective that improves the performance of Dif-                                                This research is supported in part by grants from
ferential Binary Masking (DBM) and Distributed                                       Open Philanthropy and the Stanford Institute for
Alignment Search (DAS). Multi-task DAS achieves                                            Human-Centered Artificial Intelligence (HAI).
the best results in our experiments. Results on our
attribute disentanglement task also offer insights
into the different levels of entanglement between   References
attributes and the emergence of disentangled repre-                                                      Eldar David Abraham, Karel D’Oosterlinck, Amir
sentations across layers in the Llama2-7B model.       Feder, Yair Gat, Atticus Geiger, Christopher Potts,
  The community has seen an outpouring of in-     Roi Reichart, and Zhengxuan Wu. 2022. CEBaB:
                                                         Estimating the causal effects of real-world conceptsnovative new interpretability methods. However,
                                                 on NLP model behavior. In Advances in Neural In-
these methods have not been systematically eval-                                                        formation Processing Systems (NeurIPS).
uated for whether they are faithful, generalizable,
causally effective, and able to isolate individual    Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
                                                    and Andrej Risteski. 2018. Linear algebraic structureconcepts. We release RAVEL2 to the community
                                                          of word senses, with applications to polysemy. In
and hope it will help drive the assessment and de-      Transactions of the Association of Computational
velopment of interpretability methods that satisfy       Linguistics (TACL).
these criteria.
                                                Pepa Atanasova, Oana-Maria Camburu, Christina Li-
                                                 oma, Thomas Lukasiewicz, Jakob Grue Simonsen,
Limitations                                       and Isabelle Augenstein. 2023. Faithfulness tests for
                                                             natural language explanations.  In Association for
Our attribute disentanglement results in Section 4      Computational Linguistics (ACL).
are based on the Llama2-7B model. While Llama2-
                                                  Sander Beckers, Frederick Eberhardt, and Joseph Y.
7B uses the widely adopted decoder-only Trans-
                                                        Halpern. 2020.  Approximate causal abstractions.
former architecture, different model architectures       In Uncertainty in Artificial Intelligence Conference
or training paradigms could produce LMs that fa-      (UAI).
vor different interpretability methods. Hence, when
                                                    Sander Beckers and Joseph Y. Halpern. 2019. Abstract-
deciding which interpretability method is the best                                                          ing causal models. In Conference on Artificial Intel-
to apply to a new model, we encourage people to       ligence (AAAI).
instantiate RAVEL on the new model.
                                              Nora Belrose, David Schneider-Joseph, Shauli Ravfo-
  When choosing intervention sites, we limit our                                                                  gel, Ryan Cotterell, Edward Raff, and Stella Bider-
search to the residual stream above the last entity     man. 2023. LEACE: Perfect linear concept erasure
token. However, representations of attributes can       in closed form. In Advances in Neural Information
be distributed across multiple tokens or layers. We      Processing Systems (NeurIPS).
encourage future work to explore different inter-                                                    Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Co-
vention sites when using this benchmark.               enen, Emily Reif, Fernanda B. Viégas, and Martin
                                                       Wattenberg. 2021. An interpretability illusion for
Ethics Statement                             BERT. In arXiv preprint arXiv:2104.07143.

                                                    Trenton Bricken, Adly Templeton, Joshua Batson,
In this paper, we present an interpretability bench-                                                       Brian Chen, Adam Jermyn, Tom Conerly, Nick
mark that aims to assess the faithfulness, general-      Turner, Cem Anil, Carson Denison, Amanda Askell,
izability, causal effects, and the ability to isolate      Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
individual concepts in language models. While an       Schiefer, Tim Maxwell,  Nicholas Joseph, Zac
                                                           Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Bray-
interpretability method that satisfies these criteria
                                                  den McLean, Josiah E Burke, Tristan Hume, Shan
could be useful for assessing model bias or steering       Carter, Tom Henighan, and Christopher Olah. 2023.
model behaviors, the same method might also be      Towards monosemanticity: Decomposing language
                                                    models with dictionary learning. In Transformer Cir-
   2https://github.com/explanare/ravel                     cuits Thread.


                                         9

Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah,   Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
  Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben      Chang, and Furu Wei. 2022. Knowledge neurons in
  Egan, and Swee Kiat Lim. 2020. Thread: Circuits.       pretrained transformers. In Association for Computa-
   In Distill.                                                   tional Linguistics (ACL).

Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz,   Xander  Davies,  Max  Nadeau,  Nikhil  Prakash,
  and Ivan Titov. 2020. How do decisions emerge     Tamar Rott Shaham, and David Bau. 2023. Discov-
   across layers in neural models? Interpretation with       ering variable binding circuitry with desiderata. In
   differentiable masking.  In Empirical Methods in      arXiv preprint arXiv:2307.03637.
  Natural Language Processing (EMNLP).
                                                   Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav
Nicola De Cao, Leon Schmid, Dieuwke Hupkes, and      Goldberg. 2021. Amnesic Probing: Behavioral Ex-
  Ivan Titov. 2022. Sparse interventions in language       planation with Amnesic Counterfactuals. In Transac-
  models with differentiable masking. In Proceedings       tions of the Association of Computational Linguistics
   of the Fifth BlackboxNLP Workshop on Analyzing      (TACL).
  and Interpreting Neural Networks for NLP.
                                                 Nelson Elhage,  Tristan Hume,  Catherine Olsson,
Lawrence Chan,  Adrià  Garriga-Alonso,  Nicholas                                                       Nicholas Schiefer, Tom Henighan, Shauna Kravec,
  Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishin-                                                Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
   skaya, Ansh Radhakrishnan, Buck Shlegeris, and                                                       Carol Chen, Roger Grosse, Sam McCandlish, Jared
  Nate Thomas. 2022. Causal scrubbing: a method                                                      Kaplan, Dario Amodei, Martin Wattenberg, and
   for rigorously testing interpretability hypotheses. In                                                        Christopher Olah. 2022.  Toy models of superpo-
  Alignment Forum Blog post.                                                                      sition. In arXiv preprint arXiv:2209.10652.

Pattarawat Chormai,  Jan Herrmann, Klaus-Robert                                                            Jiahai Feng and Jacob Steinhardt. 2024. How do lan-
   Müller, and Grégoire Montavon. 2022.  Disentan-                                                   guage models bind entities in context?   In Inter-
  gled explanations of neural network predictions                                                          national Conference on Learning Representations
  by finding relevant subspaces.  In arXiv preprint                                                        (ICLR).
  arXiv:2212.14855.

                                              Matthew  Finlayson,  Aaron  Mueller,   Sebastian
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
                                                 Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan
   Christopher D. Manning. 2019. What does BERT
                                                         Belinkov. 2021.   Causal  analysis of  syntactic
  look at? An analysis of BERT’s attention. In Pro-
                                                    agreement mechanisms in neural language models.
  ceedings of the 2019 ACL Workshop BlackboxNLP:
                                                          In Association for Computational Linguistics and
  Analyzing and Interpreting Neural Networks for NLP.
                                                              International Joint Conference on Natural Language
                                                        Processing (ACL-IJCNLP).Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,
  and Mor Geva. 2024. Evaluating the ripple effects of
                                                        Atticus Geiger, Hanson Lu, Thomas F Icard, and  knowledge editing in language models. In Transac-
                                                         Christopher Potts. 2021. Causal abstractions of neu-   tions of the Association of Computational Linguistics
                                                                     ral networks.  In Advances in Neural Information  (TACL).
                                                        Processing Systems (NeurIPS).
Arthur Conmy, Augustine N. Mavor-Parker, Aengus
                                                        Atticus Geiger, Christopher Potts, and Thomas Icard.  Lynch, Stefan Heimersheim, and Adrià Garriga-
                                                        2023a. Causal abstraction for faithful model interpre-  Alonso. 2023.  Towards automated circuit discov-
                                                                    tation. Ms., Stanford University.   ery for mechanistic interpretability. In Advances in
  Neural Information Processing Systems (NeurIPS).
                                                          Atticus Geiger, Kyle Richardson, and Chris Potts. 2020.
Alexis Conneau, German Kruszewski, Guillaume Lam-      Neural natural language inference models partially
   ple, Loïc Barrault, and Marco Baroni. 2018. What     embed theories of lexical entailment and negation. In
  you can cram into a single $&!#* vector: Probing      Proceedings of the 2020 EMNLP Workshop Black-
  sentence embeddings for linguistic properties.  In     boxNLP: Analyzing and Interpreting Neural Net-
  Association for Computational Linguistics (ACL).        works for NLP.

Róbert Csordás, Sjoerd van Steenkiste, and Jürgen    Atticus Geiger, Zhengxuan Wu, Christopher Potts,
  Schmidhuber. 2021. Are neural nets modular? In-     Thomas Icard, and Noah D. Goodman. 2023b. Find-
   specting functional modularity through differentiable       ing alignments between interpretable causal variables
  weight masks. In International Conference on Learn-     and distributed neural representations.  In Causal
   ing Representations (ICLR).                           Learning and Reasoning (CLeaR).

Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert   Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
  Huben, and Lee Sharkey. 2024. Sparse autoencoders      Globerson. 2023.  Dissecting recall of factual as-
   find highly interpretable features in language models.      sociations in auto-regressive language models.  In
   In International Conference on Learning Representa-      Empirical Methods in Natural Language Processing
   tions (ICLR).                                   (EMNLP).


                                         10

Mor Geva, Roei Schuster, Jonathan Berant, and Omer    Belinda Z. Li, Maxwell I. Nye, and Jacob Andreas.
   Levy. 2021. Transformer feed-forward layers are key-      2021. Implicit representations of meaning in neural
  value memories. In Empirical Methods in Natural      language models.  In Proceedings of the 59th An-
  Language Processing (EMNLP).                       nual Meeting of the Association for Computational
                                                             Linguistics and the 11th International Joint Confer-
Asma Ghandeharioun, Avi Caciularu, Adam Pearce,      ence on Natural Language Processing, ACL/IJCNLP
  Lucas Dixon, and Mor Geva. 2024.  Patchscopes:      2021, (Volume 1: Long Papers), Virtual Event, Au-
 A unifying framework for inspecting hidden repre-      gust 1-6, 2021, pages 1813–1827. Association for
   sentations of language models.  In arXiv preprint      Computational Linguistics.
  arXiv:2401.06102.
                                      Tom Lieberum, Matthew Rahtz, János Kramár, Neel
                                                   Nanda, Geoffrey Irving, Rohin Shah, and VladimirNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato,
  and Aryaman Arora. 2023.   Localizing model      Mikulik. 2023. Does circuit analysis interpretability
  behavior with path patching.   In arXiv preprint       scale? Evidence from multiple choice capabilities in
                                                                 chinchilla. In arXiv preprint arXiv:2307.09458.  arXiv:2304.05969.

                                                   Francesco Locatello, Stefan Bauer, Mario Lucic, Syl-
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine                                                           vain Gelly, Bernhard Schölkopf, and Olivier Bachem.
  Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.                                                      2018. Challenging common assumptions in the un-
  2023. Finding neurons in a haystack: Case studies                                                         supervised learning of disentangled representations.
  with sparse probing.  In Transactions on Machine                                               CoRR, abs/1811.12359.
  Learning Research (TMLR).
                                                      Christopher D. Manning, Kevin Clark, John Hewitt,
Michael Hanna, Ollie Liu, and Alexandre Variengien.      Urvashi Khandelwal, and Omer Levy. 2020. Emer-
  2023. How does GPT-2 compute greater-than?: In-      gent linguistic structure in artificial neural networks
   terpreting mathematical abilities in a pre-trained lan-       trained by self-supervision. In Proceedings of the
  guage model.  In Advances in Neural Information      National Academy of Sciences (PNAS).
  Processing Systems (NeurIPS).
                                               Samuel Marks and Max Tegmark. 2023. The geometry
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghan-      of truth: Emergent linear structure in large language
   deharioun. 2023. Does localization inform editing?      model representations of true/false datasets. In arXiv
   surprising differences in causality-based localization       preprint arXiv:2310.06824.
   vs. knowledge editing in language models. In Ad-
                                                                         J. L. McClelland, D. E. Rumelhart, and PDP Research  vances in Neural Information Processing Systems
                                                    Group, editors. 1986. Parallel Distributed Process-  (NeurIPS).
                                                                   ing. Volume 2: Psychological and Biological Models.
                                        MIT Press, Cambridge, MA.Roee Hendel, Mor Geva, and Amir Globerson. 2023.
   In-context learning creates task vectors. In Empirical                                                 Kevin Meng, David Bau, Alex Andonian, and Yonatan
  Methods in Natural Language Processing (EMNLP).                                                          Belinkov. 2022. Locating and editing factual associ-
                                                             ations in GPT. In Advances in Neural Information
Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin                                                        Processing Systems (NeurIPS).
  Meng, Martin Wattenberg, Jacob Andreas, Yonatan
   Belinkov, and David Bau. 2024. Linearity of relation    Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.
  decoding in transformer language models. In Inter-      2023. A mechanism for solving relational tasks
   national Conference on Learning Representations       in transformer language models. In arXiv preprint
  (ICLR).                                              arXiv:2305.16130.

John Hewitt, Kawin Ethayarajh, Percy Liang, and   Edmund Mills, Shiye Su, Stuart Russell, and Scott Em-
   Christopher Manning. 2021. Conditional probing:      mons. 2023. Almanacs: A simulatability benchmark
  measuring usable information beyond a baseline. In       for language model explainability. In arXiv preprint
  Empirical Methods in Natural Language Processing      arXiv:2312.12747.
  (EMNLP).
                                                        Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel
                                                  Goh, Michael Petrov, and Shan Carter. 2020. Zoom
Jing Huang,  Atticus Geiger, Karel D’Oosterlinck,
                                                                      in: An introduction to circuits. In Distill.
  Zhengxuan Wu, and Christopher Potts. 2023. Rig-
   orously assessing natural language explanations of   Kiho Park, Yo Joong Choe, and Victor Veitch. 2023.
   neurons.  In Proceedings of the 6th BlackboxNLP     The linear representation hypothesis and the geom-
  Workshop: Analyzing and Interpreting Neural Net-                                                              etry of large language models.  In arXiv preprint
  works for NLP.                                       arXiv:2311.03658.

Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.   Judea Pearl. 2001. Direct and indirect effects. In Pro-
  2018. Visualisation and “diagnostic classifiers” re-      ceedings of the Seventeenth Conference on Uncer-
   veal how recurrent and recursive neural networks       tainty in Artificial Intelligence, UAI’01, pages 411–
   process hierarchical structure. In Journal of Artificial      420, San Francisco, CA, USA. Morgan Kaufmann
   Intelligence Research (JAIR).                             Publishers Inc.


                                         11

Judea Pearl. 2009. Causality. Cambridge University    Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron
   Press.                                                 Mueller, Byron C. Wallace, and David Bau. 2024.
                                                       Function vectors in large language models.  In In-
Matthew E. Peters, Mark Neumann, Luke Zettlemoyer,       ternational Conference on Learning Representations
  and Wen-tau Yih. 2018. Dissecting contextual word      (ICLR).
  embeddings: Architecture and representation.  In
  Empirical Methods in Natural Language Processing   Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
  (EMNLP).                                                     bert, Amjad Almahairi, Yasmine Babaei, Nikolay
                                                         Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,      Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
  Ran Zmigrod, Adina Williams, and Ryan Cotterell.       Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
  2020. Information-theoretic probing for linguistic      Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
   structure. In Association for Computational Linguis-      Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
   tics (ACL).                                          thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                                                              Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael       Isabel Kloumann, Artem Korenev, Punit Singh Koura,
  Twiton, and Yoav Goldberg. 2020. Null it out: Guard-     Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
   ing protected attributes by iterative nullspace projec-      ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
   tion. In Association for Computational Linguistics        tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
  (ACL).                                               bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                                                                    stein, Rashi Rungta, Kalyan Saladi, Alan Schel-
Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and       ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-
  Ryan D Cotterell. 2022. Linear adversarial concept      ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
   erasure.  In International Conference on Machine       Taylor, Adina Williams, Jian Xiang Kuan, Puxin
  Learning (ICML).                                 Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
                                                          gela Fan, Melanie Kambadur, Sharan Narang, Au-
D. E. Rumelhart, J. L. McClelland, and PDP Research       relien Rodriguez, Robert Stojnic, Sergey Edunov,
  Group, editors. 1986. Parallel Distributed Process-     and Thomas Scialom. 2023. Llama 2: Open founda-
   ing. Volume 1: Foundations. MIT Press, Cambridge,       tion and fine-tuned chat models. In arXiv preprint
  MA.                                                 arXiv:2307.09288.

Bernhard Schölkopf, Francesco Locatello, Stefan Bauer,   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
  Nan Rosemary Ke, Nal Kalchbrenner, Anirudh       Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
  Goyal, and Yoshua Bengio. 2021. Toward causal rep-      Kaiser, and Illia Polosukhin. 2017. Attention is all
   resentation learning. Proc. IEEE, 109(5):612–634.      you need. In Advances in Neural Information Pro-
                                                           cessing Systems (NeurIPS).
Sarah Schwettmann, Tamar Rott Shaham, Joanna
  Materzynska, Neil Chowdhury, Shuang Li, Jacob    Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
  Andreas, David Bau, and Antonio Torralba. 2023.      Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
   Find: A function description benchmark for evaluat-       Shieber. 2020. Investigating gender bias in language
   ing interpretability methods. In Advances in Neural      models using causal mediation analysis. In Advances
  Information Processing Systems (NeurIPS) Track on       in Neural Information Processing Systems (NeurIPS).
  Datasets and Benchmarks.
                                                  Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Paul Smolensky. 1988. On the proper treatment of con-     Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-
   nectionism. Behavioral and Brain Sciences, 11(1):1–       pretability in the wild: a circuit for indirect object
   23.                                                           identification in GPT-2 small. In International Con-
                                                            ference on Learning Representations (ICLR).
Peter Spirtes, Clark Glymour, and Richard Scheines.
  2000.  Causation, Prediction, and Search. MIT   Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christo-
   Press.                                               pher Potts, and Noah D. Goodman. 2023.  Inter-
                                                                 pretability at scale: Identifying causal mechanisms
Alex Tamkin, Mohammad Taufeeque, and Noah D.       in alpaca. In Advances in Neural Information Pro-
  Goodman. 2023. Codebook features: Sparse and       cessing Systems (NeurIPS).
   discrete interpretability for neural networks. In arXiv
                                               Zexuan Zhong, Zhengxuan Wu, Christopher Manning,   preprint arXiv:2310.17230.
                                                           Christopher Potts, and Danqi Chen. 2023. MQuAKE:
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.      Assessing knowledge editing in language models via
  BERT rediscovers the classical NLP pipeline.  In      multi-hop questions. In Empirical Methods in Natu-
  Association for Computational Linguistics (ACL).          ral Language Processing (EMNLP).

Curt Tigges, Oskar John Hollinsworth, Atticus Geiger,
  and Neel Nanda. 2023.  Linear representations of
   sentiment in large language models. In arXiv preprint
  arXiv:2310.15154.


                                         12

A  Dataset Details


  Attributes  |AE|  Sample Values           Sample Prompts

  City
                                        city to country: Toronto is in Canada. {E} is in,
                  United  States,   China,
  Country   158                          [{"city": "Paris", "country": "France"}, {"city": "{E}",
                    Russia, Brazil, Australia
                                            "country": "
                    Asia,  Europe,   Africa,
                                        {E} is a city in the continent of,
  Continent  6     North  America,  South                                        [{"city": "{E}", "continent": "
                 America
                                        [{"city": "Rio de Janeiro", "lat": "23"},
  Latitude   122   41, 37, 47, 36, 35
                                            {"city": "{E}", "lat": ", [{"city": "{E}", "lat": "
                                        [{"city": "Rome", "long": "12.5"}, {"city": "{E}", "long": ",
  Longitude  317   30, 9, 10, 33, 11                                         "long": "122.4"}, {"city": "{E}", "long": "
                                        [{"city": "Beijing", "lang": "Chinese"},
                    English,  Spanish,  Chi-
 Language  159                              {"city": "{E}", "lang": ",
                     nese, Russian, Portuguese
                                        [{"city": "{E}", "official language": "
                  America/Chicago,
                                        Time zone in Los Angeles is America/Santiago;
                   Asia/Shanghai,
                                            Time zone in {E} is,
 Timezone  267   Asia/Kolkata,                                        [{"city": "New Delhi", "timezone": "UTC+5:30"},
                 Europe/Moscow, Amer-
                                            {"city": "{E}", "timezone": "UTC
                   ica/Sao_Paulo

 Nobel Laureate
                                        Jules A. Hoffmann won the Nobel Prize in Medicine.
                   Medicine, Physics, Chem-
  Field      7                                {E} won the Nobel Prize in,
                          istry, Literature, Peace
                                        name: {E}, award: Nobel Prize in
                  2001, 2019, 2009, 2011,  "name": {E}, "award": "Nobel Prize", "year": ",
 Award    118
                 2000                   laureate: Frances H. Arnold, year: 2018, laureate: {E}, year:
 Year
                  1918, 1940, 1943, 1911,  Alan Heeger was born in 1936. {E} was born in,
  Birth      145
                 1941                   laureate: {E}, date of birth (YYYY-MM-DD):
 Year
                  United  States,  United
                                        name: {E}, country:,
  Country   81    Kingdom,    Germany,                                        Roderick MacKinnon was born in United States. {E} was born in
  of Birth           France, Sweden
                                        name: {E}, gender:,
 Gender    4       his, male, female, her                                        David M. Lee: for his contributions in physics. {E}: for

Table 3: Attributes in RAVEL. |AE| is the number of unique attribute values. In sampled prompts, {E} is a
placeholder for the entity.


A.1  Details of Entities and Attributes

We first identify entity types from existing datasets such as the Relations Dataset (Hernandez et al., 2024)
and RIPPLEEDITS (Cohen et al., 2024), where each entity type potentially contains thousands of instances.
We then source the entities and ground truth references for attribute values from online sources.3 4 5 6 7 8
These online sources are distributed under MIT, Apache-2.0, and CC-0 licenses. Compared with similar
entity types in the Relations Dataset and RIPPLEEDITS, RAVEL has expanded the number of entities by a
factor of at least 10 and included multiple attributes per entity.
  We show the cardinality of the attributes, most frequent attribute values, and random samples of prompt
templates in Table 3.

A.2  The RAVEL Llama2-7B Instance

The RAVEL Llama2-7B instance is used for benchmarking interpretability methods in Section 4. There
are a total of 2800 entities in the Llama2-7B instance. Table 4 shows the number of entities, prompt

   3https://github.com/kevinroberts/city-timezones
   4https://github.com/open-dict-data/ipa-dict/blob/master/data/en_US.txt
   5https://github.com/monolithpl/verb.forms.dictionary
   6https://www.nobelprize.org/prizes/lists/all-nobel-prizes/
   7https://huggingface.co/datasets/corypaik/coda
   8https://www.bls.gov/ooh,https://www.bls.gov/cps


                                         13

  Attributes  |AE|  Sample Values           Sample Prompts

 Verb
                     take hold of, make certain,
                                        talk: communicate by speaking; win: achieve victory; {E}:,
  Definition  986   show, express in words,                                        like: have a positive preference; walk: move on foot; {E}:
                make
                  expanded, sealed, termi-  present tense: {E}, past tense:,
  Past      986
                     nated, escaped, answered   write: wrote; look: looked; {E}:
 Tense
  Pronun-            k@n"fju:z,     fI"nIS,   bOIl,  create: kri"eIt; become: bI"k2m; {E}:,
           986
  ciation              In"SU@r, tIp               begin: bI"gIn; change: tSeIndZ; {E}:
                   compensates,      kicks,
                                        tell: tells; create: creates; {E}:,
  Singular   986   hunts,  earns, accompa-                                        present tense: {E}, 3rd person present:
                     nies

  Physical Object
                                        bird is a type of animal: YES; rock is a type of animal: NO;
                      plant,  non-living  thing,      {E} is a type of animal:,
  Category   29
                    animal, NO, fruit         Among the categories "plant", "animal", and "non-living thing",
                                            {E} belongs to "
                                        The color of apple is usually red. The color of violet is
                    green,   white,   yellow,      usually purple. The color of {E} is usually,
  Color     12
                  brown, black             The color of apple is usually red. The color of turquoise
                                            is usually blue. The color of {E} is usually
                                        Among the units "mm", "cm", "m", and "km",
                                            the size of {E} is usually on the scale of ",
  Size      4     cm, mm, m, km                                        Given the units "mm" "cm" "m" and "km",
                                            the size of {E} usually is in "
                                         hard or soft: rock is hard; towel is soft;
                                            blackberry is soft; wood is hard; {E} is,
  Texture    2       soft, hard                                        Texture: rock: hard; towel: soft; blackberry: soft;
                                            charcoal: hard; {E}:

 Occupation
                       treat patients, teach stu-  "occupation": "photographer", "duties": "to capture
 Duty      650   dents, sell products, cre-      images using cameras"; "occupation": "{E}", "duties": "to,
                      ate art, serve food         "occupation": "{E}", "primary duties": "to
                                        The {E} left early because
 Gender    9      he, male, his, female, she                                        The newspaper praised the {E} for
  Bias
                                        "occupation": "sales manager", "industry": "retail";
                     construction, automotive,
                                            "occupation": "{E}", "industry": ",
  Industry   280   education,  health  care,                                        "occupation": "software developer", "industry":
                     agriculture
                                            "technology"; "occupation": "{E}", "industry": "
 Work
                       office, factory, hospital,  "occupation": "software developer", "environment": "office";
 Loca-     128
                    construction site, studio        "occupation": "{E}", "environment": "
  tion

                                 Table 3: Attributes in RAVEL, continued.


                                            # Prompts  # Test Examples in
                     Entity Type      # Entities                                  Accuracy (%)
                                                Templates    Entity/Context

                    City              800        90           15K/33K            97.1
                  Nobel Laureate     600        60           9K/23K            94.3
                  Verb              600        40           12K/20K            95.1
                    Physical Object     400        40            4K/6K             94.3
                   Occupation        400        30           10K/16K            96.4

Table 4: Stats of RAVEL in its Llama2-7B instance, created by sampling a subset of examples where Llama2-7B has
a high accuracy in predicting attribute values.

templates, and test examples, i.e., the number of base–source input pairs for interchange intervention in
the Llama2-7B instance.
  The RAVEL Llama2-7B instance is created by filtering examples where the pre-trained Llama2-7B has
a high accuracy in predicting attribute values. For each entity type, we take the k entities with the highest
accuracy over all prompt templates and the n prompt templates with the highest accuracy over all entities,
with the average accuracy over all prompts shown in Table 4. For most attributes, we directly compare


                                         14

model outputs against the ground truth attribute values. For “latitude” and “longitude” of a city, we relax
the match to be ±2 within the ground truth value. For “pronunciation” of a verb, we relax the match to
allow variations in the transcription. For attributes with more open-ended outputs, including “definition”
of a verb and “duty” of an occupation, we manually verify if the outputs are sensible. For “gender bias”
of an occupation, we check for the consistency of gender bias over a set of prompts that instruct the model
to output gender pronouns.

B  Method Details

B.1 PCA

For PCA, we extract the 4096-dimension hidden state representations at the target intervention site as
the inputs. The representations are first normalized to zero-mean and unit-variance using mean and
variance estimated from the training set. We use the sklearn implementation9 to compute the principal
components. We then apply L1-based feature selection10 to identify a set of dimensions that most likely
encode the target attribute A. We undo the normalization after projecting back to the original space.
  We vary the coefficient of the L1 penalty, i.e., the parameter “C” in the sklearn implementation, to
experiment with different intervention dimensions. We experiment with C ∈{0.1, 1, 10, 1000}. We
observe that regardless of the intervention dimension, the features selected have a high overlap with the
first k principal components. For most attributes, the highest Disentangle score is achieved when using
the largest intervention dimension.

B.2  Sparse Autoencoder
For the sparse autoencoder, we use a single-layer encoder-decoder model.11 The autoencoder is trained on
Wikipedia data as described below.

Model  Encoder: Fully connected layer with ReLU activations, dimensions 4096 × 16384. Decoder:
Fully connected layer, dimensions 16384 × 4096. Latent dimension: 4 × 4096. The model is trained to
optimize a combination of an L2 loss to reconstruct the representation and an L1 loss to enforce sparsity.

Training Data  For each entity type, we sample 100k sentences from the Wikipedia corpus, each
containing a mention of an entity in the training set. We extract the 4096-dimension hidden states at the
target intervention site as the input for training the sparse autoencoder.
  Similar to PCA, we apply L1-based feature selection on the latent representation to identify a set of
dimensions that most likely encode the target attribute A. We vary the coefficient C of the L1 penalty to
experiment with different intervention dimension. The optimal C varies across attributes.

B.3 RLAP

RLAP learns a set of linear probes to find the feature F. Each linear probe aims to predict the attribute value
from the entity representations. Similar to PCA and sparse autoencoders, we use the 4096-dimension
hidden state representations at the target intervention site as the initial inputs and the corresponding
attribute value as labels. In the case of attributes with extremely large output spaces, e.g., numerical
outputs, we approximate the output with the first token. Table 5 shows the linear classifier accuracy on
each attribute classification task.
  We use the official R-LACE implementation12 and extract the rank-k orthogonal matrix W from the
final null projection13 as FA. For each attribute, we experiment with rank k ∈{32, 128, 512, 2048}. We
run the algorithm for 100 iterations and select the rank with the highest Disentangle score on the dev
set. The optimal intervention dimension is usually small, i.e., 32 or 128, for attributes that have a high
accuracy linear classifier.

   9https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
  10https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection
  11https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing#scrollTo=
Kn1E_44gCa-Z
  12https://github.com/shauli-ravfogel/rlace-icml
  13https://github.com/shauli-ravfogel/rlace-icml/blob/master/rlace.py#L90


                                         15

                                      Attribute           Entity       Context

                                   City
                                Country             0.78           1.00
                                   Continent           0.96           1.00
                                    Latitude             0.18           1.00
                                 Longitude           0.13           1.00
                               Language           0.60           1.00
                               Timezone           0.68           1.00
                               Nobel Laureate
                                     Field                0.82           1.00
                             Award Year          0.08           1.00
                                     Birth Year           0.01           1.00
                                Country of Birth     0.63           1.00
                               Gender              0.93           1.00
                                Verb
                                     Definition           0.03           1.00
                                     Past Tense           0.00           1.00
                                   Pronunciation        0.00           1.00
                                    Singular             0.00           1.00
                                    Physical Object
                                 Category            0.90           1.00
                                 Color               0.49           1.00
                                   Size                0.86           1.00
                                   Texture             0.75           1.00
                                 Occupation
                              Duty                0.06           1.00
                               Gender Bias         0.17           0.99
                                    Industry             0.43           1.00
                            Work Location       0.44           1.00

Table 5: Accuracy of linear probes on dev splits using the Llama2-7B residual stream representations extracted
from layer 7 above the last entity token. For most attribute, there exists a linear classifier with significant higher
accuracy than random baseline on the entity dev split. For all attributes, there exists a linear classifier with close to
perfect accuracy on the context dev split.

B.4  DBM-based and DAS-based Methods
For DBM- and DAS-based methods, we use the implementation from the pyvene library.14 For training
data, both methods are trained on base–source pairs with interchange interventions.
  For DBM and MDBM, we use a starting temperature of 1e−2 and gradually reducing it to 1e−7. The
feature dimension is controlled by the coefficient of the L1 loss. The optimal coefficient for the DBM
penalty is around 0.001, while no penalty generally works better for MDBM, as the multi-task objective
naturally encourages the methods to select as few dimensions as possible.
  For DAS and MDAS, we do not instantiate the full rotation matrix, but only parameterize the k
orthogonal vectors that form the feature FA. The interchange intervention is defined as

     II(M, FA, x, x′) = (I −W ⊤W)(GetVals(M(x), N)) + W ⊤W(GetVals(M(x′), N))

where the rows of W are the k orthogonal vectors. We experiment with k ∈{32, 128, 512, 2048} and
select the dimension with the highest Disentangle score on the dev set. For most attributes, a larger
intervention dimension, e.g., 512 or 2048, leads to a higher Disentangle score.

B.5  Computational Cost

All models are trained and evaluated on a single NVIDIA RTX A6000 GPU.
  For training, the computational cost of sparse autoencoders is the lowest, as training sparse autoencoders
does not involve backpropagating through the original Llama2-7B model or computing orthogonal
factorization of weight matrices. Each epoch of the sparse autoencoder training, i.e., iterating over
100k examples, takes about 100 seconds with Llama2-7B features extracted offline. The computational
cost of RLAP- and DAS-based method largely depends on the rank of the nullspace or the intervention
dimension, i.e., the number of orthogonal vectors. For RLAP, it takes 1 hour per 100 iterations with

  14https://github.com/stanfordnlp/pyvene


                                         16

   Method    Continent   Country   Language    Latitude   Longitude   Timezone   Iso Cause   Disentangle

   Entity
   PCA       32.7 45.2   36.3 58.6   34.2 33.3   32.7 44.2   39.3 35.4   36.0 36.6   35.2  42.2        38.7
   SAE       82.5 15.2   40.4 70.0   91.8  5.0    92.1 17.4   93.3 21.2   91.1 13.6   81.9  23.7        52.8
   RLAP     89.4 21.0   38.2 55.8   44.6 48.0   58.1 48.2   38.2 54.0   41.1 50.0   51.6  46.2        48.9
  DBM      65.9 70.0   44.8 70.6   42.9 54.3   45.1 59.8   44.9 57.0   72.2 54.0   52.6  61.0        56.8
   DAS       67.3 86.4   30.1 83.8   36.3 74.0   52.7 63.2   50.3 56.6   71.0 74.0   51.3  73.0        62.1
  MDBM    72.6 68.2   58.6 73.0   56.7 52.3   59.1 55.2   59.9 54.4   75.7 56.4   63.8  59.9        61.8
  MDAS     92.1 69.2   82.7 65.6   86.4 51.7   91.4 47.6   93.1 46.0   92.9 62.4   89.8  57.1        73.4
   Context
   PCA       27.9 46.1   31.4 52.5   29.2 19.0   26.8 40.0   27.5 53.0   28.8 47.5   28.6  43.0        35.8
   SAE       65.6 28.9   29.3 75.4   88.6  4.5    87.0 18.0   88.4 26.5   65.8 27.0   70.8  30.0        50.4
   RLAP     86.0 21.4   22.4 84.7   36.8 43.0   46.1 55.0   28.3 72.5   34.8 51.0   42.4  54.6        48.5
  DBM      58.7 58.6   37.9 66.0   36.4 36.0   38.3 61.4   38.9 69.0   67.4 53.5   46.3  57.4        51.8
   DAS       58.9 84.9   17.7 89.3   27.7 54.0   33.9 77.6   40.9 72.5   64.6 73.5   40.6  75.3        58.0
  MDBM    65.4 56.4   50.7 67.6   52.1 32.0   51.9 58.2   53.3 66.5   70.0 55.5   57.2  56.0        56.6
  MDAS     86.6 64.9   70.5 70.7   90.3 20.0   88.0 57.0   89.8 62.0   90.0 57.5   85.9  55.4        70.6

                                                      (a) Scores of city attributes.

     Method   Award Year   Birth Year   Country of Birth     Field     Gender    Iso Cause   Disentangle

     Entity
    PCA       24.2  22.7    30.8  2.3    22.4    70.0      24.3 78.3    4.3 81.0   21.2  50.9        36.0
    SAE       79.8  0.7     80.1  0.7    39.8    49.0      43.4 54.0   71.3 63.7   62.9  33.6        48.2
    RLAP     87.3  0.3     90.3  1.0    68.0     8.7       82.5 54.0   95.3 71.0   84.7  27.0        55.8
   DBM      91.8  0.7     98.6  0.3    61.5    32.0      71.3 57.7   92.6 71.7   83.2  32.5        57.8
    DAS       57.1  5.0     72.7  2.3    80.9    25.3      80.1 72.7   80.8 77.7   74.3  36.6        55.5
   MDBM    40.8  19.3    70.2  2.0    66.9    36.3      69.2 62.3   76.4 79.7   64.7  39.9        52.3
    MDAS     83.6  4.0     85.2  2.0    88.8    28.0      86.9 58.0   93.4 78.0   87.6  34.0        60.8
     Context
    PCA       19.2  25.4    22.6  3.3    18.4    73.2      23.6 76.0    3.0 67.0   17.4  49.0        33.2
    SAE       74.9  1.0     73.8  1.0    38.1    38.3      65.1 28.0   64.8 35.0   63.3  20.7        42.0
    RLAP     88.1  0.4     90.3  0.8    54.4    67.3      77.7 67.3   94.0 61.0   80.9  39.4        60.1
   DBM      88.1  0.2     96.9  0.0    50.6    50.2      56.1 59.3   96.8 61.7   77.7  34.3        56.0
    DAS       42.7  18.4    13.9  7.5    37.1    72.8      30.2 82.3   88.0 72.7   42.4  50.7        46.5
   MDBM    38.6  20.6    69.5  2.2    65.8    54.2      66.7 65.7   91.6 72.0   66.4  42.9        54.7
    MDAS     80.2  27.4    83.9 12.3   86.6    72.8      90.2 72.0   93.4 73.0   86.9  51.5        69.2

                                             (b) Scores of Nobel laureate attributes.

                                         Table 6: Per-task results.

a feature dimension 4096 and a target rank of 128. For DAS and MDAS with the reduced parameter
formulation, the training time for an intervention dimension of 128 (out of a feature dimension of 4096)
over 1k intervention examples is about 50 seconds. The computational cost of DBM-based method is
about 35 seconds per 1k intervention examples.
  For evaluation, the inference speed of our proposed framework is 20 seconds per 1k intervention
examples.


C  Results

For all methods, we conduct hyper-parameter search on the dev set. We report a single-run test set results
using the set of hyper-parameters that achieves the highest score on the dev set. For intervention site, we
choose layer 16 for city attributes and layer 7 for the rest attributes.


C.1  Breakdown of Benchmark Results

Table 6 shows the breakdown of benchmark results in Table 2. For each method, we report a breakdown
of the highest Disentangle score per attribute, i.e., the pair of Cause score and Iso score that add up
to the highest Disentangle score. The final score in Table 2 is an average of the Disentangle score
over all five entity types. For example, for PCA, the Disentangle score under the Entity setting is
(38.7 + 36.0 + 41.3 + 43.3 + 38.1)/5 = 39.5.


                                         17

 Method    Definition   Past Tense   Pronunciation    Singular   Iso Cause   Disentangle

 Entity
 PCA        4.9  59.5    4.6  95.3    2.1   66.5      4.2 93.3    4.0  78.6        41.3
 SAE       93.4  3.5    15.4 87.3   85.4    3.0      14.3 82.3   52.1  44.0        48.1
 RLAP     22.1 42.0   15.8 87.3   23.9   45.5     13.5 85.3   18.8  65.0        41.9
 DBM      22.0 51.0   16.3 88.7   10.2   58.0     14.2 87.0   15.7  71.2        43.4
 DAS       90.3 12.0   11.9 92.0   89.4   19.5     13.6 85.8   51.3  52.3        51.8
 MDBM    55.8 30.0   32.8 70.5   66.4   20.0     25.4 75.8   45.1  49.1        47.1
 MDAS     97.6  6.5    88.4  1.2    89.5   25.0     85.4 2.5    90.2  8.8         49.5
 Context
 PCA        9.6  57.0    8.3  84.3    4.3   44.0      9.2 78.3    7.9  65.9        36.9
 SAE       84.3 10.5   16.8 77.3   74.1    5.5      16.2 73.7   47.9  41.8        44.8
 RLAP     19.5 46.5   15.0 80.7   19.1   46.5     13.9 79.3   16.9  63.2        40.0
 DBM      21.7 53.0   16.3 84.3   12.3   52.5     14.7 81.0   16.3  67.7        42.0
 DAS       69.5 36.5    8.7  93.3   77.4   49.0      7.4 89.7   40.7  67.1        53.9
 MDBM    64.4 29.5   28.4 70.0   62.9   28.0     27.5 68.0   45.8  48.9        47.3
 MDAS     94.5 21.5   74.2 17.3   84.3   44.0     70.3 24.3   80.8  26.8        53.8

                                       (c) Scores of verb attributes.

   Method    Category     Color       Size      Texture    Iso Cause   Disentangle

   Entity
   PCA       45.6 49.8   35.1 63.7   27.7 50.5   26.3 47.5   33.7  52.9        43.3
   SAE       94.2 7.9    34.2 63.2   95.0 3.0    95.3 29.0   79.6  25.8        52.7
   RLAP     85.6 30.6   83.9 8.0    62.0 28.5   58.7 47.5   72.5  28.7        50.6
  DBM      70.1 35.6   62.0 40.0   98.0 2.0    97.7 30.0   81.9  26.9        54.4
   DAS       77.3 52.0   79.7 28.7   87.2 24.0   92.0 47.5   84.0  38.1        61.1
  MDBM    59.8 48.5   53.5 59.2   74.5 27.5   81.2 49.0   67.3  46.1        56.7
  MDAS     85.1 49.8   87.0 19.8   88.5 19.5   91.5 46.5   88.0  33.9        60.9
   Context
   PCA       43.1 66.8   40.3 63.3   30.8 46.5   25.4 68.0   34.9  61.1        48.0
   SAE       39.9 70.0   43.8 62.2   91.4 6.0    90.9 34.5   66.5  43.2        54.9
   RLAP     83.6 47.2   82.3 22.5   64.6 30.0   60.9 61.0   72.8  40.2        56.5
  DBM      72.1 47.2   64.6 46.0   97.3 2.5    97.5 32.5   82.9  32.1        57.5
   DAS       70.7 75.8   72.2 67.8   82.2 53.5   85.6 64.5   77.7  65.4        71.5
  MDBM    64.3 59.0   60.6 59.7   78.6 33.0   83.2 59.5   71.7  52.8        62.2
  MDAS     84.8 73.0   83.1 61.5   87.8 46.0   86.3 65.0   85.5  61.4        73.4

                               (d) Scores of physical object attributes.

Method     Duty     Gender Bias    Industry   Work Location   Iso Cause   Disentangle

Entity
PCA       39.9 33.7   28.1  61.7    36.3 38.0   35.9   31.0      35.1  41.1        38.1
SAE       68.9 4.0    57.1  49.0    61.7 10.5   64.3   13.0      63.0  19.1        41.1
RLAP     62.1 17.7   93.8  44.0    58.9 18.5   62.0   18.0      69.2  24.5        46.9
DBM      59.3 23.3   93.2  42.7    67.2 18.3   66.4   16.0      71.5  25.1        48.3
DAS       59.8 23.0   83.7  75.7    57.9 29.3   57.9   27.0      64.9  38.7        51.8
MDBM    52.0 35.3   81.7  66.0    57.8 29.5   59.3   24.5      62.7  38.8        50.8
MDAS     82.5 12.0   85.0  70.0    82.5 17.5   83.7   14.5      83.4  28.5        56.0
Context
PCA       39.2 45.0   21.9  68.0    33.8 42.7   38.3   44.5      33.3  50.0        41.7
SAE       66.7 7.7    47.7  61.0    58.9 14.3   65.1   14.5      59.6  24.4        42.0
RLAP     60.3 23.0   92.5  51.0    56.7 23.3   62.3   24.0      68.0  30.3        49.1
DBM      49.5 14.7   87.3  29.5    56.4 18.0   56.4   21.5      62.4  20.9        41.7
DAS       46.9 49.7   79.7  85.0    44.2 55.3   46.0   46.0      54.2  59.0        56.6
MDBM    43.6 22.7   77.7  70.5    54.2 31.3   60.9   27.0      59.1  37.9        48.5
MDAS     78.7 32.0   81.0  85.5    70.1 38.7   74.1   27.0      75.9  45.8        60.9

                                   (e) Scores of occupation attributes.

                        Table 6: Per-task results, continued.





                                18

             (a) Cause score from RLAP.                (b) Cause score from DBM.       (c) Cause score from MDBM.

          Figure 4: Additional feature disentanglement results for RLAP, DBM, and MDBM methods.


C.2  Additional Attribute Disentanglement Results

In Figure 3, we show the feature entanglement results from DAS and MDAS. We provide additional
results from all other supervised methods: RLAP, DBM, and MDBM in Figure 4. Though these methods
are trained on different objectives and identify different features FA, they show similar patterns in terms
of entanglement between attribute representations. For all methods, representations of most attributes
are entangled with “continent” (and “timezone”, which for most cases starts with the continent name).
Representations of attributes such as “county–language” are also highly entangled.





                                         19