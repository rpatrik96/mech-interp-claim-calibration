              Sparse Attention Post-Training for Mechanistic Interpretability



                     Florent Draye * 1 Anson Lei * 1 2 Hsiao-Ru Pan 1 Ingmar Posner 2 Bernhard Sch¨olkopf 1 3


                         Abstract
                                                                     Base Model          We introduce a simple post-training method that
             makes transformer attention sparse without sacri-
                 ficing performance. Applying a flexible sparsity
                regularisation under a constrained-loss objective,
           we show on models up to 7B parameters that it
                   is possible to retain the original pretraining loss2026
              while reducing attention connectivity to ≈0.4%
               of its edges. Unlike sparse-attention methods de-
               signed for computational efficiency, our approachFeb
               leverages sparsity as a structural prior:  it pre-
4
                serves capability while exposing a more organized
             and interpretable connectivity pattern. We find                                                                                                                                                                                                                                                                                                 Finetuning        Sparse Model
                  that this local sparsity cascades into global circuit                simplification: task-specific circuits involve far                                                                                                       Sparsity-Regularised
              fewer components (attention heads and MLPs)
              with up to 100× fewer edges connecting them.[cs.LG]
                Additionally, using cross-layer transcoders, we
            show that sparse attention substantially simpli-
                  fies attention attribution, enabling a unified view
               of feature-based and circuit-based perspectives.
             These results demonstrate that transformer atten-
                 tion can be made orders of magnitude sparser, sug-
                gesting that much of its computation is redundant
              and that sparsity may serve as a guiding principle         Figure 1. Visualised attention patterns for a 4-layer toy model
                 for more structured and interpretable models.              trained on a simple 2-digit addition task. The main idea of this
                                                               work is to induce sparse attention between tokens via a post-
                                                                                       training procedure that optimizes for attention sparsity while main-
                                                                                  taining model performance. In this example, while both models
          1. Introduction                                                                                are able to correctly predict the sum, the sparse model solves the
                                                                    problem with a naturally interpretable circuit. Details of this toy
          Scaling has driven major advances in artificial intelligence,
                                                                              setup and more examples are provided in Appendix A
          with ever-larger models trained on internet-scale datasetsarXiv:2512.05865v2    achieving remarkable capabilities across domains. Large
          language models (LLMs) now underpin applications from
            text generation to question answering, yet their increas-   components implement specific computations and behav-
          ing complexity renders their internal mechanisms largely     iors. Recent advances in this area have successfully identi-
         opaque (Bommasani, 2021). Methods of mechanistic in-    fied interpretable circuits, features, and algorithms within
            terpretability have been developed to address this gap by   LLMs (Nanda et al., 2023; Olsson et al., 2022), showing
           reverse-engineering neural networks to uncover how internal     that large complex models can, in part, be understood mech-
                                                                                 anistically, opening avenues for improving transparency,
             *Equal contribution  1Max Planck Institute for Intelligent
                                                                                        reliability, and alignment (Bereska & Gavves, 2024).          Systems (MPI-IS), T¨ubingen, Germany 2Applied Artificial In-
             telligence Lab,  University  of Oxford,  Oxford, UK 3ETH                                                             However, interpretability is bottlenecked by the model it-
             Z¨urich, Z¨urich, Switzerland. Correspondence to: Florent Draye
                                                                                        self: even with sophisticated reverse-engineering techniques          <fdraye@tuebingen.mpg.de>.
                                                                              that can faithfully reveal internal algorithms, the underly-
            Preprint. February 6, 2026.                                   ing computations implemented by large models can still

                                                         1

                            Sparse Attention Post-Training for Mechanistic Interpretability

remain highly complex and uninterpretable. Circuits for    be understood through explicit, tractable circuits. Taken
seemingly simple tasks may span hundreds of interacting     together, these results position attention sparsity as an effec-
attention heads and MLPs with densely intertwined contri-    tive and practical inductive tool for surfacing the minimal
butions across layers (Conmy et al., 2023), and features can    functional backbone underlying model behaviour.
influence each other along combinatorially many attention-
mediated paths, complicating attention attribution (Kamath                                                        2. Related Work
et al., 2025). To exemplify this, Figure 1 (top) illustrates
the attention patterns of a small, single-head transformer     2.1. Sparse Attention
trained on a simple two-digit addition task. Here, the model
                                              As self-attention is a key component of the ubiquitous Trans-
has learned to solve the task in a highly diffused manner,
                                                        former architecture, a large number of variants of attention
where information about each token is dispersed across all
                                                  mechanisms have been explored in the literature. Related
token locations, rendering the interpretation of the underly-
                                                                  to our approach are sparse attention methods, which are pri-
ing algorithm extremely difficult even in this simple case.
                                                           marily designed to alleviate the quadratic scaling of vanilla
The crux of the problem is that models are not incentivised to     self-attention. These methods typically rely on masks based
employ simple algorithms during training. In this work, we    on fixed local and strided patterns (Child et al., 2019) or
advocate for directly embedding interpretability constraints    sliding-window and global attention patterns (Beltagy et al.,
into model design in a way that induces simple circuits while    2020; Zaheer et al., 2020) to constrain the receptive field
preserving performance. We focus our analysis on atten-    of each token. While these approaches are successful in
tion mechanisms and investigate sparsity regularisation on    reducing the computational complexity of self-attention,
attention patterns, originally proposed in (Lei et al., 2025),    they require hand-defined heuristics that do not reflect the
as an inductive bias. To demonstrate how sparse attention     internal computations learned by the model.
patterns can give rise to interpretable circuits, we return to
                                                  Beyond these fixed-pattern sparse attention methods, Top-k
the two-digit addition example: Figure 1 (bottom) shows
                                                                    attention, which enforces sparsity by dynamically selecting
the attention patterns induced by penalising attention edges
                                                             the k most relevant keys per query based on their atten-
during training. Here, the sparsity inductive bias forces the
                                                                tion scores, has also been explored (Gupta et al., 2021;
model to solve the problem with much smaller, intrinsically
                                                     DeepSeek-AI, 2025). While Top-k attention enables learn-
interpretable computation circuits.
                                                          able sparse attention, the necessity to specify k limits its
In this work, we investigate using this sparsity regularisation    scope for interpretability for two reasons. First, selecting
scheme as a post-training strategy for pre-trained LLMs. We    the optimal k is difficult, and setting k too low can degrade
propose a practical method for fine-tuning existing models    model performance. Second, and more fundamentally, Top-
without re-running pretraining, offering a flexible way to    k attention does not allow the model to choose different
induce sparse attention patterns and enhance interpretabil-   k for different attention heads based on the context. We
ity. We show, on models of up to 7B parameters, that our    argue that this flexibility is crucial for maintaining model
proposed procedure preserves the performance of the base    performance.
models on pretraining data while reducing the effective at-
                                                More recently, gated attention mechanisms (Qiu et al., 2025)
tention map to less than 0.5% of its edges. To evaluate
                                                          provide a scalable and performant framework for inducing
our central hypothesis that sparse attention facilitates inter-
                                                            sparse attention. In particular, Lei et al. (2025) introduce
pretability, we consider two complementary settings. First,
                                                        a sparsity regularisation scheme for world modelling that
we study circuit discovery, where the objective is to identify
                                                              reveals sparse token dependencies. We adopt this method
the minimal set of components responsible for task perfor-
                                                     and examine its role as an inductive bias for interpretability.
mance (Conmy et al., 2023). We find that sparsified models
yield substantially simpler computational graphs: the re-
                                                                    2.2. Circuit Discoverysulting circuits explain model behaviour using up to four
times fewer attention heads and up to two orders of mag-    Mechanistic interpretability seeks to uncover how internal
nitude fewer edges. Second, using cross-layer transcoders    components of LLMs implement specific computations. Ab-
(Ameisen et al., 2025), we analyse attribution graphs, which     lation studies assess performance drops from removing com-
capture feature-level interactions across layers. In this set-    ponents (Nanda et al., 2023), activation patching measures
ting, sparse attention mitigates the attention attribution prob-    the effect of substituting activations (Zhang & Nanda, 2023),
lem by making it possible to identify which attention heads    and attribution patching scales this approach via local lin-
give rise to a given edge, owing to the reduced number of     earisation (Syed et al., 2024). Together, these approaches
components mediating each connection. We argue that this    allow researchers to isolate sub-circuits, minimal sets of
clarity enables a tighter integration of feature-based and     attention heads and MLPs that are causally responsible for a
circuit-based perspectives, allowing feature interactions to    given behavior or task (Conmy et al., 2023). Attention itself


                                                2

                            Sparse Attention Post-Training for Mechanistic Interpretability

plays a dual role: it both routes information and exposes          post-trained models do not lose prediction performance
interpretable relational structure, making it a key substrate        compared to their fully-connected counterparts.
for mechanistic study. Our work builds on this foundation
by leveraging sparsity to simplify these circuits, amplifying                                                  To this end, we leverage the Sparse Transformer architecture
the interpretability of attention-mediated computation while                                                                 in the SPARTAN framework proposed in (Lei et al., 2025),
preserving model performance.                                                     which uses sparsity-regularised hard attention instead of the
                                                           standard softmax attention. In the following subsections,
2.3. Attribution Graph                          we describe the Sparse Transformer architecture and the
                                                             optimisation setup, highlighting how this approach satisfiesMechanistic interpretability has gradually shifted from an
                                                              the above desiderata.emphasis on explicit circuit discovery towards the analysis
of internal representations and features. Recent work on
                                                                    3.1. Sparse Attention Layerattribution graphs and circuit tracing seeks to reunify these
perspectives by approximating MLP outputs as sparse linear                                                    Given a set of token embeddings, the Sparse Transformer
combinations of features and computing causal effects along                                                              layer computes the key, query, and value embeddings,
linear paths between them (Dunefsky et al., 2024; Ameisen                                                              {ki, qi, vi}, via linear projections, analogous to the standard
et al., 2025; Lindsey et al., 2025b). This framework enables                                                            Transformer. Based on the embeddings, we sample a binary
the construction of feature-level circuits spanning the com-                                                            gating matrix from a learnable distribution parameterised
putation from input embeddings to final token predictions.                                                   by the keys and queries,
Within attribution graphs, edges correspond to direct lin-
ear causal relationships between features. However, these                                                                       Aij ∼Bern(σ(qTi kj)),               (1)
relationships are mediated by attention heads that transmit
information across token positions. Identifying which atten-   where Bern(·) is the Bernoulli distribution and σ(·) is the
tion heads give rise to a particular edge, and understanding                                                                    logistic sigmoid function. This sampling step can be made
why they do so, is essential, as this mechanism forms a fun-    differentiable via the Gumbel Softmax trick (Jang et al.,
damental component of the computational graph (Kamath                                                          2017). This binary matrix acts as a mask that controls the
et al., 2025). A key limitation of current attribution-based                                                           information flow across tokens. Next, the message passing
approaches is that individual causal edges are modulated by    step is carried out in the same way as standard softmax
dozens of attention components. We show that this leads to                                                                  attention, with the exception that we mask out the value
feature-to-feature influences that are overly complex, render-                                                    embeddings using the sampled binary mask,
ing explanations in terms of other features in the graph both
computationally expensive and conceptually challenging.
                                                     SparseAttn(Q, K, V ) = A ⊙softmax(QKT ) V,  (2)                                                                         √dk
3. Method
                                                   where dk is the dimension of the key embeddings and ⊙
Our main hypothesis is that post-training existing LLMs to    denotes element-wise multiplication. During training, we
encourage sparse attention patterns leads to the emergence     regularise the expected number of edges between tokens
of more interpretable circuits. In order to instantiate this    based on the distribution over the gating matrix. Concretely,
idea, we require a post-training pipeline that satisfies three    the expected number of edges for each layer can be calcu-
main desiderata:                                                lated as
                                                 E |A| = X σ(qTi kj).               (3)
  1. To induce sparse message passing between tokens,                                              i,j
   we need an attention mechanism that can ‘zero-out’
                                                   Note that during the forward pass, each entry of A is a     attention edges, which in turn enables effective L0-
                                                      hard binary sample that zeros out attention edges, which     regularisation on the attention weights.  This is in
                                                            serves as an effective L0 regularisation. Moreover, since     contrast to the standard softmax attention mechanism,
                                                             the functional form of the sparse attention layer after the    where naive regularisation would result in small but
                                                        hard sampling step is the same as standard softmax atten-     non-zero attention weights that still allow information
                                                                        tion, pre-trained model weights can be directly used without     flow between tokens.
                                                                     alterations.1
  2. The model architecture needs to be compatible with the                                                                           1Technically, the sampled A affects the computation. This can
     original LLM such that the pre-trained LLM weights    be mitigated by adding a positive bias term inside the sigmoid func-
    can be directly loaded at initialisation.                      tion to ensure all gates are open at initialisation. Experimentally,
                                               we found this to be unnecessary as the models quickly recover
  3. The post-training procedure needs to ensure that the      their original performance within a small number of gradient steps.

                                                3

                            Sparse Attention Post-Training for Mechanistic Interpretability

3.2. Constrained Optimisation                                                              1.0                          Benchmark Comparison
                                                                                                                                                                          OLMo-7B
In order to ensure that the models do not lose prediction per-                                                                                          Sparse OLMo-7B
                                                                                                                          0.8
formance during the post-training procedure, as per desider-
atum 3, we follow the approach proposed in (Lei et al.,            0.6
2025), which employs the GECO algorithm (Rezende &                       Accuracy
Viola, 2018). Originally developed in the context of regular-            0.4
ising VAEs, the GECO algorithm places a constraint on the
performance of the model and uses a Lagrangian multiplier             0.2
to automatically find the right strength of regularisation dur-
                                                                                                                          0.0
ing    training.             Concretely,                 we formulate
as the      following                optimisation                           problem,                                      the learning process                       TruthfulQA              PIQA            OpenBookQA              ARC-Easy
        min X E |Al|       s.t. CE ≤τ,         (4)             θ                                                 Figure 2. Comparison of model performance between the base
                           l                             OLMo model and the sparsified model evaluated on the various
where Al denotes the gating matrix at layer l, CE is the stan-    benchmarks. Across all tasks, the performance of the sparse model
                                                             remains comparable with the base model despite using substan-dard next token prediction cross-entropy loss, and τ is the
                                                                                  tially fewer attention edges.
required target loss, and θ is the model parameters. In prac-
tice, we set this target as the loss of the pre-trained baseline
models. We solve this optimisation problem via Lagrangian                                            we verify on a 7B parameter model that LoRA finetuning is
relaxation, yielding the following max-min objective,                                                                     sufficiently expressive for inducing sparse attention patterns.

      max min X E |Al| + λ(CE −τ)  .       (5)
        λ>0   θ                                        FlashAttention (Dao, 2023)  FlashAttention has become                                   l
                                                          a standard method for reducing the memory footprint of dot-
This can be solved by taking gradient steps on θ and λ alter-                                                          product attention mechanisms. In Appendix B, we discuss
nately. During training, updating λ automatically balances                                             how the sampled sparse attention can be implemented in an
the strength of the sparsity regularisation: when CE is lower                                                        analogous manner.
than the threshold, λ decreases, and hence more weight is
given to the sparsity regularisation term. This effectively
                                                               Distillation (Gu et al., 2024).  Empirically, we find thatacts as an adaptive schedule which continues to increase the
                                                       adding an auxiliary distillation loss based on the KL di-strength of the regularisation until the model performance
                                                       vergence between the base model and the sparse modeldegrades. Here, the value of τ is selected as a hyperparame-
                                                        improves training stability and ensures that the behaviour ofter to ensure that the sparse model’s performance remains
                                                              the model remains unchanged during post-training.within a certain tolerance of the original base model. In prac-
tice, the choice of τ controls a trade off between sparsity
and performance: picking a tight τ can lead to a slower train-    4. Experiments
ing process, whereas a higher tolerance can substantially
                                                To evaluate the effectiveness of our post-training pipeline,speed up training at the cost of potentially harming model
                                           we finetune pre-trained LLMs and compare their predic-performance. In Appendix C, we provide further discussion
                                                                tion performance and interpretability before and after ap-on this optimisation process and its training dynamics.
                                                          plying sparsity regularisation. We perform full finetuning
                                                  on a GPT-2 base model (Radford et al., 2019)(124M pa-
3.3. Practical Considerations
                                                           rameters) on the OpenWebText dataset (Gokaslan & Co-
One of the main strengths of our proposed method is that,    hen, 2019). To investigate the generality and scalability
architecturally, the only difference between a sparse Trans-    of our method, we perform LoRA finetuning on the larger
former and a normal one lies in how the dot-product atten-   OLMo-7B model (Groeneveld et al., 2024) on the Dolma
tion is computed. As such, most practical training tech-    dataset (Soldaini et al., 2024), which is the dataset on which
niques for optimising Transformers can be readily adapted    the base model was trained. The GPT-2 model and the
to our setting. In our experiments, we find the following   OLMo model are trained on sequences of length 64 and
techniques helpful for improving computational efficiency    512, respectively.  In the following subsections, we first
and training stability.                                         present a quantitative evaluation of model performance and
                                                                sparsity after sparse post-training. We then conduct two
LoRA finetuning (Hu et al., 2022).  Low rank finetuning     interpretability studies, using activation patching and attri-
techniques can significantly reduce the computational re-    bution graphs, to demonstrate that our method enables the
quirements for training large models. In our experiments,    discovery of substantially smaller circuits.

                                                4

                            Sparse Attention Post-Training for Mechanistic Interpretability

  Model   Base CE    τ     Final CE   Active Attn

  GPT-2     3.48     3.50    3.501      0.22%
  OLMo     2.24     2.29    2.287      0.44%

Table 1. Performance and sparsity of post-trained models. Final
cross-entropy losses closely match the specified targets, while
attention sparsity is substantially increased.



4.1. Model Performance and Sparsity

We begin by evaluating both performance retention and the
degree of sparsity achieved by post-training. We set cross-
entropy targets of 3.50 for GPT-2 (base model: 3.48) and
2.29 for OLMo (base model: 2.24). After training, the mean
cross-entropy loss for both models remains within ±0.01 of
the target, indicating that the dual optimisation scheme effec-
tively enforces a tight performance constraint. To quantify
the sparsity achieved by the models, we evaluate them on
the validation split of their respective datasets and compute
the mean number of non-zero attention edges per attention
                                                               Figure 3. Attention patterns of the heads required to explain 90%
head. We find that the sparsified GPT-2 model activates, on     of model behaviour on a copy task. The sparse model requires
average, only 0.22% of its attention edges, while the sparsi-     substantially fewer attention heads. Moreover, the selected heads
fied OLMo model activates 0.44%, indicating substantial     exhibit the characteristic ‘induction head’ pattern: each token
sparsification in both cases. Table 1 provides a summary of     attends to a previous token at a fixed relative offset, effectively
                                                              copying information forward through the sequence, a pattern well
the results. To further verify that this drastic reduction in
                                                    known to implement the copy mechanism in transformer models.
message passing between tokens does not substantially alter     Equivalent plots for OLMo can be found in Appendix D.
model behaviour, we evaluate the sparsified OLMo model
on a subset of the benchmarks used to assess the original
model. As shown in Figure 2, the sparse model largely re-    Since searching over every possible subset of model compo-
tains the performance of the base model across a diverse set    nents is infeasible due to the exponential number of potential
of tasks. In sum, our results demonstrate that sparse post-    subsets, we adopt a common heuristic to rank each model
training is effective in consolidating information flow into a    component. Specifically, for each individual component, we
small number of edges while maintaining a commensurate    compute an importance score by replacing the activations of
level of performance.                                          the component with the corrupted activations and measuring
                                                                                  its effect on the logit difference. In our experiments, we use
4.2. Circuit Discovery with Activation Patching              this ranking to select the top-k components and intervene
                                                  on the model by freezing all remaining components, with
We begin by outlining the experimental procedure used for
                                                             the goal of identifying the minimal set that accounts for at
circuit discovery. Activation patching (Nanda et al., 2023)
                                                                    least 90% of the model’s preference for the correct predic-
is a widely used technique for identifying task-specific cir-
                                                                     tion. Note that these importance scores can be computed
cuits in transformer models. In a typical setup, the model
                                                                       at two levels: (i) a single-sentence level, using a single pair
is evaluated on pairs of prompts: a clean prompt, for which
                                                            of correct and corrupted inputs, and (ii) a global level, ob-
the model predicts a correct target token, and a corrupted
                                                             tained by averaging scores across many task variants. In
prompt that shares the overall structure of the clean prompt
                                                          our experiments, we report the results using single-sentence
but is modified to induce an incorrect prediction. Here, the
                                                               scores. In Appendix D, we also provide results using the
goal is to find the set of model components that is responsi-
                                                          global scores, which are largely consistent with our main
ble for the model’s preference for the correct answer over
                                                                        results. There are also two standard approaches for freezing
the wrong one, as measured by the logit difference between
                                                  component activations: setting the activation to zero or re-
the corresponding tokens. In activation patching, individual
                                                             placing it with a mean activation value (Conmy et al., 2023).
model components, such as attention heads and individual
                                       We evaluate both variants for each model and report results
edges, can be ’switched-off’ by patching activation at the
                                                                 for the patching strategy that yields the smallest circuits.
specific positions. Circuit discovery amounts to finding a
set of components whose replacement causes the model’s   We first focus on the copy task with the following prompt:
prediction to shift from the correct to the corrupted answer.  "AJEFCKLMOPQRSTVWZS, AJEFCKLMOPQRSTVWZ",

                                                5

                            Sparse Attention Post-Training for Mechanistic Interpretability

     Greater Than                      IOI                             Docstring                          IOI Long
    1.0        4.5x                             1.0          2.2x                          1.0        2.2x                              1.0            1.4x

 Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect
  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5


                                  GPT-2                                                                                 OLMo-7B
    0.0                            Sparse GPT-2        0.0                                                   0.0                         Sparse OLMo-7B        0.0
                   50          100                             50          100                         250      500      750     1000               250      500      750     1000
               Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept

Figure 4. Logit attribution keeping only the top-k attention heads. Dotted line annotates the number of attention heads needed to explain
90% of the logit difference. Sparse models yields 1.4× to 4.5× smaller circuits. Shaded areas show standard error across 20 prompts.

     Greater Than                      IOI                             Docstring                          IOI Long
    1.0             97.0x                   1.0            42.8x                    1.0                  8.6x             1.0                  5.4x

 Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect
  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5


                                    GPT-2                                                                                 OLMo-7B
    0.0                              Sparse GPT-2      0.0                                                   0.0                           Sparse OLMo-7B      0.0
      100       101       102       103       104        100       101       102       103                100    101    102    103    104    105          100    101    102    103    104    105
                Number of Edges Kept                          Number of Edges Kept                          Number of Edges Kept                          Number of Edges Kept

Figure 5. Logit attribution per sentence keeping only the top-k attention edges. Sparse models yields 5.4× to 97× smaller circuits.
Shaded area shows standard error across 20 prompts.


where the model has to copy the letter S to the next token     string task where the model needs to predict an argument
position. This task is well studied and is widely believed to   name in a Docstring based on an implemented function.
be implemented by emergent induction heads (Elhage et al.,    Details of each task can be found in Appendix E. Figure 4
2021), which propagate token information forward in the    and 5 show the fraction of model behaviour explained as
sequence. Figure 3 illustrates the attention patterns of the    a function of the number of retained model components
set of attention heads that explains this prompt for the sparse     (attention heads and attention edges, respectively). Across
and base GPT-2 models. See Appendix D for analogous      all tasks and models, the sparse models consistently produce
results for the OLMo models. The sparse model admits     significantly smaller circuits, as measured by the number of
a substantially smaller set of attention heads (9 heads)    model components needed to explain 90% of model predic-
than its fully connected counterpart (61 heads). Moreover,     tion. This further corroborates our claim that sparse models
the identified heads in the sparse model exhibit cleaner    lead to simpler and more interpretable internal circuits.
induction head patterns, with each token attending to a
single prior position at a fixed relative offset. These results     4.3. Attribution-graph
illustrate how sparsification facilitates interpretability under
                                                          Next, we present a more fine-grained, feature-level investi-simple ranking-based methods and support our hypothesis
                                                              gation of whether sparsity in attention leads to interpretablethat sparse post-training yields models that are more
                                                                    circuits in practice using cross-layer transcoders (CLTs).amenable to mechanistic interpretability techniques.
                                                        Since training CLTs on OLMo-7B is computationally pro-
To further verify our hypothesis, we repeat the experiment     hibitive2, we focus our analysis on the GPT-2 models. For
on classical circuit discovery tasks. For GPT-2, we evaluate     the rest of the section, we perform analysis on CLTs trained
variants of the Indirect Object Identification (IOI) task, in    on the sparse and base GPT-2 models, trained with an ex-
which the model copies a person’s name from the start of    pansion factor of 32 and achieve above 80% replacement
a sentence, and the Greater Than task, in which the model    score measured with Circuit Tracer (Hanna et al., 2025). See
predicts a number that is larger than a previously mentioned    Appendix F and G for details on training and visualisation.
number. To further assess the scalability of our approach,
                                                              2The largest open-source CLT is on Gemma-2B at the time ofwe investigate more challenging and longer horizon tasks
                                                                        writing.
for OLMo, including a longer context IOI task and a Doc-


                                                6

                            Sparse Attention Post-Training for Mechanistic Interpretability

We study the problem of attention attribution, which seeks          Edges                      Heads
to understand how edges between features are mediated.            1.00                                               1.00
                                                                               16.1x                 3.4x
The key challenge here is that any given edge can be af-
                                                                                                                                                    Mass                                                               Mass
fected by a large number of model components, making            0.75                                               0.75
mediation circuits difficult to analyse both computationally                                                                                                                                                                                                                                                                                                                                                                                  Cumulative                                                                                                                                                             Cumulative
and conceptually: computationally, exhaustive enumeration
                                                                                                                                                    Mean 0.50                                                         Mean 0.50
is costly; conceptually, the resulting circuits are often large
and uninterpretable. In this experiment, we demonstrate                                                                                  NonSparseSparse
that sparse attention patterns induced via post-training sub-               100        101        102        103                   25    50    75    100   125
                                                                                                                                     Sorted Index (log scale)                                   Sorted Index
stantially alleviate these challenges, as the vast majority of
attention components have zero effect on the computation.     Figure 6. Mean cumulative distribution of the component scores
                                                                             that mediate an attribution graph edge. The components are on the
As in (Ameisen et al., 2025), we define the total attribution                                                                                   left key-query pairs within a head, and on the right full attention
score between feature n at layer ℓand position k, and feature     heads.
n′ at layer ℓ′ and position k′ as
                  aℓ′,k′,n′ℓ,k,n = fk,nℓ   Jℓ′,k′ℓ,k   gℓ′k′,n′.             (6)    edge in the attribution graph. We find that, to reach a cu-
          ℓ                                                  mulative attribution threshold of 90%, the sparse model on
Here, fk,n denotes the decoder vector corresponding to                                                         average requires 16.1× fewer key–query pairs and 3.4×
feature n at layer ℓand position k, and gℓ′k′,n′ is the cor-    fewer attention heads when compared to the dense GPT-
responding encoder vector for feature n′ at layer ℓ′ and    2 model, supporting our hypothesis that sparse attention
position k′. The term Jℓ′,k′ℓ,k   is the Jacobian from the MLP     patterns leads to simpler mediation circuits.
output at (ℓ, k) to the MLP input at (ℓ′, k′). This Jacobian
                                                         Next, we present a qualitative case-study to showcase theis computed during a forward pass in which all nonlinear-
                                                                 benefits of sparse attention patterns. For a given key–queryities are frozen using stop-gradient operations. Under this
                                                                      pair, we compute the causal effect from all other featureslinearisation, the attribution score represents the sum over
                                                                in the attribution graph to both the key and the query vec-all linear paths from the source feature to the target feature.
                                                                       tors. Figure 7 illustrates this analysis for the prompt “The
To analyse how this total effect between two features is me-    opposite of ‘large’ is”. The resulting attribution graph de-
diated by each model component, we define the component-   composes into four coherent clusters of features: features
specific attribution by subtracting the contribution of all     related to opposite, features related to large, features acti-
paths that do not pass through the component:                vating on bracketed tokens, and the final next-token logit
                                                        corresponding to small (see Appendix H for example of
  aℓ′,k′,n′ℓ,k,n  (h) = fk,nℓ   Jℓ′,k′ℓ,k   gℓ′k′,n′ −fk,nℓ    Jℓ′,k′ℓ,k  h gℓ′k′,n′.      features and visualization).

                                                                                       large                                                           Here,                                                                     the                                                                           features                                                                                     in                                                                                    the                                                                                                    cluster are                                                                                                               directly                                                                                                      connected
              denotes a modified Jacobian computed un-Here,  Jℓ′,k′ℓ,k  h                                                               small                                                                to the                                                                                      logit.                                                              The                                                                        key                                                                                     question                                                                                                                      is then                                                                                                             to understand
der the same linearization as above, but with the specific
                                             how this connection from the large to the small logit comes
attention component h additionaly frozen via stop-gradient.
                                                               about. To this end, we analyse their mediation structure. We
As such, these component-specific scores quantifies how
                                                                  find that 80% of the cumulative attribution score of the edges
much each model component impacts a particular edge be-
                                                         connecting the large cluster to the small logit is mediated
tween features.
                                                   by the same five late layer attention key–query pairs. These
Empicially, we evaluate the method on ten pruned attribution     attention components map features from token position 5
graphs, computed on the IOI, greater-than, completion, and     directly into the final-layer residual stream at position 8, and
category tasks. Similar to our previous circuit discovery    thus operate in parallel.
experiment, we compute attribution scores on the level of
                                                        For these five key–query pairs, we then compute the causal
attention heads as well as individual key–query pairs. In
                                                              influence of all other features in the graph on their key and
practice, attention sparsity yields substantial computational
                                                       query vectors. The query vectors are primarily modulated
savings: because inactive key–query pairs are known a
                                                  by features associated with bracketed tokens in the last to-
priori to have exactly zero attribution score, attribution need
                                                    ken position, while the key vectors are driven by strongly
only be computed for a small subset of components. This
                                                               active features in both the opposite and large clusters, as
reduces the computation time per attribution graph from
                                                shown in Figure 8.These results are in agreement with the
several hours to several minutes.
                                                             recent work on attention attribution and the ”opposite of”
In terms of circuit size, Figure 6 shows the mean cumula-    attribution graph (Kamath et al., 2025). In stark contrast,
tive distribution of component attribution scores for each    Figure 7 (left) shows that a similar (and more computa-

                                                7

                            Sparse Attention Post-Training for Mechanistic Interpretability

                                                                                                           All heads map key pos 5 to query pos 8             Modulated at 80% by                                                            K

                                                                                                    small
                         Q                                                                           layer 12
                                                         L11-H7     L10-H1      L9-H7      L9-H1      L8-H6
                                                                     Sparse GPT-2

                                                                                                     brackets                                                     opposite                    large                                                                                                                                      layer 0-10                                                                         layer 0-1                        layer 0-3

                                      The     opposite     of   “      large     ”    is       ”
                     GPT-2                        1          2          3    4        5        6    7        8

Figure 7. Sketch of the attribution graph for the sentence “The opposite of ‘large’ is”. The cluster of features associated with large at
token position 5 maps directly to the final next-token prediction logit small. We show the attention patterns of all key–query pairs required
to account for 80% of the cumulative attribution score. In the sparse-attention setting, this corresponds to five attention heads, compared
to more than forty heads in the dense-attention case. In the sparse model, these heads read from token position 5 and write directly to the
last token residual stream at token position 8. These heads thus compute in parallel and provide a clear picture of the internal computation.


       →Query             →Key                                                       Across a range of tasks and analyses, we show that sparsity
1. large (pos 5)         1. bracket (pos 8)    improves interpretability at the circuit level by reducing
2. large (pos 5)         2. bracket (pos 8)     the number of components involved in specific behaviours.
3. quantities (pos 5)    3. bracket (pos 8)                                                            In circuit discovery experiments, most of the model’s be-
4. comparison (pos 3)    4. bracket (pos 8)
                                                           haviour can be explained by circuits that are orders of mag-5. opposite (pos 3)      5. bracket (pos 8)
                                                            nitude smaller than in dense models; in attribution graph
                                                                analyses, the reduced number of mediating components ren-
Figure 8.  Minimal description of the top5 features activating
                                                            ders attention attribution tractable. Together, these resultsthe query and the key vectors for the attention head L8-H6 from
Figure 7.                                                      position sparse post-training of attention as a practical and
                                                                   effective tool for enhancing the mechanistic interpretability
                                                             of pre-trained models.

tionally expensive) analysis on the dense model produces a
                                                       Limitations and Future Work.  One limitation of the
much more complicated circuit. This case study illustrates
                                                              present investigation is that, while we deliberately focus on
the potential of sparse attention in the context of attribution
                                                                sparsity as a post-training intervention, it remains an open
graphs, as it enables a unified view of features and circuits.
                                                           question whether injecting a sparsity bias directly during
By jointly analyzing feature activations, attention compo-
                                                                training would yield qualitatively different or simpler cir-
nents, and their mediating roles, we obtain a more faithful
                                                                  cuit structures. Also, a comprehensive exploration of the
picture of the computational graph underlying the model’s
                                                       performance trade-offs for larger models and for tasks that
input–output behavior.
                                                             require very dense or long-range attention patterns would
                                                     be beneficial, even if beyond the computational means cur-
5. Conclusion                                              rently at our disposal. Moreover, our study is primarily
                                                                   restricted to sparsifying attention patterns, the underlying
Achieving interpretability requires innovations in both inter-
                                                               principle of leveraging sparsity to promote interpretability
pretation techniques and model design. We investigate how
                                                               naturally extends to other components of the transformer
large models can be trained to be intrinsically interpretable.
                                                                    architecture. As such, combining the proposed method with
We present a flexible post-training procedure that sparsifies
                                                    complementary approaches for training intrinsically inter-
transformer attention while preserving the original pretrain-
                                                               pretable models, such as Sparse Mixture-of-Experts (Yang
ing loss. By minimally adapting the architecture, we apply a
                                                                     et al., 2025), sparsifying model weights (Gao et al., 2024),
sparsity penalty under a constrained-loss objective, allowing
                                                             or limiting superposition () offers a promising direction for
pre-trained model to reorganise its connectivity into a much
                                                              future work. Another exciting avenue for future work is
more selective and structured pattern.
                                                                to apply the sparsity regularisation framework developed
Mechanistically, this induced sparsity gives rise to substan-    here within alternative post-training paradigms, such as rein-
tially simpler circuits: task-relevant computation concen-    forcement learning (Ouyang et al., 2022; Zhou et al., 2024)
trates into a small number of attention heads and edges.    or supervised fine-tuning (Pareja et al., 2025).

                                                8

                            Sparse Attention Post-Training for Mechanistic Interpretability

Impact Statement                                            ral Information Processing Systems, 36:16318–16352,
                                                           2023.
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal    Dao,  T.   Flashattention-2:  Faster attention with bet-
consequences of our work, none which we feel must be        ter parallelism and work partitioning.  arXiv preprint
specifically highlighted here.                                 arXiv:2307.08691, 2023.

                                                      DeepSeek-AI. Deepseek-v3.2: Pushing the frontier of open
Acknowledgment                                           large language models, 2025. URL https://arxiv.
                                             org/abs/2512.02556.F. D. acknowledges support through a fellowship from the
Hector Fellow Academy. A. L. is supported by an EPSRC                                                        Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find
Programme Grant (EP/V000748/1). I. P. holds concurrent                                                                   interpretable LLM feature circuits. Advances in Neural
appointments as a Professor of Applied AI at the University                                                             Information Processing Systems, 37:24375–24410, 2024.
of Oxford and as an Amazon Scholar. This paper describes
work performed at the University of Oxford and is not asso-    Elhage,  N.,  Nanda,  N.,   et   al.   A  mathemati-
ciated with Amazon.                                             cal  framework  for  transformer  circuits.     Trans-
                                                         former Circuits Thread, 2021.    https://transformer-
                                                               circuits.pub/2021/framework/index.html.
References
                                                    Gao, L., Rajaram, A., Coxon, J., Govande, S. V., Baker,Ameisen, E., Lindsey, J., Pearce, A., Gurnee, W., Turner,
                                                                  B., and Mossing, D.   Weight-sparse transformers  N. L., Chen, B., Citro, C., Abrahams, D., Carter, S., Hos-
                                                        have interpretable circuits.  Technical report, OpenAI,  mer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T.,
                                                         2024.  URL https://cdn.openai.com/pdf/  McDougall, C., Cunningham, H., Henighan, T., Jermyn,
                                           41df8f28-d4ef-43e9-aed2-823f9393e470/  A., Jones, A., Persic, A., Qi, Z., Ben Thompson, T., Zim-
                                             circuit-sparsity-paper.pdf.  merman, S., Rivoire, K., Conerly, T., Olah, C., and Bat-
  son, J. Circuit tracing: Revealing computational graphs                                                        Gokaslan,  A.  and  Cohen,  V.    Openwebtext  cor-
   in language models. Transformer Circuits Thread, 2025.                                                               pus.       http://Skylion007.github.io/
  URL https://transformer-circuits.pub/                                             OpenWebTextCorpus, 2019.
  2025/attribution-graphs/methods.html.
                                                        Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,
Beltagy,   I.,  Peters, M.  E., and Cohan, A.   Long-       R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,
  former: The long-document transformer. arXiv preprint     Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,
  arXiv:2004.05150, 2020.                                     K., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel,
                                                                                       J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N.,
Bereska, L. and Gavves, E. Mechanistic interpretability                                                           Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichan-
   for ai safety–a review. arXiv preprint arXiv:2404.14082,                                                                     der, A., Schwenk, D., Shah, S., Smith, W., Subramani,
  2024.                                                               N., Wortsman, M., Dasigi, P., Lambert, N., Richardson,
                                                                 K., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Ha-
Bommasani, R. e. a. On the opportunities and risks of
                                                                             jishirzi, H. Olmo: Accelerating the science of language
  foundation models. ArXiv, 2021. URL https://crfm.
                                                           models. Preprint, 2024.
  stanford.edu/assets/report.pdf.
                                                     Gu, Y., Dong, L., Wei, F., and Huang, M. MiniLLM: Knowl-
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-                                                         edge distillation of large language models. In The Twelfth
   erating long sequences with sparse transformers. arXiv                                                                International Conference on Learning Representations,
   preprint arXiv:1904.10509, 2019.                                                         2024. URL https://openreview.net/forum?
                                              id=5h0qf7IBZZ.
Conerly,    T.,   Cunningham,   H.,   Templeton,   A.,
  Lindsey,   J.,  Hosmer,  B.,  and Jermyn,  A.    Cir-    Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J.
   cuits  updates  –  january  2025,  2025.    URL      Memory-efficient transformers via top-k attention. arXiv
  https://transformer-circuits.pub/             preprint arXiv:2106.06899, 2021.
  2025/january-update/index.html.    Trans-
  former Circuits Thread.                              Hanna, M., Piotrowski, M., Lindsey,  J., and Ameisen,
                                                            E.       circuit-tracer.    https://github.com/
Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S.,     safety-research/circuit-tracer,    2025.
  and Garriga-Alonso, A. Towards automated circuit dis-     The first two authors contributed equally and are listed
  covery for mechanistic interpretability. Advances in Neu-       alphabetically.

                                                9

                            Sparse Attention Post-Training for Mechanistic Interpretability

Heimersheim, S. and Janiak, J. A circuit for python doc-      with human feedback. Advances in neural information
   strings in a 4-layer attention-only transformer. In Align-      processing systems, 35:27730–27744, 2022.
  ment Forum, 2023.
                                                               Pareja, A., Nayak, N. S., Wang, H., Killamsetty, K., Su-
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,       dalairaj, S., Zhao, W., Han, S., Bhandwaldar, A., Xu, G.,
   S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation      Xu, K., Han, L., Inglis, L., and Srivastava, A. Unveil-
  of large language models. ICLR, 1(2):3, 2022.                ing the secret recipe: A guide for supervised fine-tuning
                                                           small LLMs.  In The Thirteenth International Confer-
Jang, E., Gu, S., and Poole, B. Categorical reparameter-                                                         ence on Learning Representations, 2025. URL https:
   ization with gumbel-softmax. In International Confer-                                            //openreview.net/forum?id=eENHKMTOfW.
  ence on Learning Representations, 2017. URL https:
  //openreview.net/forum?id=rkE3y85ee.       Qiu, Z., Wang, Z., Zheng, B., Huang, Z., Wen, K., Yang,
                                                                         S., Men, R., Yu, L., Huang, F., Huang, S., Liu, D.,
Kamath, H., Ameisen, E., Kauvar, I., Luger, R., Gurnee, W.,                                                       Zhou,  J., and Lin,  J.  Gated attention for large lan-
  Pearce, A., Zimmerman, S., Batson, J., Conerly, T., Olah,                                                        guage models: Non-linearity, sparsity, and attention-sink-
   C., and Lindsey, J. Tracing attention computation through                                                                       free. In The Thirty-ninth Annual Conference on Neural
  feature  interactions.   Transformer Circuits Thread,                                                           Information Processing Systems, 2025. URL https:
  2025. URL https://transformer-circuits.                                            //openreview.net/forum?id=1b7whO4SfY.
  pub/2025/attention-qk/index.html.
                                                        Radford, A., Wu,  J., Child, R., Luan, D., Amodei, D.,
Lei, A., Sch¨olkopf, B., and Posner,  I.  SPARTAN: A                                                               Sutskever, I., et al. Language models are unsupervised
  sparse transformer world model attending to what mat-                                                                multitask learners. OpenAI blog, 1(8):9, 2019.
   ters. In The Thirty-ninth Annual Conference on Neural
  Information Processing Systems, 2025. URL https:   Rezende, D. J. and Viola, F. Taming vaes, 2018. URL
  //openreview.net/forum?id=uS5ch7GjZ4.      https://arxiv.org/abs/1810.00597.

Lindsey,  J., Ameisen, E., Nanda, N., Shabalin, S., Pi-    Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson,
  otrowski, M., McGrath, T., Hanna, M., Lewis, O., Tigges,       D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar,
   C., Merullo, J., Watts, C., Paulo, G., Batson, J., Gorton,       Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X.,
   L., Simon, E., Loeffler, M., McDougall, C., and Lin, J.      Lambert, N., Magnusson, I., Morrison, J., Muennighoff,
  The circuits research landscape: Results and perspectives.      N., Naik, A., Nam, C., Peters, M. E., Ravichander, A.,
  Neuronpedia, 2025a. URL https://neuronpedia.     Richardson, K., Shen, Z., Strubell, E., Subramani, N.,
  org/graph/info.                                         Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N. A.,
                                                                    Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J.,
Lindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce,                                                      and Lo, K. Dolma: an Open Corpus of Three Trillion
  A., Turner, N. L., Citro, C., Abrahams, D., Carter,                                                        Tokens for Language Model Pretraining Research. arXiv
   S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A.,                                                                    preprint, 2024.
  Bricken, T., McDougall, C., Cunningham, H., Henighan,
   T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thomp-    Syed, A., Rager, C., and Conmy, A. Attribution patching
  son, T. B., Zimmerman, S., Rivoire, K., Conerly, T.,      outperforms automated circuit discovery. In Proceedings
  Olah, C., and Batson, J. On the biology of a large       of the 7th BlackboxNLP Workshop: Analyzing and Inter-
  language model. Transformer Circuits Thread, 2025b.      preting Neural Networks for NLP, pp. 407–416, 2024.
 URL https://transformer-circuits.pub/
  2025/attribution-graphs/biology.html.     Yang, X., Venhoff, C., Khakzar, A., de Witt, C. S., Dokania,
                                                                             P. K., Bibi, A., and Torr, P. Mixture of experts made intrin-
Nanda, N., Chan, L., Lieberum, T., Smith, J., and Stein-       sically interpretable. arXiv preprint arXiv:2503.07639,
   hardt, J. Progress measures for grokking via mechanistic      2025.
   interpretability. arXiv preprint arXiv:2301.05217, 2023.
                                                          Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,        berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,
  N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,      Yang, L., et al. Big bird: Transformers for longer se-
  A., et al. In-context learning and induction heads. arXiv      quences.  Advances in neural information processing
  preprint arXiv:2209.11895, 2022.                            systems, 33:17283–17297, 2020.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,    Zhang, F. and Nanda, N. Towards best practices of activation
  Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,      patching in language models: Metrics and methods. arXiv
   et al. Training language models to follow instructions       preprint arXiv:2309.16042, 2023.

                                                10

                            Sparse Attention Post-Training for Mechanistic Interpretability

Zhou, Y., Zanette, A., Pan,  J., Levine, S., and Kumar,
  A.  Archer: Training language model agents via hier-
  archical multi-turn rl.  In ICML, 2024. URL https:
  //openreview.net/forum?id=b6rA0kAHT1.





                                                11

                            Sparse Attention Post-Training for Mechanistic Interpretability

A. Two-Digit Addition Study





   Non-Sparse





     Sparse





Figure 9. Simple example showing the attention patterns (shown in blue) of sparse and non-sparse transformers trained on a two digit
addition task. Both models are able to correctly predict the sum, but the attention patterns are very different: the non-sparse model solves
the task with highly dispersed information flow, while the sparse model uses a highly interpretable attention pattern: in Layer 0, the model
first attends to the corresponding digits to be added, then in Layer 1, it attends to the carry bit only if it is needed (see middle and right
columns, where the model has to carry once and twice respectively).

In the introduction, we used a two-digit addition task to demonstrate how sparse attention patterns can lead to intrinsically
interpretable circuits. The result presented is gathered in a small scale toy experiment described below. We train 4-layer
single-head Transformer models on a two-digit addition task, where the input is a sequence of digits and the model is trained
to predict the sum. In this task, there are 13 total tokens: ten digits and three symbols ”+”, ”=” and ”?”.

Within this setting, we train two models: a standard transformer model and a sparse transformer with a fixed sparsity
regularisation strength. Figure 9 shows several examples of the learned attention patterns. In these examples, we can clearly
see that the pressure of sparsity leads to the emergence of human-recognisable algorithmic patterns: in the first layer, each
digit in the answer attends to the corresponding digits in the input, while the second layer computes the carry bit when
necessary. By enforcing selective information flow through sparse message-passing, the sparse model is able to learn crisp
and localised mechanisms that are immediately amenable to interpretation.





                                                12

                            Sparse Attention Post-Training for Mechanistic Interpretability

B. Sparse Attention Implementation

For the experiments, we implemented efficient GPU kernels for the sparse attention layers using the helion domain-specific
language3. We refer to this implementation as Splash Attention (Sparse flash Attention). Our implementation follows the
same core algorithmic structure as FlashAttention-2 (Dao, 2023), including the use of online softmax computation and tiling.
Note that the sparse attention variant (Eq. 2) only differs from the standard attention by a pointwise multiplication of the
adjacency matrix, which can be easily integrated into FlashAttention by computing Aij on-the-fly. We additionally fuse the
Gumbel-softmax computation, the straight-through gradient, and the computation of the expected number of edges (required
for the penalty) into a single optimized kernel, the implementation of which will be released together with the experiment
code. Figure 10 compares our Splash Attention implementation against a naive baseline based on PyTorch-native operations.





              Figure 10. Performance comparison between our implementation (Splash) and a naive PyTorch baseline.





  3https://helionlang.com/


                                                13

                            Sparse Attention Post-Training for Mechanistic Interpretability

C. Training Details

C.1. Hyperparameters and Compute Resources



                Hyperparameter       OLMo                  GPT-2

                 Base Model             allenai/OLMo-7B-hf   gpt2
                   Context window            512                      64
                    Dataset                dolma-v1             OpenWebText
                  Batch size                 16                       256
                   Gradient accumulation steps  4                        4
                     Total steps                  400,000                     1,200,000
                   Learning rate              1 × 10−5                  1 × 10−5
               Minimum learning rate       1 × 10−6                  1 × 10−6
                   Optimizer              Adam                 Adam
                  Weight decay                 0.1                            0.1
                   Scheduler                   Cosine (1 cycle)             Cosine (1 cycle)
               Warmup steps               1,000                       1,000

                    Finetuning strategy        LoRA                         Full
              LoRA rank (r)             400                                  -
              LoRA scaling (α)           800                                  -
              LoRA dropout             0                                     -
              LoRA target modules       q,k,v,o,fc in,fc out    -

                 Dual Optimisation LR        0.01                          0.1
                    Target cross-entropy          2.29                          3.5

                      Table 2. Key hyperparameters used for sparse post-training experiments on OLMo-7B.



We provide the key hyperparameters for our experiments in table 2. All training are performed on NVIDIA H100 GPUs.
The GPT-2 model is trained on a single GPU while the OLMo model is trained on a node of 8 GPUs. The total training time
for both models is roughly 14 days. The main sparse attention code will be made available as a Transformer library wrapper.
The implementation code as well as model weights will also be released.


C.2. Training Dynamics



                                                                                                                                                                        3.0

                                                                       3000
                                                                                                                                                                        2.8
    10 1                                                                2500
                                                                                                                                                                                                           Strength 2000                                                                                                                                                                   Entropy 2.6
  Sparsity                                                                   1500                                                                                                                    Cross
                                                                                                                                                                        2.4                                                                                                                                                                                                                                                                                                                                                                    Regularisation 1000                                                                                                                                                                                                                                         Validation
    10 2
                                                                        500                                                                              2.2

                                                                          0
                                                                                                                                                                        2.0
        0          100000        200000        300000        400000        0          100000        200000        300000        400000        0          100000        200000        300000        400000
                                       Training Steps                                                                         Training Steps                                                                         Training Steps

Figure 11. The training curves for post-training OLMo-7B tacking the model sparsity (left), regularisation strength (middle), and the
cross-entropy loss (right). The black dotted line on the cross-entropy plot indicates the pre-defined threshold, τ.


A key feature of our post-training framework is that the strength of the sparsity regularisation is automatically controlled
via a constrained optimisation scheme. By pre-specifying an accepted level for the cross-entropy target, τ, the training

                                                14

                            Sparse Attention Post-Training for Mechanistic Interpretability

procedure can be written as the max-min objective:

                          max min X E |Al| + λ(CE −τ)  ,                                       (7)
                                 λ>0   θ
                                                                                     l

which can be optimised by taking alternating gradient steps in the model weight space and in the λ space. The resulting
training dynamics means that the sparsity regularisation strength increases when the model cross-entropy is lower than
the target, and decreases when the model is above the threshold. Figure 11 shows the training curves for the OLMo-7B
model. Here, we observe that the strength of sparsity regularisation keeps increasing slowly while the model cross-entropy is
clipped at the desired level. Note that during a model spike (at around 100K steps), the sparsity regularisation automatically
decreases to let the model recover.





                                                15

                            Sparse Attention Post-Training for Mechanistic Interpretability

D. Extra Experiments for Circuit Discovery

In this section, we provide additional results for the activation patching circuit discovery experiment presented in the main
text.

Figure 12 shows the attention patterns of the heads required to explain 90% of model behaviour on a copy task. To fully test
the longer context window afforded by OLMo, we use a longer prompt than the one used for GPT2 in the main text. The
result is consistent with the GPT-2 experiment: sparsified model facilitates the discovery of smaller circuits of induction
heads that implement the copy task.





Figure 12. Attention patterns of the heads required to explain 90% of model behaviour on a longer copy task. Similar to the GPT-2 results
in Figure 3, the sparse model requires substantially fewer attention heads.

Figure 13 and 14 show the fraction of explained model preference as a function of the number of model components kept
un-ablated. The difference between these plots and Figure 4 and 5 lies in the way individual model components are ranked.
Here, the ranking is performed on a task level, meaning that the importance score for each component is pooled across
different instances of the same task. Overall, the results are commensurate with results presented in the main paper, where
the ranking strategy consistently discover smaller circuits in sparse models. The only exception is the Greater Than task
for GPT-2 where the number of attention heads required for the sparse model is larger than that of the base model. We
hypothesise that this is due to the sparse model choosing different circuits to implement different instances of the same task,
rendering the task-level importance score less suitable for circuit discovery in this case. Finally, in Figure 15, we provide a
qualitative visualisation of the edges required to completed the IOI task.





                                                16

                            Sparse Attention Post-Training for Mechanistic Interpretability



     Greater Than                      IOI                             Docstring                          IOI Long
    1.0           0.6x                         1.0               2.0x                  1.0        1.3x                               1.0       1.6x

 Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect
  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5


                                  GPT-2                                                                                 OLMo-7B
    0.0                            Sparse GPT-2        0.0                                                   0.0                         Sparse OLMo-7B        0.0
                   50          100                             50          100                         250      500      750     1000               250      500      750     1000
               Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept                          Number of Heads Kept

Figure 13. Logit attribution per sentence keeping only the top-k attention heads based on a global ranking score. Dotted line annotates the
number of attention heads needed to explain 90% of the logit difference. With the exception of the Greater Than task for GPT-2, the
sparse models admits smaller circuits.





     Greater Than                      IOI                             Docstring                          IOI Long
    1.0              41.9x                1.0              14.9x                1.0                   5.5x           1.0                   3.1x

 Effect                                                                                               Effect                                                                                               Effect                                                                                               Effect
  Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5                                                                                                                                     Explained 0.5


                                    GPT-2                                                                                 OLMo-7B
    0.0                              Sparse GPT-2      0.0                                                   0.0                           Sparse OLMo-7B      0.0
      100       101       102       103       104        100       101       102       103                100    101    102    103    104    105          100    101    102    103    104    105
            Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)                  Number of Edges Kept (log scale)

Figure 14. Logit attribution per sentence keeping only the top-k attention edges based on a global ranking score. Dotted line annotates the
number of attention heads needed to explain 90% of the logit difference.





                            Sparse GPT-2                             GPT-2 (baseline)


                         Layers





Figure 15. An example of the attention-head edges required to reach 0.9 cumulative score based on the averaged scores for the IOI task.




                                                17

                            Sparse Attention Post-Training for Mechanistic Interpretability

E. Circuit Discovery Tasks

In the following, we provide the details and the prompts for the various tasks used in section 4.2.


E.1. Greater-Than Task

Each example contains a clean prompt, a corrupt prompt, and two disjoint sets of candidate continuations, answers and
wrong answers. A typical entry is:

{
  "clean":   "The demonstrations lasted from the year 1363 to 13",
  "corrupt": "The demonstrations lasted from the year 1301 to 13",
  "answers": ["64", "65", ..., "99"],
  "wrong_answers": ["00", "01", ..., "63"]
}

For the clean prompt, any token in answers yields an end year strictly greater than the start year (e.g. "1364"–"1399"),
whereas tokens in wrong answers correspond to years that are less than or equal to the start year. The corrupt prompt
changes only the starting year, shifting which continuations correspond to valid end years. We use the logit difference
between the aggregated probability mass on answers vs. wrong answers in clean vs. corrupt contexts as our signal, in
the spirit of prior mechanistic studies on simple algorithmic tasks (Elhage et al., 2021; Nanda et al., 2023).


E.2. Indirect Object Identification (IOI) Task

Our IOI setup follows the standard indirect object identification paradigm for mechanistic interpretability (Elhage et al.,
2021; Conmy et al., 2023). Each example is generated by combining:

   • a pair of names (A, B), e.g. (" Mary", " John");

   • a natural-language template with placeholders [A], [B], and [S].


We instantiate templates such as:

"Then, [B] and [A] went to the park. [S] gave a ball to"

"When [B] and [A] got a snack at the cafe, [S] decided to give it to"

"After the lunch, [B] and [A] went to the mall. [S] gave a gift to"

by sampling a name pair and substituting [A] and [B], then choosing the subject [S] (either one of the pair). The correct
continuation is the indirect object, i.e. the other member of the pair.

For example, with (A, B) = (" John", " Mary") and S = B, one instance is:


   Then, Mary and John went to the park.  Mary gave a ball to

The correct continuation is " John", while " Mary" and any distractor names are treated as incorrect candidates.

In the OLMo experiments, in order to further test the capability of our approach, we use a different set of IOI task with
increased complexity and prompt length. Example templates include:

"After several months without any contact due to conflicting schedules and
unexpected personal obligations, [B] and [A] finally met again at the park,
where they spent a long afternoon catching up on past events, sharing stories,
and reflecting on how much had changed. As the day came to an end, [S] gave
a ball to"



                                                18

                            Sparse Attention Post-Training for Mechanistic Interpretability

"Although [B] and [A] had previously been involved in a long and emotionally
charged argument that left several issues unresolved, they agreed to meet in
order to clarify their misunderstandings. After a tense but honest conversation,
[S] said to"


E.3. Docstring Task

We also test the OLMo models on a more complex Docstring task (Heimersheim & Janiak, 2023; Conmy et al., 2023), where
the model needs to attend to a specific argument for a specified function in order to complete a Docstring. Similarly to the
Greater Than task, each example contains a clean prompt, a corrupt prompt, and two disjoint sets of candidate continuations.
A typical entry is:

{
  "clean": "def model(self, results, old, option):
                """
                stage agency security vision spot tone joy session river unit
                :param results: bone paper selection sky
                :param old: host action hell miss
                :param",
  "corrupt": "def model(self, command, output, state):
                """
                stage agency security vision spot tone joy session river unit
                :param old: bone paper selection sky
                :param results: host action hell miss
                param",
  "answers": [" option"],
  "wrong_answers": [" results"," old"]
}





                                                19

                            Sparse Attention Post-Training for Mechanistic Interpretability

                        Category                 Setting
                       Model                 GPT-2 (HookedTransformer)
                            Input dimension (din)      768
                            Latent dimension (dlatent)   24 576
                         Expansion factor         32
                          Context size             64
                         Batch size (tokens)       1 024
                             Precision               Mixed (FP32 / AMP)
                         Device            CUDA
                             Distributed training     DDP

                          Optimizer            Adam
                          Learning rate            2 × 10−4
                   Adam β1 / β2              0.9 / 0.999
                          Learning rate warm-up     Cosine (1 000 steps)
                          Learning rate decay steps  1 874
                             Final LR scale             0.1

                      L0 coefficient            2
                         Optimal L0              3
                      L0 warm-up               Linear (18 749 steps)
                      Dead feature penalty      10−5
                      Dead feature window      250

                              Table 3. Training configuration for the GPT-2 cross-layer-transcoders.



F. Cross-Layer-Transcoder

To implement a cross-layer transcoder, let hℓ∈Rdmodel denote the input to the MLP at layer ℓfor a single token position.
This representation is projected into a sparse feature space via an encoder,


                         zℓ= ReLU Wℓenchℓ+ bℓenc  ∈Rdfeatures,                                       (8)

where Wℓenc ∈Rdfeatures×dmodel and bℓenc ∈Rdfeatures are layer-specific encoder parameters.
The CLT reconstructs the MLP output at a target layer ℓ′ by linearly aggregating feature activations originating from all
preceding layers,

                                                 ˆmℓ′ = X Wℓ→ℓ′dec zℓ+ bℓ′dec,                                             (9)
                                                         ℓ≤ℓ′


where Wℓ→ℓ′dec   ∈Rdmodel×dfeatures denotes the decoder mapping from layer ℓto layer ℓ′.

The summation over layers reflects the fact that a given semantic feature may manifest in different representations across
multiple MLP layers. For example, a feature that emerges in the MLP at layer ℓmay reappear, potentially in a transformed
form, in the outputs of subsequent MLPs. Without accounting for these layer-dependent variations, such duplicated
representations would lead to redundant nodes in the attribution graph. By allowing features to be represented differently
across layers while being linked through a shared latent space, the cross-layer transcoder avoids this duplication and yields
a more compact and interpretable attribution structure. For a detailed comparison between cross-layer transcoders and
standard transcoders, we refer the reader to Lindsey et al. (2025a).

Following the training procedure proposed by Anthropic (Ameisen et al., 2025), the final objective combines reconstruction


                                                20

                            Sparse Attention Post-Training for Mechanistic Interpretability

accuracy with sparsity and dead-feature regularization:

                     L = X ∥ˆmℓ′ −mℓ′∥22
                                                             ℓ′
                                     | MSE reconstruction{z      }
                     + λ0 X tanh C (zℓ⊙∥Wℓdec∥)
                                                          ℓ
                                          |         L0 sparsity{z           }
                     + λdf X ReLU exp(τ) −hpreℓ  ∥Wℓdec∥ ,                               (10)
                                                           ℓ
                                           |              dead-feature{z penalty         }

where Wℓdec denotes the concatenated decoder weights associated with layer ℓ, hpreℓ   are the corresponding pre-activation
values, τ is a threshold parameter, and C is a scaling constant. The hyperparameters λ0 and λdf control the strength of the
sparsity and dead-feature regularization terms. We initialize the weights with following circuits updates (Conerly et al.,
2025). The encoder biais is initialize to have a fixed proportion of the features active at initialization. We provide in
Figure 16 the training curves of the sparsity value, the sparsity coefficient, the explained variance, and the amount of dead
features. We hope this can help the community in training their own cross-layer transcoders.





                      (a) L0 vs steps                                                          (b) L0 coefficient vs steps





                  (c) Dead features vs steps                                                  (d) Explained variance vs steps

Figure 16. Training dynamics of the cross-layer transcoder, showing sparsity, regularization strength, dead features, and reconstruction
quality over training.



                                                21

                            Sparse Attention Post-Training for Mechanistic Interpretability

G. Attribution-Graph

Following Ameisen et al. (2025), we define the attribution score between feature n at layer ℓand position k, and feature n′
at layer ℓ′ and position k′, as
                                                   aℓ′,k′,n′ℓ,k,n = X fk,nℓ→s Jℓ′,k′s,k   gℓ′k′,n′,                                     (11)
                                                   ℓ≤s≤ℓ′
where fk,nℓ→s denotes the decoder vector associated with feature n projecting from layer ℓto layer s, Jℓ′,k′s,k   is the Jacobian
mapping the MLP output at (ℓ, k) to the MLP input at (ℓ′, k′), and gℓ′k′,n′ is the corresponding encoder feature at layer ℓ′ and
position k′. The sum in this equation reflects the cross-layer mapping of the cross-layer transcoder.

The Jacobian is computed during a modified forward pass in which all nonlinear operations, including normalization layers,
attention mechanisms, and MLPs, are frozen using stop-gradient operations. The resulting attribution graph is pruned by
retaining only those features that cumulatively explain 80% of the contribution to the final logit, and only those edges that
account for 95% of the total edge-level effect. All attribution computations are performed using the circuit-tracer
library (Hanna et al., 2025). For a complete description of the attribution graph computation and pruning, we refer the user
to reading (Ameisen et al., 2025).

For the visualization and the autointerp, we write our own pipeline. In Figure 17, we show a screenshot of the interface for
the ’The opposite of ”large” is ”’ attribution graph. The features are colored with respect to their corresponding clusters.




                                             Clusters: opposite – large – brackets – say small





                Figure 17. Circuit-tracing interface example for the ’The opposite of ”large” is ”’ with GPT2-sparse.





                                                22

                            Sparse Attention Post-Training for Mechanistic Interpretability

H. Graph: The opposite of ”large” is ”

We obtain a replacement score of 0.82, with 459 features identified before pruning and 82 features remaining after pruning.
The majority of features in the resulting attribution graph fall into four dominant clusters:



   • Opposition cluster: features associated with opposition and comparison, primarily localized at the token position
     corresponding to opposite.



   • Magnitude cluster: features related to notions of size (e.g., large, big, full, medium), predominantly located in the
     residual stream at the large token position.



   • Bracket cluster: features that activate on tokens enclosed in brackets.



   • Final-logit cluster: mainly the final logit itself and a couple of features that activate before the token ”small” or related
     terms.



In the boxes below, we present the top activations of representative feature sets for each cluster.


   Feature 1117                                                                                    ”Opposite” cluster
    in Washington has now adopted the wider measure of student debt outstanding. This new
   the situation in Syria, Iran and the wider region. ”The
   recharged by the wider dense forests of Sanjay Van and its overflow drained
    public, with interesting accounts of Oswald’s demeanor at this significant moment
   has a slightly wider range. Specifically, the Atom-powered NANO
   56 becoming part of the wider Seven Years’ War in which Britain and France




   Feature 1337                                                                                    ”Opposite” cluster
    opposite, piece of Mexico’s cultural identity. I made the hour

    opposite shows, or something bigger, “where there’s villains

    opposite sides of Mars in 2004 and used their instruments to discover geologic evidence

    opposite, but not anymore. Now everything he says to me is some kind

    opposite direction, and had little trouble finding space at the campsites.

   always seem to be just the opposite.

   show a growing trend to cast “no” votes , opposing how much salary

   and the occupation of the opposing forces was generally limited to mutual observation.

   work hand in hand for the purpose of opposing  all movements of the thinking part

   the defense’s inability to stop opposing run games. The Bills have

    ing opposing quarterbacks. The Seahawks not only had depth, they were versatile.

    to win more hand battles particularly when the opposing tackle neutralizes his initial



                                                23

                         Sparse Attention Post-Training for Mechanistic Interpretability


Feature 901                                                                                      ”Large” cluster
Let’s be honest: When someone advocates for large-scale Muslim

robot provides a tragicomic reminder of why RWD needs to consider large as

what kind of social safety nets should be in place to protect people from large

advocates to limit the power of large, established corporations, analysts say.

of large law firms is that they are so great that the only reason anyone

that by scaling up tests, the method would be conducive for use on larger




Feature 933                                                                                      ”Large” cluster
people healthy and anticipating health issues before they become a problem . Big Data is

 Big brown bucks with funny accents.” Judy flinched at

 BIG UP UBUNTU: Ubuntu releases are named after industry.¡—endoftext—¿
 BIG LEAGUE: Barron’s Says The
they need to submit their content in the same way  . Big enough

apps and offering alternatives routes  . Big data and optical fiber




Feature 1004                                                                                     ”Large” cluster
guide said was ?  full of drinking saloons, dime museums, small
would have 2 mana sources next turn (unless his hand was  full of fast
’s house, it  ’s full of adventure itself.?
statement that all German Catholics had a right to  full transparency”
glimpsing a lobby  full of construction debris. The front hallway was  full of
Jokubas had recently been reading a newspaper article which was  full of




Feature 412                                                                                       “Brackets” cluster
group answered either “very” or “somewhat” attached – except
some work colleagues. Wilcox said she found it ‘highly unlikely

very rare , “very likely ,” “high  risk,” she says.
ulent. Pentagon spokesman Peter Cook said the sample was “
on of PopMatters called the album “brilliant” and said
arlene Lowe, described him as being “one of my biggest supporters”.




Feature 518                                                                                       “Brackets” cluster
Kerry said Washington and Hanoi will “continue to have differences in opinions
the United States will “take care of it.” He told reporters after the
legislation would “provide new enforcement tools for protecting our citizens and will help

Gary Ross, said in a statement that the Air Force is currently “short
said, and Syrian President Bashar al-Assad would “have to go”.
introduces politics into consumer policies,” said Palmor, adding that it would ”



                                             24