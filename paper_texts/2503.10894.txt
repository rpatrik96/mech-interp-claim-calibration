                      Published as a conference paper at ICLR 2025


         HYPERDAS: TOWARDS AUTOMATING MECHANISTIC
           INTERPRETABILITY WITH HYPERNETWORKS

                      Jiuding Sun♡,♦      Jing Huang♦      Sidharth Baskaran♠      Karel D’Oosterlinck♣

                      Christopher Potts♦                Michael Sklar∗♠                  Atticus Geiger∗♡

                     ♡Pr(Ai)2R Group    ♦Stanford University    ♠Confirm Labs   ♣Ghent University


                                       ABSTRACT2025

                              Mechanistic interpretability has made great strides in identifying neural networkApr                           features (e.g., directions in hidden activation space) that mediate concepts (e.g.,
                                 the birth year of a person) and enable predictable manipulation.  Distributed
                              alignment search (DAS) leverages supervision from counterfactual data to learn25
                              concept features within hidden states, but DAS assumes we can afford to con-
                               duct a brute force search over potential feature locations. To address this, we
                                present HyperDAS, a transformer-based hypernetwork architecture that (1) au-
                                 tomatically locates the token-positions of the residual stream that a concept is
                                  realized in and (2) constructs features of those residual stream vectors for the con-
                                   cept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art per-[cs.CL]                       formance on the RAVEL benchmark for disentangling concepts in hidden states.
                                In addition, we review the design decisions we made to mitigate the concern that
                        HyperDAS (like all powerful interpretabilty methods) might inject new informa-
                                  tion into the target model rather than faithfully interpreting it. Code available at:
                              https://github.com/jiudingsun01/HyperDAS



                1  INTRODUCTION


                      Mechanistic interpretability methods promise to demystify the internal workings of black-box lan-
                    guage models (LMs), thereby helping us to more accurately control these models and predict how
                       they will behave. Automating such efforts is critical for interpreting our largest and most performant
                      models, and strides toward this goal have been made for circuit discovery (Conmy et al., 2023; Ra-
                     jaram et al., 2024) and neuron / feature labeling (Bills et al., 2023; Huang et al., 2023; SchwettmannarXiv:2503.10894v3
                           et al., 2023; Shaham et al., 2024). In the present paper, we complement these efforts by taking the
                               first steps toward automating interpretability for identifying features of hidden representations (e.g.,
                         directions in activation space) that mediate concepts (Mueller et al., 2024; Geiger et al., 2024a).

                        Interventions on model-internal states are the building blocks of mechanistic interpretability (Saphra
             & Wiegreffe, 2024). To establish that features of a hidden representation are mediators of a concept,
                      a large number of interchange intervention (Vig et al., 2020; Geiger et al., 2020) experiments are
                     performed on the LM. Interchange interventions change features to values they would take on if a
                        counterfactual input were processed. For example, if the concept is C = the birth year of a person,
                 we can fix the features F of an LM processing the input Albert Einstein was born in to the value
                       they take for Marie Curie was a chemist. If the output changes from 1879 to 1934, we have a piece
                       of evidence that F mediates C. The field has developed a variety of methods for learning such
                         interventions, but all of them require a brute-force search through potential hidden representations.

                   To address this significant bottlebeck, we propose HyperDAS, a method to automate this search pro-
                       cess via a hypernetwork. In the HyperDAS architecture, a transformer-based hypernetwork localizes
                      a concept within the residual stream of a fixed layer in a target LM by:


                                                           1

Published as a conference paper at ICLR 2025





                                               Expected Output                           Select Token     Identify a Feature
                                                                                                  Positions        Subspace       I loveParis[self]              France
          Vie                ④
                                                    Intervention in the Country Subspace                               Dot-ProductAttention           Norm
          nna                                                                                                               R
                                                                                               “Paris”                 MLP                MLP
           is                                            “nna”           ②
                                                                 (Vienna)
           in                                                            Model States      Add & Norm    ③
                                                                                               at Layer L
                Select Token Positions         Identify a Feature Subspace                                       Masked
             ②            ③                                                        Cross-attentionMulti-Head

                                                                                Base
                                                                                                               Add & Norm                 Hypernetwork                                Model States
                                             N ⨉           Multi-HeadMasked          ①
                                                                                                                                              Cross-attention
                      ...           ...            Counterfactual
                      ...           ...           Model States
     the country of     ...           ...                                            Add & Norm
         a city            ...           ...                                      Masked                      ...           ...                                           Multi-Head
                                                                                                                                                         Self-attention
                         Vie nna is in          I love Paris
   Natural Language Instruction      Model States When Processing Base (Left)        ①
                                  and Counterfactual (Right) Inputs                   Natural Language Instruction

Figure 1: The HyperDAS framework, used here to find the features that mediate the concept of
“country”. (1) Concept Encoding A natural language description that specifies which concept to
localize, “The country of a city”, is encoded by a transformer hypernetwork with two additional
cross-attention blocks attending to the hidden states of the target LM prompted with the base text
“Vienna is in” and the counterfactual text “I love Paris”. (2) Selecting Token-Positions With the
encoding from step 1 as a query, HyperDAS uses selects the tokens “nna” and “Paris” as the location
of the concept “country” for the base and counterfactual, respectively. (3) Identifying a Subspace
With the representation from step 1 as the encoding, HyperDAS constructs a matrix whose orthogo-
nal columns are the features for “country”. (4) Interchange Intervention With the token-positions
from step 2 and subspace from step 3, HyperDAS performs a intervention by patching the subspace
of the hidden vector for the token “nna” to the value it takes on in the hidden vector for the token
“Paris”, leading the model to predict “France” from the base prompt “Vienna is in”.



  1. Encoding a language description (e.g., the birth year of a person) of a concept using a trans-
    former that can attend to the target LM processing a base prompt (e.g., Albert Einstein was
    born in) and a counterfactual prompt (e.g., Marie Curie was a chemist).

  2. Pairing tokens in the base and counterfactual prompts (e.g., align “Cur” with “Ein”) with an
     attention mechanism using the encoding from (1) as a query and token-pairs as keys/values.

  3. Selecting features of the residual stream via a fixed orthogonal matrix that undergoes a House-
     holder transformation (Householder, 1958) using the encoding from (1).

  4. Patching the selected residual stream features of aligned tokens from the base prompt to the
     values they take on in the residual stream of aligned tokens from the counterfactual prompt.

We benchmark HyperDAS on the RAVEL interpretability benchmark (Huang et al., 2024), in which
concepts related to a type of entity are disentangled. For example, we might seek to separate features
for the birth year and field of study of a Nobel laureate. HyperDAS achieves state-of-the-art perfor-
mance on RAVEL with a single model. Greater gains are achieved when a separate HyperDAS is
trained for each entity type (e.g., Nobel laureates).

Finally, we address the issue of whether HyperDAS is faithful to the target model. As we use more
complex machine learning tools for interpretability, there is an increasing concern that we are not
uncovering latent causal structure, but instead injecting new information to steer or edit a model
(Meng et al., 2022; Ghandeharioun et al., 2024). If we allow our supervised interpretability models
too much power, we run the risk of false-positive signals. Thus, we conclude with a discussion of
how our decisions about architecture, training, and evaluation were made to mitigate these concerns.


                                       2

Published as a conference paper at ICLR 2025




2  BACKGROUND

Automating Interpretability Workflows  The growing size and complexity of language models
demands scalable techniques for interpretability. Major directions include circuit analysis (Conmy
et al., 2023; Marks et al., 2024; Rajaram et al., 2024; Ferrando & Voita, 2024), unsupervised feature
learning (Huben et al., 2024; Braun et al., 2024), and feature labeling with natural language descrip-
tions (Mu & Andreas, 2021; Hernandez et al., 2022; Bills et al., 2023; Huang et al., 2023; Shaham
et al., 2024). In this work, we take steps towards automating the process of identifying features that
are causal mediators of concepts using supervision from counterfactual data.

Identifying Features that Mediate Concepts  Interchange interventions identify neural represen-
tations that are causal mediators of high-level concepts (Vig et al., 2020; Geiger et al., 2020; Fin-
layson et al., 2021; Stolfo et al., 2023). Geiger et al. (2024b) and Wu et al. (2024) further extend
interchange interventions to localizing concepts in hidden vector subspaces. However, these meth-
ods require an exhaustive search over all layers and tokens to measure causal effects at each position.
In practice, the lack of an effective search method leads to heuristics in token selection. For example,
in knowledge editing and model inspection, a widely held assumption is that the entity information is
localized to the last entity token (Meng et al., 2022; 2023; Hernandez et al., 2024; Geva et al., 2023;
Ghandeharioun et al., 2024), but this does not hold for all entities (Meng et al., 2022). Our proposed
method directly addresses this problem by using an end-to-end optimization to automatically select
the intervention site across all tokens, conditioned on the concept to localize.

The RAVEL Benchmark  The RAVEL benchmark evaluates how well an interpretability method
can localize and disentangle entity attributes through causal interventions. An example consists of
a base prompt that queries a specific attribute of an entity (e.g., Albert Einstein studied the field), a
counterfactual prompt containing a different entity of the same type (e.g., Poland declared 2011 the
Year of Marie Curie), an attribute targeted for intervention (e.g., field of study or birth year), and a
counterfactual label for the base prompt. The label would be physics if the targeted attribute is birth
year (the intervention should not affect the field of study attribute), and it would be chemistry if the
targeted attribute is field of study.

Distributed Interchange Interventions  RAVEL supports evaluations with distributed inter-
change interventions on features of a hidden representation H that encode an attribute in the original
model M. In our experiments, features are orthogonal directions that form the columns of a low-
rank matrix R. Given a base prompt ¯x and a counterfactual prompt ˆx, we perform an intervention
that fixes the linear subspace spanned by R:
               H ←¯h + R⊤(R(ˆh) −R(¯h))                                (1)
where ¯h and ˆh are the values that variable H has when the model M is run on ¯x and ˆx, respectively.

RAVEL Metrics  The metric from the RAVEL dataset has two components. The Cause score is
the proportion of interchange interventions that successfully change the attribute that was targeted,
and the Iso score is the proportion of interchange interventions that successfully do not change an
attribute that was not targeted. The Disentangle score is the average of these two.

Distributed Alignment Search  The RAVEL evaluations use distributed alignment search (DAS;
Geiger et al. 2024b) as a baseline for learning a feature of a hidden representation that mediate an
attribute. A rotation matrix is optimized on a RAVEL example with base input ¯x, counterfactual
input ˆx, and counterfactual label y using the following loss:
                     LDAS = CE(MH←¯h+R⊤(R(ˆh)−R(¯h))(¯x), y)                         (2)
where Mγ(¯x) is the output of the model M run on input ¯x with an intervention γ. Only the
parameters R are updated while the parameters of the target model M are frozen.

3  HYPERDAS

To localize a concept in the layer l of a target model M, a HyperDAS architecture consists of a
hypernetwork H that takes in a text specification x of the target concept and dynamically selects


                                       3

Published as a conference paper at ICLR 2025




token positions in the base text ¯x and counterfactual text ˆx and identifies a linear subspace R that
mediates the target concept. Selecting tokens is a discrete operation, so we “soften” the selection
during training and force discrete decisions during evaluation. Our specific model is as follows.

3.1  REPRESENTING THE TARGET CONCEPT AS A VECTOR

Token Embedding A token sequence x of length E that specifies the concept to localize, e.g., the
country a city is in, is encoded with the embeddings of the target model M to form e0 = Emb(x) ∈
RE×d, the zeroth layer of the residual stream for the hypernetwork H.

Cross-attention Decoder Layers  After embedding the target concept, we run a transformer with
N decoder layers. Besides the standard multi-headed self-attention (MHA) and feed-forward layers
(MLP), each decoder block has two additional cross-attention modules to incorporate information
from the target model M procesing the base and counterfactual prompts ¯x and ˆx.
Let ¯h ∈RB×L×d and ˆh ∈RC×L×d be the stacks of base and counterfactual hidden states from the
base and the counterfactual input, where L is the number of layers in M, d is the hidden dimension,
and B and C are the sequence length of the base and counterfactual examples, respectively. Two
multi-headed cross-attention modules MHA¯  and MHAˆ   allow H to attend to ¯h and ˆh. Each layer of
the hypernetwork H can attend to every layer of the target model.

For the p-th decoder layer of the hypernetwork H, the three attention mechanisms are as follows:
                                     e′p = MHA(Q(ep), K(ep), V(ep))                            (3)
                                             e′′p = MHA(¯   ¯Q(e′p), ¯K(¯h), ¯V(¯h))                              (4)
                          ep+1 = MHA(ˆ    ˆQ(e′′p), ˆK(ˆh), ˆV(ˆh))                              (5)

After the final transformer block is applied, the residual stream vector at the last token position
 (N) ∈Rd encodes information about the concept targeted for intervention and the target model’se E
base and counterfactual runs. This representation is used to select token-positions in the base and
source texts and identify a linear subspace for intervention.

3.2  DYNAMICALLY SELECTING TOKEN-POSITIONS THAT CONTAIN THE TARGET CONCEPT

The next step is selecting the tokens in the base and counterfactual prompts that contain the concept
            (N)
encoded as e         E   . We construct an “intervention score” matrix G ∈RB×(C+1) with elements ranging
from 0 to 1, where B and C are the number of tokens in the base and counterfactual prompts,
respectively. Each element G(b,c) denotes the degree to which the b-th base token is aligned with
c-th counterfactual token for intervention. The additional column G(b,c) corresponds to the score for
retaining the b-th base token without any intervention.

In Figure 1, only the base-token “nna” (the last token of the entity “Vienna”) receives an intervention
score of 1 when paired with the counterfactual=token “Paris”. All other base tokens are not selected
for intervention. The concept of “country” is localized to the last token of the city entities.
To compute the element G(b,c), we use ¯h(l)b  and ˆh(l)c  , the l-th layer residual stream representation of
the target model at b-th base-token and c-th counterfactual-token. We first linearly combine the two:
                                                                        (l)                                           g(b,c) = F([¯hb  ; ˆh(l)c ])                                    (6)
where F(.) ∶R2d →Rd is a linear projection that condenses the concatenated representation into
the original dimension d. For the extra column that indicates no intervention, the representation is
simply the original base token representation:
                                                                            (l)
                                         g(b,C+1) = ¯hb                                          (7)
Then, we weight each merged representation ̃h using the concept encoding e (N)E  ∈Rd and M
                                               d                       d
“query” and “key” matrices Q(i) ∈Rd×M×                           M and K(i) ∈Rd×M×                                         M :
                   ∑M                                             i=1 (e (N)E  )Q(i) ⋅(̃h)K(i)
                       =        √                                          (8)                          G′(b,c)
                     M  d


                                       4

Published as a conference paper at ICLR 2025



Finally, we apply a column-wise softmax G = ColumnSoftmax(G′).

Using the matrix G, we can construct the representation that we will use to intervene on each token
in the base prompt. For the b-th base-token hidden states ¯hlb, the interventio representation is:
                                      C
                                        ̃h(l)b                      = G(b,C+1)¯h(l)b + ∑ G(b,c)ˆh(l)c                               (9)
                                                      c=1
This is how the counterfactual representation is constructed for a weighted interchange intervention
(Wu et al., 2024). The counterfactual hidden states remain identical to the base hidden states (i.e.,
no intervention on token b) when G(b,C+1) = 1. Conversely, if G(b,c) = 1 for a specific position c,
the b-th base token is entirely replaced by the hidden vector for the c-th counterfactual token.

3.3  DYNAMICALLY IDENTIFYING A LINEAR SUBSPACE THAT CONTAINS THE CONCEPT

In addition to selecting token-positions, HyperDAS also dynamically identifies a linear subspace
that contains the target concept, encoded as a low-rank matrix with orthogonal columns. First, we
                                (N)
apply a multi-layer perceptron to e                        E  ∈Rd in order to produce a new vector
                                             (N)
                         v = MLP(e E  ) ∈Rd.
In DAS, there is a fixed low-rank matrix with orthogonal columns R′ representing a fixed subspace
targeted for intervention. We use a linear algebra operation known as the Householder transforma-
tion to change R′ conditional on v into a new matrix R that still has orthogonal columns. Given a
non-zero vector v ∈Rd, the Householder transformation H is defined as:
                  H = I −2vv⊤                                     (10)                                     v⊤v,
where I is the identity matrix. The matrix H is orthogonal and R′ has orthogonal columns, which
means R′H has orthogonal columns. Utilizing this property, we can dynamically select the subspace
                                      (N)based on the intervention representation e E  by computing R = R′H.

3.4  INTERVENING ON THE SUBSPACE AT THE SELECTED TOKEN-POSITIONS

After selecting token-positions and identifying a subspace, we finally intervene. For each base-token
b in the l-th layer of the target model M, we perform a weighted interchange intervention with the
counterfactual hidden states ̃h(l) and low-rank orthogonal matrix R:
                                           (l)
             H                                    b ←¯h(l)b + R⊤(R(̃h(l)b ) −R(¯h(l)b )).                        (11)

3.5  TRAINING

We train HyperDAS on the RAVEL with two losses. The first simply measures success on RAVEL.
The second incentivizes the model to select unique token-pairings so that performance is maintained
when the token alignment matrix G is snapped to binary values during test-time evaluation.

RAVEL Loss A RAVEL example consists of a base input ¯x, counterfactual input ˆx, target concept
input x, and a counterfactual label y. When the target concept matches the attribute queried in the
base input, the label y is the attribute of ˆx. Otherwise, y is the attribute of ¯x. The loss is:

                                                                           (l)                  LRAVEL = CE( MH                                                      b ←¯h (l)b +RT(R(̃h (l)b )−R(¯h (l)b ))(¯x), y)                 (12)

Sparse Attention Loss  The construction of G allows for a single counterfactual token to be paired
with a weighted combination of multiple base tokens, however during test-time evaluations we will
enforce a 1-1 correspondence. Thus, we include a sparse attention loss that penalizes cases where
one counterfactual token attends strongly to multiple base tokens in each row of matrix G:

                          1                                                                          if Sum(G(∗,c)) > 1            ∑                                                                                           (13)                         C {Sum(G(∗,c))                          Lsparse =                  C                                0                    if Sum(G(∗,c)) ≤1                                   c=1
The final loss is L = LRAVEL + λLsparse, where λ is a real-valued weight scheduled during training.


                                       5

Published as a conference paper at ICLR 2025




                             a) Raw Intervnention Score                              b) Intervention Score at Inference Time           1.0
        [BOS]  0    0    0    0    0    0    0    0    .01   0              [BOS]  0   0   0   0   0   0   0   0   0   0
         The  0    0    0    0    0    0    0    0    .01   0              The  0   0   0   0   0   0   0   0   0   0
                                                                                                                                                           0.8  Tokens       city  .03   0    0    0    0    0    0    0    0    0                    city  0   0   0   0   0   0   0   0   0   0
             of  .01   0    0    0    0    0    0    0    0    0                    of  0   0   0   0   0   0   0   0   0   0
       Mach  .06   0    .24   0    0    0    0    0    0    0            Mach  0   0   0   0   0   0   0   0   0   0             0.6   Sentence     eng  0    0    .67   0    0    0    0    0    .01   0              eng  0   0   1   0   0   0   0   0   0   0
                's  .03   0    .07   0    0    0    0    0    0    0                         's  0   0   0   0   0   0   0   0   0   0             0.4
          official  .05   0    0    0    0    0    0    0    .01   0                  official  0   0   0   0   0   0   0   0   0   0
     language  .03   0    0    0    0    0    0    0    0    0          language  0   0   0   0   0   0   0   0   0   0                                                                                                                                                           0.2     Counterfactual          is  .01   0    0    0    0    0    0    0    0    0                         is  0   0   0   0   0   0   0   0   0   0
        [SELF]  .78   .99   .02   1    1    1    .99   .98   .95   .99             [SELF]  1   1   0   1   1   1   1   1   1   1
                     is a      in          of                      is a    in       of            0.0
                        [BOS]      Spring     field                   city         the       country              China                       [BOS]   Spring   field               city      the    country          China
                             Base Sentence Tokens                                        Base Sentence Tokens

Figure 2: The “intervention score” matrix G for the counterfactual prompt “The city of Macheng’s
official language is” and base prompt “Springfield is a city in the country of”, for which the target
model will output “China”. The attribute targeted for intervention is country, so the output should
be “The United States”. The raw intervention (left) is produced by the token-position selection
discussed in Sec 3.2, and a column-wise and row-wise argmax is applied at inference time to enforce
an 1-1 correspondence between the base and counterfactual tokens, detailed in Sec. 3.6.


3.6  EVALUATION

HyperDAS is end-to-end differentiable because discrete operations like aligning base and counter-
factual tokens are “softened” using softmax operators. During the evaluation, we force these discrete
decisions. In many cases, the matrix G contains non-zero weight for multiple base-counterfactual
sentence token pairs (left panel in Figure 2). At inference time, we perform a double argmax oper-
ation on the intervention score to select the most important location for the intervention. For each
base-counterfactual token pair, it is set to 1 if and only if this position gets the highest intervention
score across its row and column.

                                                      if G(b,c) = max(G(b,∗), G(∗,c))                           G(b,c) = {1                                                       (14)                             0   otherwise

The [SELF] row, representing no intervention on a base-token, is readjusted according to the discrete
intervention weight (right panel in Figure 2).
                              G(b,C+1) = 1 if max(G(b,∗)) = 0                            (15)


4  EXPERIMENTS

We benchmark HyperDAS on RAVEL with Llama3-8B (Meta, 2024) as the target model. We both
train a HyperDAS model on all of RAVEL at once and also train a separate HyperDAS model for
each of the five entity domains in the RAVEL benchmark, i.e., cities, Nobel laureates, occupations,
physical objects, and verbs. We experimented with initializing the transformer hypernetwork from
pre-trained parameters, but found no advantage for this task.

Crucial Hyperparameters  We use 8 decoder blocks for the hypernetwork and 32 attention heads
for computing the pairwise token position attention. The sparsity loss weight is scheduled to linearly
increase from 0 to 1.5, starting at 50% of the total steps. A learning rate between 2 × 104 to 2 × 105
is chosen depending on the dataset. Discussion of these choices concerning the sparsity loss is in
Section 4.2. For the feature subspace, we experiment with dimensions from 32 up to 2048 (out of
4096 dimensions) and use a subspace of dimension 128.

Masking of the Base Prompt  As the hypernetwork has access to the target attribute information
from the instruction and the base attribute information from the base model states, a trivial solution


                                       6

Published as a conference paper at ICLR 2025





                               City      Nobel Laureate   Occupation    Physical Object      Verb       Average
Methods
                           Causal    Iso   Causal     Iso    Causal    Iso   Causal     Iso    Causal    Iso   Disentangle

MDAS                      55.8    77.9    56.0     93.5     50.7    88.1    85.0     97.9     74.3    79.6      76.0
HyperDAS
 - Asymmetric                70.8    93.9    55.4     95.1     50.4    99.1    92.7     97.2     93.0    98.9      84.7
 - Asymmetric All Domains    58.8    90.5    47.6     92.0     75.7    82.1    92.9     94.5     86.9    95.8      80.7
 - Symmetric                 76.9    90.9    59.2     88.4     47.1    89.1    94.8     97.5     42.3    82.9      76.9
 - Symmetric All Domains     16.8    94.7     2.0      98.8      6.1    97.3    21.6     99.3     13.6    97.8      54.8

(a) Main results of HyperDAS on five domains of RAVEL with Llama3-8B. For each method, we report the
results from the best layer between 10 and 15. HyperDAS achieves state-of-the-art attribute disentangling
performance across the board. The MDAS baseline intervenes on fixed tokens.

                   MDAS                                 HyperDAS-Asym                             HyperDAS-Sym     1.0                                                   1.0                                                   1.0
  Score                                                                         Score                                                                         Score
     0.8                                                   0.8                                                   0.8

     0.6                                                   0.6                                                   0.6

     0.4                                                   0.4                                                   0.4
              Cause
     0.2         Iso                                        0.2                                                   0.2
               Disentangle       Cause/Iso/Disentangle                                                                                                                                                                                                                                                                                                                 Cause/Iso/Disentangle                                                                                                                                                                                                                                                                                                                 Cause/Iso/Disentangle     0.0                                                   0.0                                                   0.0
       0     5    10    15    20    25    30         0     5    10    15    20    25    30             5     10    15    20    25    30
                     Intervention Layer                                  Intervention Layer                                  Intervention Layer

(b) The cause/iso/disentangle score of the baseline method and HyperDAS (Both symmetric and asymmetric
implementation) for the entity type of “city” across the layers of Llama3-8B. For both HyperDAS and MDAS,
the peak Disentangle Score happens around L15.

       Figure 3: RAVEL benchmark results. HyperDAS establishes a new state-of-the-art.



the hypernetwork can learn is to condition the intervention location on whether the target attribute
matches the base attribute.  In other words, if the two attributes match, attend to a location that
has causal effect on the output, otherwise attend to the extra [self] row (see Appendix A.3 for
an example). This solution, however, does not find the actual concept subspace. To prevent the
hypernetwork from learning this trivial solution, we apply an attention mask to the base prompt to
mask out the attribute information. With the masking, the hypernetwork no longer has access to the
base attribute information, and hence the localization prediction is only conditioned on the target
attribute in the natural language instruction.


Symmetry   Intuitively, if we have localized a concept, then “get” operations that retrieve the con-
cept and “set” operations that fix the concept should both target the same features and hidden rep-
resentations. For this reason, we consider a variant of HyperDAS that enforces symmetry in the
localization of base and counterfactual prompts. We can enforce symmetry between base and coun-
terfactual inputs during token selection by randomly flipping the order of the concatenation between
base and counterfactual hidden representations in Equation 6. We report full results for symmetric
and asymmetric models.


Multi-task DAS (MDAS) Baseline  The current state-of-the-art method on RAVEL is MDAS,
which uses a multi-task learning objective to satisfy multiple high-level causal criteria. MDAS
requires supervised training data like HyperDAS, however, MDAS relies on manually selected token
position for intervention, which in our case is the final token of the entity, e.g., “nna” in Figure 1.


Results  In Table 3a, we show results on RAVEL for layer 15 of Llama3-8B. In Figure 3b, we also
run HyperDAS targeting every 2 layers in Llama3-8B starting from the embedding layer. The peak
performance of attribute disentanglement for both MDAS and HyperDAS is around layer 15.


4.1  LAYER-SPECIFIC INTERVENTION BEHAVIORS OF HYPERDAS

HyperDAS searches for an optimal location to intervene within the target hidden state in a chosen
layer. We evaluate MDAS and HyperDAS on 16 layers across the model (Figure 3b) and chose an
early layer, middle layer, and deep layer for detailed study: Layer 7, Layer 15, and Layer 29.


                                       7

Published as a conference paper at ICLR 2025





                              1600


                              1400


                              1200
                                  Cases  1000
              of   800      83.5%                             97.7%            98.7%            89.8%
                                         Number   600

                               400
                                                                                                                        Token Types
                                                                                                             BOS Token
                               200                                                                                                                Entity Tokens
                                                                                                                           JSON Syntax
                                                                                                                                        Others
                                 0
                                           Shallow Layer      Shallow Layer       Middle Layer       Middle Layer      Deep Layer       Deep Layer
                                            Counterfactual         Base          Counterfactual         Base          Counterfactual         Base
                                                               Intervention Layer / Sentence

Figure 4: The intervention location, in counterfactual and base sentence, picked by HyperDAS when
targeting shallow (7), middle (15) and deep (29) decoder layers.


Analysis presented in Figure 4 reveals that HyperDAS consistently targets entity tokens in the coun-
terfactual input across all layers, suggesting robust detection of attribute information in the entity
token’s residual stream from an early stage. However, the choice of intervention location within the
base input shows significant variation. For each example in the “city” entity split, we categorize the
base and counterfactual token pair that gets the largest intervention weight, and classify them into
the following categories: (1) BOS Token represents the beginning-of-sentence token. (2) Entity
Token refers to tokens representing entities. (3) JSON Syntax includes special characters and syn-
tactic tokens typical of JSON formatted text (e.g., opening curly brace “{”). (4) Others comprises
all tokens irrelevant to the current analysis, with “is” following the entity token being a common
example in both shallow (36%) and deep (29%) layer bases.

At very early layers, HyperDAS displays turbulent behavior, targeting random or even beginning-of-
sentence tokens in the base sentence. By the middle layers, the model consistently favors the entity
token for intervention, aligning with findings from Huang et al. (2024) and Geva et al. (2023).
In contrast, at deeper layers, the hypernetwork learns to intervene on unintuitive positions such as
syntax tokens within a JSON-formatted prompt, which were previously unknown to store attributes.


4.2  DISCUSSION

HyperDAS establishes a new state-of-the-art performance on RAVEL  Our results show that
HyperDAS outperforms MDAS, the previous state-of-the-art, across all entity splits at layer 15 in
Llama3-8B and across all layers of Llama3-8B for the “cities” entity split.

HyperDAS requires more compute than MDAS.  HyperDAS is more powerful than MDAS, but
also more computationally expensive. Training our HyperDAS model for one epoch on disentan-
gling the country attribute in the city domain takes 468,923 TeraFLOPs, while training an MDAS
model for one epoch on the same task takes 193,833. Thus, HyperDAS requires ≈2.4x compute.

On the other hand, HyperDAS is more memory efficient for tasks like RAVEL. Our target Llama
model requires 16GB of RAM. The HyperDAS model requires 52GB more in total for RAVEL,
whereas MDAS requires only 4.1GB more per attribute. The memory usage of HyperDAS does not
go up with additional attributes, so when trained on all of RAVEL together (23 attributes), MDAS
(23 * 4.1 + 16 = 110.3GB) exceeds the memory usage of HyperDAS (52 + 16 = 68GB).

Householder vectors analysis provides a window into attribute features.  To analyze the House-
holder vectors generated by the model, we collected vectors from each test example and categorized
them according to their respective attributes. For each attribute category, a subset of 1,000 sam-
ples was randomly selected. We then computed the similarity scores between pairs of attributes by
calculating the average cosine similarity across these 1,000 pairs of selected Householder vectors.


                                       8

  Published as a conference paper at ICLR 2025





                                                                                                                                     Country    Continent   Timezone   Longitude    Latitude    Language


                                                                                                                                         0.99        0.87        0.79        0.69        0.75        0.84                                                                               Attributes                                                                                                                                                                                                                                                                                                         Country                                                                       Country                                                                                                                                                                                                                                                                            0.95
                                                                          Continent
                                                                 Language
                                                                              Latitude         0.3                                        0.87        0.97        0.83        0.70        0.76        0.88
                                                                         Longitude                                                                                                                                                                                                                                                                                                                                                                                              Continent                                                                                                      0.90                                                                  Timezone       0.2
                                                                                                          0.1
                                                                                                                                         0.79        0.83        1.00        0.71        0.77        0.80
                                                                                                         0.0                                                                                                                                                                                                                                                                                                                                                    Timezone
                                                                                                                                                                                                                                                                            0.85                                                                                                          0.1
                                                                                                          0.2                                        0.69        0.70        0.71        0.99        0.87        0.71                                                                                                         0.3                                                                                                  Longitude                                                                                                      0.80
                                                                                                        0.4
                                                                                                    0.3
                                                                                                0.2                                                0.75        0.76        0.77        0.87        0.98        0.75                                                                                                                                                                                                                                                                                                                                                    Latitude
                                                                                                                                                                                                                                                                            0.75                               0.4                                                           0.1
                                      0.3                                                0.0
                                             0.2
                                                    0.1                                                                                                                                         0.84        0.88        0.80        0.71        0.75        0.98                                                          0.0                                                                                  0.2                                                                                      0.1                                                                                                                                    Language                                                                 0.1
                                                                                                                                                                                                                                                                            0.70                                                                         0.2  0.3

                                                Figure 6: The cosine similarity between the  Figure 5:  The relative position between the
                                             Householder vectors of different attributes in the Householder vector (after PCA) of attributes for
                                                        city domain, computed using 100,000 samples  all the correct predictions in city domain. The
                                           from each attribute. Notably, HyperDAS effec-  clustering indicates that HyperDAS learns dif-
                                                         tively learns a highly similar subspace for the  ferent subspace for each attribute.
                                                         attributes ‘Longitude’ and ‘Latitude’.
                     With Sparsity Loss                      With No Sparsity Loss              With Too Much Sparsity Loss           1.0       [BOS]            att
          orney's                                                                                                                                                                                                                         0.8Sentence      duty:    represent                                                                                                                                                                                                                         0.6        clients             in
          court;                                                                                                                                                                                                                         0.4         play
      therapist's                                                                                                                                                                                                                         0.2
          duty:Counterfactual    [SELF]                                                                                                                                                                                                                                             0.0
                                   [BOS]" ":" ","duties":"tocode,test,and    and ";" ":" laborer","duties":"tohelp             [BOS]" ":" ","duties":"tocode,test,and    and ";" ":" laborer","duties":"tohelp          [BOS]" " "duties":"tocode,test,and  and " "laborer","duties":"tohelp
                                                                                 occupation     softwaredeveloper                                                      maintaincomputerprograms applications       occupation        construction                                             children                               occupation     softwaredeveloper                                                      maintaincomputerprograms applications       occupation        construction                                             children                       occupation":softwaredeveloper",                                      maintaincomputerprogramsapplications";occupation":construction                                   children
                     Base Sentence Tokens                      Base Sentence Tokens                   Base Sentence Tokens

  Figure 7: Intervention locations for a base/counterfactual sentences pair with Occupation entity-type
  selected by HyperDAS trained with different amounts of sparsity loss. This comparison illustrates
  the intervention locations generated by HyperDAS when trained under three different sparsity loss
  conditions. All three models achieved a Disentangle Score ≈94.0% using weighted interventions.
 With no sparsity loss (middle), HyperDAS tends to intervene from the last subject token in the
  counterfactual sentence to most tokens in the base sentence, which yields adequate performance
  under many-to-one constraints but not under strict one-to-one constraints. With too much sparsity
  loss (right), the pairwise token selection attention within HyperDAS fails, resulting in interventions
  that blend all hidden states. Although this approach achieves a near-perfect disentangle score with
 weighted intervention, the model’s does not have interpretable intervention patterns and fails entirely
  during test time when masks are snapped to align base and source tokens one-to-one.



 We analyze the geometry of the learned householder vectors, with the PCA projection shown in
  Figure 5. We also compute the average pairwise cosine similarity of Householder vectors sampled
 from within the same attribute or cross two different attributes, as shown in Figure 6. Despite an
  overall high cosine similarity among all Householder vectors associated with the same entity type,
  the Householder vectors associated with the same attribute form a tighter cluster, with a higher
  cosine similarity score than pairs of vectors associated with two different attributes. These per-
  attribute clusters might explain why the learned subspace can disentangle different attributes of the
 same entity, as different attributes are localized into different subspaces of the entity representation.


 How do we know HyperDAS uncovers actual causal structures faithful to the target model?
 On one hand, we should leverage the power of supervised machine learning to develop increasingly
  sophisticated interpretability methods. On the other hand, such methods are incentivized to “hack”
  evaluations without uncovering actual causal structure in the target model. We have taken several
  steps to maintain fidelity to the underlying model structures when training and evaluating Hyper-
 DAS, by constraining optimization flexibility to prevent inadvertently steering or editing the model
  with out-of-distribution interventions.


                                         9

Published as a conference paper at ICLR 2025





                                                    Symmetric Model                                                          Asymmetric Model       Source Position (Text)
                                                                                                                                                                                                 1st Entity Token
                       600                                                                        600                                                        2nd3rd EntityEntity TokenToken
                       400                                                                        400                                                               Others                                                                                                                (Counterfactual)200                                                                        200
                                   Count  0     Country    Continent  Language  Timezone    Latitude   Longitude       0     Country    Continent  Language  Timezone    Latitude   Longitude
                       600
                                          (Base)400                                                                        400
                                   Count200                                                                        200
                         0     Country    Continent  Language  Timezone    Latitude   Longitude       0     Country    Continent  Language  Timezone    Latitude   Longitude
                                                                     Attribute                                                                                Attribute

Figure 8: The count of intervention location picked by HyperDAS at the counterfactual prompt
(upper) v.s. at the base prompt (bottom) across all the attributes in the city domain on entities with
three tokens. The asymmetric variant (right) of HyperDAS favors getting the attribute information
from the last entity token for the majority of the counterfactual prompts (≥95%), and intervene on
the second last entity token. The symmetric variant (left) favors last entity token consistently for
both base and counterfactual prompt.



The weighted interchange interventions used in training hacks the objective without soft con-
straints via loss terms.  The loss term Lsparse is crucial for ensuring that HyperDAS learns a one-
to-one alignment between base tokens and counterfactual tokens (Figure 2). When no sparsity loss
is applied, the model aligns the final entity token (e.g., “nna” from Figure 1) to many tokens in
the base sentence. These solutions fail during evaluations where token alignments are snapped to
be one-to-one. Conversely, with excessive sparsity loss, the model constructs a counterfactual hid-
den representation that is a linear combination of many hidden states, resulting in a high flexibility
optimization scheme that is closer to model steering or editing. This also fails during one-to-one
evaluations. See Figure 7 for an example of these pathological settings.


Often only one token is aligned between base and counterfactual inputs.  The MDAS baseline
performs well on the RAVEL benchmark by one token in the base and one token in the source.
However, our new state-of-the-art HyperDAS model will select multiple tokens 53% of the time.


Asymmetric HyperDAS targets different tokens for base and counterfactual examples.  Fig-
ure 8 shows the tokens selected by the symmetric and asymmetric variants of HyperDAS. When
allowed asymmetric parametrization, networks break symmetry in positional assignments; for a sin-
gle input prompt, HyperDAS will select different tokens depending on whether that input is the base
or counterfactual.


5  CONCLUSION


In this work, we introduced HyperDAS, a novel hypernetwork-based approach for automating causal
interpretability work. HyperDAS achieves state-of-the-art performance on the RAVEL benchmark,
demonstrating its effectiveness in localizing and disentangling entity attributes through causal inter-
ventions. Our method’s ability to dynamically select hidden representations and learn linear features
that mediate target concepts represents a significant advancement in interpretability techniques for
language models. We are optimistic that HyperDAS will open new avenues for understanding and
interpreting the internal workings of complex language models.


Limitations  HyperDAS will only be successful if the target concept is mediated by linear features.
However, there is emerging evidence that non-linear mediators are a possibility (Csord´as et al.,
2024; Engels et al., 2024). As discussed extensively in the main text, applying supervised machine
learning to interpretability has the potential to lead to false positive results. While we have taken
steps to maintain fidelity to underlying model structures, future work should continue to explore the
delicate balance between uncovering latent causal relationships and the risk of model steering.


                                       10

Published as a conference paper at ICLR 2025




ACKNOWLEDGEMENTS

This research was in part supported by a grant from Open Philanthropy.

REFERENCES

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,
  Jan Leike, Jeff Wu, and William Saunders.  Language models can explain neurons in lan-
  guage models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index.
  html.(Date accessed: 14.05. 2023), 2, 2023.

Dan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey.  Identifying functionally
  important features with end-to-end sparse dictionary learning, 2024. URL https://arxiv.org/
  abs/2405.12241.

Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a
  Garriga-Alonso.  Towards automated circuit discovery for mechanistic interpretability.  In Al-
   ice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine
   (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neu-
   ral Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, Decem-
  ber 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/
  34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html.

R´obert Csord´as, Christopher Potts, Christopher D. Manning, and Atticus Geiger. Recurrent neural
  networks learn to store and generate sequences using non-linear representations, 2024. URL
  https://arxiv.org/abs/2408.10920.

Joshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, and Max Tegmark. Not all language model
   features are linear, 2024. URL https://arxiv.org/abs/2405.14860.

Javier Ferrando and Elena Voita.  Information flow routes: Automatically interpreting language
  models at scale, 2024. URL https://arxiv.org/abs/2403.00824.

Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan
  Belinkov.  Causal analysis of syntactic agreement mechanisms in neural language models.  In
  Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th An-
  nual Meeting of the Association for Computational Linguistics and the 11th International Joint
  Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1828–1843, Online,
  August 2021. Association for Computational Linguistics.  doi: 10.18653/v1/2021.acl-long.144.
  URL https://aclanthology.org/2021.acl-long.144.

Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models
   partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov,
  Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of
  the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp.
  163–173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/
  2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.

Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang,
  Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. Causal
   abstraction: A theoretical foundation for mechanistic interpretability, 2024a. URL https://
  arxiv.org/abs/2301.04709.

Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman.  Find-
  ing alignments between interpretable causal variables and distributed neural representations. In
  Causal Learning and Reasoning, pp. 160–187. PMLR, 2024b.

Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.  Dissecting recall of factual
  associations in auto-regressive language models.  In Houda Bouamor, Juan Pino, and Kalika
  Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language
  Processing, pp. 12216–12235, Singapore, December 2023. Association for Computational Lin-
   guistics.  doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/2023.
  emnlp-main.751.


                                       11

Published as a conference paper at ICLR 2025





Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva.  Patchscopes:
 A unifying framework for inspecting hidden representations of language models. In Forty-first
  International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.
  OpenReview.net, 2024. URL https://openreview.net/forum?id=5uwBzcn885.

Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob
  Andreas. Natural language descriptions of deep visual features. In International Conference on
  Learning Representations, 2022. URL https://arxiv.org/abs/2201.11114.

Evan Hernandez, Belinda Z. Li, and Jacob Andreas.   Inspecting and editing knowledge rep-
   resentations in language models.  In First Conference on Language Modeling, 2024. URL
  https://openreview.net/forum?id=ADtL6fgNRv.

Alston S Householder. Unitary triangularization of a nonsymmetric matrix. Journal of the ACM
  (JACM), 5(4):339–342, 1958.

Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigor-
  ously assessing natural language explanations of neurons.  In Yonatan Belinkov, Sophie Hao,
  Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi (eds.), Proceedings of the
  6th BlackboxNLP Workshop:  Analyzing and Interpreting Neural Networks for NLP, Black-
  boxNLP@EMNLP 2023, Singapore, December 7, 2023, pp. 317–331. Association for Com-
   putational Linguistics, 2023.  doi: 10.18653/V1/2023.BLACKBOXNLP-1.24. URL https:
  //doi.org/10.18653/v1/2023.blackboxnlp-1.24.

Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. RAVEL: Evaluating
   interpretability methods on disentangling language model representations. In Lun-Wei Ku, Andre
  Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association
   for Computational Linguistics (Volume 1: Long Papers), pp. 8669–8687, Bangkok, Thailand,
  August 2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.acl-long.470.
  URL https://aclanthology.org/2024.acl-long.470.

Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. Sparse
  autoencoders find highly interpretable features in language models. In The Twelfth International
  Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
  F76bwRSLeK.

Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.
  Sparse feature circuits: Discovering and editing interpretable causal graphs in language mod-
   els. Computing Research Repository, arXiv:2403.19647, 2024. URL https://arxiv.org/abs/
  2403.19647.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual as-
   sociations in GPT. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and
  A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on
  Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-
  ber 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2022/
  hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html.

Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing
  memory in a transformer. In The Eleventh International Conference on Learning Representations,
  2023. URL https://openreview.net/forum?id=MkbcAHIYgyS.

AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024.

Jesse Mu and Jacob Andreas. Compositional explanations of neurons, 2021. URL https://arxiv.
  org/abs/2006.14032.

Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can
  Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and
  Yonatan Belinkov. The quest for the right mediator: A history, survey, and theoretical grounding
  of causal interpretability, 2024. URL https://arxiv.org/abs/2408.01416.


                                       12

Published as a conference paper at ICLR 2025





Achyuta Rajaram, Neil Chowdhury, Antonio Torralba, Jacob Andreas, and Sarah Schwettmann.
  Automatic discovery of visual circuits. CoRR, abs/2404.14349, 2024.  doi: 10.48550/ARXIV.
  2404.14349. URL https://doi.org/10.48550/arXiv.2404.14349.

Naomi Saphra and Sarah Wiegreffe. Mechanistic?, 2024. URL https://arxiv.org/abs/2410.
  09087.

Sarah Schwettmann, Tamar  Rott Shaham,  Joanna Materzynska,  Neil Chowdhury,  Shuang
   Li,  Jacob  Andreas,  David  Bau,  and  Antonio  Torralba.   FIND: A  function  descrip-
   tion benchmark  for  evaluating  interpretability  methods.     In  Alice Oh,  Tristan Nau-
  mann,  Amir  Globerson,  Kate  Saenko,  Moritz  Hardt,  and  Sergey  Levine  (eds.),  Ad-
  vances in Neural Information Processing Systems 36:  Annual Conference on Neural In-
  formation Processing Systems 2023, NeurIPS 2023, New Orleans,  LA, USA, December
  10  - 16,  2023,  2023.  URL http://papers.nips.cc/paper files/paper/2023/hash/
  ef0164c1112f56246224af540857348f-Abstract-Datasets and Benchmarks.html.

Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob
  Andreas, and Antonio Torralba. A multimodal automated interpretability agent.  In Forty-first
  International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.
  OpenReview.net, 2024. URL https://openreview.net/forum?id=mDw42ZanmE.

Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. A mechanistic interpretation of arith-
  metic reasoning in language models using causal mediation analysis. In Houda Bouamor, Juan
  Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natu-
   ral Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 7035–7052. Asso-
   ciation for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.435. URL
  https://doi.org/10.18653/v1/2023.emnlp-main.435.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,
  and Stuart Shieber.   Investigating gender bias in language models using causal mediation
   analysis.   In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
  vances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran Asso-
   ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/
  92650b2e92217715fe312e6fa7b90d82-Paper.pdf.

Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretabil-
   ity at scale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing
  Systems, 36, 2024.





                                       13

Published as a conference paper at ICLR 2025




A  APPENDIX


A.1  HYPERDAS OVER ALL DOMAINS

Our results at Table 3a show that we can train HyperDAS to achieve state of the art performance
on the RAVEL benchmark by training a separate model for each entity type split, which is the set
up used to train the previous state of the art MDAS. To test the scalibility and generalizability of
HyperDAS, we train a single model across all the entity type splits and evaluate its performance.


Experiment Set-up  We aggregate the training split of the dataset from all 5 domains and train
HyperDAS for 5 epochs. We adjuste the learning rate from 2 × 10−5 to 5 × 10−4 and schedule the
sparsity weight λ ranging from 0.75 to 1.5 starting after 50% of the total steps. This set-up allows
the model to first find a stable solution across all domains with soft intervention before forcing it to
converge to a single token selection.


Result  We report the performance of HyperDAS trained over all entity type split in Table 3a.
The model performs better than MDAS but slightly worse than HyperDAS trained on individual
entity type split by 4.0%. Specifically, HyperDAS-All-Domain performs worse over city and nobel
laureate split, better over occupation split, and on-par over physical object and verb split.


A.2  DATASET SPECIFICATION


          Domain/Attribute  # of Cause Example  # of IsolateExample  # of Entity

         City                 34899/7016         49500/9930      3552/3374
         Country               7925/1544           8250/1655       3528/2411
         Language             6207/1252           8250/1655       3471/2221
          Continent             8254/1658           8250/1655       3543/2567
         Timezone             5371/1144           8250/1655       3414/1900
          Latitude               3813/743           8250/1655       3107/1519
         Longitude             3329/675           8250/1655       2989/1357

         Nobel Laureate       39771/6754         44628/7600       928/928
         Country of Birth       7218/1356           8908/1520        928/909
        Award Year           11037/1904          8930/1520        928/926
         Gender                 854/96            8930/1520        592/149
          Field                 9518/1558           8930/1520        928/922
          Birth Year            11144/1840          8930/1520        928/927

         Occupation          54444/1582          29052/864        799/785
        Work Location         24216/724           9684/288        799/708
         Duty                 12090/371           9684/288        785/522
          Industry              18138/487           9684/288        799/600

          Physical Object       49114/4659         35285/3636       563/563
         Color                14707/1518           8825/909        563/563
         Category             13540/1273           8820/909        563/562
          Texture              14666/1265           8821/909        563/561
          Size                  6201/603            8819/909        563/528

        Verb                 70003/3806          14396/782        986/984
          Past Tense            34043/1848           7188/391        986/975
          Singular              35960/1958           7208/391        986/978

Table 1: The details of the dataset used for the experiment, in the format of train/test splits. For
every model in each setting. Methods are trained on the full dataset of that setting with 5 epochs.
The prompts used by the train/test splits are completely disjoint.



                                       14

Published as a conference paper at ICLR 2025





A.3  DATASET PREPROCESSING

HyperDAS uses an attention mechanism to gather information from the hidden states of the target
model M when running the base and counterfactual sentences.  This makes HyperDAS overtly
powerful in some situations. Consider the following input:



     Base               ¯x = Vienna, known for its Imperial palaces, is a city in the country of
    Counterfactual    ˆx = I love Paris                                                       (16)
     Instruction    x = Localize the latitude of the city                      ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩


If the model works as intended, it will intervene on the ‘Latitude’ subspace, which will leave the
‘Country’ features intact and therefore the target model will predict Austria.
However, since the model can access the hidden states M(¯x), it knows that the queried attribute in
the sentence is ‘Country’, which is different than the targeted attribute ‘Latitude’. Through train-
ing, HyperDAS learns a shortcut to a trivial solution—performing no intervention when the target
attribute is different from the one mentioned in the sentence. With this shortcut, the Isolate objec-
tive no longer works and the HyperDAS fails to learn disentangled feature subspaces for different
attributes.

Figure 9 shows how the HyperDAS may learn a trivial solution to the RAVEL benchmark if the
relevant information (base prompt attribute) can be accessed by the model.




                        Instruction: Goroka ; Zurich - Country   Label: Oceania  Pred: Oceania
                    [BOS] 000000000000000000000000000
                            city 000000000000000000000000000
                          to 000000000000000000000000000           0.8
                   country 000000000000000000000000000
                                         : 000000000000000000000000000
                 Buenos 000000000000000000000000000           0.6
                        Aires 000000000000000000000000000
                                  is 000000000000000000000000000
                             in 000000000000000000000000000                                                                             0.4
                 Argentina 000000000000000000000000000
                                            . 000000000000000000000000000
                     Zurich 000000000000000000000000000                                                                             0.2
                                  is 000000000000000000000000000
                             in 000000000000000000000000000
                     [SELF] 111111111111111111111111111
                                          [BOS][{"city":"NewDelhi","continent":"Asia"},{"city":"Goroka","continent":"Oceania

Figure 9: The trivial solution learnt by the HyperDAS on isolate examples when no mask is applied
on the attribute token in the prompt. HyperDAS learns to not intervene if it sees the base prompt
attribute to be different than the attribute in the instruction.


Therefore, for each pair of prompts ¯x, ˆx at training, we apply an intervention mask to all the tokens
starting from the attribute mention. The hidden states from token with intervention mask is not
visible to HyperDAS and therefore cannot be selected for intervention. The example becomes:


                                       15

Published as a conference paper at ICLR 2025





     Base               ¯x = Vienna, known for its Imperial palaces, is a city in the country of
    Counterfactual    ˆx = I love Paris                                                       (17)
     Instruction    x = Localize the latitude of the city                      ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩

where the hidden states of the red text is masked from the HyperDAS .


A.4  LOADING HYPERDAS WITH PRE-TRAINED PARAMETERS

We have also explored initializing the HyperDAS from a pretrained LM instead of initializing it from
scratch. With Llama3-8b (Meta, 2024) as the target LM, we initialize the modules of HyperDAS ,
besides the multi-head cross-attention heads and pairwise token position scores attention heads, as
the copy of the parameters from the target model. We then evaluate the performance of this variation
of the model on the city dataset of RAVEL (Huang et al., 2024).


                                                                                                                                                            Initialization    6                                                                                                                                                                              Initialize from pretrained
    5                                                                                                                                                                       Initialize from scratch
 loss4
    3   Training2
    1
    0
           0               2000              4000              6000              8000             10000             12000
                                                                      Step

Figure 10: The loss curve of HyperDAS initialized from scratch or from pretrained LM while train-
ing on the city dataset of RAVEL.

In Figure 10, we observe that there is no significant difference between the model initialized from
scratch and the model initialized from Llama3-8b parameters. However, it remains unknown how
would this difference change as the training of HyperDAS scales.


A.5  SPARSE AUTOENCODERS

We experiment with different feature subspace dimension, as shown in Figure 11. We add an trained
sparse autoencoder as another baseline. Following the exact same setting in (Huang et al., 2024),
we train sparse autoencoder that projects the target hidden states into a higher-dimensional sparse
feature space and then reconstruct the original hidden states.


A.6  ABLATION RESULTS

See ablations in Table 12.


A.7  INTERVENTION PATTERNS

Here we include a few demonstrations of the intervention pattern that HyperDAS generates on
RAVEL, as shown in Figure 13.





                                       16

Published as a conference paper at ICLR 2025





                    50.0%
            1.0                                                                  1.6%          3.1%          GOAL                                                       25.0%
                    0.8%                          12.5%


            0.9



            0.8


                                   0.7%                                                          12.5%            0.7                                                                                               3.1%
           Score                                                                                    1.6% 25.0%
                                         1.6%                                              0.8%
                Isolate 0.6                                  3.0%                                                  50.0%



            0.5



            0.4


                                                                                   12.8%
                   HyperDAS                                                       50.5%
            0.3      MDAS
                   SAE                                                             26.1%
                   0.0                 0.2                 0.4                 0.6                 0.8                 1.0
                                                         Causal Score

Figure 11: Cause (x-axis) and Iso (y-axis) scores trade-off for HyperDAS, MDAS, and SAE when
using different feature size shown as the ratio %. GOAL (1, 1) indicates the score with which the
method is able to disentangle the feature subspace perfectly.





                Ablation           Causal   Iso   Disentangle

              HyperDAS            70.8    93.9      82.4
               -No Cross Attention    68.2    83.9      76.1
               -No DAS               0.8     97.4      49.1
               -No Hypernetwork     15.1    46.9      31.0

Figure 12: Ablation results for HyperDAS. No DAS has no rotation matrix and intervenes on entire
hidden representations. No Hypernetwork replaces concept encoding via transformer with a vector
lookup. No Cross Attention removes attention head submodules connecting the hypernetwork and
target model.





                                       17

Published as a conference paper at ICLR 2025





                             a) Soft Intervention                                        b) One-to-Many Intervention                                 c) One-to-One Intervention                1.0
        [BOS][{"                                                                            [BOS][{"                                                                            [BOS][{"
                city":                                                                                                                                                  city":                                                                                                                                                                                                                                                                                    city":
                                                                                                                                                                                                                         0.8         Los"                                                                               Los"                                                                                                                                                      Los"  Tokens Angeles",             "                                                                                Angeles",                                                                                                   "                                                                                Angeles",                                                                                                                                                                                        "
                                                                                                                                                                                                                                      long":             "                                                                                                       long":
   Sentence      long":118.2"},{"                                                                                                                   118.2"},{""                                                                                                                                                                                                                          118.2"},{""                                                         0.60.4                city":                                                                                                                            city":                                                                                                                            city":
             ig"K                                                                                    ig"K                                                                                    ig"K
                                                                                                                                                                                                                         0.2     Counterfactual    oma","                                                                                    oma","                                                                                    oma","
            long":"                                                                                                                         long":"                                                                                                                                                                                                                                      long":"       [SELF]                                                                                 [SELF]                                                                                                                                                           [SELF]                                                                                                                                                                                                                         0.0
                     [BOS] city to : New Delhi is in India . Bag h lan is in                                                                                                                                        [BOS] city to : New Delhi is in India . Bag h lan is in                                                                                                                                                                                                                                                           [BOS]cityto :NewDelhiisinIndia.Baghlanisin                                                     country                                                                                                                 Tanzania                                                 country                                                                                                                 Tanzania                                            country                                                                                         Tanzania
                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens

                                                          (a)

                             a)                                Soft                                   Intervention                                                                                       b)                                                                              One-to-Many                                                                                                       Intervention                                                                                                                                                              c)                                                                                                                                 One-to-One                                                                                                                                                                 Intervention                                                                                                                                                                                                                         1.0     [BOS]                                                                          [BOS]                                                                                                                                              [BOS]
              [                                                                                                                   [                                                                                                                                                                                                                       [
         {"                                                                                {"                                                                                                                                                        {"
         city                                                                                                city                                                                                                                                                                                      city
             ":                                                                                                               ":                                                                                                                                                                                                                  ":
            "                                                                                                 "                                                                                                                                                                                      "                                                                                                                                                                                                                         0.8        Sy                                                                        Sy                                                                                                                                        Sy  Tokens      dney                                                                     dney                                                                                                                                    dney
             ",                                                                                                                  ",                                                                                                                                                                                                                       ",
            "                                                                                                 "                                                                                                                                                                                      "
       lang                                                                                lang                                                                                                                                                        lang                                                                                                                                                                                                                         0.6             ":                                                                                                               ":                                                                                                                                                                                                                  ":
            "                                                                                                 "                                                                                                                                                                                      "
     English                                                                                 English                                                                                                                                                             English   Sentence          "},                                                                                             "},                                                                                                                                                                                "},
         {"                                                                                {"                                                                                                                                                        {"
         city                                                                                                city                                                                                                                                                                                      city                                                                                                                                                                                                                         0.4             ":                                                                                                               ":                                                                                                                                                                                                                  ":
            "                                                                                                 "                                                                                                                                                                                      "
         F                                                                              F                                                                                                                                                  F
        uk                                                                        uk                                                                                                                                         uk
           ui                                                                                                 ui                                                                                                                                                                                       ui
             ",                                                                                                                  ",                                                                                                                                                                                                                       ",                                                                                                                                                                                                                         0.2     Counterfactual            "                                                                                                 "                                                                                                                                                                                      "
       lang                                                                                lang                                                                                                                                                        lang
             ":                                                                                                               ":                                                                                                                                                                                                                  ":
            "                                                                                                 "                                                                                                                                                                                      "
     [SELF]                                                                               [SELF]                                                                                                                                                         [SELF]                                                                                                                                                                                                                         0.0
                                          City                                                                                                                                  York                                                                                                                                                                New                                      . Photo taken in Mou ila , Japan                                                           .PhototakeninMouila,Japan                  [BOS] Photo taken in New York                                                                                                                                      [BOS] Photo taken in New                                                                                                                                                                                                                                                         [BOS]Phototakenin                                                                                  States . Photo taken in Mou ila , Japan            , United                                                                                                                                      City , United States                                                                                                                                                                                                                         YorkCity,UnitedStates
                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens

                                                       (b)

                              a) Soft Intervention                                                                                        b) One-to-Many Intervention                                                                                                                                                               c) One-to-One Intervention               1.0       [BOS]                                                                            [BOS]                                                                                                                                                [BOS]
          Sy                                                              Sy                                                              Sy
                                                                                                                                                                                                                          0.8  Tokens   dney:                                                           dney:                                                           dney:
     Australia                                                                       Australia                                                                       Australia
             /S                                                                               /S                                                                               /S                                                        0.6   Sentence       y                                                                 y                                                                 y
       dney                                                           dney                                                           dney                                                        0.4
                     .                                                                                                                          .                                                                                                                          .
           Br                                                                     Br                                                                     Br
        aga                                                            aga                                                            aga                                                        0.2     Counterfactual             :                                                                                                                   :                                                                                                                   :
       [SELF]                                                                                 [SELF]                                                                                                                                                           [SELF]
                                                                                                                                                                                                                          0.0
                      city to :     is in . So chi is in                                                                                                                   city to :     is in . So chi is in                                                                                                                                                                                                              cityto :    isin .Sochiisin
                      [BOS]                   country       Kuala Lumpur                     Malaysia                                                  Portugal                   [BOS]                   country       Kuala Lumpur                     Malaysia                                                  Portugal                   [BOS]             country     KualaLumpur               Malaysia                                       Portugal
                           Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens

                                                          (c)

                            a)                               Soft                                   Intervention                                                                                      b)                                                                             One-to-Many                                                                                                       Intervention                                                                                                                                                              c)                                                                                                                                 One-to-One                                                                                                                                                                Intervention                                                                                                                                                                                                                         1.0     [BOS]                                                                         [BOS]                                                                                                                                             [BOS]             [                                                                                                                  [                                                                                                                                                                                                                      [        {"                                                                                {"                                                                                                                                                       {"        city                                                                                               city                                                                                                                                                                                     city            ":                                                                                                              ":                                                                                                                                                                                                                 ":
                                                                                                                                                                                                                         0.8       C"                                                               C"                                                                                                                       C"  Tokens     Townape                                                                   Townape                                                                                                                                  Townape            ",                                                                                                                 ",                                                                                                                                                                                                                      ",           "                                                                                                "                                                                                                                                                                                      "          lat                                                                                                      lat                                                                                                                                                                                                 lat                                                                                                                                                                                                                         0.6            ":                                                                                                              ":                                                                                                                                                                                                                 ":           "                                                                                                "                                                                                                                                                                                      "   Sentence       34                                                                    34                                                                                                                                  34         "},                                                                                            "},                                                                                                                                                                               "},        {"                                                                                {"                                                                                                                                                       {"        city                                                                                               city                                                                                                                                                                                     city                                                                                                                                                                                                                         0.4            ":                                                                                                              ":                                                                                                                                                                                                                 ":           "                                                                                                "                                                                                                                                                                                      "      Q                                                        Q                                                                                                          Q        ing                                                                                    ing                                                                                                                                                                ing       an                                                                      an                                                                                                                                     an                                                                                                                                                                                                                         0.2            ",                                                                                                                 ",                                                                                                                                                                                                                      ",     Counterfactual           "                                                                                                "                                                                                                                                                                                      "          lat                                                                                                      lat                                                                                                                                                                                                 lat            ":                                                                                                              ":                                                                                                                                                                                                                 ":           "                                                                                                "                                                                                                                                                                                      "    [SELF]                                                                              [SELF]                                                                                                                                                        [SELF]                                                                                                                                                                                                                         0.0            a                     is
                                                                             in  the     of                                                                                                                       in the   of                                                        city in  the     of                                                                   is a   city                                                                                                              is a city                                                                                      Tim im                                                                                                                                                         Tim im                Tim im  oun                                                                                                oun                                                                                                                                                                  oun
                   [BOS]                                                                                                    country            China                 [BOS]                                                                                                    country            China                [BOS]                                                                               country         China
                          Base Sentence Tokens                                         Base Sentence Tokens                                    Base Sentence Tokens

                                                       (d)

                        Figure 13: Four types of intervention patterns.





                                       18