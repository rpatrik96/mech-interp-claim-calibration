paper_id,title,authors,year,venue,primary_method,valid_claims,overclaim_rate,replication_status
2104.08164,Editing Factual Knowledge in Language Models,De Cao et al.,2021,EMNLP,Knowledge Editing,5,0.4,
2202.05262,Locating and Editing Factual Associations in GPT,Meng et al.,2022,NeurIPS,Causal Tracing + ROME,4,0.5,0.5
2211.00593,Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small,Wang et al.,2022,NeurIPS,Activation Patching,6,0.8333333333333334,0.5
2301.04709,Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability,Geiger et al.,2023,arXiv,DAS/Interchange Intervention,3,0.0,
2301.05217,Progress measures for grokking via mechanistic interpretability,Nanda et al.,2023,ICLR,Circuit Analysis,3,1.0,0.0
2304.14997,Towards Automated Circuit Discovery for Mechanistic Interpretability,Conmy et al.,2023,NeurIPS,Activation Patching,4,0.5,
2309.16042,Towards Best Practices of Activation Patching,Zhang & Nanda,2023,ICLR,Activation Patching,2,0.0,
2311.17030,Is This the Subspace You Are Looking for? Interpretability Illusion,Makelov et al.,2023,NeurIPS Workshop,Activation Patching,4,0.25,
2402.17700,RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,Huang et al.,2024,ACL,DAS/Probing,5,0.4,
2403.07809,pyvene: A Library for Understanding and Improving PyTorch Models via Interventions,Wu et al.,2024,arXiv,Intervention Library,3,0.0,
2404.03592,ReFT: Representation Finetuning for Language Models,Wu et al.,2024,ACL,Representation Intervention,5,0.6,
2404.03646,Locating and Editing Factual Associations in Mamba,Sharma et al.,2024,arXiv,Causal Tracing,3,0.3333333333333333,
2404.15255,How to use and interpret activation patching,Heimersheim & Nanda,2024,arXiv,Activation Patching,2,0.0,
2406.11779,Compact Proofs of Model Performance via Mechanistic Interpretability,Gross et al.,2024,arXiv,Circuit Analysis,5,0.4,
2407.14008,Investigating the Indirect Object Identification circuit in Mamba,Ensign & Garriga-Alonso,2024,arXiv,Activation Patching,4,0.75,
2409.04478,Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small,Chaudhary & Geiger,2024,arXiv,SAE Attribution,2,0.0,0.0
2410.08417,Bilinear MLPs enable weight-based mechanistic interpretability,Pearce et al.,2024,arXiv,Weight Analysis,5,0.6,
2411.08745,Separating Tongue from Thought: Language-Agnostic Concepts,Dumas et al.,2024,arXiv,Activation Patching,4,0.5,
2411.16105,Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability,Nainani et al.,2024,arXiv,Circuit Analysis,4,0.75,
2501.17148,AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders,Wu et al.,2025,arXiv,Steering Vectors + SAE,3,0.0,0.0
2502.03714,Universal SAEs: Interpretable Cross-Model Concept Alignment,Thasarathan et al.,2025,arXiv,SAE,3,1.0,
2503.10894,HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks,Sun et al.,2025,arXiv,DAS/Hypernetworks,5,0.4,
2504.02976,Localized Definitions Distributed Reasoning via Patching,Bahador,2025,arXiv,Activation Patching,3,1.0,
2505.14685,Language Models use Lookbacks to Track Beliefs,Prakash et al.,2025,arXiv,Circuit Analysis,4,1.0,
2505.22637,Understanding Steering Vector Reliability,Braun et al.,2025,ICLR Workshop,Steering Vectors,3,0.6666666666666666,
2505.24859,Beyond Multiple Choice: Steering for Summarization,Braun et al.,2025,ICML Workshop,Steering Vectors,3,0.0,
2506.03292,HyperSteer: Activation Steering at Scale with Hypernetworks,Sun et al.,2025,arXiv,Steering Vectors,5,0.0,
2506.18167,Understanding Reasoning in Thinking Language Models via Steering Vectors,Venhoff et al.,2025,arXiv,Steering Vectors,5,0.4,
2507.08802,The Non-Linear Representation Dilemma: Causal Abstraction Limits,Sutter et al.,2025,NeurIPS Spotlight,Causal Abstraction,4,0.5,
2507.20936,Dissecting Persona-Driven Reasoning via Activation Patching,Poonia & Jain,2025,EMNLP Findings,Activation Patching,3,0.6666666666666666,
2508.11214,How Causal Abstraction Underpins Computational Explanation,Geiger et al.,2025,arXiv,Causal Abstraction,2,0.0,
2508.21258,RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching,Jafari et al.,2025,arXiv,Activation Patching,5,1.0,
2509.06608,Small Vectors Big Effects: RL-Induced Reasoning via Steering,Sinii et al.,2025,arXiv,Steering Vectors,3,0.6666666666666666,
2509.18127,Safe-SAIL: Fine-grained Safety Landscape via SAE,Weng et al.,2025,arXiv,SAE Attribution,3,1.0,
2510.01070,Eliciting Secret Knowledge from Language Models,Cywiński et al.,2025,arXiv,Probing + Steering,5,0.8,
2510.06182,Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context,Gur-Arieh et al.,2025,arXiv,Activation Analysis,4,0.75,
2511.05923,Causal Tracing in VLMs for Hallucination Mitigation,Li et al.,2025,AAAI,Causal Tracing,3,0.6666666666666666,
2511.09432,Group Equivariance Meets MI: Equivariant SAEs,Erdogan & Lucic,2025,NeurIPS Workshop,SAE,3,1.0,
2511.22662,Difficulties with Evaluating a Deception Detector for AIs,Smith et al.,2025,arXiv,Probing,4,0.75,
2512.05534,On the Theoretical Foundation of Sparse Dictionary Learning in MI,Tang et al.,2025,arXiv,SAE Theory,3,0.6666666666666666,
2512.05794,Mechanistic Interpretability of Antibody Language Models Using SAEs,Haque et al.,2025,arXiv,SAE Attribution,5,0.6,
2512.05865,Sparse Attention Post-Training for Mechanistic Interpretability,Draye et al.,2025,arXiv,Attention Analysis,5,0.6,
2512.06681,Mechanistic Interpretability of GPT-2 Sentiment Analysis,Hatua,2025,arXiv,Activation Patching,3,1.0,
2512.13568,Superposition as Lossy Compression: Measure with SAE,Bereska et al.,2025,TMLR,SAE Analysis,4,0.5,
2512.18092,Faithful and Stable Neuron Explanations for MI,Yan et al.,2025,arXiv,Neuron Attribution,3,0.0,
2601.02989,MI of Large-Scale Counting in LLMs,Hasani et al.,2026,arXiv,Circuit Analysis,3,0.6666666666666666,
2601.03047,When the Coffee Feature Activates on Coffins: SAE Critique,Ronge et al.,2026,arXiv,SAE Attribution + Steering,4,0.25,
2601.03595,Controllable LLM Reasoning via SAE-Based Steering,Fang et al.,2026,arXiv,SAE + Steering,4,0.5,
2601.05679,Falsifying SAE Reasoning Features in Language Models,Ma et al.,2026,arXiv,SAE Attribution,3,0.6666666666666666,
2601.11516,Building Production-Ready Probes For Gemini,Kramár et al.,2026,arXiv,Linear Probing,2,0.0,
